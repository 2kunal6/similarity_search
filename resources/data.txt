Wrapper for only doing the rendering on request (drastically reduces memory)
If the next line is an include statement, inserts the contents         of the included file into the pending buffer.
Takes a FortranObject and adds it to the appropriate list, if         not already present.
Returns the node corresponding to obj. If does not already exist         then it will create it.
Adds nodes and edges to the graph as long as the maximum number         of nodes is not exceeded.         All edges are expected to have a reference to an entry in nodes.         If the list of nodes is not added in the first hop due to graph         size limitations, they are stored in hopNodes.         If the graph was extended the function returns True, otherwise the         result will be False.
Adds nodes and edges for generating the graph showing the relationship         between modules and submodules listed in nodes.
Adds edges showing dependencies between source files listed in         the nodes.
Adds edges indicating the call-tree for the procedures listed in         the nodes.
What to return when there's an exception.
Return a timestamp for the provided datestring, described by RFC 7231.
Get the ttl from headers.
Get the wrapped object.
Get the entity that corresponds to URL.
Return (expiration, obj) corresponding to provided url, exercising the         cache_policy as necessary.
Return true if the provided URL is allowed to agent.
Return (expiration, Agent) for the robots.txt at the provided URL.
Time this block.
Additional command line arguments for the behave management command
Additional command line arguments extracted directly from behave
Add behave's and our command line arguments to the command
Get a list of those command line arguments specified with the         management command that are meant as arguments for running behave.
Apply fixtures that are registered with the @fixtures decorator.
Integrate behave_django in behave via before/after scenario hooks
Patches the context to add utility functions          Sets up the base_url, and the get_url() utility function.
Sets up fixtures
Prepare and execute a HTTP POST call to AppCommand.xml end point.          Returns XML ElementTree on success and None on fail.
Get status XML via HTTP and return it as XML ElementTree.
Send command via HTTP get to receiver.
Send command via HTTP post to receiver.
Create instances of additional zones for the receiver.
Get the latest status information from device.          Method executes the update method for the current receiver type.
Get the latest status information from device.          Method queries device via HTTP and updates instance attributes.         Returns "True" on success and "False" on fail.         This method is for pre 2016 AVR(-X) devices
Get the latest status information from device.          Method queries device via HTTP and updates instance attributes.         Returns "True" on success and "False" on fail.         This method is for AVR-X  devices built in 2016 and later.
Update sources list from receiver.          Internal method which updates sources list of receiver after getting         sources and potential renaming information from receiver.
Get name of receiver from web interface if not set.
Get receivers zone name if not set yet.
Get if sound mode is supported from device.          Method executes the method for the current receiver type.
Get if sound mode is supported from device.          Method queries device via HTTP.         Returns "True" if sound mode supported and "False" if not.         This method is for pre 2016 AVR(-X) devices
Get renamed and deleted sources lists from receiver .          Internal method which queries device via HTTP to get names of renamed         input sources.
Get renamed and deleted sources lists from receiver .          Internal method which queries device via HTTP to get names of renamed         input sources. In this method AppCommand.xml is used.
Get sources list from receiver.          Internal method which queries device via HTTP to get the receiver's         input sources.         This method also determines the type of the receiver         (avr, avr-x, avr-x-2016).
Update media data for playing devices.          Internal method which queries device via HTTP to update media         information (title, artist, etc.) and URL of cover image.
Get relevant status tags from XML structure with this internal method.          Status is saved to internal attributes.         Return dictionary of tags not found in XML.
Set input_func of device.          Valid values depend on the device and should be taken from         "input_func_list".         Return "True" on success and "False" on fail.
Set All Zone Stereo option on the device.          Calls command to activate/deactivate the mode         Return "True" when successfully sent.
Set sound_mode of device.          Valid values depend on the device and should be taken from         "sound_mode_list".         Return "True" on success and "False" on fail.
Set the matching dictionary used to match the raw sound mode.
Construct the sm_match_dict.          Reverse the key value structure. The sm_match_dict is bigger,         but allows for direct matching using a dictionary key access.         The sound_mode_dict is uses externally to set this dictionary         because that has a nicer syntax.
Match the raw_sound_mode to its corresponding sound_mode.
Toggle play pause media player.
Send play command to receiver command via HTTP post.
Send pause command to receiver command via HTTP post.
Send previous track command to receiver command via HTTP post.
Turn off receiver via HTTP get command.
Turn off receiver via HTTP get command.
Volume up receiver via HTTP get command.
Volume down receiver via HTTP get command.
Set receiver volume via HTTP get command.          Volume is send in a format like -50.0.         Minimum is -80.0, maximum at 18.0
Mute receiver via HTTP get command.
Return the matched current sound mode as a string.
Identify DenonAVR using SSDP and SCPD queries.      Returns a list of dictionaries which includes all discovered Denon AVR     devices with keys "host", "modelName", "friendlyName", "presentationURL".
Send SSDP broadcast message to discover UPnP devices.      Returns a list of dictionaries with "address" (IP, PORT) and "URL"     of SCPD XML for all discovered devices.
Get and evaluate SCPD XML to identified URLs.      Returns dictionary with keys "host", "modelName", "friendlyName" and     "presentationURL" if a Denon AVR device was found and "False" if not.
Initialize all discovered Denon AVR receivers in LAN zone.      Returns a list of created Denon AVR instances.     By default SSDP broadcasts are sent up to 3 times with a 2 seconds timeout.
Parses webvtt and returns timestamps for words and lines     Tested on automatically generated subtitles from YouTube
setter for the framerate attribute         :param framerate:         :return:
Converts the given timecode to frames
Converts frames back to timecode          :returns str: the string representation of the current time code
parses timecode string frames '00:00:00:00' or '00:00:00;00' or         milliseconds '00:00:00:000'
Get ngrams from a text     Sourced from:     https://gist.github.com/dannguyen/93c2c43f4e65328b85af
Converts an array of ordered timestamps into an EDL string
Convert an srt timespan into a start and end timestamp.
Convert an srt timestamp into seconds.
Remove damaging line breaks and numbers from srt files and return a     dictionary.
Search for and remove temp log files found in the output directory.
Print out timespans to be cut followed by the line number in the srt.
Concatenate video clips together and output finished video file to the     output directory.
Create & concatenate video clips in groups of size BATCH_SIZE and output     finished video file to output directory.
Return True if search term is found in given line, False otherwise.
Return a list of subtitle files.
Return a list of vtt files.
Takes a list of subtitle (srt) filenames, search term and search type     and, returns a list of timestamps for composing a supercut.
Takes transcripts created by audiogrep/pocketsphinx, a search and search type     and returns a list of timestamps for creating a supercut
Search through and find all instances of the search term in an srt or transcript,     create a supercut around that instance, and output a new video file     comprised of those supercuts.
INPUT: XML file with captions             OUTPUT: parsed object like:                 [{'texlines': [u"So, I'm going to rewrite this", 'in a more concise form as'],                 'time': {'hours':'1', 'min':'2','sec':44,'msec':232} }]
INPUT: array with captions, i.e.                 [{'texlines': [u"So, I'm going to rewrite this", 'in a more concise form as'],                 'time': {'hours':'1', 'min':'2','sec':44,'msec':232} }]             OUTPUT: srtformated string
调整音量大小
屏幕上下滚动          :params incrment: 1 向下滚动                           -1 向上滚动
返回总字符数(考虑英文和中文在终端所占字块)          return: int
切换页面线程
接受按键, 存入queue
第一次载入时查找歌词
通过歌词生成屏幕需要显示的内容
16 colors supported
256 colors supported
Call color function base on name
解析json列表,转换成utf-8
解析json字典,转换成utf-8
提供登陆的认证          这里顺带增加了 volume, channel, theme_id , netease, run_times的默认值
记录退出时的播放状态
统计用户信息
获取配置并检查是否更改
存储历史记录和登陆信息
获取channel列表
这个貌似没啥用         :params fcid, tcid: string
这里包装了一个函数,发送post_data         :param ptype: n 列表无歌曲,返回新列表                       e 发送歌曲完毕                       b 不再播放,返回新列表                       s 下一首,返回新的列表                       r 标记喜欢                       u 取消标记喜欢
初始获取歌曲          :params return: json
获取歌词          如果测试频繁会发如下信息:         {'msg': 'You API access rate limit has been exceeded.                  Contact api-master@douban.com if you want higher limit. ',          'code': 1998,          'request': 'GET /j/v2/lyric'}
运行播放器（若当前已有正在运行的，强制推出）          extra_cmd: 额外的参数 (list)
监控正在运行的播放器（独立线程）          播放器退出后将会设置 _exit_event
pasue状态下如果取时间会使歌曲继续, 这里加了一个_pause状态
Send a command to MPlayer.          cmd: the command string         expect: expect the output starts with a certain string         The result, if any, is returned as a string.
互斥锁
更新队列线程
返回当前播放歌曲歌词
设置api发送的FM频道          :params channel_num: channel_list的索引值 int
获取每日推荐歌曲
获取单个歌曲
获取歌曲, 对外统一接口
设置显示信息
生成输出行          注意: 多线程终端同时输出会有bug, 导致起始位置偏移, 需要在每行加\r
每个controller需要提供run方法, 来提供启动
标题时间显示
从queue里取出字符执行命令
发送桌面通知
发送Linux桌面通知
发送Mac桌面通知
获取专辑封面
第一次桌面通知时加入图片
需要解码一下,通知需要unicode编码
时间状态
从queue里取出字符执行命令
登陆界面
通过帐号,密码请求token,返回一个dict     {     "user_info": {         "ck": "-VQY",         "play_record": {             "fav_chls_count": 4,             "liked": 802,             "banned": 162,             "played": 28368         },         "is_new_user": 0,         "uid": "taizilongxu",         "third_party_info": null,         "url": "http://www.douban.com/people/taizilongxu/",         "is_dj": false,         "id": "2053207",         "is_pro": false,         "name": "刘小备"     },     "r": 0     }
切换歌曲时刷新
打开豆瓣网页
从queue里取出字符执行命令
根据歌曲名搜索歌曲          : params : song_title: 歌曲名                    limit: 搜索数量
根据歌名获取歌曲id
根据歌名搜索320k地址
因为历史列表动态更新,需要刷新
界面执行程序
Instantiate a new Trade from a dict (generally from loading a JSON         response). The data used to instantiate the Trade is a shallow copy of         the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new TradeSummary from a dict (generally from loading a         JSON response). The data used to instantiate the TradeSummary is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new CalculatedTradeState from a dict (generally from         loading a JSON response). The data used to instantiate the         CalculatedTradeState is a shallow copy of the dict passed in, with any         complex child types instantiated appropriately.
Create, replace and cancel a Trade's dependent Orders (Take Profit,         Stop Loss and Trailing Stop Loss) through the Trade itself          Args:             accountID:                 Account Identifier             tradeSpecifier:                 Specifier for the Trade             takeProfit:                 The specification of the Take Profit to create/modify/cancel.                 If takeProfit is set to null, the Take Profit Order will be                 cancelled if it exists. If takeProfit is not provided, the                 exisiting Take Profit Order will not be modified. If a sub-                 field of takeProfit is not specified, that field will be set to                 a default value on create, and be inherited by the replacing                 order on modify.             stopLoss:                 The specification of the Stop Loss to create/modify/cancel. If                 stopLoss is set to null, the Stop Loss Order will be cancelled                 if it exists. If stopLoss is not provided, the exisiting Stop                 Loss Order will not be modified. If a sub-field of stopLoss is                 not specified, that field will be set to a default value on                 create, and be inherited by the replacing order on modify.             trailingStopLoss:                 The specification of the Trailing Stop Loss to                 create/modify/cancel. If trailingStopLoss is set to null, the                 Trailing Stop Loss Order will be cancelled if it exists. If                 trailingStopLoss is not provided, the exisiting Trailing Stop                 Loss Order will not be modified. If a sub-field of                 trailngStopLoss is not specified, that field will be set to a                 default value on create, and be inherited by the replacing                 order on modify.          Returns:             v20.response.Response containing the results from submitting the             request
Instantiate a new Transaction from a dict (generally from loading a         JSON response). The data used to instantiate the Transaction is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new ClientConfigureTransaction from a dict (generally         from loading a JSON response). The data used to instantiate the         ClientConfigureTransaction is a shallow copy of the dict passed in,         with any complex child types instantiated appropriately.
Instantiate a new ClientConfigureRejectTransaction from a dict         (generally from loading a JSON response). The data used to instantiate         the ClientConfigureRejectTransaction is a shallow copy of the dict         passed in, with any complex child types instantiated appropriately.
Instantiate a new TransferFundsTransaction from a dict (generally from         loading a JSON response). The data used to instantiate the         TransferFundsTransaction is a shallow copy of the dict passed in, with         any complex child types instantiated appropriately.
Instantiate a new TransferFundsRejectTransaction from a dict (generally         from loading a JSON response). The data used to instantiate the         TransferFundsRejectTransaction is a shallow copy of the dict passed in,         with any complex child types instantiated appropriately.
Instantiate a new MarketOrderTransaction from a dict (generally from         loading a JSON response). The data used to instantiate the         MarketOrderTransaction is a shallow copy of the dict passed in, with         any complex child types instantiated appropriately.
Instantiate a new OrderFillTransaction from a dict (generally from         loading a JSON response). The data used to instantiate the         OrderFillTransaction is a shallow copy of the dict passed in, with any         complex child types instantiated appropriately.
Instantiate a new OrderClientExtensionsModifyTransaction from a dict         (generally from loading a JSON response). The data used to instantiate         the OrderClientExtensionsModifyTransaction is a shallow copy of the         dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new OrderClientExtensionsModifyRejectTransaction from a         dict (generally from loading a JSON response). The data used to         instantiate the OrderClientExtensionsModifyRejectTransaction is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new TradeClientExtensionsModifyTransaction from a dict         (generally from loading a JSON response). The data used to instantiate         the TradeClientExtensionsModifyTransaction is a shallow copy of the         dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new TradeClientExtensionsModifyRejectTransaction from a         dict (generally from loading a JSON response). The data used to         instantiate the TradeClientExtensionsModifyRejectTransaction is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new TradeOpen from a dict (generally from loading a JSON         response). The data used to instantiate the TradeOpen is a shallow copy         of the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new TradeReduce from a dict (generally from loading a         JSON response). The data used to instantiate the TradeReduce is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new LiquidityRegenerationSchedule from a dict (generally         from loading a JSON response). The data used to instantiate the         LiquidityRegenerationSchedule is a shallow copy of the dict passed in,         with any complex child types instantiated appropriately.
Instantiate a new LiquidityRegenerationScheduleStep from a dict         (generally from loading a JSON response). The data used to instantiate         the LiquidityRegenerationScheduleStep is a shallow copy of the dict         passed in, with any complex child types instantiated appropriately.
Instantiate a new OpenTradeFinancing from a dict (generally from         loading a JSON response). The data used to instantiate the         OpenTradeFinancing is a shallow copy of the dict passed in, with any         complex child types instantiated appropriately.
Instantiate a new PositionFinancing from a dict (generally from loading         a JSON response). The data used to instantiate the PositionFinancing is         a shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Get a list of Transactions pages that satisfy a time-based Transaction         query.          Args:             accountID:                 Account Identifier             fromTime:                 The starting time (inclusive) of the time range for the                 Transactions being queried.             toTime:                 The ending time (inclusive) of the time range for the                 Transactions being queried.             pageSize:                 The number of Transactions to include in each page of the                 results.             type:                 A filter for restricting the types of Transactions to retreive.          Returns:             v20.response.Response containing the results from submitting the             request
Get a stream of Transactions for an Account starting from when the         request is made.          Args:             accountID:                 Account Identifier          Returns:             v20.response.Response containing the results from submitting the             request
Instantiate a new PriceBucket from a dict (generally from loading a         JSON response). The data used to instantiate the PriceBucket is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new Price from a dict (generally from loading a JSON         response). The data used to instantiate the Price is a shallow copy of         the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new Candlestick from a dict (generally from loading a         JSON response). The data used to instantiate the Candlestick is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new CandlestickData from a dict (generally from loading a         JSON response). The data used to instantiate the CandlestickData is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new OrderBook from a dict (generally from loading a JSON         response). The data used to instantiate the OrderBook is a shallow copy         of the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new OrderBookBucket from a dict (generally from loading a         JSON response). The data used to instantiate the OrderBookBucket is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new PositionBook from a dict (generally from loading a         JSON response). The data used to instantiate the PositionBook is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new PositionBookBucket from a dict (generally from         loading a JSON response). The data used to instantiate the         PositionBookBucket is a shallow copy of the dict passed in, with any         complex child types instantiated appropriately.
Fetch candlestick data for an instrument.          Args:             instrument:                 Name of the Instrument             price:                 The Price component(s) to get candlestick data for. Can contain                 any combination of the characters "M" (midpoint candles) "B"                 (bid candles) and "A" (ask candles).             granularity:                 The granularity of the candlesticks to fetch             count:                 The number of candlesticks to return in the reponse. Count                 should not be specified if both the start and end parameters                 are provided, as the time range combined with the graularity                 will determine the number of candlesticks to return.             fromTime:                 The start of the time range to fetch candlesticks for.             toTime:                 The end of the time range to fetch candlesticks for.             smooth:                 A flag that controls whether the candlestick is "smoothed" or                 not.  A smoothed candlestick uses the previous candle's close                 price as its open price, while an unsmoothed candlestick uses                 the first price from its time range as its open price.             includeFirst:                 A flag that controls whether the candlestick that is covered by                 the from time should be included in the results. This flag                 enables clients to use the timestamp of the last completed                 candlestick received to poll for future candlesticks but avoid                 receiving the previous candlestick repeatedly.             dailyAlignment:                 The hour of the day (in the specified timezone) to use for                 granularities that have daily alignments.             alignmentTimezone:                 The timezone to use for the dailyAlignment parameter.                 Candlesticks with daily alignment will be aligned to the                 dailyAlignment hour within the alignmentTimezone.  Note that                 the returned times will still be represented in UTC.             weeklyAlignment:                 The day of the week used for granularities that have weekly                 alignment.          Returns:             v20.response.Response containing the results from submitting the             request
Fetch a price for an instrument. Accounts are not associated in any way         with this endpoint.          Args:             instrument:                 Name of the Instrument             time:                 The time at which the desired price is in effect. The current                 price is returned if no time is provided.          Returns:             v20.response.Response containing the results from submitting the             request
Instantiate a new Position from a dict (generally from loading a JSON         response). The data used to instantiate the Position is a shallow copy         of the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new PositionSide from a dict (generally from loading a         JSON response). The data used to instantiate the PositionSide is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new CalculatedPositionState from a dict (generally from         loading a JSON response). The data used to instantiate the         CalculatedPositionState is a shallow copy of the dict passed in, with         any complex child types instantiated appropriately.
Instantiate a new DynamicOrderState from a dict (generally from loading         a JSON response). The data used to instantiate the DynamicOrderState is         a shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new Order from a dict (generally from loading a JSON         response). The data used to instantiate the Order is a shallow copy of         the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new LimitOrder from a dict (generally from loading a JSON         response). The data used to instantiate the LimitOrder is a shallow         copy of the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new TakeProfitOrderRequest from a dict (generally from         loading a JSON response). The data used to instantiate the         TakeProfitOrderRequest is a shallow copy of the dict passed in, with         any complex child types instantiated appropriately.
Instantiate a new UnitsAvailableDetails from a dict (generally from         loading a JSON response). The data used to instantiate the         UnitsAvailableDetails is a shallow copy of the dict passed in, with any         complex child types instantiated appropriately.
Instantiate a new UnitsAvailable from a dict (generally from loading a         JSON response). The data used to instantiate the UnitsAvailable is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new GuaranteedStopLossOrderEntryData from a dict         (generally from loading a JSON response). The data used to instantiate         the GuaranteedStopLossOrderEntryData is a shallow copy of the dict         passed in, with any complex child types instantiated appropriately.
Get a list of Orders for an Account          Args:             accountID:                 Account Identifier             ids:                 List of Order IDs to retrieve             state:                 The state to filter the requested Orders by             instrument:                 The instrument to filter the requested orders by             count:                 The maximum number of Orders to return             beforeID:                 The maximum Order ID to return. If not provided the most recent                 Orders in the Account are returned          Returns:             v20.response.Response containing the results from submitting the             request
Replace an Order in an Account by simultaneously cancelling it and         creating a replacement Order          Args:             accountID:                 Account Identifier             orderSpecifier:                 The Order Specifier             order:                 Specification of the replacing Order          Returns:             v20.response.Response containing the results from submitting the             request
Cancel a pending Order in an Account          Args:             accountID:                 Account Identifier             orderSpecifier:                 The Order Specifier          Returns:             v20.response.Response containing the results from submitting the             request
Update the Client Extensions for an Order in an Account. Do not set,         modify, or delete clientExtensions if your account is associated with         MT4.          Args:             accountID:                 Account Identifier             orderSpecifier:                 The Order Specifier             clientExtensions:                 The Client Extensions to update for the Order. Do not set,                 modify, or delete clientExtensions if your account is                 associated with MT4.             tradeClientExtensions:                 The Client Extensions to update for the Trade created when the                 Order is filled. Do not set, modify, or delete clientExtensions                 if your account is associated with MT4.          Returns:             v20.response.Response containing the results from submitting the             request
Shortcut to create a Market Order in an Account          Args:             accountID : The ID of the Account             kwargs : The arguments to create a MarketOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to create a Limit Order in an Account          Args:             accountID : The ID of the Account             kwargs : The arguments to create a LimitOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to replace a pending Limit Order in an Account          Args:             accountID : The ID of the Account             orderID : The ID of the Limit Order to replace             kwargs : The arguments to create a LimitOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to create a Stop Order in an Account          Args:             accountID : The ID of the Account             kwargs : The arguments to create a StopOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to replace a pending Stop Order in an Account          Args:             accountID : The ID of the Account             orderID : The ID of the Stop Order to replace             kwargs : The arguments to create a StopOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to create a MarketIfTouched Order in an Account          Args:             accountID : The ID of the Account             kwargs : The arguments to create a MarketIfTouchedOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to replace a pending MarketIfTouched Order in an Account          Args:             accountID : The ID of the Account             orderID : The ID of the MarketIfTouched Order to replace             kwargs : The arguments to create a MarketIfTouchedOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to create a Take Profit Order in an Account          Args:             accountID : The ID of the Account             kwargs : The arguments to create a TakeProfitOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to replace a pending Take Profit Order in an Account          Args:             accountID : The ID of the Account             orderID : The ID of the Take Profit Order to replace             kwargs : The arguments to create a TakeProfitOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to create a Stop Loss Order in an Account          Args:             accountID : The ID of the Account             kwargs : The arguments to create a StopLossOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to replace a pending Stop Loss Order in an Account          Args:             accountID : The ID of the Account             orderID : The ID of the Stop Loss Order to replace             kwargs : The arguments to create a StopLossOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to create a Trailing Stop Loss Order in an Account          Args:             accountID : The ID of the Account             kwargs : The arguments to create a TrailingStopLossOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Shortcut to replace a pending Trailing Stop Loss Order in an Account          Args:             accountID : The ID of the Account             orderID : The ID of the Take Profit Order to replace             kwargs : The arguments to create a TrailingStopLossOrderRequest          Returns:             v20.response.Response containing the results from submitting             the request
Instantiate a new Instrument from a dict (generally from loading a JSON         response). The data used to instantiate the Instrument is a shallow         copy of the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new InstrumentCommission from a dict (generally from         loading a JSON response). The data used to instantiate the         InstrumentCommission is a shallow copy of the dict passed in, with any         complex child types instantiated appropriately.
Instantiate a new GuaranteedStopLossOrderLevelRestriction from a dict         (generally from loading a JSON response). The data used to instantiate         the GuaranteedStopLossOrderLevelRestriction is a shallow copy of the         dict passed in, with any complex child types instantiated         appropriately.
Set the token for the v20 context          Args:             token: The token used to access the v20 REST api
Set the Accept-Datetime-Format header to an acceptable         value          Args:             format: UNIX or RFC3339
Perform an HTTP request through the context          Args:             request: A v20.request.Request object          Returns:             A v20.response.Response object
Instantiate a new Account from a dict (generally from loading a JSON         response). The data used to instantiate the Account is a shallow copy         of the dict passed in, with any complex child types instantiated         appropriately.
Instantiate a new AccountChangesState from a dict (generally from         loading a JSON response). The data used to instantiate the         AccountChangesState is a shallow copy of the dict passed in, with any         complex child types instantiated appropriately.
Instantiate a new AccountSummary from a dict (generally from loading a         JSON response). The data used to instantiate the AccountSummary is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new CalculatedAccountState from a dict (generally from         loading a JSON response). The data used to instantiate the         CalculatedAccountState is a shallow copy of the dict passed in, with         any complex child types instantiated appropriately.
Instantiate a new AccountChanges from a dict (generally from loading a         JSON response). The data used to instantiate the AccountChanges is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Get a list of all Accounts authorized for the provided token.          Args:          Returns:             v20.response.Response containing the results from submitting the             request
Get the list of tradeable instruments for the given Account. The list         of tradeable instruments is dependent on the regulatory division that         the Account is located in, thus should be the same for all Accounts         owned by a single user.          Args:             accountID:                 Account Identifier             instruments:                 List of instruments to query specifically.          Returns:             v20.response.Response containing the results from submitting the             request
Set the client-configurable portions of an Account.          Args:             accountID:                 Account Identifier             alias:                 Client-defined alias (name) for the Account             marginRate:                 The string representation of a decimal number.          Returns:             v20.response.Response containing the results from submitting the             request
Fetch the user information for the specified user. This endpoint is         intended to be used by the user themself to obtain their own         information.          Args:             userSpecifier:                 The User Specifier          Returns:             v20.response.Response containing the results from submitting the             request
Instantiate a new ClientPrice from a dict (generally from loading a         JSON response). The data used to instantiate the ClientPrice is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Instantiate a new QuoteHomeConversionFactors from a dict (generally         from loading a JSON response). The data used to instantiate the         QuoteHomeConversionFactors is a shallow copy of the dict passed in,         with any complex child types instantiated appropriately.
Instantiate a new HomeConversions from a dict (generally from loading a         JSON response). The data used to instantiate the HomeConversions is a         shallow copy of the dict passed in, with any complex child types         instantiated appropriately.
Get pricing information for a specified list of Instruments within an         Account.          Args:             accountID:                 Account Identifier             instruments:                 List of Instruments to get pricing for.             since:                 Date/Time filter to apply to the response. Only prices and home                 conversions (if requested) with a time later than this filter                 (i.e. the price has changed after the since time) will be                 provided, and are filtered independently.             includeUnitsAvailable:                 Flag that enables the inclusion of the unitsAvailable field in                 the returned Price objects.             includeHomeConversions:                 Flag that enables the inclusion of the homeConversions field in                 the returned response. An entry will be returned for each                 currency in the set of all base and quote currencies present in                 the requested instruments list.          Returns:             v20.response.Response containing the results from submitting the             request
Get a stream of Account Prices starting from when the request is made.         This pricing stream does not include every single price created for the         Account, but instead will provide at most 4 prices per second (every         250 milliseconds) for each instrument being requested. If more than one         price is created for an instrument during the 250 millisecond window,         only the price in effect at the end of the window is sent. This means         that during periods of rapid price movement, subscribers to this stream         will not be sent every price. Pricing windows for different connections         to the price stream are not all aligned in the same way (i.e. they are         not all aligned to the top of the second). This means that during         periods of rapid price movement, different subscribers may observe         different prices depending on their alignment.          Args:             accountID:                 Account Identifier             instruments:                 List of Instruments to stream Prices for.             snapshot:                 Flag that enables/disables the sending of a pricing snapshot                 when initially connecting to the stream.          Returns:             v20.response.Response containing the results from submitting the             request
Esse método trata a data recebida de acordo com o timezone do     usuário. O seu retorno é dividido em duas partes:     1) A data em si;     2) As horas;     :param cDateUTC: string contendo as informações da data     :param timezone: timezone do usuário do sistema     :return: data e hora convertidos para a timezone do usuário
Remove special characters and strip spaces
Format datetime
Format date
Get a list of program files by expanding a list of path patterns         and interpreting it as relative to the executable.         This method can be used as helper for implementing the method program_files().         Contrary to the default implementation of program_files(), this method does not explicitly         add the executable to the list of returned files, it assumes that required_paths         contains a path that covers the executable.         @param executable: the path to the executable of the tool (typically the result of executable())         @param required_paths: a list of required path patterns         @param parent_dir: whether required_paths are relative to the directory of executable or the parent directory         @return a list of paths as strings, suitable for result of program_files()
Get version of a tool by executing it with argument "--version"         and returning stdout.         @param executable: the path to the executable of the tool (typically the result of executable())         @param arg: an argument to pass to the tool to let it print its version         @param use_stderr: True if the tool prints version on stderr, False for stdout         @return a (possibly empty) string of output of the tool
Mark the benchmark as erroneous, e.g., because the benchmarking tool crashed.         The message is intended as explanation for the user.
This method writes information about benchmark and system into txt_file.
The method output_before_run_set() calculates the length of the         first column for the output in terminal and stores information         about the runSet in XML.         @param runSet: current run set
This function writes a simple message to terminal and logfile,         when a run set is skipped.         There is no message about skipping a run set in the xml-file.
This method writes the information about a run set into the txt_file.
The method output_before_run() prints the name of a file to terminal.         It returns the name of the logfile.         @param run: a Run object
The method output_after_run() prints filename, result, time and status         of a run to terminal and stores all data in XML
The method output_after_run_set() stores the times of a run set in XML.         @params cputime, walltime: accumulated times of the run set
This function creates the XML structure for a list of runs
This function adds the result values to the XML representation of a run.
This function adds the result values to the XML representation of a runSet.
This function returns the name of the file for a run set         with an extension ("txt", "xml").
Formats the file name of a program for printing on console.
Write a rough string version of the XML (for temporary files).
Writes a nicely formatted XML file with DOCTYPE, and compressed if necessary.
returns a function that extracts the value for a column.
Calculate an assignment of the available CPU cores to a number     of parallel benchmark executions such that each run gets its own cores     without overlapping of cores between runs.     In case the machine has hyper-threading, this method tries to avoid     putting two different runs on the same physical core     (but it does not guarantee this if the number of parallel runs is too high to avoid it).     In case the machine has multiple CPUs, this method avoids     splitting a run across multiple CPUs if the number of cores per run     is lower than the number of cores per CPU     (splitting a run over multiple CPUs provides worse performance).     It will also try to split the runs evenly across all available CPUs.      A few theoretically-possible cases are not implemented,     for example assigning three 10-core runs on a machine     with two 16-core CPUs (this would have unfair core assignment     and thus undesirable performance characteristics anyway).      The list of available cores is read from the cgroup file system,     such that the assigned cores are a subset of the cores     that the current process is allowed to use.     This script does currently not support situations     where the available cores are asymmetrically split over CPUs,     e.g. 3 cores on one CPU and 5 on another.      @param coreLimit: the number of cores for each run     @param num_of_threads: the number of parallel benchmark executions     @param coreSet: the list of CPU cores identifiers provided by a user, None makes benchexec using all cores     @return a list of lists, where each inner list contains the cores for one run
This method does the actual work of _get_cpu_cores_per_run     without reading the machine architecture from the file system     in order to be testable. For description, c.f. above.     Note that this method might change the input parameters!     Do not call it directly, call getCpuCoresPerRun()!     @param allCpus: the list of all available cores     @param cores_of_package: a mapping from package (CPU) ids to lists of cores that belong to this CPU     @param siblings_of_core: a mapping from each core to a list of sibling cores including the core itself (a sibling is a core sharing the same physical core)
Get an assignment of memory banks to runs that fits to the given coreAssignment,     i.e., no run is allowed to use memory that is not local (on the same NUMA node)     to one of its CPU cores.
Get all memory banks the kernel lists in a given directory.     Such a directory can be /sys/devices/system/node/ (contains all memory banks)     or /sys/devices/system/cpu/cpu*/ (contains all memory banks on the same NUMA node as that core).
Check whether the desired amount of parallel benchmarks fits in the memory.     Implemented are checks for memory limits via cgroup controller "memory" and     memory bank restrictions via cgroup controller "cpuset",     as well as whether the system actually has enough memory installed.     @param memLimit: the memory limit in bytes per run     @param num_of_threads: the number of parallel benchmark executions     @param memoryAssignment: the allocation of memory banks to runs (if not present, all banks are assigned to all runs)
Get the size of a memory bank in bytes.
Returns a BenchExec result status based on the output of SMACK
Parse the output of the tool and extract the verification result.         This method always needs to be overridden.         If the tool gave a result, this method needs to return one of the         benchexec.result.RESULT_* strings.         Otherwise an arbitrary string can be returned that will be shown to the user         and should give some indication of the failure reason         (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Handle the options specified by add_basic_container_args().     @return: a dict that can be used as kwargs for the ContainerExecutor constructor
Define command-line arguments for output of a container (result files).     @param argument_parser: an argparse parser instance
Handle the options specified by add_container_output_args().     @return: a dict that can be used as kwargs for the ContainerExecutor.execute_run()
A simple command-line interface for the containerexecutor module of BenchExec.
Given the temp directory that is created for each run, return the path to the directory         where files created by the tool are stored.
This method executes the command line and waits for the termination of it,         handling all setup and cleanup.         @param args: the command line to run         @param rootDir: None or a root directory that contains all relevant files for starting a new process         @param workingDir: None or a directory which the execution should use as working directory         @param output_dir: the directory where to write result files (required if result_files_pattern)         @param result_files_patterns: a list of patterns of files to retrieve as result files
Execute the given command and measure its resource usage similarly to super()._start_execution(),         but inside a container implemented using Linux namespaces.         The command has no network access (only loopback),         a fresh directory as /tmp and no write access outside of this,         and it does not see other processes except itself.
Setup the filesystem layout in the container.          As first step, we create a copy of all existing mountpoints in mount_base, recursively,         and as "private" mounts (i.e., changes to existing mountpoints afterwards won't propagate         to our copy).         Then we iterate over all mountpoints and change them         according to the mode the user has specified (hidden, read-only, overlay, or full-access).         This has do be done for each mountpoint because overlays are not recursive.         Then we chroot into the new mount hierarchy.          The new filesystem layout still has a view of the host's /proc.         We do not mount a fresh /proc here because the grandchild still needs the old /proc.          We do simply iterate over all existing mount points and set them to read-only/overlay them,         because it is easier to create a new hierarchy and chroot into it.         First, we still have access to the original mountpoints while doing so,         and second, we avoid race conditions if someone else changes the existing mountpoints.          @param temp_dir: The base directory under which all our directories should be created.
Setup the filesystem layout in the given root directory.         Create a copy of the existing proc- and dev-mountpoints in the specified root         directory. Afterwards we chroot into it.          @param root_dir: The path of the root directory that is used to execute the process.
Transfer files created by the tool in the container to the output directory.         @param tool_output_dir: The directory under which all tool output files are created.         @param working_dir: The absolute working directory of the tool in the container.         @param output_dir: the directory where to write result files         @param patterns: a list of patterns of files to retrieve as result files
Read an parse the XML of a table-definition file.     @return: an ElementTree object for the table definition
Load all results in files that are listed in the given table-definition file.     @return: a list of RunSetResult objects
Load results from given files with column definitions taken from a table-definition file.     @return: a list of RunSetResult objects
Extract all columns mentioned in the result tag of a table definition file.
Extract columns that are relevant for the diff table.      @param columns_to_show: (list) A list of columns that should be shown     @return: (set) Set of columns that are relevant for the diff table. If              none is marked relevant, the column named "status" will be              returned in the set.
Return a unique identifier for a given task.     @param task: the XML element that represents a task     @return a tuple with filename of task as first element
Load the module with the tool-specific code.
Version of load_result for multiple input files that will be loaded concurrently.
Completely handle loading a single result file.     @param result_file the file to parse     @param options additional options     @param run_set_id the identifier of the run set     @param columns the list of columns     @param columns_relevant_for_diff a set of columns that is relevant for                                      the diff table     @return a fully ready RunSetResult instance or None
This function parses an XML file that contains the results of the execution of a run set.     It returns the "result" XML tag.     @param resultFile: The file name of the XML file that contains the results.     @param run_set_id: An optional identifier of this set of results.
This function merges the results of all RunSetResult objects.     If necessary, it can merge lists of names: [A,C] + [A,B] --> [A,B,C]     and add dummy elements to the results.     It also ensures the same order of tasks.
Set the filelists of all RunSetResult elements so that they contain the same files     in the same order. For missing files a dummy element is inserted.
Create list of rows with all data. Each row consists of several RunResults.
Find all rows with differences in the status column.
Find out which of the entries in Row.id are equal for all given rows.     @return: A list of True/False values according to whether the i-th part of the id is always equal.
Calculcate number of true/false tasks and maximum achievable score.
This function returns the numbers of the statistics.     @param runResults: All the results of the execution of one run set (as list of RunResult objects)
Count the number of regressions, i.e., differences in status of the two right-most results     where the new one is not "better" than the old one.     Any change in status between error, unknown, and wrong result is a regression.     Different kind of errors or wrong results are also a regression.
Create tables and write them to files.     @return a list of futures to allow waiting for completion
Append the result for one run. Needs to be called before collect_data().
Load the actual result values from the XML file and the log files.         This may take some time if many log files have to be opened and parsed.
This function extracts everything necessary for creating a RunSetResult object         from the "result" XML tag of a benchmark result file.         It returns a RunSetResult object, which is not yet fully initialized.         To finish initializing the object, call collect_data()         before using it for anything else         (this is to separate the possibly costly collect_data() call from object instantiation).
This function collects the values from one run.         Only columns that should be part of the table are collected.
generate output representation of rows
Find the path to the executable file that will get executed.         This method always needs to be overridden,         and most implementations will look similar to this one.         The path returned should be relative to the current directory.
Determine whether the version is greater than some given version
Parse the output of the tool and extract the verification result.         This method always needs to be overridden.         If the tool gave a result, this method needs to return one of the         benchexec.result.RESULT_* strings.         Otherwise an arbitrary string can be returned that will be shown to the user         and should give some indication of the failure reason         (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
A simple command-line interface for the runexecutor module of BenchExec.
Get the user account info from the passwd database. Only works on Linux.     @param user The name of a user account or a numeric uid prefixed with '#'     @return a tuple that corresponds to the members of the passwd structure     @raise KeyError: If user account is unknown     @raise ValueError: If uid is not a valid number
This function shrinks a file.     We remove only the middle part of a file,     the file-start and the file-end remain unchanged.
Segmentation faults and some memory failures reference a file     with more information (hs_err_pid_*). We append this file to the log.     The format that we expect is a line     "# An error report file with more information is saved as:"     and the file name of the dump file on the next line.     @param output_filename name of log file with tool output     @param base_path string that needs to be preprended to paths for lookup of files
Join a thread, but if the thread doesn't terminate for some time, ignore it     instead of waiting infinitely.
This function initializes the cgroups for the limitations and measurements.
Build the final command line for executing the given command,         using sudo if necessary.
Try to send signal to given process, either directly of with sudo.         Because we cannot send signals to the sudo process itself,         this method checks whether the target is the sudo process         and redirects the signal to sudo's child in this case.
Send signal to given process, either directly or with sudo.         If the target is the sudo process itself, the signal will be lost,         because we do not have the rights to send signals to sudo.         Use _kill_process() because of this.
Return the list of files in a directory, assuming that our user can read it.
This method creates the CGroups for the following execution.         @param my_cpus: None or a list of the CPU cores to use         @param memlimit: None or memory limit in bytes         @param memory_nodes: None or a list of memory nodes of a NUMA system to use         @param cgroup_values: dict of additional values to set         @return cgroups: a map of all the necessary cgroups for the following execution.                          Please add the process of the following execution to all those cgroups!
Create a temporary directory for the run.
Delete given temporary directory and all its contents.
Return map with desired environment variables for run.
Open and prepare output file.
Start time-limit handler.         @return None or the time-limit handler for calling cancel()
Start memory-limit handler.         @return None or the memory-limit handler for calling cancel()
Setup time limit with ulimit for the current process.
Start thread that enforces any file-hiearchy limits.
This function executes a given command with resource limits,         and writes the output to a file.         @param args: the command line to run         @param output_filename: the file where the output should be written to         @param stdin: What to uses as stdin for the process (None: /dev/null, a file descriptor, or a file object)         @param hardtimelimit: None or the CPU time in seconds after which the tool is forcefully killed.         @param softtimelimit: None or the CPU time in seconds after which the tool is sent a kill signal.         @param walltimelimit: None or the wall time in seconds after which the tool is forcefully killed (default: hardtimelimit + a few seconds)         @param cores: None or a list of the CPU cores to use         @param memlimit: None or memory limit in bytes         @param memory_nodes: None or a list of memory nodes in a NUMA system to use         @param environments: special environments for running the command         @param workingDir: None or a directory which the execution should use as working directory         @param maxLogfileSize: None or a number of bytes to which the output of the tool should be truncated approximately if there is too much output.         @param cgroupValues: dict of additional cgroup values to set (key is tuple of subsystem and option, respective subsystem needs to be enabled in RunExecutor; cannot be used to override values set by BenchExec)         @param files_count_limit: None or maximum number of files that may be written.         @param files_size_limit: None or maximum size of files that may be written.         @param error_filename: the file where the error output should be written to (default: same as output_filename)         @param write_headers: Write informational headers to the output and the error file if separate (default: True)         @param **kwargs: further arguments for ContainerExecutor.execute_run()         @return: dict with result of run (measurement results and process exitcode)
This method executes the command line and waits for the termination of it,         handling all setup and cleanup, but does not check whether arguments are valid.
This method calculates the exact results for time and memory measurements.         It is not important to call this method as soon as possible after the run.
Check that the user account's home directory now does not contain more files than         when this instance was created, and warn otherwise.         Does nothing if no user account was given to RunExecutor.         @return set of newly created files
Set the host name of the machine.
Basic utility to check the availability and permissions of cgroups.     This will log some warnings for the user if necessary.     On some systems, daemons such as cgrulesengd might interfere with the cgroups     of a process soon after it was started. Thus this function starts a process,     waits a configurable amount of time, and check whether the cgroups have been changed.     @param wait: a non-negative int that is interpreted as seconds to wait during the check     @raise SystemExit: if cgroups are not usable
Run check_cgroup_availability() in a separate thread to detect the following problem:     If "cgexec --sticky" is used to tell cgrulesengd to not interfere     with our child processes, the sticky flag unfortunately works only     for processes spawned by the main thread, not those spawned by other threads     (and this will happen if "benchexec -N" is used).
A simple command-line interface for the cgroups check of BenchExec.
This function checks, if all the words appear in the given order in the text.
This function prints the given String immediately and flushes the output.
This function returns True, if  a line of the file contains bracket '{'.
This function searches for all "option"-tags and returns a list with all attributes and texts.
Get a single child tag from an XML element.     Similar to "elem.find(tag)", but warns if there are multiple child tags with the given name.
This method returns a shallow copy of a XML-Element.     This method is for compatibility with Python 2.6 or earlier..     In Python 2.7 you can use  'copyElem = elem.copy()'  instead.
Parse a comma-separated list of strings.     The list may additionally contain ranges such as "1-5",     which will be expanded into "1,2,3,4,5".
Parse a string that consists of a integer number and an optional unit.     @param s a non-empty string that starts with an int and is followed by some letters     @return a triple of the number (as int) and the unit
Parse a string that contains a number of bytes, optionally with a unit like MB.     @return the number of bytes encoded by the string
Parse a string that contains a time span, optionally with a unit like s.     @return the number of seconds encoded by the string
Expand a file name pattern containing wildcards, environment variables etc.      @param pattern: The pattern string to expand.     @param base_dir: The directory where relative paths are based on.     @return: A list of file names (possibly empty).
Replace certain keys with respective values in a string.     @param template: the string in which replacements should be made     @param replacements: a dict or a list of pairs of keys and values
Suited as onerror handler for (sh)util.rmtree() that logs a warning.
Same as shutil.rmtree, but supports directories without write or execute permissions.
Copy all lines from an input file object to an output file object.
Simply write some content to a file, overriding the file if necessary.
Shrink a text file to approximately maxSize bytes     by removing lines from the middle of the file.
Read the full content of a file.
Read key value pairs from a file (each pair on a separate line).     Key and value are separated by ' ' as often used by the kernel.     @return a generator of tuples
Add and commit all files given in a list into a git repository in the     base_dir directory. Nothing is done if the git repository has     local changes.      @param files: the files to commit     @param description: the commit message
Setup the logging framework with a basic configuration
Interrupt running process, and provide a python prompt for interactive debugging.     This code is based on http://stackoverflow.com/a/133384/396730
This function executes the tool with a sourcefile with options.         It also calls functions for output before and after the run.
Parse the output of the tool and extract the verification result.         This method always needs to be overridden.         If the tool gave a result, this method needs to return one of the         benchexec.result.RESULT_* strings.         Otherwise an arbitrary string can be returned that will be shown to the user         and should give some indication of the failure reason         (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Check whether Turbo Boost (scaling CPU frequency beyond nominal frequency)     is active on this system.     @return: A bool, or None if Turbo Boost is not supported.
Check whether any of the CPU cores monitored by this instance has         throttled since this instance was created.         @return a boolean value
Check whether any swapping occured on this system since this instance was created.         @return a boolean value
Add some basic options for an executor to an argparse argument_parser.
Handle the options specified by add_basic_executor_options().
Try to send signal to given process.
Actually start the tool and the measurements.         @param parent_setup_fn a function without parameters that is called in the parent process             immediately before the tool is started         @param child_setup_fn a function without parameters that is called in the child process             before the tool is started         @param parent_cleanup_fn a function that is called in the parent process             immediately after the tool terminated, with three parameters:             the result of parent_setup_fn, the result of the executed process as ProcessExitCode,             and the base path for looking up files as parameter values         @return: a tuple of PID of process and a blocking function, which waits for the process             and a triple of the exit code and the resource usage of the process             and the result of parent_cleanup_fn (do not use os.wait)
Wait for the given process to terminate.         @return tuple of exit code and resource usage
Return a pretty-printed XML string for the Element.     Also allows setting a document type.
Determine the file paths to be adopted
This method replaces special substrings from a list of string     and return a new list.
Open and parse a task-definition file in YAML format.
Load the tool-info class.     @param tool_name: The name of the tool-info module.     Either a full Python package name or a name within the benchexec.tools package.     @return: A tuple of the full name of the used tool-info module and an instance of the tool-info class.
This function builds a list of SourcefileSets (containing filename with options).         The files and their options are taken from the list of sourcefilesTags.
Get the task-definition files from the XML definition. Task-definition files are files         for which we create a run (typically an input file or a YAML task definition).
Create a Run from a direct definition of the main input file (without task definition)
Create a Run from a task definition in yaml format
The function expand_filename_pattern expands a filename pattern to a sorted list         of filenames. The pattern can contain variables and wildcards.         If base_dir is given and pattern is not absolute, base_dir and pattern are joined.
Set the result of this run.         Use this method instead of manually setting the run attributes and calling after_execution(),         this method handles all this by itself.         @param values: a dictionary with result values as returned by RunExecutor.execute_run(),             may also contain arbitrary additional values         @param visible_columns: a set of keys of values that should be visible by default             (i.e., not marked as hidden), apart from those that BenchExec shows by default anyway
Return status according to result and output of tool.
try to find out whether the tool terminated because of a timeout
Create a dict of property->ExpectedResult from information encoded in a filename.
Return the achieved score of a task according to the SV-COMP scoring scheme.     @param category: result category as determined by get_result_category     @param result: the result given by the tool
Return the possible score of task, depending on whether the result is correct or not.
Classify the given result into "true" (property holds),     "false" (property does not hold), "unknown", and "error".     @param result: The result given by the tool (needs to be one of the RESULT_* strings to be recognized).     @return One of RESULT_CLASS_* strings
This function determines the relation between actual result and expected result     for the given file and properties.     @param filename: The file name of the input file.     @param result: The result given by the tool (needs to be one of the RESULT_* strings to be recognized).     @param properties: The list of property names to check.     @return One of the CATEGORY_* strings.
Create a Property instance by attempting to parse the given property file.         @param propertyfile: A file name of a property file         @param allow_unknown: Whether to accept unknown properties
Create a Property instance from a list of well-known property names         @param property_names: a non-empty list of property names
The function get_file_list expands a short filename to a sorted list     of filenames. The short filename can contain variables and wildcards.
Open a URL and ensure that the result is seekable,     copying it into a buffer if necessary.
Split a string into two parts: a prefix and a suffix. Splitting is done from the end,     so the split is done around the position of the last digit in the string     (that means the prefix may include any character, mixing digits and chars).     The flag 'numbers_into_suffix' determines whether the suffix consists of digits or non-digits.
Helper function for formatting the content of the options line
Take a tuple (values, counts), remove consecutive values and increment their count instead.
Returns a list where sequences of post-fixed entries are shortened to their common prefix.     This might be useful in cases of several similar values,     where the prefix is identical for several entries.     If less than 'number_of_needed_commons' are identically prefixed, they are kept unchanged.     Example: ['test', 'pc1', 'pc2', 'pc3', ... , 'pc10'] -> ['test', 'pc*']
Filter out duplicate values while keeping order.
If the value is a number (or number followed by a unit),     this function returns a string-representation of the number     with the specified number of significant digits,     optionally aligned at the decimal point.
Returns the type of the given column based on its row values on the given RunSetResult.     @param column: the column to return the correct ColumnType for     @param column_values: the column values to consider     @return: a tuple of a type object describing the column - the concrete ColumnType is stored in the attribute 'type',         the display unit of the column, which may be None,         the source unit of the column, which may be None,         and the scale factor to convert from the source unit to the display unit.         If no scaling is necessary for conversion, this value is 1.
Returns the amount of decimal digits of the given regex match, considering the number of significant     digits for the provided column.      @param decimal_number_match: a regex match of a decimal number, resulting from REGEX_MEASURE.match(x).     @param number_of_significant_digits: the number of significant digits required     @return: the number of decimal digits of the given decimal number match's representation, after expanding         the number to the required amount of significant digits
Format a value nicely for human-readable output (including rounding).          @param value: the value to format         @param isToAlign: if True, spaces will be added to the returned String representation to align it to all             other values in this column, correctly         @param format_target the target the value should be formatted for         @return: a formatted String representation of the given value.
Find the path to the executable file that will get executed.         This method always needs to be overridden,         and most implementations will look similar to this one.         The path returned should be relative to the current directory.
Parse the output of the tool and extract the verification result.         This method always needs to be overridden.         If the tool gave a result, this method needs to return one of the         benchexec.result.RESULT_* strings.         Otherwise an arbitrary string can be returned that will be shown to the user         and should give some indication of the failure reason         (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
OPTIONAL, this method is only necessary for situations when the benchmark environment         needs to know all files belonging to a tool         (to transport them to a cloud service, for example).         Returns a list of files or directories that are necessary to run the tool.
Return a Cgroup object with the cgroups of the current process.     Note that it is not guaranteed that all subsystems are available     in the returned object, as a subsystem may not be mounted.     Check with "subsystem in <instance>" before using.     A subsystem may also be present but we do not have the rights to create     child cgroups, this can be checked with require_subsystem().     @param cgroup_paths: If given, use this instead of reading /proc/self/cgroup.
Return the information which subsystems are mounted where.     @return a generator of tuples (subsystem, mountpoint)
For all subsystems, return the information in which (sub-)cgroup this process is in.     (Each process is in exactly cgroup in each hierarchy.)     @return a generator of tuples (subsystem, cgroup)
Parse a /proc/*/cgroup file into tuples of (subsystem,cgroup).     @param content: An iterable over the lines of the file.     @return: a generator of tuples
Tell cgrulesengd daemon to not move the given process into other cgroups,     if libcgroup is available.
Check whether the given subsystem is enabled and is writable         (i.e., new cgroups can be created for it).         Produces a log message for the user if one of the conditions is not fulfilled.         If the subsystem is enabled but not writable, it will be removed from         this instance such that further checks with "in" will return "False".         @return A boolean value.
Create child cgroups of the current cgroup for at least the given subsystems.         @return: A Cgroup instance representing the new child cgroup(s).
Add a process to the cgroups represented by this instance.
Return a generator of all PIDs currently in this cgroup for the given subsystem.
Kill all tasks in this cgroup and all its children cgroups forcefully.         Additionally, the children cgroups will be deleted.
Check whether the given value exists in the given subsystem.         Does not make a difference whether the value is readable, writable, or both.         Do not include the subsystem name in the option name.         Only call this method if the given subsystem is available.
Read the given value from the given subsystem.         Do not include the subsystem name in the option name.         Only call this method if the given subsystem is available.
Read the lines of the given file from the given subsystem.         Do not include the subsystem name in the option name.         Only call this method if the given subsystem is available.
Read the lines of the given file from the given subsystem         and split the lines into key-value pairs.         Do not include the subsystem name in the option name.         Only call this method if the given subsystem is available.
Write the given value for the given subsystem.         Do not include the subsystem name in the option name.         Only call this method if the given subsystem is available.
Remove all cgroups this instance represents from the system.         This instance is afterwards not usable anymore!
Parse a time stamp in the "year-month-day hour-minute" format.
The main method of BenchExec for use in a command-line script.     In addition to calling benchexec.start(argv),     it also handles signals and keyboard interrupts.     It does not return but calls sys.exit().     @param benchexec: An instance of BenchExec for executing benchmarks.     @param argv: optionally the list of command-line options to use
Start BenchExec.         @param argv: command-line options for BenchExec
Create a parser for the command-line options.         May be overwritten for adding more configuration options.         @return: an argparse.ArgumentParser instance
Configure the logging framework.
Execute a single benchmark as defined in a file.         If called directly, ensure that config and executor attributes are set up.         @param benchmark_file: the name of a benchmark-definition XML file         @return: a result value from the executor module
Check and abort if the target directory for the benchmark results         already exists in order to avoid overwriting results.
Stop the execution of a benchmark.         This instance cannot be used anymore afterwards.         Timely termination is not guaranteed, and this method may return before         everything is terminated.
Allocate some memory that can be used as a stack.     @return: a ctypes void pointer to the *top* of the stack.
Execute a function in a child process in separate namespaces.     @param func: a parameter-less function returning an int (which will be the process' exit value)     @return: the PID of the created child process
Write uid_map and gid_map in /proc to create a user mapping     that maps our user from outside the container to the same user inside the container     (and no other users are mapped).     @see: http://man7.org/linux/man-pages/man7/user_namespaces.7.html     @param pid: The PID of the process in the container.
Bring up the given network interface.     @raise OSError: if interface does not exist or permissions are missing
Get all current mount points of the system.     Changes to the mount points during iteration may be reflected in the result.     @return a generator of (source, target, fstype, options),     where options is a list of bytes instances, and the others are bytes instances     (this avoids encoding problems with mount points with problematic characters).
Remount an existing mount point with additional flags.     @param mountpoint: the mount point as bytes     @param existing_options: dict with current mount existing_options as bytes     @param mountflags: int with additional mount existing_options (cf. libc.MS_* constants)
Make a bind mount.     @param source: the source directory as bytes     @param target: the target directory as bytes     @param recursive: whether to also recursively bind mount all mounts below source     @param private: whether to mark the bind as private, i.e., changes to the existing mounts         won't propagate and vice-versa (changes to files/dirs will still be visible).
Drop all capabilities this process has.     @param keep: list of capabilities to not drop
Install all signal handler that forwards all signals to the given process.
Wait for a child to terminate and in the meantime forward all signals the current process     receives to this child.     @return a tuple of exit code and resource usage of the child as given by os.waitpid
Close all open file descriptors except those in a given set.     @param keep_files: an iterable of file descriptors or file-like objects.
Create a minimal system configuration for use in a container.     @param basedir: The directory where the configuration files should be placed as bytes.     @param mountdir: If present, bind mounts to the configuration files will be added below         this path (given as bytes).
Determine whether a given file is one of the files created by setup_container_system_config().     @param file: Absolute file path as string.
Compose the command line to execute from the name of the executable,         the user-specified options, and the inputfile to analyze.         This method can get overridden, if, for example, some options should         be enabled or if the order of arguments must be changed.          All paths passed to this method (executable, tasks, and propertyfile)         are either absolute or have been made relative to the designated working directory.          @param executable: the path to the executable of the tool (typically the result of executable())         @param options: a list of options, in the same order as given in the XML-file.         @param tasks: a list of tasks, that should be analysed with the tool in one run.                             In most cases we we have only _one_ inputfile.         @param propertyfile: contains a specification for the verifier.         @param rlimits: This dictionary contains resource-limits for a run,                         for example: time-limit, soft-time-limit, hard-time-limit, memory-limit, cpu-core-limit.                         All entries in rlimits are optional, so check for existence before usage!
Parse the output of the tool and extract the verification result.         This method always needs to be overridden.         If the tool gave a result, this method needs to return one of the         benchexec.result.RESULT_* strings.         Otherwise an arbitrary string can be returned that will be shown to the user         and should give some indication of the failure reason         (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
OPTIONAL, this method is only necessary for situations when the benchmark environment         needs to know all files belonging to a tool         (to transport them to a cloud service, for example).         Returns a list of files or directories that are necessary to run the tool.
Add content to the represented file.         If keep is False, the new content will be forgotten during the next call         to this method.
Take the result of an energy measurement and return a flat dictionary that contains all values.
Starts the external measurement program.
Stops the external measurement program and returns the measurement result,         if the measurement was running.
Parse the output of the tool and extract the verification result.         This method always needs to be overridden.         If the tool gave a result, this method needs to return one of the         benchexec.result.RESULT_* strings.         Otherwise an arbitrary string can be returned that will be shown to the user         and should give some indication of the failure reason         (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Render the error document
Call Paste's FileApp (a WSGI application) to serve the file         at the specified path
Show login form.
Extract prefix attributes from arbitary dict.
Extract pool attributes from arbitary dict.
List VRFs and return JSON encoded result.
Perform a smart VRF search.              The "smart" search function tries extract a query from             a text string. This query is then passed to the search_vrf             function, which performs the search.
Add a new VRF to NIPAP and return its data.
Remove a VRF.
List pools and return JSON encoded result.
Perform a smart pool search.              The "smart" search function tries extract a query from             a text string. This query is then passed to the search_pool             function, which performs the search.
Edit a pool.
Remove a pool.
List prefixes and return JSON encoded result.
Search prefixes. Does not yet incorporate all the functions of the             search_prefix API function due to difficulties with transferring             a complete 'dict-to-sql' encoded data structure.              Instead, a list of prefix attributes can be given which will be             matched with the 'equals' operator if notheing else is specified. If             multiple attributes are given, they will be combined with the 'and'             operator. Currently, it is not possible to specify different             operators for different attributes.
Perform a smart search.              The smart search function tries extract a query from             a text string. This query is then passed to the search_prefix             function, which performs the search.
Add prefix according to the specification.              The following keys can be used:              vrf             ID of VRF to place the prefix in             prefix          the prefix to add if already known             family          address family (4 or 6)             description     A short description             expires         Expiry time of assignment             comment         Longer comment             node            Hostname of node             type            Type of prefix; reservation, assignment, host             status          Status of prefix; assigned, reserved, quarantine             pool            ID of pool             country         Country where the prefix is used             order_id        Order identifier             customer_id     Customer identifier             vlan            VLAN ID             alarm_priority  Alarm priority of prefix             monitor         If the prefix should be monitored or not              from-prefix     A prefix the prefix is to be allocated from             from-pool       A pool (ID) the prefix is to be allocated from             prefix_length   Prefix length of allocated prefix
Remove a prefix.
Add VRF to filter list session variable
Remove VRF to filter list session variable
Return VRF filter list from session variable              Before returning list, make a search for all VRFs currently in the             list to verify that they still exist.
List Tags and return JSON encoded result.
Read the configuration file
Display NIPAP version info
Initialize auth backends.
Returns an authentication object.                  Examines the auth backend given after the '@' in the username and             returns a suitable instance of a subclass of the BaseAuth class.              * `username` [string]                 Username to authenticate as.             * `password` [string]                 Password to authenticate with.             * `authoritative_source` [string]                 Authoritative source of the query.             * `auth_options` [dict]                 A dict which, if authenticated as a trusted user, can override                 `username` and `authoritative_source`.
Verify authentication.              Returns True/False dependant on whether the authentication             succeeded or not.
Set up database              Creates tables required for the authentication module.
Verify authentication.              Returns True/False dependant on whether the authentication             succeeded or not.
Fetch the user from the database              The function will return None if the user is not found
Add user to SQLite database.              * `username` [string]                 Username of new user.             * `password` [string]                 Password of new user.             * `full_name` [string]                 Full name of new user.             * `trusted` [boolean]                 Whether the new user should be trusted or not.             * `readonly` [boolean]                 Whether the new user can only read or not
Remove user from the SQLite database.              * `username` [string]                 Username of user to remove.
Modify user in SQLite database.              Since username is used as primary key and we only have a single             argument for it we can't modify the username right now.
List all users.
Generate password hash.
Adds readwrite authorization          This will check if the user is a readonly user and if so reject the         query. Apply this decorator to readwrite functions.
Parse the 'expires' attribute, guessing what format it is in and         returning a datetime
Create the INET type and an Inet adapter.
Return true if given arg is a valid IPv4 address
Return address-family (4 or 6) for IP or None if invalid address
Open database connection
Execute query, catch and log errors.
Expand a dict so it fits in a INSERT clause
Expand a dict so it fits in a INSERT clause
Get rows updated by last update query              * `function` [function]                 Function to use for searching (one of the search_* functions).              Helper function used to fetch all rows which was updated by the             latest UPDATE ... RETURNING id query.
Split a query string into its parts
Get the schema version of the nipap psql db.
Install nipap database schema
Upgrade nipap database schema
Expand VRF specification to SQL.              id [integer]                 internal database id of VRF              name [string]                 name of VRF              A VRF is referenced either by its internal database id or by its             name. Both are used for exact matching and so no wildcard or             regular expressions are allowed. Only one key may be used and an             error will be thrown if both id and name is specified.
Add a new VRF.              * `auth` [BaseAuth]                 AAA options.             * `attr` [vrf_attr]                 The news VRF's attributes.              Add a VRF based on the values stored in the `attr` dict.              Returns a dict describing the VRF which was added.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.add_vrf` for full understanding.
Remove a VRF.              * `auth` [BaseAuth]                 AAA options.             * `spec` [vrf_spec]                 A VRF specification.              Remove VRF matching the `spec` argument.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_vrf` for full             understanding.
Return a list of VRFs matching `spec`.              * `auth` [BaseAuth]                 AAA options.             * `spec` [vrf_spec]                 A VRF specification. If omitted, all VRFs are returned.              Returns a list of dicts.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.list_vrf` for full             understanding.
Get a VRF based on prefix spec              Shorthand function to reduce code in the functions below, since             more or less all of them needs to perform the actions that are             specified here.              The major difference to :func:`list_vrf` is that we always return             results - empty results if no VRF is specified in prefix spec.
Update VRFs matching `spec` with attributes `attr`.              * `auth` [BaseAuth]                 AAA options.             * `spec` [vrf_spec]                 Attibutes specifying what VRF to edit.             * `attr` [vrf_attr]                 Dict specifying fields to be updated and their new values.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_vrf` for full             understanding.
Perform a smart search on VRF list.              * `auth` [BaseAuth]                 AAA options.             * `query_str` [string]                 Search string             * `search_options` [options_dict]                 Search options. See :func:`search_vrf`.             * `extra_query` [dict_to_sql]                 Extra search terms, will be AND:ed together with what is                 extracted from the query string.              Return a dict with three elements:                 * :attr:`interpretation` - How the query string was interpreted.                 * :attr:`search_options` - Various search_options.                 * :attr:`result` - The search result.                  The :attr:`interpretation` is given as a list of dicts, each                 explaining how a part of the search key was interpreted (ie. what                 VRF attribute the search operation was performed on).                  The :attr:`result` is a list of dicts containing the search result.              The smart search function tries to convert the query from a text             string to a `query` dict which is passed to the             :func:`search_vrf` function.  If multiple search keys are             detected, they are combined with a logical AND.              It will basically just take each search term and try to match it             against the name or description column with regex match or the VRF             column with an exact match.              See the :func:`search_vrf` function for an explanation of the             `search_options` argument.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_vrf` for full             understanding.
Parse a smart search query for VRFs              This is a helper function to smart_search_vrf for easier unit             testing of the parser.
Create a pool according to `attr`.              * `auth` [BaseAuth]                 AAA options.             * `attr` [pool_attr]                 A dict containing the attributes the new pool should have.              Returns a dict describing the pool which was added.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.add_pool` for full             understanding.
Remove a pool.              * `auth` [BaseAuth]                 AAA options.             * `spec` [pool_spec]                 Specifies what pool(s) to remove.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_pool` for full             understanding.
Return a list of pools.              * `auth` [BaseAuth]                 AAA options.             * `spec` [pool_spec]                 Specifies what pool(s) to list. Of omitted, all will be listed.              Returns a list of dicts.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.list_pool` for full             understanding.
Check pool attributes.
Get a pool.              Shorthand function to reduce code in the functions below, since             more or less all of them needs to perform the actions that are             specified here.              The major difference to :func:`list_pool` is that an exception             is raised if no pool matching the spec is found.
Update pool given by `spec` with attributes `attr`.              * `auth` [BaseAuth]                 AAA options.             * `spec` [pool_spec]                 Specifies what pool to edit.             * `attr` [pool_attr]                 Attributes to update and their new values.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_pool` for full             understanding.
Perform a smart search on pool list.              * `auth` [BaseAuth]                 AAA options.             * `query_str` [string]                 Search string             * `search_options` [options_dict]                 Search options. See :func:`search_pool`.             * `extra_query` [dict_to_sql]                 Extra search terms, will be AND:ed together with what is                 extracted from the query string.              Return a dict with three elements:                 * :attr:`interpretation` - How the query string was interpreted.                 * :attr:`search_options` - Various search_options.                 * :attr:`result` - The search result.                  The :attr:`interpretation` is given as a list of dicts, each                 explaining how a part of the search key was interpreted (ie. what                 pool attribute the search operation was performed on).                  The :attr:`result` is a list of dicts containing the search result.              The smart search function tries to convert the query from a text             string to a `query` dict which is passed to the             :func:`search_pool` function.  If multiple search keys are             detected, they are combined with a logical AND.              It will basically just take each search term and try to match it             against the name or description column with regex match.              See the :func:`search_pool` function for an explanation of the             `search_options` argument.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_pool` for full             understanding.
Parse a smart search query for pools              This is a helper function to smart_search_pool for easier unit             testing of the parser.
Expand prefix specification to SQL.
Expand prefix query dict into a WHERE-clause.              If you need to prefix each column reference with a table             name, that can be supplied via the table_name argument.
Add a prefix and return its ID.              * `auth` [BaseAuth]                 AAA options.             * `attr` [prefix_attr]                 Prefix attributes.             * `args` [add_prefix_args]                 Arguments explaining how the prefix should be allocated.              Returns a dict describing the prefix which was added.              Prefixes can be added in three ways; manually, from a pool or             from a prefix.              Manually                 All prefix data, including the prefix itself is specified in the                 `attr` argument. The `args` argument shall be omitted.              From a pool                 Most prefixes are expected to be automatically assigned from a pool.                 In this case, the :attr:`prefix` key is omitted from the `attr` argument.                 Also the :attr:`type` key can be omitted and the prefix type will then be                 set to the pools default prefix type. The :func:`find_free_prefix`                 function is used to find available prefixes for this allocation                 method, see its documentation for a description of how the                 `args` argument should be formatted.              From a prefix                 A prefix can also be selected from another prefix. Also in this case                 the :attr:`prefix` key is omitted from the `attr` argument. See the                 documentation for the :func:`find_free_prefix` for a description of how                 the `args` argument is to be formatted.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.add_prefix` for full             understanding.
Update prefix matching `spec` with attributes `attr`.              * `auth` [BaseAuth]                 AAA options.             * `spec` [prefix_spec]                 Specifies the prefix to edit.             * `attr` [prefix_attr]                 Prefix attributes.              Note that there are restrictions on when and how a prefix's type             can be changed; reservations can be changed to assignments and vice             versa, but only if they contain no child prefixes.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_prefix` for full             understanding.
Finds free prefixes in the sources given in `args`.              * `auth` [BaseAuth]                 AAA options.             * `vrf` [vrf]                 Full VRF-dict specifying in which VRF the prefix should be                 unique.             * `args` [find_free_prefix_args]                 Arguments to the find free prefix function.              Returns a list of dicts.              Prefixes can be found in two ways: from a pool of from a prefix.              From a pool             The `args` argument is set to a dict with key :attr:`from-pool` set to a             pool spec. This is the pool from which the prefix will be assigned.             Also the key :attr:`family` needs to be set to the adress family (integer             4 or 6) of the requested prefix.  Optionally, also the key             :attr:`prefix_length` can be added to the `attr` argument, and will then             override the default prefix length.              Example::                  args = {                     'from-pool': { 'name': 'CUSTOMER-' },                     'family': 6,                     'prefix_length': 64                 }              From a prefix                 Instead of specifying a pool, a prefix which will be searched                 for new prefixes can be specified. In `args`, the key                 :attr:`from-prefix` is set to list of prefixes you want to                 allocate from and the key :attr:`prefix_length` is set to                  the wanted prefix length.              Example::                  args = {                     'from-prefix': ['192.0.2.0/24'],                     'prefix_length': 27                 }              The key :attr:`count` can also be set in the `args` argument to specify             how many prefixes that should be returned. If omitted, the default             value is 1000.              The internal backend function :func:`find_free_prefix` is used             internally by the :func:`add_prefix` function to find available             prefixes from the given sources. It's also exposed over XML-RPC,             please see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.find_free_prefix` for full             understanding.
List prefixes matching the `spec`.              * `auth` [BaseAuth]                 AAA options.             * `spec` [prefix_spec]                 Specifies prefixes to list. If omitted, all will be listed.              Returns a list of dicts.              This is a quite blunt tool for finding prefixes, mostly useful for             fetching data about a single prefix. For more capable alternatives,             see the :func:`search_prefix` or :func:`smart_search_prefix` functions.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.list_prefix` for full             understanding.
Do the underlying database operations to delete a prefix
Remove prefix matching `spec`.              * `auth` [BaseAuth]                 AAA options.             * `spec` [prefix_spec]                 Specifies prefixe to remove.             * `recursive` [bool]                 When set to True, also remove child prefixes.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_prefix` for full             understanding.
Search prefix list for prefixes matching `query`.              * `auth` [BaseAuth]                 AAA options.             * `query` [dict_to_sql]                 How the search should be performed.             * `search_options` [options_dict]                 Search options, see below.              Returns a list of dicts.              The `query` argument passed to this function is designed to be             able to express quite advanced search filters. It is internally             expanded to an SQL WHERE-clause.              The `query` is a dict with three elements, where one specifies the             operation to perform and the two other specifies its arguments. The             arguments can themselves be `query` dicts, i.e. nested, to build             more complex queries.              The :attr:`operator` key specifies what operator should be used for the             comparison. Currently the following operators are supported:              * :data:`and` - Logical AND             * :data:`or` - Logical OR             * :data:`equals_any` - Equality of any element in array             * :data:`equals` - Equality; =             * :data:`not_equals` - Inequality; !=             * :data:`less` - Less than; <             * :data:`less_or_equal` - Less than or equal to; <=             * :data:`greater` - Greater than; >             * :data:`greater_or_equal` - Greater than or equal to; >=             * :data:`like` - SQL LIKE             * :data:`regex_match` - Regular expression match             * :data:`regex_not_match` - Regular expression not match             * :data:`contains` - IP prefix contains             * :data:`contains_equals` - IP prefix contains or is equal to             * :data:`contained_within` - IP prefix is contained within             * :data:`contained_within_equals` - IP prefix is contained within or equals              The :attr:`val1` and :attr:`val2` keys specifies the values which             are subjected to the comparison. :attr:`val1` can be either any             prefix attribute or a query dict. :attr:`val2` can be either the             value you want to compare the prefix attribute to, or a `query`             dict.              Example 1 - Find the prefixes which contains 192.0.2.0/24::                  query = {                     'operator': 'contains',                     'val1': 'prefix',                     'val2': '192.0.2.0/24'                 }              This will be expanded to the pseudo-SQL query::                  SELECT * FROM prefix WHERE prefix contains '192.0.2.0/24'              Example 2 - Find for all assignments in prefix 192.0.2.0/24::                  query = {                     'operator': 'and',                     'val1': {                         'operator': 'equals',                         'val1': 'type',                         'val2': 'assignment'                     },                     'val2': {                         'operator': 'contained_within',                         'val1': 'prefix',                         'val2': '192.0.2.0/24'                     }                 }              This will be expanded to the pseudo-SQL query::                  SELECT * FROM prefix WHERE (type == 'assignment') AND (prefix contained within '192.0.2.0/24')              If you want to combine more than two expressions together with a             boolean expression you need to nest them. For example, to match on             three values, in this case the tag 'foobar' and a prefix-length             between /10 and /24, the following could be used::                  query = {                     'operator': 'and',                     'val1': {                         'operator': 'and',                         'val1': {                             'operator': 'greater',                             'val1': 'prefix_length',                             'val2': 9                         },                         'val2': {                             'operator': 'less_or_equal',                             'val1': 'prefix_length',                             'val2': 24                         }                     },                     'val2': {                         'operator': 'equals_any',                         'val1': 'tags',                         'val2': 'foobar'                     }                 }               The `options` argument provides a way to alter the search result to             assist in client implementations. Most options regard parent and             children prefixes, that is the prefixes which contain the prefix(es)             matching the search terms (parents) or the prefixes which are             contained by the prefix(es) matching the search terms. The search             options can also be used to limit the number of rows returned.              The following options are available:                 * :attr:`parents_depth` - How many levels of parents to return. Set to :data:`-1` to include all parents.                 * :attr:`children_depth` - How many levels of children to return. Set to :data:`-1` to include all children.                 * :attr:`include_all_parents` - Include all parents, no matter what depth is specified.                 * :attr:`include_all_children` - Include all children, no matter what depth is specified.                 * :attr:`max_result` - The maximum number of prefixes to return (default :data:`50`).                 * :attr:`offset` - Offset the result list this many prefixes (default :data:`0`).              The options above gives the possibility to specify how many levels             of parent and child prefixes to return in addition to the prefixes             that actually matched the search terms. This is done by setting the             :attr:`parents_depth` and :attr:`children depth` keys in the             `search_options` dict to an integer value.  In addition to this it             is possible to get all all parents and/or children included in the             result set even though they are outside the limits set with             :attr:`*_depth`.  The extra prefixes included will have the             attribute :attr:`display` set to :data:`false` while the other ones             (the actual search result togther with the ones included due to             given depth) :attr:`display` set to :data:`true`. This feature is             usable obtain search results with some context given around them,             useful for example when displaying prefixes in a tree without the             need to implement client side IP address logic.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.search_prefix` for full             understanding.
Perform a smart search on prefix list.              * `auth` [BaseAuth]                 AAA options.             * `query_str` [string]                 Search string             * `search_options` [options_dict]                 Search options. See :func:`search_prefix`.             * `extra_query` [dict_to_sql]                 Extra search terms, will be AND:ed together with what is                 extracted from the query string.              Return a dict with three elements:                 * :attr:`interpretation` - How the query string was interpreted.                 * :attr:`search_options` - Various search_options.                 * :attr:`result` - The search result.                  The :attr:`interpretation` is given as a list of dicts, each                 explaining how a part of the search key was interpreted (ie. what                 prefix attribute the search operation was performed on).                  The :attr:`result` is a list of dicts containing the search result.              The smart search function tries to convert the query from a text             string to a `query` dict which is passed to the             :func:`search_prefix` function.  If multiple search keys are             detected, they are combined with a logical AND.              It tries to automatically detect IP addresses and prefixes and put             these into the `query` dict with "contains_within" operators and so             forth.              See the :func:`search_prefix` function for an explanation of the             `search_options` argument.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_prefix` for full             understanding.
Parse a smart search query for prefixes              This is a helper function to smart_search_prefix for easier unit             testing of the parser.
List AS numbers matching `spec`.              * `auth` [BaseAuth]                 AAA options.             * `spec` [asn_spec]                 An automous system number specification. If omitted, all ASNs                 are returned.              Returns a list of dicts.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.list_asn` for full             understanding.
Add AS number to NIPAP.              * `auth` [BaseAuth]                 AAA options.             * `attr` [asn_attr]                 ASN attributes.              Returns a dict describing the ASN which was added.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.add_asn` for full             understanding.
Edit AS number              * `auth` [BaseAuth]                 AAA options.             * `asn` [integer]                 AS number to edit.             * `attr` [asn_attr]                 New AS attributes.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_asn` for full             understanding.
Remove an AS number.              * `auth` [BaseAuth]                 AAA options.             * `spec` [asn]                 An ASN specification.              Remove ASNs matching the `asn` argument.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_asn` for full             understanding.
Search ASNs for entries matching 'query'              * `auth` [BaseAuth]                 AAA options.             * `query` [dict_to_sql]                 How the search should be performed.             * `search_options` [options_dict]                 Search options, see below.              Returns a list of dicts.              The `query` argument passed to this function is designed to be             able to specify how quite advanced search operations should be             performed in a generic format. It is internally expanded to a SQL             WHERE-clause.              The `query` is a dict with three elements, where one specifies the             operation to perform and the two other specifies its arguments. The             arguments can themselves be `query` dicts, to build more complex             queries.              The :attr:`operator` key specifies what operator should be used for the             comparison. Currently the following operators are supported:              * :data:`and` - Logical AND             * :data:`or` - Logical OR             * :data:`equals` - Equality; =             * :data:`not_equals` - Inequality; !=             * :data:`like` - SQL LIKE             * :data:`regex_match` - Regular expression match             * :data:`regex_not_match` - Regular expression not match              The :attr:`val1` and :attr:`val2` keys specifies the values which are subjected             to the comparison. :attr:`val1` can be either any prefix attribute or an             entire query dict. :attr:`val2` can be either the value you want to             compare the prefix attribute to, or an entire `query` dict.              The search options can also be used to limit the number of rows             returned or set an offset for the result.              The following options are available:                 * :attr:`max_result` - The maximum number of prefixes to return (default :data:`50`).                 * :attr:`offset` - Offset the result list this many prefixes (default :data:`0`).              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.search_tag` for full             understanding.
Perform a smart search operation among AS numbers              * `auth` [BaseAuth]                 AAA options.             * `query_str` [string]                 Search string             * `search_options` [options_dict]                 Search options. See :func:`search_asn`.             * `extra_query` [dict_to_sql]                 Extra search terms, will be AND:ed together with what is                 extracted from the query string.              Return a dict with three elements:                 * :attr:`interpretation` - How the query string was interpreted.                 * :attr:`search_options` - Various search_options.                 * :attr:`result` - The search result.                  The :attr:`interpretation` is given as a list of dicts, each                 explaining how a part of the search key was interpreted (ie. what                 ASN attribute the search operation was performed on).                  The :attr:`result` is a list of dicts containing the search result.              The smart search function tries to convert the query from a text             string to a `query` dict which is passed to the             :func:`search_asn` function.  If multiple search keys are             detected, they are combined with a logical AND.              See the :func:`search_asn` function for an explanation of the             `search_options` argument.              This is the documentation of the internal backend function. It's             exposed over XML-RPC, please also see the XML-RPC documentation for             :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_asn` for full             understanding.
Parse a smart search query for ASNs              This is a helper function to smart_search_asn for easier unit             testing of the parser.
Expand Tag query dict into a WHERE-clause.              If you need to prefix each column reference with a table             name, that can be supplied via the table_name argument.
Create, configure and return the routes Mapper
Set up the global pynipap connection object
Returns pool to work with          Returns a pynipap.Pool object representing the pool we are working with.
Returns VRF to work in          Returns a pynipap.VRF object representing the VRF we are working         in. If there is a VRF set globally, return this. If not, fetch the         VRF named 'arg'. If 'arg' is None, fetch the default_vrf         attribute from the config file and return this VRF.
List pools matching a search criteria
List VRFs matching a search criteria
List prefixes matching 'arg'
Return a prefix based on options passed from command line          Used by add_prefix() and add_prefix_from_pool() to avoid duplicate         parsing
Add prefix to NIPAP
Add prefix using from-pool to NIPAP
Add VRF to NIPAP
Add a pool.
View a single VRF
View a single pool
View a single prefix.
Remove VRF
Remove pool
Remove prefix
Modify a VRF with the options set in opts
Modify a pool with the options set in opts
Expand a pool with the ranges set in opts
Shrink a pool by removing the ranges in opts from it
Modify the prefix 'arg' with the options 'opts'
Add attributes to a prefix
Add attributes to a VRF
Remove attributes from a prefix
Add attributes to a pool
Remove attributes from a prefix
Returns valid string completions          Takes the string 'key' and compares it to each of the strings in         'haystack'. The ones which beginns with 'key' are returned as result.
Complete NIPAP prefix type
Complete member prefixes of pool
Complete node hostname          This function is currently a bit special as it looks in the config file         for a command to use to complete a node hostname from an external         system.          It is configured by setting the config attribute "complete_node_cmd" to         a shell command. The string "%search_string%" in the command will be         replaced by the current search string.
Returns list of matching pool names
Returns list of matching VRFs
Returns list of matching VRFs          Includes "virtual" VRF 'all' which is used in search         operations
Parse a smart search string and return it in an AST like form
Parse matching expression in form key <op> value              For example;                 vlan > 1                 node = FOO-BAR
Do magic matching of single words or quoted string
Do magic matching of single words or quoted string
Examine the current matching key              Extracts information, such as function to execute and command             options, from the current key (passed to function as 'key_name' and             'key_val').
Extract command and options from string.              The tree argument should contain a specifically formatted dict             which describes the available commands, options, arguments and             callbacks to methods for completion of arguments.             TODO: document dict format              The inp_cmd argument should contain a list of strings containing             the complete command to parse, such as sys.argv (without the first             element which specified the command itself).
Return list of valid completions              Returns a list of valid completions on the current level in the             tree. If an element of type 'value' is found, its complete callback             function is called (if set).
Return list of valid next values
Add a pool.
Edit a pool.
Remove pool.
Remove a prefix from pool 'id'.
Add hosts from ipplan to networks object.
Gather network and host information from ipplan export files.
Put your network information in the prefix object.
Put your host information in the prefix object.
Get version of nipapd we're connected to.          Maps to the function :py:func:`nipap.xmlrpc.NipapXMLRPC.version` in the         XML-RPC API. Please see the documentation for the XML-RPC function for         information regarding the return value.
Get schema version of database we're connected to.          Maps to the function :py:func:`nipap.backend.Nipap._get_db_version` in         the backend. Please see the documentation for the backend function for         information regarding the return value.
Converts XML-RPC Fault objects to Pynipap-exceptions.          TODO: Is this one neccesary? Can be done inline...
Create new Tag-object from dict.              Suitable for creating objects from XML-RPC data.             All available keys must exist.
Search tags.              For more information, see the backend function             :py:func:`nipap.backend.Nipap.search_tag`.
List VRFs.              Maps to the function :py:func:`nipap.backend.Nipap.list_vrf` in the             backend. Please see the documentation for the backend function for             information regarding input arguments and return values.
Create new VRF-object from dict.              Suitable for creating objects from XML-RPC data.             All available keys must exist.
Get the VRF with id 'id'.
Search VRFs.              Maps to the function :py:func:`nipap.backend.Nipap.search_vrf` in             the backend. Please see the documentation for the backend function             for information regarding input arguments and return values.
Perform a smart VRF search.              Maps to the function             :py:func:`nipap.backend.Nipap.smart_search_vrf` in the backend.             Please see the documentation for the backend function for             information regarding input arguments and return values.
Save changes made to object to NIPAP.              If the object represents a new VRF unknown to NIPAP (attribute `id`             is `None`) this function maps to the function             :py:func:`nipap.backend.Nipap.add_vrf` in the backend, used to             create a new VRF. Otherwise it maps to the function             :py:func:`nipap.backend.Nipap.edit_vrf` in the backend, used to             modify the VRF. Please see the documentation for the backend             functions for information regarding input arguments and return             values.
Remove VRF.              Maps to the function :py:func:`nipap.backend.Nipap.remove_vrf` in             the backend. Please see the documentation for the backend function             for information regarding input arguments and return values.
Save changes made to pool to NIPAP.              If the object represents a new pool unknown to NIPAP (attribute             `id` is `None`) this function maps to the function             :py:func:`nipap.backend.Nipap.add_pool` in the backend, used to             create a new pool. Otherwise it maps to the function             :py:func:`nipap.backend.Nipap.edit_pool` in the backend, used to             modify the pool. Please see the documentation for the backend             functions for information regarding input arguments and return             values.
Get the pool with id 'id'.
Search pools.              Maps to the function :py:func:`nipap.backend.Nipap.search_pool` in             the backend. Please see the documentation for the backend function             for information regarding input arguments and return values.
Perform a smart pool search.              Maps to the function             :py:func:`nipap.backend.Nipap.smart_search_pool` in the backend.             Please see the documentation for the backend function for             information regarding input arguments and return values.
Create new Pool-object from dict.              Suitable for creating objects from XML-RPC data.             All available keys must exist.
List pools.              Maps to the function :py:func:`nipap.backend.Nipap.list_pool` in             the backend. Please see the documentation for the backend function             for information regarding input arguments and return values.
Get the prefix with id 'id'.
Finds a free prefix.              Maps to the function             :py:func:`nipap.backend.Nipap.find_free_prefix` in the backend.             Please see the documentation for the backend function for             information regarding input arguments and return values.
Search for prefixes.              Maps to the function :py:func:`nipap.backend.Nipap.search_prefix`             in the backend. Please see the documentation for the backend             function for information regarding input arguments and return             values.
Perform a smart prefix search.              Maps to the function             :py:func:`nipap.backend.Nipap.smart_search_prefix` in the backend.             Please see the documentation for the backend function for             information regarding input arguments and return values.
List prefixes.              Maps to the function :py:func:`nipap.backend.Nipap.list_prefix` in             the backend. Please see the documentation for the backend function             for information regarding input arguments and return values.
Save prefix to NIPAP.              If the object represents a new prefix unknown to NIPAP (attribute             `id` is `None`) this function maps to the function             :py:func:`nipap.backend.Nipap.add_prefix` in the backend, used to             create a new prefix. Otherwise it maps to the function             :py:func:`nipap.backend.Nipap.edit_prefix` in the backend, used to             modify the VRF. Please see the documentation for the backend             functions for information regarding input arguments and return             values.
Remove the prefix.              Maps to the function :py:func:`nipap.backend.Nipap.remove_prefix`             in the backend. Please see the documentation for the backend             function for information regarding input arguments and return             values.
Create a Prefix object from a dict.              Suitable for creating Prefix objects from XML-RPC input.
Edit a VRF
Add a new VRF.
Removes a VRF.
Place any commands to setup nipapwww here
Create a Pylons WSGI application and return it      ``global_conf``         The inherited configuration for this application. Normally from         the [DEFAULT] section of the Paste ini file.      ``full_stack``         Whether this application provides a full WSGI stack (by default,         meaning it handles its own exceptions and errors). Disable         full_stack when this application is "managed" by another WSGI         middleware.      ``static_files``         Whether this application serves its own static files; disable         when another web server is responsible for serving them.      ``app_conf``         The application's local configuration. Normally specified in         the [app:<name>] section of the Paste ini file (where <name>         defaults to main).
Add a prefix.
Edit a prefix.
Remove a prefix.
Configure the Pylons environment via the ``pylons.config``     object
Get prefix data from NIPAP
Write the config to file
Detach a process from the controlling terminal and run it in the    background as a daemon.
Parse one line
Mangle prefix result
Class decorator for XML-RPC functions that requires auth
An echo function              An API test function which simply echoes what is is passed in the             'message' element in the args-dict..              Valid keys in the `args`-struct:              * `auth` [struct]                 Authentication options passed to the :class:`AuthFactory`.             * `message` [string]                 String to echo.             * `sleep` [integer]                 Number of seconds to sleep before echoing.              Returns a string.
Add a new VRF.              Valid keys in the `args`-struct:              * `auth` [struct]                 Authentication options passed to the :class:`AuthFactory`.             * `attr` [struct]                 VRF attributes.              Returns the internal database ID for the VRF.
Add a prefix.              Valid keys in the `args`-struct:              * `auth` [struct]                 Authentication options passed to the :class:`AuthFactory`.             * `attr` [struct]                 Attributes to set on the new prefix.             * `args` [srgs]                 Arguments for addition of prefix, such as what pool or prefix                 it should be allocated from.              Returns ID of created prefix.
Remove a prefix.              Valid keys in the `args`-struct:              * `auth` [struct]                 Authentication options passed to the :class:`AuthFactory`.             * `prefix` [struct]                 Attributes used to select what prefix to remove.             * `recursive` [boolean]                 When set to 1, also remove child prefixes.
Find a free prefix.              Valid keys in the `args`-struct:              * `auth` [struct]                 Authentication options passed to the :class:`AuthFactory`.             * `args` [struct]                 Arguments for the find_free_prefix-function such as what prefix                 or pool to allocate from.
Read content of specified NEWS file
Parse content of DCH file
Formats a record and serializes it as a JSON str. If record message isnt         already a dict, initializes a new dict and uses `default_msg_fieldname`         as a key as the record msg as the value.
:type record: aiologger.loggers.json.LogRecord
:type record: aiologger.loggers.json.LogRecord
Open the current base file with the (original) mode and encoding.         Return the resulting stream.
Emit a record.          Output the record to the file, catering for rollover as described         in `do_rollover`.
Modify the filename of a log file when rotating.          This is provided so that a custom filename can be provided.          :param default_name: The default name for the log file.
When rotating, rotate the current log.          The default implementation calls the 'rotator' attribute of the         handler, if it's callable, passing the source and dest arguments to         it. If the attribute isn't callable (the default is None), the source         is simply renamed to the destination.          :param source: The source filename. This is normally the base                        filename, e.g. 'test.log'         :param dest:   The destination filename. This is normally                        what the source is rotated to, e.g. 'test.log.1'.
Work out the rollover time based on the specified time.          If we are rolling over at midnight or weekly, then the interval is         already known. need to figure out is WHEN the next interval is.         In other words, if you are rolling over at midnight, then your base         interval is 1 day, but you want to start that one day clock at midnight,          not now. So, we have to fudge the `rollover_at` value in order to trigger          the first rollover at the right time.  After that, the regular interval          will take care of the rest.  Note that this code doesn't care about          leap seconds. :)
Determine if rollover should occur.          record is not used, as we are just comparing times, but it is needed so         the method signatures are the same
Determine the files to delete when rolling over.
do a rollover; in this case, a date/time stamp is appended to the filename         when the rollover happens.  However, you want the file to be named for the         start of the interval, not the current time.  If there is a backup count,         then we have to get a list of matching filenames, sort them and remove         the one with the oldest suffix.
Low-level logging routine which creates a LogRecord and then calls         all the handlers of this logger to handle the record.          Overwritten to properly handle log methods kwargs
Conditionally emit the specified logging record.         Emission depends on filters which may have been added to the handler.
Actually log the specified logging record to the stream.
Pass a record to all relevant handlers.          Loop through all handlers for this logger and its parents in the         logger hierarchy. If no handler was found, raises an error. Stop         searching up the hierarchy whenever a logger with the "propagate"         attribute set to zero is found - that will be the last logger         whose handlers are called.
Call the handlers for the specified record.          This method is used for unpickled records received from a socket, as         well as those created locally. Logger-level filtering is applied.
Creates an asyncio.Task for a msg if logging is enabled for level.         Returns a dummy task otherwise.
Log msg with severity 'DEBUG'.          To pass exception information, use the keyword argument exc_info with         a true value, e.g.          await logger.debug("Houston, we have a %s", "thorny problem", exc_info=1)
Log msg with severity 'INFO'.          To pass exception information, use the keyword argument exc_info with         a true value, e.g.          await logger.info("Houston, we have an interesting problem", exc_info=1)
Log msg with severity 'WARNING'.          To pass exception information, use the keyword argument exc_info with         a true value, e.g.          await logger.warning("Houston, we have a bit of a problem", exc_info=1)
Log msg with severity 'ERROR'.          To pass exception information, use the keyword argument exc_info with         a true value, e.g.          await logger.error("Houston, we have a major problem", exc_info=1)
Log msg with severity 'CRITICAL'.          To pass exception information, use the keyword argument exc_info with         a true value, e.g.          await logger.critical("Houston, we have a major disaster", exc_info=1)
Convenience method for logging an ERROR with exception information.
Perform any cleanup actions in the logging system (e.g. flushing         buffers).          Should be called at application exit.
From an array-like input, infer the correct vega typecode     ('ordinal', 'nominal', 'quantitative', or 'temporal')      Parameters     ----------     data: Numpy array or Pandas Series         data for which the type will be inferred     ordinal_threshold: integer (default: 0)         integer data will result in a 'quantitative' type, unless the         number of unique values is smaller than ordinal_threshold.      Adapted from code at http://github.com/altair-viz/altair/     Licence: BSD-3
Unpivot a dataframe for use with Vega/Vega-Lite      The input is a frame with any number of columns,     output is a frame with three columns: x value, y values,     and variable names.
Validate an aggregation for use in Vega-Lite.      Translate agg to one of the following supported named aggregations:     ['mean', 'sum', 'median', 'min', 'max', 'count']      Parameters     ----------     agg : string or callable         A string      Supported reductions are ['mean', 'sum', 'median', 'min', 'max', 'count'].      If agg is a numpy function, the return value is the string representation.      If agg is unrecognized, raise a ValueError
Obtain the packge version from a python file e.g. pkg/__init__.py     See <https://packaging.python.org/en/latest/single_source_version.html>.
Draw a matrix of scatter plots.      The result is an interactive pan/zoomable plot, with linked-brushing     enabled by holding the shift key.      Parameters     ----------     frame : DataFrame         The dataframe for which to draw the scatter matrix.     c : string (optional)         If specified, the name of the column to be used to determine the         color of each point.     s : string (optional)         If specified, the name of the column to be used to determine the         size of each point,     figsize : tuple (optional)         A length-2 tuple speficying the size of the figure in inches     dpi : float (default=72)         The dots (i.e. pixels) per inch used to convert the figure size from         inches to pixels.      Returns     -------     chart: alt.Chart object         The alt.Chart representation of the plot.      See Also     --------     pandas.plotting.scatter_matrix : matplotlib version of this routine
Generates an Andrews curves visualization for visualising clusters of     multivariate data.      Andrews curves have the functional form:      f(t) = x_1/sqrt(2) + x_2 sin(t) + x_3 cos(t) +            x_4 sin(2t) + x_5 cos(2t) + ...      Where x coefficients correspond to the values of each dimension and t is     linearly spaced between -pi and +pi. Each row of frame then corresponds to     a single curve.      Parameters:     -----------     data : DataFrame         Data to be plotted, preferably normalized to (0.0, 1.0)     class_column : string         Name of the column containing class names     samples : integer         Number of points to plot in each curve     alpha: float, optional         The transparency of the lines     width : int, optional         the width of the plot in pixels     height : int, optional         the height of the plot in pixels     **kwds: keywords         Additional options      Returns:     --------     chart: alt.Chart object
Parallel coordinates plotting.      Parameters     ----------     frame: DataFrame     class_column: str         Column name containing class names     cols: list, optional         A list of column names to use     alpha: float, optional         The transparency of the lines     interactive : bool, optional         if True (default) then produce an interactive plot     width : int, optional         the width of the plot in pixels     height : int, optional         the height of the plot in pixels     var_name : string, optional         the legend title     value_name : string, optional         the y-axis label      Returns     -------     chart: alt.Chart object         The altair representation of the plot.      See Also     --------     pandas.plotting.parallel_coordinates : matplotlib version of this routine
Lag plot for time series.      Parameters     ----------     data: pandas.Series         the time series to plot     lag: integer         The lag of the scatter plot, default=1     kind: string         The kind of plot to use (e.g. 'scatter', 'line')     **kwds:         Additional keywords passed to data.vgplot.scatter      Returns     -------     chart: alt.Chart object
Return a hash of the contents of a dictionary
Exec a code block & return evaluation of the last line
Import the object given by clsname.     If default_module is specified, import from this module.
Strip the vega-lite extension (either vl.json or json) from filename
Utility to return (prev, this, next) tuples from an iterator
Get TurboCARTO CartoCSS based on input parameters
Create a custom scheme.      Args:         colors (list of str): List of hex values for styling data         bins (int, optional): Number of bins to style by. If not given, the           number of colors will be used.         bin_method (str, optional): Classification method. One of the values           in :obj:`BinMethod`. Defaults to `quantiles`, which only works with           quantitative data.
Return a custom scheme based on CARTOColors.      Args:         name (str): Name of a CARTOColor.         bins (int or iterable): If an `int`, the number of bins for classifying           data. CARTOColors have 7 bins max for quantitative data, and 11 max           for qualitative data. If `bins` is a `list`, it is the upper range           for classifying data. E.g., `bins` can be of the form ``(10, 20, 30,           40, 50)``.         bin_method (str, optional): One of methods in :obj:`BinMethod`.           Defaults to ``quantiles``. If `bins` is an interable, then that is           the bin method that will be used and this will be ignored.      .. Warning::         Input types are particularly sensitive in this function, and little        feedback is given for errors. ``name`` and ``bin_method`` arguments        are case-sensitive.
Checks if credentials allow for authenticated carto access
Read a table from CARTO into a pandas DataFrames.          Args:             table_name (str): Name of table in user's CARTO account.             limit (int, optional): Read only `limit` lines from                 `table_name`. Defaults to ``None``, which reads the full table.             decode_geom (bool, optional): Decodes CARTO's geometries into a               `Shapely <https://github.com/Toblerity/Shapely>`__               object that can be used, for example, in `GeoPandas               <http://geopandas.org/>`__.             shared_user (str, optional): If a table has been shared with you,               specify the user name (schema) who shared it.             retry_times (int, optional): If the read call is rate limited,               number of retries to be made          Returns:             pandas.DataFrame: DataFrame representation of `table_name` from             CARTO.          Example:             .. code:: python                  import cartoframes                 cc = cartoframes.CartoContext(BASEURL, APIKEY)                 df = cc.read('acadia_biodiversity')
List all tables in user's CARTO account          Returns:             :obj:`list` of :py:class:`Table <cartoframes.analysis.Table>`
Write a DataFrame to a CARTO table.          Examples:             Write a pandas DataFrame to CARTO.              .. code:: python                  cc.write(df, 'brooklyn_poverty', overwrite=True)              Scrape an HTML table from Wikipedia and send to CARTO with content             guessing to create a geometry from the country column. This uses             a CARTO Import API param `content_guessing` parameter.              .. code:: python                  url = 'https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy'                 # retrieve first HTML table from that page                 df = pd.read_html(url, header=0)[0]                 # send to carto, let it guess polygons based on the 'country'                 #   column. Also set privacy to 'public'                 cc.write(df, 'life_expectancy',                          content_guessing=True,                          privacy='public')                 cc.map(layers=Layer('life_expectancy',                                     color='both_sexes_life_expectancy'))          Args:             df (pandas.DataFrame): DataFrame to write to ``table_name`` in user                 CARTO account             table_name (str): Table to write ``df`` to in CARTO.             temp_dir (str, optional): Directory for temporary storage of data                 that is sent to CARTO. Defaults are defined by `appdirs                 <https://github.com/ActiveState/appdirs/blob/master/README.rst>`__.             overwrite (bool, optional): Behavior for overwriting ``table_name``                 if it exits on CARTO. Defaults to ``False``.             lnglat (tuple, optional): lng/lat pair that can be used for                 creating a geometry on CARTO. Defaults to ``None``. In some                 cases, geometry will be created without specifying this. See                 CARTO's `Import API                 <https://carto.com/developers/import-api/reference/#tag/Standard-Tables>`__                 for more information.             encode_geom (bool, optional): Whether to write `geom_col` to CARTO                 as `the_geom`.             geom_col (str, optional): The name of the column where geometry                 information is stored. Used in conjunction with `encode_geom`.             **kwargs: Keyword arguments to control write operations. Options                 are:                  - `compression` to set compression for files sent to CARTO.                   This will cause write speedups depending on the dataset.                   Options are ``None`` (no compression, default) or ``gzip``.                 - Some arguments from CARTO's Import API. See the `params                   listed in the documentation                   <https://carto.com/developers/import-api/reference/#tag/Standard-Tables>`__                   for more information. For example, when using                   `content_guessing='true'`, a column named 'countries' with                   country names will be used to generate polygons for each                   country. Another use is setting the privacy of a dataset. To                   avoid unintended consequences, avoid `file`, `url`, and other                   similar arguments.          Returns:             :py:class:`Dataset <cartoframes.datasets.Dataset>`          .. note::             DataFrame indexes are changed to ordinary columns. CARTO creates             an index called `cartodb_id` for every table that runs from 1 to             the length of the DataFrame.
gets current privacy of a table
Updates the privacy of a dataset
Delete a table in user's CARTO account.          Args:             table_name (str): Name of table to delete          Returns:             bool: `True` if table is removed
Pull the result from an arbitrary SELECT SQL query from a CARTO account         into a pandas DataFrame.          Args:             query (str): SELECT query to run against CARTO user database. This data               will then be converted into a pandas DataFrame.             decode_geom (bool, optional): Decodes CARTO's geometries into a               `Shapely <https://github.com/Toblerity/Shapely>`__               object that can be used, for example, in `GeoPandas               <http://geopandas.org/>`__.          Returns:             pandas.DataFrame: DataFrame representation of query supplied.             Pandas data types are inferred from PostgreSQL data types.             In the case of PostgreSQL date types, dates are attempted to be             converted, but on failure a data type 'object' is used.          Examples:             This query gets the 10 highest values from a table and             returns a dataframe.              .. code:: python                  topten_df = cc.query(                     '''                       SELECT * FROM                       my_table                       ORDER BY value_column DESC                       LIMIT 10                     '''                 )              This query joins points to polygons based on intersection, and             aggregates by summing the values of the points in each polygon. The             query returns a dataframe, with a geometry column that contains             polygons.              .. code:: python                  points_aggregated_to_polygons = cc.query(                     '''                       SELECT polygons.*, sum(points.values)                       FROM polygons JOIN points                       ON ST_Intersects(points.the_geom, polygons.the_geom)                       GROUP BY polygons.the_geom, polygons.cartodb_id                     ''',                     decode_geom=True                 )
Pull the result from an arbitrary SQL SELECT query from a CARTO account         into a pandas DataFrame. This is the default behavior, when `is_select=True`          Can also be used to perform database operations (creating/dropping tables,         adding columns, updates, etc.). In this case, you have to explicitly         specify `is_select=False`          This method is a helper for the `CartoContext.fetch` and `CartoContext.execute`         methods. We strongly encourage you to use any of those methods depending on the         type of query you want to run. If you want to get the results of a `SELECT` query         into a pandas DataFrame, then use `CartoContext.fetch`. For any other query that         performs an operation into the CARTO database, use `CartoContext.execute`          Args:             query (str): Query to run against CARTO user database. This data               will then be converted into a pandas DataFrame.             table_name (str, optional): If set (and `is_select=True`), this will create a new               table in the user's CARTO account that is the result of the SELECT               query provided. Defaults to None (no table created).             decode_geom (bool, optional): Decodes CARTO's geometries into a               `Shapely <https://github.com/Toblerity/Shapely>`__               object that can be used, for example, in `GeoPandas               <http://geopandas.org/>`__. It only works for SELECT queries when `is_select=True`             is_select (bool, optional): This argument has to be set depending on the query               performed. True for SELECT queries, False for any other query.               For the case of a SELECT SQL query (`is_select=True`) the result will be stored into a               pandas DataFrame.               When an arbitrary SQL query (`is_select=False`) it will perform a database               operation (UPDATE, DROP, INSERT, etc.)               By default `is_select=None` that means that the method will return a dataframe if               the `query` starts with a `select` clause, otherwise it will just execute the query               and return `None`          Returns:             pandas.DataFrame: When `is_select=True` and the query is actually a SELECT query             this method returns a pandas DataFrame representation of query supplied otherwise             returns None.             Pandas data types are inferred from PostgreSQL data types.             In the case of PostgreSQL date types, dates are attempted to be             converted, but on failure a data type 'object' is used.          Raises:             CartoException: If there's any error when executing the query          Examples:             Query a table in CARTO and write a new table that is result of             query. This query gets the 10 highest values from a table and             returns a dataframe, as well as creating a new table called             'top_ten' in the CARTO account.              .. code:: python                  topten_df = cc.query(                     '''                       SELECT * FROM                       my_table                       ORDER BY value_column DESC                       LIMIT 10                     ''',                     table_name='top_ten'                 )              This query joins points to polygons based on intersection, and             aggregates by summing the values of the points in each polygon. The             query returns a dataframe, with a geometry column that contains             polygons and also creates a new table called             'points_aggregated_to_polygons' in the CARTO account.              .. code:: python                  points_aggregated_to_polygons = cc.query(                     '''                       SELECT polygons.*, sum(points.values)                       FROM polygons JOIN points                       ON ST_Intersects(points.the_geom, polygons.the_geom)                       GROUP BY polygons.the_geom, polygons.cartodb_id                     ''',                     table_name='points_aggregated_to_polygons',                     decode_geom=True                 )              Drops `my_table`              .. code:: python                  cc.query(                     '''                       DROP TABLE my_table                     '''                 )              Updates the column `my_column` in the table `my_table`              .. code:: python                  cc.query(                     '''                       UPDATE my_table SET my_column = 1                     '''                 )
Produce a CARTO map visualizing data layers.          Examples:             Create a map with two data :py:class:`Layer             <cartoframes.layer.Layer>`\s, and one :py:class:`BaseMap             <cartoframes.layer.BaseMap>` layer::                  import cartoframes                 from cartoframes import Layer, BaseMap, styling                 cc = cartoframes.CartoContext(BASEURL, APIKEY)                 cc.map(layers=[BaseMap(),                                Layer('acadia_biodiversity',                                      color={'column': 'simpson_index',                                             'scheme': styling.tealRose(7)}),                                Layer('peregrine_falcon_nest_sites',                                      size='num_eggs',                                      color={'column': 'bird_id',                                             'scheme': styling.vivid(10))],                        interactive=True)              Create a snapshot of a map at a specific zoom and center::                  cc.map(layers=Layer('acadia_biodiversity',                                     color='simpson_index'),                        interactive=False,                        zoom=14,                        lng=-68.3823549,                        lat=44.3036906)         Args:             layers (list, optional): List of zero or more of the following:                  - :py:class:`Layer <cartoframes.layer.Layer>`: cartoframes                   :py:class:`Layer <cartoframes.layer.Layer>` object for                   visualizing data from a CARTO table. See :py:class:`Layer                   <cartoframes.layer.Layer>` for all styling options.                 - :py:class:`BaseMap <cartoframes.layer.BaseMap>`: Basemap for                   contextualizng data layers. See :py:class:`BaseMap                   <cartoframes.layer.BaseMap>` for all styling options.                 - :py:class:`QueryLayer <cartoframes.layer.QueryLayer>`: Layer                   from an arbitrary query. See :py:class:`QueryLayer                   <cartoframes.layer.QueryLayer>` for all styling options.              interactive (bool, optional): Defaults to ``True`` to show an                 interactive slippy map. Setting to ``False`` creates a static                 map.             zoom (int, optional): Zoom level of map. Acceptable values are                 usually in the range 0 to 19. 0 has the entire earth on a                 single tile (256px square). Zoom 19 is the size of a city                 block. Must be used in conjunction with ``lng`` and ``lat``.                 Defaults to a view to have all data layers in view.             lat (float, optional): Latitude value for the center of the map.                 Must be used in conjunction with ``zoom`` and ``lng``. Defaults                 to a view to have all data layers in view.             lng (float, optional): Longitude value for the center of the map.                 Must be used in conjunction with ``zoom`` and ``lat``. Defaults                 to a view to have all data layers in view.             size (tuple, optional): List of pixel dimensions for the map.                 Format is ``(width, height)``. Defaults to ``(800, 400)``.             ax: matplotlib axis on which to draw the image. Only used when                 ``interactive`` is ``False``.          Returns:             IPython.display.HTML or matplotlib Axes: Interactive maps are             rendered as HTML in an `iframe`, while static maps are returned as             matplotlib Axes objects or IPython Image.
gets geometry type(s) of specified layer
Find all boundaries available for the world or a `region`. If         `boundary` is specified, get all available boundary polygons for the         region specified (if any). This method is espeically useful for getting         boundaries for a region and, with :py:meth:`CartoContext.data         <cartoframes.context.CartoContext.data>` and         :py:meth:`CartoContext.data_discovery         <cartoframes.context.CartoContext.data_discovery>`, getting tables of         geometries and the corresponding raw measures. For example, if you want         to analyze how median income has changed in a region (see examples         section for more).          Examples:              Find all boundaries available for Australia. The columns             `geom_name` gives us the name of the boundary and `geom_id`             is what we need for the `boundary` argument.              .. code:: python                  import cartoframes                 cc = cartoframes.CartoContext('base url', 'api key')                 au_boundaries = cc.data_boundaries(region='Australia')                 au_boundaries[['geom_name', 'geom_id']]              Get the boundaries for Australian Postal Areas and map them.              .. code:: python                  from cartoframes import Layer                 au_postal_areas = cc.data_boundaries(boundary='au.geo.POA')                 cc.write(au_postal_areas, 'au_postal_areas')                 cc.map(Layer('au_postal_areas'))              Get census tracts around Idaho Falls, Idaho, USA, and add median             income from the US census. Without limiting the metadata, we get             median income measures for each census in the Data Observatory.              .. code:: python                  cc = cartoframes.CartoContext('base url', 'api key')                 # will return DataFrame with columns `the_geom` and `geom_ref`                 tracts = cc.data_boundaries(                     boundary='us.census.tiger.census_tract',                     region=[-112.096642,43.429932,-111.974213,43.553539])                 # write geometries to a CARTO table                 cc.write(tracts, 'idaho_falls_tracts')                 # gather metadata needed to look up median income                 median_income_meta = cc.data_discovery(                     'idaho_falls_tracts',                     keywords='median income',                     boundaries='us.census.tiger.census_tract')                 # get median income data and original table as new dataframe                 idaho_falls_income = cc.data(                     'idaho_falls_tracts',                     median_income_meta,                     how='geom_refs')                 # overwrite existing table with newly-enriched dataframe                 cc.write(idaho_falls_income,                          'idaho_falls_tracts',                          overwrite=True)          Args:             boundary (str, optional): Boundary identifier for the boundaries               that are of interest. For example, US census tracts have a               boundary ID of ``us.census.tiger.census_tract``, and Brazilian               Municipios have an ID of ``br.geo.municipios``. Find IDs by               running :py:meth:`CartoContext.data_boundaries               <cartoframes.context.CartoContext.data_boundaries>`               without any arguments, or by looking in the `Data Observatory               catalog <http://cartodb.github.io/bigmetadata/>`__.             region (str, optional): Region where boundary information or,               if `boundary` is specified, boundary polygons are of interest.               `region` can be one of the following:                  - table name (str): Name of a table in user's CARTO account                 - bounding box (list of float): List of four values (two                   lng/lat pairs) in the following order: western longitude,                   southern latitude, eastern longitude, and northern latitude.                   For example, Switzerland fits in                   ``[5.9559111595,45.8179931641,10.4920501709,47.808380127]``             timespan (str, optional): Specific timespan to get geometries from.               Defaults to use the most recent. See the Data Observatory catalog               for more information.             decode_geom (bool, optional): Whether to return the geometries as               Shapely objects or keep them encoded as EWKB strings. Defaults               to False.             include_nonclipped (bool, optional): Optionally include               non-shoreline-clipped boundaries. These boundaries are the raw               boundaries provided by, for example, US Census Tiger.          Returns:             pandas.DataFrame: If `boundary` is specified, then all available             boundaries and accompanying `geom_refs` in `region` (or the world             if `region` is ``None`` or not specified) are returned. If             `boundary` is not specified, then a DataFrame of all available             boundaries in `region` (or the world if `region` is ``None``)
Discover Data Observatory measures. This method returns the full         Data Observatory metadata model for each measure or measures that         match the conditions from the inputs. The full metadata in each row         uniquely defines a measure based on the timespan, geographic         resolution, and normalization (if any). Read more about the metadata         response in `Data Observatory         <https://carto.com/docs/carto-engine/data/measures-functions/#obs_getmetaextent-geometry-metadata-json-max_timespan_rank-max_score_rank-target_geoms>`__         documentation.          Internally, this method finds all measures in `region` that match the         conditions set in `keywords`, `regex`, `time`, and `boundaries` (if         any of them are specified). Then, if `boundaries` is not specified, a         geographical resolution for that measure will be chosen subject to the         type of region specified:            1. If `region` is a table name, then a geographical resolution that              is roughly equal to `region size / number of subunits`.           2. If `region` is a country name or bounding box, then a geographical              resolution will be chosen roughly equal to `region size / 500`.          Since different measures are in some geographic resolutions and not         others, different geographical resolutions for different measures are         oftentimes returned.          .. tip::              To remove the guesswork in how geographical resolutions are             selected, specify one or more boundaries in `boundaries`. See             the boundaries section for each region in the `Data Observatory             catalog <http://cartodb.github.io/bigmetadata/>`__.          The metadata returned from this method can then be used to create raw         tables or for augmenting an existing table from these measures using         :py:meth:`CartoContext.data <cartoframes.context.CartoContext.data>`.         For the full Data Observatory catalog, visit         https://cartodb.github.io/bigmetadata/. When working with the metadata         DataFrame returned from this method, be careful to only remove rows not         columns as `CartoContext.data <cartoframes.context.CartoContext.data>`         generally needs the full metadata.          .. note::             Narrowing down a discovery query using the `keywords`, `regex`, and             `time` filters is important for getting a manageable metadata             set. Besides there being a large number of measures in the DO, a             metadata response has acceptable combinations of measures with             demonimators (normalization and density), and the same measure from             other years.              For example, setting the region to be United States counties with             no filter values set will result in many thousands of measures.          Examples:              Get all European Union measures that mention ``freight``.              .. code::                  meta = cc.data_discovery('European Union',                                          keywords='freight',                                          time='2010')                 print(meta['numer_name'].values)          Arguments:             region (str or list of float): Information about the region of               interest. `region` can be one of three types:                  - region name (str): Name of region of interest. Acceptable                   values are limited to: 'Australia', 'Brazil', 'Canada',                   'European Union', 'France', 'Mexico', 'Spain',                   'United Kingdom', 'United States'.                 - table name (str): Name of a table in user's CARTO account                   with geometries. The region will be the bounding box of                   the table.                    .. Note:: If a table name is also a valid Data Observatory                       region name, the Data Observatory name will be chosen                       over the table.                  - bounding box (list of float): List of four values (two                   lng/lat pairs) in the following order: western longitude,                   southern latitude, eastern longitude, and northern latitude.                   For example, Switzerland fits in                   ``[5.9559111595,45.8179931641,10.4920501709,47.808380127]``                  .. Note:: Geometry levels are generally chosen by subdividing                     the region into the next smallest administrative unit. To                     override this behavior, specify the `boundaries` flag. For                     example, set `boundaries` to                     ``'us.census.tiger.census_tract'`` to choose US census                     tracts.              keywords (str or list of str, optional): Keyword or list of               keywords in measure description or name. Response will be matched               on all keywords listed (boolean `or`).             regex (str, optional): A regular expression to search the measure               descriptions and names. Note that this relies on PostgreSQL's               case insensitive operator ``~*``. See `PostgreSQL docs               <https://www.postgresql.org/docs/9.5/static/functions-matching.html>`__               for more information.             boundaries (str or list of str, optional): Boundary or list of               boundaries that specify the measure resolution. See the               boundaries section for each region in the `Data Observatory               catalog <http://cartodb.github.io/bigmetadata/>`__.             include_quantiles (bool, optional): Include quantiles calculations               which are a calculation of how a measure compares to all measures               in the full dataset. Defaults to ``False``. If ``True``,               quantiles columns will be returned for each column which has it               pre-calculated.          Returns:             pandas.DataFrame: A dataframe of the complete metadata model for             specific measures based on the search parameters.          Raises:             ValueError: If `region` is a :obj:`list` and does not consist of               four elements, or if `region` is not an acceptable region             CartoException: If `region` is not a table in user account
Get an augmented CARTO dataset with `Data Observatory         <https://carto.com/data-observatory>`__ measures. Use         `CartoContext.data_discovery         <#context.CartoContext.data_discovery>`__ to search for available         measures, or see the full `Data Observatory catalog         <https://cartodb.github.io/bigmetadata/index.html>`__. Optionally         persist the data as a new table.          Example:             Get a DataFrame with Data Observatory measures based on the             geometries in a CARTO table.              .. code::                  cc = cartoframes.CartoContext(BASEURL, APIKEY)                 median_income = cc.data_discovery('transaction_events',                                                   regex='.*median income.*',                                                   time='2011 - 2015')                 df = cc.data('transaction_events',                              median_income)              Pass in cherry-picked measures from the Data Observatory catalog.             The rest of the metadata will be filled in, but it's important to             specify the geographic level as this will not show up in the column             name.              .. code::                  median_income = [{'numer_id': 'us.census.acs.B19013001',                                   'geom_id': 'us.census.tiger.block_group',                                   'numer_timespan': '2011 - 2015'}]                 df = cc.data('transaction_events', median_income)          Args:             table_name (str): Name of table on CARTO account that Data                 Observatory measures are to be added to.             metadata (pandas.DataFrame): List of all measures to add to                 `table_name`. See :py:meth:`CartoContext.data_discovery                 <cartoframes.context.CartoContext.data_discovery>` outputs                 for a full list of metadata columns.             persist_as (str, optional): Output the results of augmenting                 `table_name` to `persist_as` as a persistent table on CARTO.                 Defaults to ``None``, which will not create a table.             how (str, optional): **Not fully implemented**. Column name for                 identifying the geometry from which to fetch the data. Defaults                 to `the_geom`, which results in measures that are spatially                 interpolated (e.g., a neighborhood boundary's population will                 be calculated from underlying census tracts). Specifying a                 column that has the geometry identifier (for example, GEOID for                 US Census boundaries), results in measures directly from the                 Census for that GEOID but normalized how it is specified in the                 metadata.          Returns:             pandas.DataFrame: A DataFrame representation of `table_name` which             has new columns for each measure in `metadata`.          Raises:             NameError: If the columns in `table_name` are in the               ``suggested_name`` column of `metadata`.             ValueError: If metadata object is invalid or empty, or if the               number of requested measures exceeds 50.             CartoException: If user account consumes all of Data Observatory               quota
Checks if query from Layer or QueryLayer is valid
Return the bounds of all data layers involved in a cartoframes map.          Args:             layers (list): List of cartoframes layers. See `cartoframes.layers`                 for all types.          Returns:             dict: Dictionary of northern, southern, eastern, and western bounds                 of the superset of data layers. Keys are `north`, `south`,                 `east`, and `west`. Units are in WGS84.
CARTO VL-powered interactive map      Args:         layers (list of Layer-types): List of layers. One or more of           :py:class:`Layer <cartoframes.contrib.vector.Layer>`,           :py:class:`QueryLayer <cartoframes.contrib.vector.QueryLayer>`, or           :py:class:`LocalLayer <cartoframes.contrib.vector.LocalLayer>`.         context (:py:class:`CartoContext <cartoframes.context.CartoContext>`):           A :py:class:`CartoContext <cartoframes.context.CartoContext>`           instance         size (tuple of int or str): a (width, height) pair for the size of the map.           Default is None, which makes the map 100% wide and 640px tall. If specified as int,           will be used as pixels, but you can also use string values for the CSS attributes.           So, you could specify it as size=('75%', 250).         basemap (str):           - if a `str`, name of a CARTO vector basemap. One of `positron`,             `voyager`, or `darkmatter` from the :obj:`BaseMaps` class           - if a `dict`, Mapbox or other style as the value of the `style` key.             If a Mapbox style, the access token is the value of the `token`             key.         bounds (dict or list): a dict with `east`,`north`,`west`,`south`           properties, or a list of floats in the following order: [west,           south, east, north]. If not provided the bounds will be automatically           calculated to fit all features.         viewport (dict): Configure where and how map will be centered. If not specified, or             specified without lat / lng, automatic bounds or the bounds argument will be used             to center the map. You can specify only zoom, bearing or pitch if you desire             automatic bounds but want to tweak the viewport.             - lng (float): Longitude to center the map on. Must specify lat as well.             - lat (float): Latitude to center the map on. Must specify lng as well.             - zoom (float): Zoom level.             - bearing (float): A bearing, or heading, is the direction you're facing,                 measured clockwise as an angle from true north on a compass.                 (north is 0, east is 90, south is 180, and west is 270).             - pitch (float): The angle towards the horizon measured in degrees, with a                 range between 0 and 60 degrees. Zero degrees results in a two-dimensional                 map, as if your line of sight forms a perpendicular angle with                 the earth's surface.      Example:          .. code::              from cartoframes.contrib import vector             from cartoframes import CartoContext             cc = CartoContext(                 base_url='https://your_user_name.carto.com',                 api_key='your api key'             )             vector.vmap([vector.Layer('table in your account'), ], cc)          CARTO basemap style.          .. code::              from cartoframes.contrib import vector             from cartoframes import CartoContext             cc = CartoContext(                 base_url='https://your_user_name.carto.com',                 api_key='your api key'             )             vector.vmap(                 [vector.Layer('table in your account'), ],                 context=cc,                 basemap=vector.BaseMaps.darkmatter             )          Custom basemap style. Here we use the Mapbox streets style, which         requires an access token.          .. code::              from cartoframes.contrib import vector             from cartoframes import CartoContext             cc = CartoContext(                 base_url='https://<username>.carto.com',                 api_key='your api key'             )             vector.vmap(                 [vector.Layer('table in your account'), ],                 context=cc,                 basemap={                     'style': 'mapbox://styles/mapbox/streets-v9',                     'token: '<your mapbox token>'                 }             )          Custom bounds          .. code::              from cartoframes.contrib import vector             from cartoframes import CartoContext             cc = CartoContext(                 base_url='https://<username>.carto.com',                 api_key='your api key'             )             vector.vmap(                 [vector.Layer('table in your account'), ],                 context=cc,                 bounds={'west': -10, 'east': 10, 'north': -10, 'south': 10}             )          Adjusting the map's viewport.          .. code::              from cartoframes.contrib import vector             from cartoframes import CartoContext             cc = CartoContext(                 base_url='https://<username>.carto.com',                 api_key='your api key'             )             vector.vmap(                 [vector.Layer('table in your account'), ],                 context=cc,                 viewport={'lng': 10, 'lat': 15, 'zoom': 10, 'bearing': 90, 'pitch': 45}             )
Aggregates bounding boxes of all local layers          return: dict of bounding box of all bounds in layers
Takes two bounding boxes dicts and gives a new bbox that encompasses     them both
Appends `prop` with `style` to layer styling
Adds interactivity syntax to the styling
normalize country name to match data obs
Creates a map named based on supplied parameters
Creates a map template based on custom parameters supplied
Setup the color scheme
Parse time inputs
Parse size inputs
Validate the options in the styles
Setups layers once geometry types and data types are known, and when         a map is requested to be rendered from zero or more data layers
Choose color scheme
generates CartoCSS for time-based maps (torque)
Generate cartocss for class properties
Given an arbitrary column name, translate to a SQL-normalized column         name a la CARTO's Import API will translate to          Examples             * 'Field: 2' -> 'field_2'             * '2 Items' -> '_2_items'             * 'Unnamed: 0' -> 'unnamed_0',             * '201moore' -> '_201moore',             * '201moore' -> '_201moore_1',             * 'Acadia 1.2.3' -> 'acadia_1_2_3',             * 'old_soaker' -> 'old_soaker',             * '_testingTesting' -> '_testingtesting',             * 1 -> '_1',             * 1.0 -> '_1_0',             * 'public' -> 'public',             * 'SELECT' -> '_select',             * 'à' -> 'a',             * 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabesplittedrightnow' -> \              'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabespli',             * 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabesplittedrightnow' -> \              'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabe_1',             * 'all' -> '_all'          Args:             column_names (list): List of column names that will be SQL normalized         Returns:             list: List of SQL-normalized column names
Function to get CartoCSS from Python dicts
htmlify string
Temporarily ignores warnings like those emitted by the carto python sdk
Get list of cartoframes.columns.Column
Get column names and types from a query
decorator for encoding and decoding geoms
Decode encoded wkb into a shapely geometry
Checks to see if table exists
Create the read (COPY TO) query
Get column names and types from a table
Get column names and types from a table
Saves current user credentials to user directory.          Args:             config_loc (str, optional): Location where credentials are to be                 stored. If no argument is provided, it will be send to the                 default location.          Example:              .. code::                  from cartoframes import Credentials                 creds = Credentials(username='eschbacher', key='abcdefg')                 creds.save()  # save to default location
Retrives credentials from a file. Defaults to the user config         directory
Deletes the credentials file specified in `config_file`. If no         file is specified, it deletes the default user credential file.          Args:             config_file (str): Path to configuration file. Defaults to delete                 the user default location if `None`.          .. Tip::              To see if there is a default user credential file stored, do the             following::                  >>> creds = Credentials()                 >>> print(creds)                 Credentials(username=eschbacher, key=abcdefg,                         base_url=https://eschbacher.carto.com/)
Update the credentials of a Credentials instance instead with new         values.          Args:             key (str): API key of user account. Defaults to previous value if                 not specified.             username (str): User name of account. This parameter is optional if                 `base_url` is not specified, but defaults to the previous                 value if not set.             base_url (str): Base URL of user account. This parameter is                 optional if `username` is specified and on CARTO's                 cloud-based account. Generally of the form                 ``https://your_user_name.carto.com/`` for cloud-based accounts.                 If on-prem or otherwise, contact your admin.          Example:              .. code::                  from cartoframes import Credentials                 # load credentials saved in previous session                 creds = Credentials()                 # set new API key                 creds.set(key='new_api_key')                 # save new creds to default user config directory                 creds.save()          Note:             If the `username` is specified but the `base_url` is not, the             `base_url` will be updated to ``https://<username>.carto.com/``.
Return or set `base_url`.          Args:             base_url (str, optional): If set, updates the `base_url`. Otherwise                 returns current `base_url`.          Note:             This does not update the `username` attribute. Separately update             the username with ``Credentials.username`` or update `base_url` and             `username` at the same time with ``Credentials.set``.          Example:              .. code::                  >>> from cartoframes import Credentials                 # load credentials saved in previous session                 >>> creds = Credentials()                 # returns current base_url                 >>> creds.base_url()                 'https://eschbacher.carto.com/'                 # updates base_url with new value                 >>> creds.base_url('new_base_url')
Quick setup for a chatroom.      :param str room: Roomname, if not given, a random sequence is generated and printed.     :param MediaStream stream: The media stream to share, if not given a CameraStream will be created.     :rtype: WebRTCRoom
Adds an object as a child in the scene graph.
Conveniience function: Adds objects as children in the scene graph.
Returns a copy of the projection matrix
Updates Camera.aspect to match the viewport's aspect ratio.
np.array: The Camera's lens-shift matrix.
np.array: The Camera's Projection Matrix.  Will be an Orthographic matrix if ortho_mode is set to True.
Save Camera to a pickle file, given a filename.
Loads and Returns a Camera from a pickle file, given a filename.
Converges the two cameras to look at the specific point
Bind the FBO.  Anything drawn afterward will be stored in the FBO's texture.
Unbind the FBO.
Returns int pointing to an OpenGL texture
Makes GLfloat or GLuint vector containing float or uint args.         By default, newtype is 'float', but can be set to 'int' to make         uint list.
Return Nx3 normal array from Nx3 vertex array.
Experimental anaglyph drawing function for VR system with red/blue glasses, used in Sirota lab.     Draws a virtual scene in red and blue, from subject's (heda trackers) perspective in active scene.      Note: assumes shader uses playerPos like ratcave's default shader      Args:         cube_fbo: texture frameBuffer object.         vr_scene: virtual scene object         active_scene: active scene object         eye_poses: the eye positions      Returns:
Draw each visible mesh in the scene from the perspective of the scene's camera and lit by its light.
Draw each visible mesh in the scene from the perspective of the scene's camera and lit by its light, and         applies it to each face of cubetexture, which should be currently bound to an FBO.
Load data into a vbo
Returns a copy of the Mesh.
Loads and Returns a Mesh from a pickle file, given a filename.
Resets the uniforms to the Mesh object to the ""global"" coordinate system
Return a Mesh with (vertices, normals, texcoords) as arrays, in that order.            Useful for when you want a standardized array location format across different amounts of info in each mesh.
Put array location in VAO for shader in same order as arrays given to Mesh.
Draw the Mesh if it's visible, from the perspective of the camera and lit by the light. The function sends the uniforms
Returns a 3x3 cross-product matrix from a 3-element vector.
Returns a rotation matrix to rotate from 3d vector "from_vec" to 3d vector "to_vec".     Equation from https://math.stackexchange.com/questions/180418/calculate-rotation-matrix-to-align-vector-a-to-vector-b-in-3d
Generates combinations of named coordinate values, mapping them to the internal array.         For Example: x, xy, xyz, y, yy, zyx, etc
The maximum number of textures available for this graphic card's fragment shader.
Applies some hard-coded texture filtering settings.
Attach the texture to a bound FBO object, for rendering to texture.
Uses Pyglet's image.load function to generate a Texture from an image file. If 'mipmap', then texture will         have mipmap layers calculated.
Generate an empty texture in OpenGL
Builds Mesh from geom name in the wavefront file.  Takes all keyword arguments that Mesh takes.
Sends all the key-value pairs to the graphics card.         These uniform variables will be available in the currently-bound shader.
Activate this Shader, making it the currently-bound program.          Any Mesh.draw() calls after bind() will have their data processed by this Shader.  To unbind, call Shader.unbind().          Example::             shader.bind()            mesh.draw()            shader.unbind()          .. note:: Shader.bind() and Shader.unbind() can be also be called implicitly by using the 'with' statement.          Example of with statement with Shader::             with shader:                 mesh.draw()
Reads the shader programs, given the vert and frag filenames          Arguments:             - vert (str): The filename of the vertex shader program (ex: 'vertshader.vert')             - frag (str): The filename of the fragment shader program (ex: 'fragshader.frag')          Returns:             - shader (Shader): The Shader using these files.
link the program, making it the active shader.          .. note:: Shader.bind() is preferred here, because link() Requires the Shader to be compiled already.
Rotate so orientation is toward (x, y, z) coordinates.
Adds an object as a child in the scene graph. With modify=True, model_matrix_transform gets change from identity and prevents the changes of the coordinates of the child
Setup an example Treeview
Put widgets in the grid
Take a screenshot, crop and save
Take a screenshot for all themes available
Load the themes into the Tkinter interpreter
Append a theme dir to the Tk interpreter auto_path
Set new theme to use. Uses a direct tk call to allow usage         of the themes supplied with this package.          :param theme_name: name of theme to activate
Load an advanced theme that is dynamically created          Applies the given modifiers to the images of the theme given and         then creates a theme from these new images with the name         'advanced' and then applies this theme. Is not available without         support for PNG-based themes, then raises RuntimeError.
Setup all the files required to enable an advanced theme.          Copies all the files over and creates the required directories         if they do not exist.          :param theme_name: theme to copy the files over from         :param output_dir: output directory to place the files in
Apply modifiers to the images of a theme          Modifies the images using the PIL.ImageEnhance module. Using         this function, theme images are modified to given them a         unique look and feel. Works best with PNG-based images.
Redirect the set_theme call to also set Tk background color
Setup Toplevel.__init__ hook for background color
configure redirect to support additional options
cget redirect to support additional options
:param command: command to run on os.system     :return: exit code
Build a binary distribution wheel and install it
Run the most common CI tasks
Setup Travis-CI macOS for wheel building
Set a new theme to use or return current theme name          :param theme_name: name of theme to use         :returns: active theme name
Like os.chdir(), but always restores the old working directory      For example, code like this...          old_curdir = os.getcwd()         os.chdir('stuff')         do_some_stuff()         os.chdir(old_curdir)      ...leaves the current working directory unchanged if do_some_stuff()     raises an error, so it should be rewritten like this:          old_curdir = os.getcwd()         os.chdir('stuff')         try:             do_some_stuff()         finally:             os.chdir(old_curdir)      Or equivalently, like this:          with utils.temporary_chdir('stuff'):             do_some_stuff()
Return an absolute path to an existing temporary directory
Return an absolute path the to /themes directory
Create directory but first delete it if it exists
Shifts the hue of an image in HSV format.     :param image: PIL Image to perform operation on     :param hue: value between 0 and 2.0
Turn all black pixels in an image into transparent ones
Setup platform specific network settings
Connects to WIFI
Disables any Accesspoint
Depending on the interval:          returns True if its time for an update,         returns False if its not yet time for an update
Return the property id from topic as integer
add a node class of HomieNode to this device
subscribe to all registered device and node topics
publish device and node properties
publish node data if node has updates
publish device and node properties, run forever
Factory method for shared memory arrays supporting all numpy dtypes.
Get configuration value from PYMP/OMP env variables.
Print synchronized.
Get the correctly distributed parallel chunks.          This corresponds to using the OpenMP 'static' schedule.
Get an iterator for this threads chunk of work.          This corresponds to using the OpenMP 'dynamic' schedule.
Iterate over an iterable.          The iterator is executed in the host thread. The threads dynamically         grab the elements. The iterator elements must hence be picklable to         be transferred through the queue.          If there is only one thread, no special operations are performed.         Otherwise, effectively n-1 threads are used to process the iterable         elements, and the host thread is used to provide them.          You can specify a timeout for the clients to adhere.
Iterator implementation.
Iterator implementation.
creates (if needed)  and updates the value of the key in the config with a      value entered by the user      Parameters     ----------     config: ConfigParser object         existing configuration     key: string         key to update     instruction: string         text to show in the prompt     is_sensitive: bool         if true, require confirmation and do not show typed characters      Notes     -----     sets key in config passed in
Configure information about Databricks account and default behavior.      Configuration is stored in a `.apparatecfg` file. A config file must exist      before this package can be used, and can be supplied either directly as a      text file or generated using this configuration tool.
upload an egg to the Databricks filesystem.      Parameters     ----------     filename: string         local location of file to upload     match: FilenameMatch object         match object with library_type, library_name, and version     folder: string         Databricks folder to upload to         (e.g. '/Users/htorrence@shoprunner.com/')     token: string         Databricks API key     host: string         Databricks host (e.g. https://my-organization.cloud.databricks.com)      Side Effects     ------------     uploads egg to Databricks
get a list of jobs using the major version of the given library      Parameters     ----------     logger: logging object         configured in cli_commands.py     match: FilenameMatch object         match object with suffix     library_mapping: dict         first element of get_library_mapping output     token: string         Databricks API key     host: string         Databricks host (e.g. https://my-organization.cloud.databricks.com)      Returns     -------     list of dictionaries containing the job id, job name, and library path      for each job
returns a pair of library mappings, the first mapping library uri to a      library name for all libraries in the production folder, and the second      mapping library name to info for libraries in the production folder with      parsable versions      Parameters     ----------     logger: logging object         configured in cli_commands.py     prod_folder: string         name of folder in Databricks UI containing production libraries     token: string         Databricks API key     host: string         Databricks account url         (e.g. https://fake-organization.cloud.databricks.com)      Returns     -------     dictionary mapping a library uri to a library name     dictionary mapping library UI path to base name, major version,         minor version, and id number
update libraries on jobs using same major version      Parameters     ----------     logger: logging object         configured in cli_commands.py     job_list: list of strings         output of get_job_list     match: FilenameMatch object         match object with suffix     new_library_path: string         path to library in dbfs (including uri)     token: string         Databricks API key with admin permissions     host: string         Databricks account url         (e.g. https://fake-organization.cloud.databricks.com)      Side Effects     ------------     jobs now require updated version of library
delete any other versions of the same library where:         it has the same major version         it has a smaller minor version         it lives in prod_folder      Parameters     ----------     logger: logging object         configured in cli_commands.py     match: FilenameMatch object         match object with library_name_, major_version, minor_version     id_nums: dict         second output of get_library_mapping     token: string         Databricks API key with admin permissions     prod_folder: string         name of folder in Databricks UI containing production libraries     host: string         Databricks account url         (e.g. https://fake-organization.cloud.databricks.com)      Side Effects     ------------     delete any other versions of the same library with the same major version         and smaller minor versions
upload library, update jobs using the same major version,     and delete libraries with the same major and lower minor versions     (depending on update_jobs and cleanup flags)      Parameters     ----------     logger: logging object         configured in cli_commands.py     path: string         path with name of egg as output from setuptools         (e.g. dist/new_library-1.0.0-py3.6.egg)     token: string         Databricks API key     folder: string         Databricks folder to upload to         (e.g. '/Users/my_email@fake_organization.com/')     update_jobs: bool         if true, jobs using this library will be updated to point to the             new version         if false, will not touch jobs or other library versions at all     cleanup: bool         if true, outdated libraries will be deleted         if false, nothing will be deleted      Side Effects     ------------     new library in Databricks     if update_jobs is true, then updated jobs     if update_jobs and cleanup are true, removed outdated libraries
True if self can safely replace other          based on version numbers only - snapshot and branch tags are ignored
Resolve input entered as option values with config values      If option values are provided (passed in as `variable`), then they are      returned unchanged. If `variable` is None, then we first look for a config      value to use.     If no config value is found, then raise an error.      Parameters     ----------     variable: string or numeric         value passed in as input by the user     variable_name: string         name of the variable, for clarity in the error message     config_key: string         key in the config whose value could be used to fill in the variable     config: ConfigParser         contains keys/values in .apparatecfg
The egg that the provided path points to will be uploaded to Databricks.
The egg that the provided path points to will be uploaded to Databricks.      All jobs which use the same major version of the library will be updated      to use the new version, and all version of this library in the production      folder with the same major version and a lower minor version will      be deleted.      Unlike `upload`, `upload_and_update` does not ask for a folder because it      relies on the production folder specified in the config. This is to      protect against accidentally updating jobs to versions of a library still      in testing/development.      All egg names already in Databricks must be properly formatted      with versions of the form <name>-0.0.0.
Parse a SAS token into its components.      :param sas_token: The SAS token.     :type sas_token: str     :rtype: dict[str, str]
The offset of the event data object.          :rtype: ~azure.eventhub.common.Offset
The enqueued timestamp of the event data object.          :rtype: datetime.datetime
The partition key of the event data object.          :rtype: bytes
Set the partition key of the event data object.          :param value: The partition key to set.         :type value: str or bytes
Application defined properties on the message.          :param value: The application properties for the EventData.         :type value: dict
The body of the event data as a string if the data is of a         compatible type.          :param encoding: The encoding to use for decoding message data.          Default is 'UTF-8'         :rtype: str or unicode
The body of the event loaded as a JSON object is the data is compatible.          :param encoding: The encoding to use for decoding message data.          Default is 'UTF-8'         :rtype: dict
Creates a selector expression of the offset.          :rtype: bytes
Returns an auth token dictionary for making calls to eventhub         REST API.          :rtype: str
Returns an auth token for making calls to eventhub REST API.          :rtype: str
Open the Sender using the supplied conneciton.         If the handler has previously been redirected, the redirect         context will be used to create a new handler before opening it.          :param connection: The underlying client shared connection.         :type: connection: ~uamqp.connection.Connection
Whether the handler has completed all start up processes such as         establishing the connection, session, link and authentication, and         is not ready to process messages.         **This function is now deprecated and will be removed in v2.0+.**          :rtype: bool
Close down the handler. If the handler has already closed,         this will be a no op. An optional exception can be passed in to         indicate that the handler was shutdown due to error.          :param exception: An optional exception if the handler is closing          due to an error.         :type exception: Exception
Sends an event data and blocks until acknowledgement is         received or operation times out.          :param event_data: The event to be sent.         :type event_data: ~azure.eventhub.common.EventData         :raises: ~azure.eventhub.common.EventHubError if the message fails to          send.         :return: The outcome of the message send.         :rtype: ~uamqp.constants.MessageSendResult
Transfers an event data and notifies the callback when the operation is done.          :param event_data: The event to be sent.         :type event_data: ~azure.eventhub.common.EventData         :param callback: Callback to be run once the message has been send.          This must be a function that accepts two arguments.         :type callback: callable[~uamqp.constants.MessageSendResult, ~azure.eventhub.common.EventHubError]
Wait until all transferred events have been sent.
Called when the outcome is received for a delivery.          :param outcome: The outcome of the message delivery - success or failure.         :type outcome: ~uamqp.constants.MessageSendResult
The EventProcessorHost can't pass itself to the AzureStorageCheckpointLeaseManager         constructor because it is still being constructed. Do other initialization here         also because it might throw and hence we don't want it in the constructor.
Get the checkpoint data associated with the given partition.         Could return null if no checkpoint has been created for that partition.          :param partition_id: The partition ID.         :type partition_id: str         :return: Given partition checkpoint info, or `None` if none has been previously stored.         :rtype: ~azure.eventprocessorhost.checkpoint.Checkpoint
Create the given partition checkpoint if it doesn't exist.Do nothing if it does exist.         The offset/sequenceNumber for a freshly-created checkpoint should be set to StartOfStream/0.          :param partition_id: The partition ID.         :type partition_id: str         :return: The checkpoint for the given partition, whether newly created or already existing.         :rtype: ~azure.eventprocessorhost.checkpoint.Checkpoint
Update the checkpoint in the store with the offset/sequenceNumber in the provided checkpoint         checkpoint:offset/sequeceNumber to update the store with.          :param lease: The stored lease to be updated.         :type lease: ~azure.eventprocessorhost.lease.Lease         :param checkpoint: The checkpoint to update the lease with.         :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint
Create the lease store if it does not exist, do nothing if it does exist.          :return: `True` if the lease store already exists or was created successfully, `False` if not.         :rtype: bool
Return the lease info for the specified partition.         Can return null if no lease has been created in the store for the specified partition.          :param partition_id: The partition ID.         :type partition_id: str         :return: lease info for the partition, or `None`.         :rtype: ~azure.eventprocessorhost.lease.Lease
Return the lease info for all partitions.         A typical implementation could just call get_lease_async() on all partitions.          :return: A list of lease info.         :rtype: list[~azure.eventprocessorhost.lease.Lease]
Create in the store the lease info for the given partition, if it does not exist.         Do nothing if it does exist in the store already.          :param partition_id: The ID of a given parition.         :type partition_id: str         :return: the existing or newly-created lease info for the partition.         :rtype: ~azure.eventprocessorhost.lease.Lease
Delete the lease info for the given partition from the store.         If there is no stored lease for the given partition, that is treated as success.          :param lease: The stored lease to be deleted.         :type lease: ~azure.eventprocessorhost.lease.Lease
Acquire the lease on the desired partition for this EventProcessorHost.         Note that it is legal to acquire a lease that is already owned by another host.         Lease-stealing is how partitions are redistributed when additional hosts are started.          :param lease: The stored lease to be acquired.         :type lease: ~azure.eventprocessorhost.lease.Lease         :return: `True` if the lease was acquired successfully, `False` if not.         :rtype: bool
Renew a lease currently held by this host.         If the lease has been stolen, or expired, or released, it is not possible to renew it.         You will have to call getLease() and then acquireLease() again.          :param lease: The stored lease to be renewed.         :type lease: ~azure.eventprocessorhost.lease.Lease         :return: `True` if the lease was renewed successfully, `False` if not.         :rtype: bool
Give up a lease currently held by this host. If the lease has been stolen, or expired,         releasing it is unnecessary, and will fail if attempted.          :param lease: The stored lease to be released.         :type lease: ~azure.eventprocessorhost.lease.Lease         :return: `True` if the lease was released successfully, `False` if not.         :rtype: bool
Update the store with the information in the provided lease. It is necessary to currently         hold a lease in order to update it. If the lease has been stolen, or expired, or released,         it cannot be updated. Updating should renew the lease before performing the update to         avoid lease expiration during the process.          :param lease: The stored lease to be updated.         :type lease: ~azure.eventprocessorhost.lease.Lease         :return: `True` if the updated was performed successfully, `False` if not.         :rtype: bool
Open the Receiver using the supplied conneciton.         If the handler has previously been redirected, the redirect         context will be used to create a new handler before opening it.          :param connection: The underlying client shared connection.         :type: connection: ~uamqp.async_ops.connection_async.ConnectionAsync
Whether the handler has completed all start up processes such as         establishing the connection, session, link and authentication, and         is not ready to process messages.         **This function is now deprecated and will be removed in v2.0+.**          :rtype: bool
Create a shared access signiture token as a string literal.     :returns: SAS token as string literal.     :rtype: str
Create an EventHubClient from an existing auth token or token generator.          :param address: The Event Hub address URL         :type address: str         :param sas_token: A SAS token or function that returns a SAS token. If a function is supplied,          it will be used to retrieve subsequent tokens in the case of token expiry. The function should          take no arguments.         :type sas_token: str or callable         :param eventhub: The name of the EventHub, if not already included in the address URL.         :type eventhub: str         :param debug: Whether to output network trace logs to the logger. Default          is `False`.         :type debug: bool         :param http_proxy: HTTP proxy settings. This must be a dictionary with the following          keys: 'proxy_hostname' (str value) and 'proxy_port' (int value).          Additionally the following keys may also be present: 'username', 'password'.         :type http_proxy: dict[str, Any]         :param auth_timeout: The time in seconds to wait for a token to be authorized by the service.          The default value is 60 seconds. If set to 0, no timeout will be enforced from the client.         :type auth_timeout: int
Create an EventHubClient from a connection string.          :param conn_str: The connection string.         :type conn_str: str         :param eventhub: The name of the EventHub, if the EntityName is          not included in the connection string.         :type eventhub: str         :param debug: Whether to output network trace logs to the logger. Default          is `False`.         :type debug: bool         :param http_proxy: HTTP proxy settings. This must be a dictionary with the following          keys: 'proxy_hostname' (str value) and 'proxy_port' (int value).          Additionally the following keys may also be present: 'username', 'password'.         :type http_proxy: dict[str, Any]         :param auth_timeout: The time in seconds to wait for a token to be authorized by the service.          The default value is 60 seconds. If set to 0, no timeout will be enforced from the client.         :type auth_timeout: int
Create an EventHubClient from an IoTHub connection string.          :param conn_str: The connection string.         :type conn_str: str         :param debug: Whether to output network trace logs to the logger. Default          is `False`.         :type debug: bool         :param http_proxy: HTTP proxy settings. This must be a dictionary with the following          keys: 'proxy_hostname' (str value) and 'proxy_port' (int value).          Additionally the following keys may also be present: 'username', 'password'.         :type http_proxy: dict[str, Any]         :param auth_timeout: The time in seconds to wait for a token to be authorized by the service.          The default value is 60 seconds. If set to 0, no timeout will be enforced from the client.         :type auth_timeout: int
Format the properties with which to instantiate the connection.         This acts like a user agent over HTTP.          :rtype: dict
Run the EventHubClient in blocking mode.         Opens the connection and starts running all Sender/Receiver clients.         Returns a list of the start up results. For a succcesful client start the         result will be `None`, otherwise the exception raised.         If all clients failed to start, then run will fail, shut down the connection         and raise an exception.         If at least one client starts up successfully the run command will succeed.          :rtype: list[~azure.eventhub.common.EventHubError]
Stop the EventHubClient and all its Sender/Receiver clients.
Get details on the specified EventHub.         Keys in the details dictionary include:             -'name'             -'type'             -'created_at'             -'partition_count'             -'partition_ids'          :rtype: dict
Add a receiver to the client for a particular consumer group and partition.          :param consumer_group: The name of the consumer group.         :type consumer_group: str         :param partition: The ID of the partition.         :type partition: str         :param offset: The offset from which to start receiving.         :type offset: ~azure.eventhub.common.Offset         :param prefetch: The message prefetch count of the receiver. Default is 300.         :type prefetch: int         :operation: An optional operation to be appended to the hostname in the source URL.          The value must start with `/` character.         :type operation: str         :rtype: ~azure.eventhub.receiver.Receiver
Add a sender to the client to EventData object to an EventHub.          :param partition: Optionally specify a particular partition to send to.          If omitted, the events will be distributed to available partitions via          round-robin.         :type parition: str         :operation: An optional operation to be appended to the hostname in the target URL.          The value must start with `/` character.         :type operation: str         :param send_timeout: The timeout in seconds for an individual event to be sent from the time that it is          queued. Default value is 60 seconds. If set to 0, there will be no timeout.         :type send_timeout: int         :param keep_alive: The time interval in seconds between pinging the connection to keep it alive during          periods of inactivity. The default value is 30 seconds. If set to `None`, the connection will not          be pinged.         :type keep_alive: int         :param auto_reconnect: Whether to automatically reconnect the sender if a retryable error occurs.          Default value is `True`.         :rtype: ~azure.eventhub.sender.Sender
Create an ~uamqp.authentication.cbs_auth_async.SASTokenAuthAsync instance to authenticate         the session.          :param username: The name of the shared access policy.         :type username: str         :param password: The shared access key.         :type password: str
Run the EventHubClient asynchronously.         Opens the connection and starts running all AsyncSender/AsyncReceiver clients.         Returns a list of the start up results. For a succcesful client start the         result will be `None`, otherwise the exception raised.         If all clients failed to start, then run will fail, shut down the connection         and raise an exception.         If at least one client starts up successfully the run command will succeed.          :rtype: list[~azure.eventhub.common.EventHubError]
Stop the EventHubClient and all its Sender/Receiver clients.
Get details on the specified EventHub async.          :rtype: dict
Add an async receiver to the client for a particular consumer group and partition.          :param consumer_group: The name of the consumer group.         :type consumer_group: str         :param partition: The ID of the partition.         :type partition: str         :param offset: The offset from which to start receiving.         :type offset: ~azure.eventhub.common.Offset         :param prefetch: The message prefetch count of the receiver. Default is 300.         :type prefetch: int         :operation: An optional operation to be appended to the hostname in the source URL.          The value must start with `/` character.         :type operation: str         :rtype: ~azure.eventhub.async_ops.receiver_async.ReceiverAsync
Add an async sender to the client to send ~azure.eventhub.common.EventData object         to an EventHub.          :param partition: Optionally specify a particular partition to send to.          If omitted, the events will be distributed to available partitions via          round-robin.         :type partition: str         :operation: An optional operation to be appended to the hostname in the target URL.          The value must start with `/` character.         :type operation: str         :param send_timeout: The timeout in seconds for an individual event to be sent from the time that it is          queued. Default value is 60 seconds. If set to 0, there will be no timeout.         :type send_timeout: int         :param keep_alive: The time interval in seconds between pinging the connection to keep it alive during          periods of inactivity. The default value is 30 seconds. If set to `None`, the connection will not          be pinged.         :type keep_alive: int         :param auto_reconnect: Whether to automatically reconnect the sender if a retryable error occurs.          Default value is `True`.         :type auto_reconnect: bool         :rtype: ~azure.eventhub.async_ops.sender_async.SenderAsync
Creates a new Checkpoint from an existing checkpoint.          :param checkpoint: Existing checkpoint.         :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint
Init Azure Blob Lease with existing blob.
Init Azure Blob Lease from existing.
Check and return Azure Blob Lease state using Storage API.
Makes pump sync so that it can be run in a thread.
Updates pump status and logs update to console.
Sets a new partition lease to be processed by the pump.          :param lease: The lease to set.         :type lease: ~azure.eventprocessorhost.lease.Lease
Opens partition pump.
Safely closes the pump.          :param reason: The reason for the shutdown.         :type reason: str
Process pump events.          :param events: List of events to be processed.         :type events: list[~azure.eventhub.common.EventData]
Eventhub Override for on_open_async.
Responsible for establishing connection to event hub client         throws EventHubsException, IOException, InterruptedException, ExecutionException.
Resets the pump swallows all exceptions.
Overides partition pump on closing.          :param reason: The reason for the shutdown.         :type reason: str
Runs the async partion reciever event loop to retrive messages from the event queue.
Handles processing errors this is never called since python recieve client doesn't         have error handling implemented (TBD add fault pump handling).          :param error: An error the occurred.         :type error: Exception
Init with partition Id.          :param partition_id: ID of a given partition.         :type partition_id: str
Init with existing lease.          :param lease: An existing Lease.         :type lease: ~azure.eventprocessorhost.lease.Lease
Starts the host.
Updates offset based on event.          :param event_data: A received EventData with valid offset and sequenceNumber.         :type event_data: ~azure.eventhub.common.EventData
Gets the initial offset for processing the partition.          :rtype: str
Generates a checkpoint for the partition using the curren offset and sequenceNumber for         and persists to the checkpoint manager.          :param event_processor_context An optional custom state value for the Event Processor.          This data must be in a JSON serializable format.         :type event_processor_context: str or dict
Stores the offset and sequenceNumber from the provided received EventData instance,         then writes those values to the checkpoint store via the checkpoint manager.         Optionally stores the state of the Event Processor along the checkpoint.          :param event_data: A received EventData with valid offset and sequenceNumber.         :type event_data: ~azure.eventhub.common.EventData         :param event_processor_context An optional custom state value for the Event Processor.          This data must be in a JSON serializable format.         :type event_processor_context: str or dict         :raises: ValueError if suplied event_data is None.         :raises: ValueError if the sequenceNumber is less than the last checkpointed value.
Returns the parition context in the following format:         "PartitionContext({EventHubPath}{ConsumerGroupName}{PartitionId}{SequenceNumber})"          :rtype: str
Persists the checkpoint, and - optionally - the state of the Event Processor.          :param checkpoint: The checkpoint to persist.         :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint         :param event_processor_context An optional custom state value for the Event Processor.          This data must be in a JSON serializable format.         :type event_processor_context: str or dict
Called by processor host to indicate that the event processor is being stopped.         :param context: Information about the partition         :type context: ~azure.eventprocessorhost.PartitionContext
Called by the processor host when a batch of events has arrived.         This is where the real work of the event processor is done.         :param context: Information about the partition         :type context: ~azure.eventprocessorhost.PartitionContext         :param messages: The events to be processed.         :type messages: list[~azure.eventhub.common.EventData]
Open the Receiver using the supplied conneciton.         If the handler has previously been redirected, the redirect         context will be used to create a new handler before opening it.          :param connection: The underlying client shared connection.         :type: connection: ~uamqp.connection.Connection
Receive events from the EventHub.          :param max_batch_size: Receive a batch of events. Batch size will          be up to the maximum specified, but will return as soon as service          returns no new events. If combined with a timeout and no events are          retrieve before the time, the result will be empty. If no batch          size is supplied, the prefetch size will be the maximum.         :type max_batch_size: int         :rtype: list[~azure.eventhub.common.EventData]
Returns a list of all the event hub partition IDs.          :rtype: list[str]
Intializes the partition checkpoint and lease store and then calls run async.
Terminiates the partition manger.
Starts the run loop and manages exceptions and cleanup.
Intializes the partition checkpoint and lease store ensures that a checkpoint         exists for all partitions. Note in this case checkpoint and lease stores are         the same storage manager construct.          :return: Returns the number of partitions.         :rtype: int
Make attempt_renew_lease async call sync.
Throws if it runs out of retries. If it returns, action succeeded.
This is the main execution loop for allocating and manging pumps.
Updates the lease on an exisiting pump.          :param partition_id: The partition ID.         :type partition_id: str         :param lease: The lease to be used.         :type lease: ~azure.eventprocessorhost.lease.Lease
Create a new pump thread with a given lease.          :param partition_id: The partition ID.         :type partition_id: str         :param lease: The lease to be used.         :type lease: ~azure.eventprocessorhost.lease.Lease
Stops a single partiton pump.          :param partition_id: The partition ID.         :type partition_id: str         :param reason: A reason for closing.         :type reason: str
Stops all partition pumps         (Note this might be wrong and need to await all tasks before returning done).          :param reason: A reason for closing.         :type reason: str         :rtype: bool
Determines and return which lease to steal         If the number of leases is a multiple of the number of hosts, then the desired         configuration is that all hosts own the name number of leases, and the         difference between the "biggest" owner and any other is 0.          If the number of leases is not a multiple of the number of hosts, then the most         even configurationpossible is for some hosts to have (self, leases/hosts) leases         and others to have (self, (self, leases/hosts) + 1). For example, for 16 partitions         distributed over five hosts, the distribution would be 4, 3, 3, 3, 3, or any of the         possible reorderings.          In either case, if the difference between this host and the biggest owner is 2 or more,         then thesystem is not in the most evenly-distributed configuration, so steal one lease         from the biggest. If there is a tie for biggest, we pick whichever appears first in the         list because it doesn't really matter which "biggest" is trimmed down.          Stealing one at a time prevents flapping because it reduces the difference between the         biggest and this host by two at a time. If the starting difference is two or greater,         then the difference cannot end up below 0. This host may become tied for biggest, but it         cannot become larger than the host that it is stealing from.          :param stealable_leases: List of leases to determine which can be stolen.         :type stealable_leases: list[~azure.eventprocessorhost.lease.Lease]         :param have_lease_count: Lease count.         :type have_lease_count: int         :rtype: ~azure.eventprocessorhost.lease.Lease
Returns a dictionary of leases by current owner.
Make attempt_renew_lease async call sync.
Attempts to renew a potential lease if possible and         marks in the queue as none adds to adds to the queue.
Open the Sender using the supplied conneciton.         If the handler has previously been redirected, the redirect         context will be used to create a new handler before opening it.          :param connection: The underlying client shared connection.         :type: connection: ~uamqp.async_ops.connection_async.ConnectionAsync
Close down the handler. If the handler has already closed,         this will be a no op. An optional exception can be passed in to         indicate that the handler was shutdown due to error.          :param exception: An optional exception if the handler is closing          due to an error.         :type exception: Exception
Wait until all transferred events have been sent.
This method opens an IP connection on the IP device          :return: None
This method writes sends data to the IP device         :param data:          :return: None
Setup pins
Function that gets called again as soon as it finishes (forever).
Retrieve the last data update for the specified digital pin.         It is intended for a polling application.          :param pin: Digital pin number          :returns: Last value reported for the digital pin
This command enables the rotary encoder (2 pin + ground) and will         enable encoder reporting.          This is a FirmataPlus feature.          Encoder data is retrieved by performing a digital_read from         pin a (encoder pin 1)          :param pin_a: Encoder pin 1.          :param pin_b: Encoder pin 2.          :param cb: callback function to report encoder changes          :param cb_type: Constants.CB_TYPE_DIRECT = direct call                         or Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :param hall_encoder: wheel hall_encoder - set to True to select                              hall encoder support support.          :returns: No return value
This method retrieves the latest encoder data value.         It is a FirmataPlus feature.          :param pin: Encoder Pin          :returns: encoder data value
Enables digital reporting. By turning reporting on for all         8 bits in the "port".         This is part of Firmata's protocol specification.          :param pin: Pin and all pins for this port          :returns: No return value
This method will send an extended-data analog write command         to the selected pin..          :param pin: 0 - 127          :param data: 0 - 0-0x4000 (14 bits)          :returns: No return value
A list is returned containing the latch state for the pin, the         latched value, and the time stamp         [pin_num, latch_state, latched_value, time_stamp]         If the the latch state is LATCH_LATCHED, the table is reset         (data and timestamp set to zero)         It is intended to be used for a polling application.          :param pin: Pin number.          :returns:  [latched_state, threshold_type, threshold_value,                     latched_data, time_stamp]
This method requests and returns an analog map.          :param cb: Optional callback reference          :returns: An analog map response or None if a timeout occurs
This method retrieves the Firmata capability report          :param raw: If True, it either stores or provides the callback                     with a report as list.                     If False, prints a formatted report to the console          :param cb: Optional callback reference to receive a raw report          :returns: capability report
This method retrieves the Firmata firmware version          :param cb: Reference to a callback function          :returns:If no callback is specified, the firmware version
This method retrieves a pin state report for the specified pin          :param pin: Pin of interest          :param cb: optional callback reference          :returns: pin state report
This method retrieves the PyMata version number          :returns: PyMata version number.
This method configures Arduino i2c with an optional read delay time.          :param read_delay_time: firmata i2c delay time          :returns: No return value
Retrieve result of last data read from i2c device.         i2c_read_request should be called before trying to retrieve data.         It is intended for use by a polling application.          :param address: i2c          :returns: last data read or None if no data is present.
This method issues an i2c read request for a single read,continuous         read or a stop, specified by the read_type.         Because different i2c devices return data at different rates,         if a callback is not specified, the user must first call this method         and then call i2c_read_data  after waiting for sufficient time for the         i2c device to respond.         Some devices require that transmission be restarted         (e.g. MMA8452Q accelerometer).         Use I2C_READ | I2C_RESTART_TX for those cases.          :param address: i2c device          :param register: i2c register number          :param number_of_bytes: number of bytes to be returned          :param read_type:  Constants.I2C_READ, Constants.I2C_READ_CONTINUOUSLY                            or Constants.I2C_STOP_READING.          Constants.I2C_RESTART_TX may be OR'ed when required         :param cb: optional callback reference          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :returns: No return value
Write data to an i2c device.          :param address: i2c device address          :param args: A variable number of bytes to be sent to the device                      passed in as a list.          :returns: No return value
Periodically send a keep alive message to the Arduino.         Frequency of keep alive transmission is calculated as follows:         keep_alive_sent = period - (period * margin)           :param period: Time period between keepalives.                        Range is 0-10 seconds. 0 disables                        the keepalive mechanism.          :param margin: Safety margin to assure keepalives                         are sent before period expires. Range is 0.1 to 0.9          :returns: No return value
This method will call the Tone library for the selected pin.         It requires FirmataPlus to be loaded onto the arduino          If the tone command is set to TONE_TONE, then the specified         tone will be played.          Else, if the tone command is TONE_NO_TONE, then any currently         playing tone will be disabled.           :param pin: Pin number          :param tone_command: Either TONE_TONE, or TONE_NO_TONE          :param frequency: Frequency of tone          :param duration: Duration of tone in milliseconds          :returns: No return value
Send a Firmata reset command          :returns: No return value
This method configures the Arduino for servo operation.          :param pin: Servo control pin          :param min_pulse: Minimum pulse width          :param max_pulse: Maximum pulse width          :returns: No return value
This method "arms" an analog pin for its data to be latched and         saved in the latching table.         If a callback method is provided, when latching criteria is achieved,         the callback function is called with latching data notification.          :param pin: Analog pin number (value following an 'A' designator,                     i.e. A5 = 5          :param threshold_type: Constants.LATCH_GT | Constants.LATCH_LT  |                                Constants.LATCH_GTE | Constants.LATCH_LTE          :param threshold_value: numerical value - between 0 and 1023          :param cb: callback method          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :returns: True if successful, False if parameter data is invalid
This method sets the  pin mode for the specified pin.          :param pin_number: Arduino Pin Number          :param pin_state: INPUT/OUTPUT/ANALOG/PWM/PULLUP - for SERVO use                           servo_config()          :param callback: Optional: A reference to a call back function to be                          called when pin data value changes          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :returns: No return value
This method sets the sampling interval for the Firmata loop method          :param interval: time in milliseconds          :returns: No return value
Perform an asyncio sleep for the time specified in seconds. T         his method should be used in place of time.sleep()          :param time: time in seconds         :returns: No return value
Shutdown the application and exit          :returns: No return value
Retrieve Ping (HC-SR04 type) data. The data is presented as a         dictionary.         The 'key' is the trigger pin specified in sonar_config() and the         'data' is the current measured distance (in centimeters)         for that pin. If there is no data, the value is set to None.         This is a FirmataPlus feature.          :param trigger_pin: trigger pin specified in sonar_config          :returns: active_sonar_map
Configure the pins,ping interval and maximum distance for an HC-SR04         type device.         Single pin configuration may be used. To do so, set both the trigger         and echo pins to the same value.         Up to a maximum of 6 SONAR devices is supported         If the maximum is exceeded a message is sent to the console and the         request is ignored.         NOTE: data is measured in centimeters          This is FirmataPlus feature.          :param trigger_pin: The pin number of for the trigger (transmitter).          :param echo_pin: The pin number for the received echo.          :param cb: optional callback function to report sonar data changes          :param ping_interval: Minimum interval between pings. Lowest number                               to use is 33 ms.Max is 127          :param max_distance: Maximum distance in cm. Max is 200.          :param cb_type: direct call or asyncio yield from          :returns: No return value
Configure stepper motor prior to operation.         This is a FirmataPlus feature.          :param steps_per_revolution: number of steps per motor revolution          :param stepper_pins: a list of control pin numbers - either 4 or 2          :returns: No return value
Move a stepper motor for the number of steps at the specified speed         This is a FirmataPlus feature.          :param motor_speed: 21 bits of data to set motor speed          :param number_of_steps: 14 bits for number of steps & direction                                 positive is forward, negative is reverse
Initialize Pixy and will enable Pixy block reporting.         This is a FirmataPlusRB feature.          :param cb: callback function to report Pixy blocks          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :param max_blocks: Maximum number of Pixy blocks to report when many                            signatures are found.          :returns: No return value.
Sends the setServos Pixy command.         This method sets the pan/tilt servos that are plugged into Pixy's two servo ports.          :param s0: value 0 to 1000          :param s1: value 0 to 1000          :returns: No return value.
Sends the setBrightness Pixy command.         This method sets the brightness (exposure) of Pixy's camera.          :param brightness: range between 0 and 255 with 255 being the                            brightest setting          :returns: No return value.
Sends the setLed Pixy command.         This method sets the RGB LED on front of Pixy.          :param r: red range between 0 and 255          :param g: green range between 0 and 255          :param b: blue range between 0 and 255          :returns: No return value.
This is an asyncio adapted version of pyserial write. It provides a         non-blocking  write and returns the number of bytes written upon         completion          :param data: Data to be written         :return: Number of bytes written
This is an asyncio adapted version of pyserial read.         It provides a non-blocking read and returns a line of data read.          :return: A line of data
Prints the Pixy blocks data.
Prints the Pixy blocks data.
This method checks verifies the device ID.         @return: True if valid, False if not
Put the device into standby mode so that the registers can be set.         @return: No return value
Set the device scale register.         Device must be in standby before calling this function         @param scale: scale factor         @return: No return value
Set the device output data rate.         Device must be in standby before calling this function         @param output_data_rate: Desired data rate         @return: No return value.
This method checks to see if new xyz data is available         @return: Returns 0 if not available. 1 if it is available
The device returns an MSB and LSB (in that order) for each axis.         These are 12 bit values - that is only the upper 4 bits of the LSB are used.          To make things more confusing, firmata returns each axis as 4 bytes, and reverses the order because         it looks at the world as lsb, msb order.         :param callback: Callback function         :returns: callback data is set with x,y,z raw (integers) followed by x,y,z corrected ( floating point)         Call available() first to make sure new data is really available.
This is a utility function to wait for return data call back         @return: Returns resultant data from callback
Basically the same as drive(), but omitting the right motor.
Basically the same as drive(), but omitting the left motor.
allows left motor to coast to a stop
allows left motor to coast to a stop
allows left motor to coast to a stop
allows left motor to coast to a stop
pivot() controls the pivot speed of the RedBot. The values of the pivot function inputs             range from -255:255, with -255 indicating a full speed counter-clockwise rotation.             255 indicates a full speed clockwise rotation
This function reads the portrait/landscape status register of the MMA8452Q.         It will return either PORTRAIT_U, PORTRAIT_D, LANDSCAPE_R, LANDSCAPE_L,         or LOCKOUT. LOCKOUT indicates that the sensor is in neither p or ls.         :param callback: Callback function         :returns: See above.
This method sets the tap thresholds.         Device must be in standby before calling this function.         Set up single and double tap - 5 steps:         for more info check out this app note:         http://cache.freescale.com/files/sensors/doc/app_note/AN4072.pdf         Set the threshold - minimum required acceleration to cause a tap.         @param x_ths: x tap threshold         @param y_ths: y tap threshold         @param z_ths: z tap threshold         @return: No return value.
This function returns any taps read by the MMA8452Q. If the function         returns 0, no new taps were detected. Otherwise the function will return the         lower 7 bits of the PULSE_SRC register.         :param callback: Callback function         :returns: 0 or lower 7 bits of the PULSE_SRC register.
This is a utility function to wait for return data call back         @return: Returns resultant data from callback
Prints the Pixy blocks data.
Set digital pin 6 as a PWM output and set its output value to 128     @param my_board: A PymataCore instance     @return: No Return Value
Blink LED 13     @return: No Return Value
turns RedBot to the Right
turns RedBot to the Left
This method must be called immediately after the class is instantiated.         It instantiates the serial interface and then performs auto pin         discovery.         It is intended for use by pymata3 applications that do not         use asyncio coroutines directly.          :returns: No return value.
Set the selected pin to the specified value.          :param pin: PWM pin number          :param value: Pin value (0 - 0x4000)          :returns: No return value
Set the specified pin to the specified value directly without port manipulation.          :param pin: pin number          :param value: pin value          :returns: No return value
Set the specified pin to the specified value.          :param pin: pin number          :param value: pin value          :returns: No return value
Disables analog reporting for a single analog pin.          :param pin: Analog pin number. For example for A0, the number is 0.          :returns: No return value
Disables digital reporting. By turning reporting off for this pin,         Reporting is disabled for all 8 bits in the "port"          :param pin: Pin and all pins for this port          :returns: No return value
This command enables the rotary encoder support and will         enable encoder reporting.          This command is not part of StandardFirmata. For 2 pin + ground         encoders, FirmataPlus is required to be used for 2 pin rotary encoder,         and for hell effect wheel encoder support, FirmataPlusRB is required.          Encoder data is retrieved by performing a digital_read from pin a         (encoder pin_a).          When using 2 hall effect sensors (e.g. 2 wheel robot)         specify pin_a for 1st encoder and pin_b for 2nd encoder.          :param pin_a: Encoder pin 1.          :param pin_b: Encoder pin 2.          :param cb: callback function to report encoder changes          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :param hall_encoder: wheel hall_encoder - set to                              True to select hall encoder support support.          :returns: No return value
Enables analog reporting. By turning reporting on for a single pin,          :param pin: Analog pin number. For example for A0, the number is 0.          :returns: No return value
Enables digital reporting. By turning reporting on for all 8 bits         in the "port" - this is part of Firmata's protocol specification.          :param pin: Pin and all pins for this port          :returns: No return value
This method will send an extended-data analog write command to the         selected pin.          :param pin: 0 - 127          :param data: 0 - 0xfffff          :returns: No return value
A list is returned containing the latch state for the pin, the         latched value, and the time stamp         [latched_state, threshold_type, threshold_value,          latched_data, time_stamp]          :param pin: Pin number.          :returns:  [latched_state, threshold_type, threshold_value,                     latched_data, time_stamp]
This method requests a Firmata analog map query and returns the results.          :returns: An analog map response or None if a timeout occurs
This method requests and returns a Firmata capability query report          :returns: A capability report in the form of a list
A list is returned containing the latch state for the pin, the         latched value, and the time stamp         [pin_num, latch_state, latched_value, time_stamp]          :param pin: Pin number.          :returns:  [latched_state, threshold_type, threshold_value,                    latched_data, time_stamp]
This method retrieves the Firmata firmware version          :returns: Firmata firmware version
This method returns the major and minor values for the protocol         version, i.e. 2.4          :returns: Firmata protocol version
This method retrieves a pin state report for the specified pin          :param pin: Pin of interest          :returns: pin state report
NOTE: THIS METHOD MUST BE CALLED BEFORE ANY I2C REQUEST IS MADE         This method initializes Firmata for I2c operations.          :param read_delay_time (in microseconds): an optional parameter,                                                   default is 0          :returns: No Return Value
This method retrieves cached i2c data to support a polling mode.          :param address: I2C device address          :returns: Last cached value read
This method requests the read of an i2c device. Results are retrieved         by a call to i2c_get_read_data(). or by callback.          If a callback method is provided, when data is received from the         device it will be sent to the callback method.         Some devices require that transmission be restarted         (e.g. MMA8452Q accelerometer).         Use Constants.I2C_READ | Constants.I2C_END_TX_MASK for those cases.          :param address: i2c device address          :param register: register number (can be set to zero)          :param number_of_bytes: number of bytes expected to be returned          :param read_type: I2C_READ  or I2C_READ_CONTINUOUSLY. I2C_END_TX_MASK                           may be OR'ed when required          :param cb: Optional callback function to report i2c data as a                    result of read command          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :returns: No return value.
Write data to an i2c device.          :param address: i2c device address          :param args: A variable number of bytes to be sent to the device                      passed in as a list          :returns: No return value.
Periodically send a keep alive message to the Arduino.         Frequency of keep alive transmission is calculated as follows:         keep_alive_sent = period - (period * margin)           :param period: Time period between keepalives. Range is 0-10 seconds.                        0 disables the keepalive mechanism.          :param margin: Safety margin to assure keepalives are sent before                     period expires. Range is 0.1 to 0.9         :returns: No return value
This method will call the Tone library for the selected pin.         It requires FirmataPlus to be loaded onto the arduino          If the tone command is set to TONE_TONE, then the specified         tone will be played.          Else, if the tone command is TONE_NO_TONE, then any currently         playing tone will be disabled.          :param pin: Pin number          :param tone_command: Either TONE_TONE, or TONE_NO_TONE          :param frequency: Frequency of tone          :param duration: Duration of tone in milliseconds          :returns: No return value
Configure a pin as a servo pin. Set pulse min, max in ms.         Use this method (not set_pin_mode) to configure a pin for servo         operation.          :param pin: Servo Pin.          :param min_pulse: Min pulse width in ms.          :param max_pulse: Max pulse width in ms.          :returns: No return value
This method "arms" an analog pin for its data to be latched and saved         in the latching table         If a callback method is provided, when latching criteria is achieved,         the callback function is called with latching data notification.          Data returned in the callback list has the pin number as the         first element,          :param pin: Analog pin number                     (value following an 'A' designator, i.e. A5 = 5          :param threshold_type: ANALOG_LATCH_GT | ANALOG_LATCH_LT  |                                ANALOG_LATCH_GTE | ANALOG_LATCH_LTE          :param threshold_value: numerical value - between 0 and 1023          :param cb: callback method          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :returns: True if successful, False if parameter data is invalid
This method "arms" a digital pin for its data to be latched and         saved in the latching table         If a callback method is provided, when latching criteria is achieved,         the callback function is called with latching data notification.          Data returned in the callback list has the pin number as the         first element,          :param pin: Digital pin number          :param threshold_value: 0 or 1          :param cb: callback function          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :returns: True if successful, False if parameter data is invalid
This method sets the pin mode for the specified pin.         For Servo, use servo_config() instead.          :param pin_number: Arduino Pin Number          :param pin_state: INPUT/OUTPUT/ANALOG/PWM/PULLUP - for SERVO use                           servo_config()          :param callback: Optional: A reference to a call back function to be                          called when pin data value changes          :param callback_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :returns: No return value
This method sends the desired sampling interval to Firmata.         Note: Standard Firmata  will ignore any interval less than               10 milliseconds          :param interval: Integer value for desired sampling interval                          in milliseconds          :returns: No return value.
This method attempts an orderly shutdown         If any exceptions are thrown, just ignore them.          :returns: No return value
This method is a proxy method for asyncio.sleep          :param sleep_time: Sleep interval in seconds          :returns: No return value.
Configure the pins,ping interval and maximum distance for an HC-SR04         type device.         Single pin configuration may be used. To do so, set both the trigger         and echo pins to the same value.         Up to a maximum of 6 SONAR devices is supported         If the maximum is exceeded a message is sent to the console and the         request is ignored.         NOTE: data is measured in centimeters          :param trigger_pin: The pin number of for the trigger (transmitter).          :param echo_pin: The pin number for the received echo.          :param cb: optional callback function to report sonar data changes          :param ping_interval: Minimum interval between pings. Lowest number                               to use is 33 ms. Max is 127ms.          :param max_distance: Maximum distance in cm. Max is 200.          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine         :returns: No return value.
Retrieve Ping (HC-SR04 type) data. The data is presented as a         dictionary.         The 'key' is the trigger pin specified in sonar_config()         and the 'data' is the current measured distance (in centimeters)         for that pin. If there is no data, the value is set to None.          :param trigger_pin: key into sonar data map          :returns: active_sonar_map
Configure stepper motor prior to operation.         This is a FirmataPlus feature.          :param steps_per_revolution: number of steps per motor revolution          :param stepper_pins: a list of control pin numbers - either 4 or 2          :returns: No return value.
Move a stepper motor for the number of steps at the specified speed         This is a FirmataPlus feature.          :param motor_speed: 21 bits of data to set motor speed          :param number_of_steps: 14 bits for number of steps & direction                                 positive is forward, negative is reverse          :returns: No return value.
Initialize Pixy and enable Pixy block reporting.         This is a FirmataPlusRB feature.          :param cb: callback function to report Pixy blocks          :param cb_type: Constants.CB_TYPE_DIRECT = direct call or                         Constants.CB_TYPE_ASYNCIO = asyncio coroutine          :param max_blocks: Maximum number of Pixy blocks to report when many signatures are found.          :returns: No return value.
Sends the setServos Pixy command.         This method sets the pan/tilt servos that are plugged into Pixy's two servo ports.          :param s0: value 0 to 1000          :param s1: value 0 to 1000          :returns: No return value.
Sends the setBrightness Pixy command.         This method sets the brightness (exposure) of Pixy's camera.          :param brightness: range between 0 and 255 with 255 being the brightest setting          :returns: No return value.
Sends the setLed Pixy command.         This method sets the RGB LED on front of Pixy.          :param r: red range between 0 and 255          :param g: green range between 0 and 255          :param b: blue range between 0 and 255          :returns: No return value.
This is a private method.         It continually accepts and interprets data coming from Firmata,and then         dispatches the correct handler to process the data.          :returns: This method never returns
This is a private message handler method.         It is a message handler for analog messages.          :param data: message data          :returns: None - but saves the data in the pins structure
This is a private message handler method.         It is a message handler for Digital Messages.          :param data: digital message          :returns: None - but update is saved in pins structure
This is a private message handler method.         It handles encoder data messages.          :param data: encoder data          :returns: None - but update is saved in the digital pins structure
This is a private message handler method.         It handles pixy data messages.          :param data: pixy data          :returns: None - but update is saved in the digital pins structure
This is a private message handler method.         It handles replies to i2c_read requests. It stores the data         for each i2c device address in a dictionary called i2c_map.         The data may be retrieved via a polling call to i2c_get_read_data().         It a callback was specified in pymata.i2c_read, the raw data is sent         through the callback          :param data: raw data returned from i2c device
This is a private message handler method.         This method handles the sysex 'report firmware' command sent by         Firmata (0x79).         It assembles the firmware version by concatenating the major and          minor version number components and         the firmware identifier into a string.         e.g. "2.3 StandardFirmata.ino"          :param sysex_data: Sysex data sent from Firmata          :returns: None
This is a private message handler method.         This method reads the following 2 bytes after the report version         command (0xF9 - non sysex).         The first byte is the major number and the second byte is the         minor number.          :returns: None
This method handles the incoming sonar data message and stores         the data in the response table.          :param data: Message data from Firmata          :returns: No return value.
This is a private message handler method.         It is the message handler for String data messages that will be         printed to the console.         :param data:  message          :returns: None - message is sent to console
This is a private utility method.         When a data change message is received this method checks to see if         latching needs to be processed          :param key: encoded pin number          :param data: data change          :returns: None
This is a private utility method.         This method attempts to discover the com port that the arduino         is connected to.          :returns: Detected Comport
This is a private utility method.         This method formats a capability report if the user wishes to         send it to the console.         If log_output = True, no output is generated          :param data: Capability report          :returns: None
This is a private utility method.         This method process latching events and either returns them via         callback or stores them in the latch map          :param key: Encoded pin          :param latching_entry: a latch table entry          :returns: Callback or store data in latch map
This is a private utility method.         The method sends a non-sysex command to Firmata.          :param command:  command data          :returns: length of data sent
This is a private utility method.         This method sends a sysex command to Firmata.          :param sysex_command: sysex command          :param sysex_data: data for command          :returns : No return value.
This is a private utility method.         This method accumulates the requested number of bytes and         then returns the full command          :param current_command:  command id          :param number_of_bytes:  how many bytes to wait for          :returns: command
This method writes a value to an analog pin.          It is used to set the output of a PWM pin or the angle of a Servo.          :param command: {"method": "analog_write", "params": [PIN, WRITE_VALUE]}         :returns: No return message.
This method reads and returns the last reported value for a digital pin.         Normally not used since digital pin updates will be provided automatically         as they occur with the digital_message_reply being sent to the client after set_pin_mode is called..         (see enable_digital_reporting for message format)          :param command: {"method": "digital_read", "params": [PIN]}         :returns: {"method": "digital_read_reply", "params": [PIN, DIGITAL_DATA_VALUE]}
This method writes a zero or one to a digital pin.          :param command: {"method": "digital_write", "params": [PIN, DIGITAL_DATA_VALUE]}         :returns: No return message..
Disable Firmata reporting for an analog pin.          :param command: {"method": "disable_analog_reporting", "params": [PIN]}         :returns: No return message..
Disable Firmata reporting for a digital pin.          :param command: {"method": "disable_digital_reporting", "params": [PIN]}         :returns: No return message.
Enable Firmata reporting for an analog pin.          :param command: {"method": "enable_analog_reporting", "params": [PIN]}         :returns: {"method": "analog_message_reply", "params": [PIN, ANALOG_DATA_VALUE]}
Enable Firmata reporting for a digital pin.          :param command: {"method": "enable_digital_reporting", "params": [PIN]}         :returns: {"method": "digital_message_reply", "params": [PIN, DIGITAL_DATA_VALUE]}
Configure 2 pins for FirmataPlus encoder operation.          :param command: {"method": "encoder_config", "params": [PIN_A, PIN_B]}         :returns: {"method": "encoder_data_reply", "params": [ENCODER_DATA]}
This is a polling method to read the last cached FirmataPlus encoder value.         Normally not used. See encoder config for the asynchronous report message format.          :param command: {"method": "encoder_read", "params": [PIN_A]}         :returns: {"method": "encoder_read_reply", "params": [PIN_A, ENCODER_VALUE]}
This method retrieves the Firmata capability report.          Refer to http://firmata.org/wiki/Protocol#Capability_Query          The command format is: {"method":"get_capability_report","params":["null"]}          :returns: {"method": "capability_report_reply", "params": [RAW_CAPABILITY_REPORT]}
This method retrieves a Firmata pin_state report for a pin..          See: http://firmata.org/wiki/Protocol#Pin_State_Query          :param command: {"method": "get_pin_state", "params": [PIN]}         :returns: {"method": "get_pin_state_reply", "params": [PIN_NUMBER, PIN_MODE, PIN_STATE]}
This method retrieves the Firmata protocol version.          JSON command: {"method": "get_protocol_version", "params": ["null"]}          :returns: {"method": "protocol_version_reply", "params": [PROTOCOL_VERSION]}
This method initializes the I2c and sets the optional read delay (in microseconds).          It must be called before doing any other i2c operations for a given device.         :param command: {"method": "i2c_config", "params": [DELAY]}         :returns: No Return message.
This method retrieves the last value read for an i2c device identified by address.         This is a polling implementation and i2c_read_request and i2c_read_request_reply may be         a better alternative.         :param command: {"method": "i2c_read_data", "params": [I2C_ADDRESS ]}         :returns:{"method": "i2c_read_data_reply", "params": i2c_data}
This method sends an I2C read request to Firmata. It is qualified by a single shot, continuous         read, or stop reading command.         Special Note: for the read type supply one of the following string values:           "0" = I2C_READ           "1" = I2C_READ | I2C_END_TX_MASK"           "2" = I2C_READ_CONTINUOUSLY           "3" = I2C_READ_CONTINUOUSLY | I2C_END_TX_MASK           "4" = I2C_STOP_READING          :param command: {"method": "i2c_read_request", "params": [I2C_ADDRESS, I2C_REGISTER,                 NUMBER_OF_BYTES, I2C_READ_TYPE ]}         :returns: {"method": "i2c_read_request_reply", "params": [DATA]}
This method performs an I2C write at a given I2C address,         :param command: {"method": "i2c_write_request", "params": [I2C_DEVICE_ADDRESS, [DATA_TO_WRITE]]}         :returns:No return message.
Periodically send a keep alive message to the Arduino.         Frequency of keep alive transmission is calculated as follows:         keep_alive_sent = period - (period * margin)          :param command:  {"method": "keep_alive", "params": [PERIOD, MARGIN]}         Period is time period between keepalives. Range is 0-10 seconds. 0 disables the keepalive mechanism.         Margin is a  safety margin to assure keepalives are sent before period expires. Range is 0.1 to 0.9         :returns: No return value
This method controls a piezo device to play a tone. It is a FirmataPlus feature.         Tone command is TONE_TONE to play, TONE_NO_TONE to stop playing.         :param command: {"method": "play_tone", "params": [PIN, TONE_COMMAND, FREQUENCY(Hz), DURATION(MS)]}         :returns:No return message.
This method sets the an analog latch for a given analog pin, providing the threshold type, and         latching threshold.         :param command: {"method": "set_analog_latch", "params": [PIN, THRESHOLD_TYPE, THRESHOLD_VALUE]}         :returns:{"method": "analog_latch_data_reply", "params": [PIN, DATA_VALUE_LATCHED, TIMESTAMP_STRING]}
This method sets the a digital latch for a given digital pin, the threshold type, and latching threshold.         :param command:{"method": "set_digital_latch", "params": [PIN, THRESHOLD (0 or 1)]}         :returns:{"method": digital_latch_data_reply", "params": [PIN, DATA_VALUE_LATCHED, TIMESTAMP_STRING]}
This method sets the pin mode for the selected pin. It handles: Input, Analog(Input) PWM, and OUTPUT. Servo         is handled by servo_config().         :param command: {"method": "set_pin_mode", "params": [PIN, MODE]}         :returns:No return message.
This method sets the Firmata sampling interval in ms.         :param command:{"method": "set_sampling_interval", "params": [INTERVAL]}         :returns:No return message.
This method configures 2 pins to support HC-SR04 Ping devices.         This is a FirmataPlus feature.         :param command: {"method": "sonar_config", "params": [TRIGGER_PIN, ECHO_PIN, PING_INTERVAL(default=50),          MAX_DISTANCE(default= 200 cm]}         :returns:{"method": "sonar_data_reply", "params": [DISTANCE_IN_CM]}
This method retrieves the last sonar data value that was cached.         This is a polling method. After sonar config, sonar_data_reply messages will be sent automatically.         :param command: {"method": "sonar_read", "params": [TRIGGER_PIN]}         :returns:{"method": "sonar_read_reply", "params": [TRIGGER_PIN, DATA_VALUE]}
This method configures a pin for servo operation. The servo angle is set by using analog_write().         :param command: {"method": "servo_config", "params": [PIN, MINIMUM_PULSE(ms), MAXIMUM_PULSE(ms)]}         :returns:No message returned.
This method configures 4 pins for stepper motor operation.         This is a FirmataPlus feature.         :param command: {"method": "stepper_config", "params": [STEPS_PER_REVOLUTION, [PIN1, PIN2, PIN3, PIN4]]}         :returns:No message returned.
This method activates a stepper motor motion.         This is a FirmataPlus feature.         :param command: {"method": "stepper_step", "params": [SPEED, NUMBER_OF_STEPS]}         :returns:No message returned.
This method handles the analog message received from pymata_core         :param data: analog callback message         :returns:{"method": "analog_message_reply", "params": [PIN, DATA_VALUE}
This method handles analog_latch data received from pymata_core         :param data: analog latch callback message         :returns:{"method": "analog_latch_data_reply", "params": [ANALOG_PIN, VALUE_AT_TRIGGER, TIME_STAMP_STRING]}
Prints the Pixy blocks data.
Helper method to shutdown the RedBot if Ctrl-c is pressed
Set digital pin 6 as a PWM output and set its output value to 128     @return:
Exchange a code (and 'state' token) for a bearer token
Returns the complete tag record for a single tag.          Parameters         ----------         tag : {Id} The tag to get.         [params] : {Object} Parameters for the request
Updates the properties of a tag. Only the fields provided in the `data`         block will be updated; any unspecified fields will remain unchanged.                  When using this method, it is best to specify only those fields you wish         to change, or else you may overwrite changes made by another user since         you last retrieved the task.                  Returns the complete updated tag record.          Parameters         ----------         tag : {Id} The tag to update.         [data] : {Object} Data for the request
A specific, existing tag can be deleted by making a DELETE request         on the URL for that tag.                  Returns an empty data record.          Parameters         ----------         tag : {Id} The tag to delete.
Returns the compact tag records for all tags in the workspace.          Parameters         ----------         workspace : {Id} The workspace or organization to find tags in.         [params] : {Object} Parameters for the request
Returns the compact task records for all tasks with the given tag.         Tasks can have more than one tag at a time.          Parameters         ----------         tag : {Id} The tag to fetch tasks from.         [params] : {Object} Parameters for the request
Returns details of a previously-requested Organization export.          Parameters         ----------         organization_export : {Id} Globally unique identifier for the Organization export.         [params] : {Object} Parameters for the request
Returns the complete definition of a custom field's metadata.          Parameters         ----------         custom_field : {Id} Globally unique identifier for the custom field.         [params] : {Object} Parameters for the request
A specific, existing custom field can be updated by making a PUT request on the URL for that custom field. Only the fields provided in the `data` block will be updated; any unspecified fields will remain unchanged                  When using this method, it is best to specify only those fields you wish to change, or else you may overwrite changes made by another user since you last retrieved the custom field.                  A custom field's `type` cannot be updated.                  An enum custom field's `enum_options` cannot be updated with this endpoint. Instead see "Work With Enum Options" for information on how to update `enum_options`.                  Returns the complete updated custom field record.          Parameters         ----------         custom_field : {Id} Globally unique identifier for the custom field.         [data] : {Object} Data for the request
A specific, existing custom field can be deleted by making a DELETE request on the URL for that custom field.                  Returns an empty data record.          Parameters         ----------         custom_field : {Id} Globally unique identifier for the custom field.
Creates an enum option and adds it to this custom field's list of enum options. A custom field can have at most 50 enum options (including disabled options). By default new enum options are inserted at the end of a custom field's list.                  Returns the full record of the newly created enum option.          Parameters         ----------         custom_field : {Id} Globally unique identifier for the custom field.         [data] : {Object} Data for the request           - name : {String} The name of the enum option.           - [color] : {String} The color of the enum option. Defaults to 'none'.           - [insert_before] : {Id} An existing enum option within this custom field before which the new enum option should be inserted. Cannot be provided together with after_enum_option.           - [insert_after] : {Id} An existing enum option within this custom field after which the new enum option should be inserted. Cannot be provided together with before_enum_option.
Updates an existing enum option. Enum custom fields require at least one enabled enum option.                  Returns the full record of the updated enum option.          Parameters         ----------         enum_option : {Id} Globally unique identifier for the enum option.         [data] : {Object} Data for the request           - name : {String} The name of the enum option.           - [color] : {String} The color of the enum option. Defaults to 'none'.           - [enabled] : {Boolean} Whether or not the enum option is a selectable value for the custom field.
Moves a particular enum option to be either before or after another specified enum option in the custom field.          Parameters         ----------         custom_field : {Id} Globally unique identifier for the custom field.         [data] : {Object} Data for the request           - enum_option : {Id} The ID of the enum option to relocate.           - name : {String} The name of the enum option.           - [color] : {String} The color of the enum option. Defaults to 'none'.           - [before_enum_option] : {Id} An existing enum option within this custom field before which the new enum option should be inserted. Cannot be provided together with after_enum_option.           - [after_enum_option] : {Id} An existing enum option within this custom field after which the new enum option should be inserted. Cannot be provided together with before_enum_option.
Returns the compact project membership records for the project.          Parameters         ----------         project : {Id} The project for which to fetch memberships.         [params] : {Object} Parameters for the request           - [user] : {String} If present, the user to filter the memberships to.
Returns the project membership record.          Parameters         ----------         project_membership : {Id} Globally unique identifier for the project membership.         [params] : {Object} Parameters for the request
Returns the full workspace record for a single workspace.          Parameters         ----------         workspace : {Id} Globally unique identifier for the workspace or organization.         [params] : {Object} Parameters for the request
A specific, existing workspace can be updated by making a PUT request on         the URL for that workspace. Only the fields provided in the data block         will be updated; any unspecified fields will remain unchanged.                  Currently the only field that can be modified for a workspace is its `name`.                  Returns the complete, updated workspace record.          Parameters         ----------         workspace : {Id} The workspace to update.         [data] : {Object} Data for the request
Retrieves objects in the workspace based on an auto-completion/typeahead         search algorithm. This feature is meant to provide results quickly, so do         not rely on this API to provide extremely accurate search results. The         result set is limited to a single page of results with a maximum size,         so you won't be able to fetch large numbers of results.          Parameters         ----------         workspace : {Id} The workspace to fetch objects from.         [params] : {Object} Parameters for the request           - type : {Enum} The type of values the typeahead should return. You can choose from           one of the following: custom_field, project, tag, task, and user.           Note that unlike in the names of endpoints, the types listed here are           in singular form (e.g. `task`). Using multiple types is not yet supported.           - [query] : {String} The string that will be used to search for relevant objects. If an           empty string is passed in, the API will currently return an empty           result set.           - [count] : {Number} The number of results to return. The default is `20` if this           parameter is omitted, with a minimum of `1` and a maximum of `100`.           If there are fewer results found than requested, all will be returned.
The user can be referenced by their globally unique user ID or their email address.         Returns the full user record for the invited user.          Parameters         ----------         workspace : {Id} The workspace or organization to invite the user to.         [data] : {Object} Data for the request           - user : {String} An identifier for the user. Can be one of an email address,           the globally unique identifier for the user, or the keyword `me`           to indicate the current user making the request.
The user making this call must be an admin in the workspace.         Returns an empty data record.          Parameters         ----------         workspace : {Id} The workspace or organization to invite the user to.         [data] : {Object} Data for the request           - user : {String} An identifier for the user. Can be one of an email address,           the globally unique identifier for the user, or the keyword `me`           to indicate the current user making the request.
Returns the full record for a single attachment.          Parameters         ----------         attachment : {Id} Globally unique identifier for the attachment.         [params] : {Object} Parameters for the request
Returns the compact records for all attachments on the task.          Parameters         ----------         task : {Id} Globally unique identifier for the task.         [params] : {Object} Parameters for the request
Upload an attachment for a task. Accepts a file object or string, file name, and optional file Content-Type
Returns the full record for a single team.          Parameters         ----------         team : {Id} Globally unique identifier for the team.         [params] : {Object} Parameters for the request
Returns the compact records for all teams in the organization visible to         the authorized user.          Parameters         ----------         organization : {Id} Globally unique identifier for the workspace or organization.         [params] : {Object} Parameters for the request
Returns the compact records for all teams to which user is assigned.          Parameters         ----------         user : {String} An identifier for the user. Can be one of an email address,         the globally unique identifier for the user, or the keyword `me`         to indicate the current user making the request.         [params] : {Object} Parameters for the request           - [organization] : {Id} The workspace or organization to filter teams on.
Returns the compact records for all users that are members of the team.          Parameters         ----------         team : {Id} Globally unique identifier for the team.         [params] : {Object} Parameters for the request
The user making this call must be a member of the team in order to add others.         The user to add must exist in the same organization as the team in order to be added.         The user to add can be referenced by their globally unique user ID or their email address.         Returns the full user record for the added user.          Parameters         ----------         team : {Id} Globally unique identifier for the team.         [data] : {Object} Data for the request           - user : {String} An identifier for the user. Can be one of an email address,           the globally unique identifier for the user, or the keyword `me`           to indicate the current user making the request.
The user to remove can be referenced by their globally unique user ID or their email address.         Removes the user from the specified team. Returns an empty data record.          Parameters         ----------         team : {Id} Globally unique identifier for the team.         [data] : {Object} Data for the request           - user : {String} An identifier for the user. Can be one of an email address,           the globally unique identifier for the user, or the keyword `me`           to indicate the current user making the request.
Changes the parent of a task. Each task may only be a subtask of a single         parent, or no parent task at all. Returns an empty data block.          Parameters         ----------         task : {Id} Globally unique identifier for the task.         [data] : {Object} Data for the request           - parent : {Id} The new parent of the task, or `null` for no parent.
Creates a new section in a project.                  Returns the full record of the newly created section.          Parameters         ----------         project : {Id} The project to create the section in         [data] : {Object} Data for the request           - name : {String} The text to be displayed as the section name. This cannot be an empty string.
Returns the compact records for all sections in the specified project.          Parameters         ----------         project : {Id} The project to get sections from.         [params] : {Object} Parameters for the request
Returns the complete record for a single section.          Parameters         ----------         section : {Id} The section to get.         [params] : {Object} Parameters for the request
A specific, existing section can be updated by making a PUT request on         the URL for that project. Only the fields provided in the `data` block         will be updated; any unspecified fields will remain unchanged. (note that         at this time, the only field that can be updated is the `name` field.)                  When using this method, it is best to specify only those fields you wish         to change, or else you may overwrite changes made by another user since         you last retrieved the task.                  Returns the complete updated section record.          Parameters         ----------         section : {Id} The section to update.         [data] : {Object} Data for the request
A specific, existing section can be deleted by making a DELETE request         on the URL for that section.                  Note that sections must be empty to be deleted.                  The last remaining section in a board view cannot be deleted.                  Returns an empty data block.          Parameters         ----------         section : {Id} The section to delete.
Move sections relative to each other in a board view. One of         `before_section` or `after_section` is required.                  Sections cannot be moved between projects.                  At this point in time, moving sections is not supported in list views, only board views.                  Returns an empty data block.          Parameters         ----------         project : {Id} The project in which to reorder the given section         [data] : {Object} Data for the request           - section : {Id} The section to reorder           - [before_section] : {Id} Insert the given section immediately before the section specified by this parameter.           - [after_section] : {Id} Insert the given section immediately after the section specified by this parameter.
Returns the full record for a single story.          Parameters         ----------         story : {Id} Globally unique identifier for the story.         [params] : {Object} Parameters for the request
Adds a comment to a task. The comment will be authored by the         currently authenticated user, and timestamped when the server receives         the request.                  Returns the full record for the new story added to the task.          Parameters         ----------         task : {Id} Globally unique identifier for the task.         [data] : {Object} Data for the request           - text : {String} The plain text of the comment to add.
Updates the story and returns the full record for the updated story.         Only comment stories can have their text updated, and only comment stories and         attachment stories can be pinned. Only one of `text` and `html_text` can be specified.          Parameters         ----------         story : {Id} Globally unique identifier for the story.         [data] : {Object} Data for the request           - [text] : {String} The plain text with which to update the comment.           - [html_text] : {String} The rich text with which to update the comment.           - [is_pinned] : {Boolean} Whether the story should be pinned on the resource.
Deletes a story. A user can only delete stories they have created. Returns an empty data record.          Parameters         ----------         story : {Id} Globally unique identifier for the story.
Creating a new task is as easy as POSTing to the `/tasks` endpoint         with a data block containing the fields you'd like to set on the task.         Any unspecified fields will take on default values.                  Every task is required to be created in a specific workspace, and this         workspace cannot be changed once set. The workspace need not be set         explicitly if you specify a `project` or a `parent` task instead.          Parameters         ----------         workspace : {Id} The workspace to create a task in.         [data] : {Object} Data for the request
Returns the complete task record for a single task.          Parameters         ----------         task : {Id} The task to get.         [params] : {Object} Parameters for the request
A specific, existing task can be updated by making a PUT request on the         URL for that task. Only the fields provided in the `data` block will be         updated; any unspecified fields will remain unchanged.                  When using this method, it is best to specify only those fields you wish         to change, or else you may overwrite changes made by another user since         you last retrieved the task.                  Returns the complete updated task record.          Parameters         ----------         task : {Id} The task to update.         [data] : {Object} Data for the request
A specific, existing task can be deleted by making a DELETE request on the         URL for that task. Deleted tasks go into the "trash" of the user making         the delete request. Tasks can be recovered from the trash within a period         of 30 days; afterward they are completely removed from the system.                  Returns an empty data record.          Parameters         ----------         task : {Id} The task to delete.
Returns the compact task records for all tasks within the given project,         ordered by their priority within the project.          Parameters         ----------         projectId : {Id} The project in which to search for tasks.         [params] : {Object} Parameters for the request
Returns the compact task records for all tasks with the given tag.          Parameters         ----------         tag : {Id} The tag in which to search for tasks.         [params] : {Object} Parameters for the request
<b>Board view only:</b> Returns the compact section records for all tasks within the given section.          Parameters         ----------         section : {Id} The section in which to search for tasks.         [params] : {Object} Parameters for the request
The search endpoint allows you to build complex queries to find and fetch exactly the data you need from Asana. For a more comprehensive description of all the query parameters and limitations of this endpoint, see our [long-form documentation](/developers/documentation/getting-started/search-api) for this feature.          Parameters         ----------         workspace : {Id} The workspace or organization in which to search for tasks.         [params] : {Object} Parameters for the request
Returns the compact representations of all of the dependencies of a task.          Parameters         ----------         task : {Id} The task to get dependencies on.         [params] : {Object} Parameters for the request
Returns the compact representations of all of the dependents of a task.          Parameters         ----------         task : {Id} The task to get dependents on.         [params] : {Object} Parameters for the request
Marks a set of tasks as dependencies of this task, if they are not         already dependencies. *A task can have at most 15 dependencies.*          Parameters         ----------         task : {Id} The task to add dependencies to.         [data] : {Object} Data for the request           - dependencies : {Array} An array of task IDs that this task should depend on.
Marks a set of tasks as dependents of this task, if they are not already         dependents. *A task can have at most 30 dependents.*          Parameters         ----------         task : {Id} The task to add dependents to.         [data] : {Object} Data for the request           - dependents : {Array} An array of task IDs that should depend on this task.
Unlinks a set of dependencies from this task.          Parameters         ----------         task : {Id} The task to remove dependencies from.         [data] : {Object} Data for the request           - dependencies : {Array} An array of task IDs to remove as dependencies.
Unlinks a set of dependents from this task.          Parameters         ----------         task : {Id} The task to remove dependents from.         [data] : {Object} Data for the request           - dependents : {Array} An array of task IDs to remove as dependents.
Adds each of the specified followers to the task, if they are not already         following. Returns the complete, updated record for the affected task.          Parameters         ----------         task : {Id} The task to add followers to.         [data] : {Object} Data for the request           - followers : {Array} An array of followers to add to the task.
Removes each of the specified followers from the task if they are         following. Returns the complete, updated record for the affected task.          Parameters         ----------         task : {Id} The task to remove followers from.         [data] : {Object} Data for the request           - followers : {Array} An array of followers to remove from the task.
Returns a compact representation of all of the projects the task is in.          Parameters         ----------         task : {Id} The task to get projects on.         [params] : {Object} Parameters for the request
Adds the task to the specified project, in the optional location         specified. If no location arguments are given, the task will be added to         the end of the project.                  `addProject` can also be used to reorder a task within a project or section that         already contains it.                  At most one of `insert_before`, `insert_after`, or `section` should be         specified. Inserting into a section in an non-order-dependent way can be         done by specifying `section`, otherwise, to insert within a section in a         particular place, specify `insert_before` or `insert_after` and a task         within the section to anchor the position of this task.                  Returns an empty data block.          Parameters         ----------         task : {Id} The task to add to a project.         [data] : {Object} Data for the request           - project : {Id} The project to add the task to.           - [insert_after] : {Id} A task in the project to insert the task after, or `null` to           insert at the beginning of the list.           - [insert_before] : {Id} A task in the project to insert the task before, or `null` to           insert at the end of the list.           - [section] : {Id} A section in the project to insert the task into. The task will be           inserted at the bottom of the section.
Removes the task from the specified project. The task will still exist         in the system, but it will not be in the project anymore.                  Returns an empty data block.          Parameters         ----------         task : {Id} The task to remove from a project.         [data] : {Object} Data for the request           - project : {Id} The project to remove the task from.
Returns a compact representation of all of the tags the task has.          Parameters         ----------         task : {Id} The task to get tags on.         [params] : {Object} Parameters for the request
Adds a tag to a task. Returns an empty data block.          Parameters         ----------         task : {Id} The task to add a tag to.         [data] : {Object} Data for the request           - tag : {Id} The tag to add to the task.
Removes a tag from the task. Returns an empty data block.          Parameters         ----------         task : {Id} The task to remove a tag from.         [data] : {Object} Data for the request           - tag : {Id} The tag to remove from the task.
Returns a compact representation of all of the subtasks of a task.          Parameters         ----------         task : {Id} The task to get the subtasks of.         [params] : {Object} Parameters for the request
Creates a new subtask and adds it to the parent task. Returns the full record         for the newly created subtask.          Parameters         ----------         task : {Id} The task to add a subtask to.         [data] : {Object} Data for the request
Returns a compact representation of all of the stories on the task.          Parameters         ----------         task : {Id} The task containing the stories to get.         [params] : {Object} Parameters for the request
Adds a comment to a task. The comment will be authored by the         currently authenticated user, and timestamped when the server receives         the request.                  Returns the full record for the new story added to the task.          Parameters         ----------         task : {Id} Globally unique identifier for the task.         [data] : {Object} Data for the request           - text : {String} The plain text of the comment to add.
Returns the complete record for a single status update.          Parameters         ----------         project-status : {Id} The project status update to get.         [params] : {Object} Parameters for the request
Deletes a specific, existing project status update.                  Returns an empty data record.          Parameters         ----------         project-status : {Id} The project status update to delete.
Dispatches a request to the Asana HTTP API
Sleep based on the type of :class:`RetryableAsanaError`
Parses GET request options and dispatches a request.
Get a collection from a collection endpoint.          Parses GET request options for a collection endpoint and dispatches a         request.
Parses PUT request options and dispatches a request.
Select all unknown options.          Select all unknown options (not query string, API, or request         options)
Select API options out of the provided options object.          Selects API string options out of the provided options object and         formats for either request body (default) or query string.
Select request options out of the provided options object.           Select and formats options to be passed to the 'requests' library's         request methods.
Select the provided keys out of an options object.           Selects the provided keys (or everything except the provided keys) out         of an options object.
Generate the client version header to send on each request.
Returns the full record for the given webhook.          Parameters         ----------         webhook : {Id} The webhook to get.         [params] : {Object} Parameters for the request
This method permanently removes a webhook. Note that it may be possible         to receive a request that was already in flight after deleting the         webhook, but no further requests will be issued.          Parameters         ----------         webhook : {Id} The webhook to delete.
Dispatches a GET request to /events of the API to get a set of recent changes to a resource.
Returns a tuple containing the next page of events and a sync token for the given query (and optional 'sync' token)
Returns an event iterator for the given query (and optional 'sync' token)
Creates a project shared with the given team.                  Returns the full record of the newly created project.          Parameters         ----------         team : {Id} The team to create the project in.         [data] : {Object} Data for the request
Returns the complete project record for a single project.          Parameters         ----------         project : {Id} The project to get.         [params] : {Object} Parameters for the request
A specific, existing project can be updated by making a PUT request on the         URL for that project. Only the fields provided in the `data` block will be         updated; any unspecified fields will remain unchanged.                  When using this method, it is best to specify only those fields you wish         to change, or else you may overwrite changes made by another user since         you last retrieved the task.                  Returns the complete updated project record.          Parameters         ----------         project : {Id} The project to update.         [data] : {Object} Data for the request
A specific, existing project can be deleted by making a DELETE request         on the URL for that project.                  Returns an empty data record.          Parameters         ----------         project : {Id} The project to delete.
Returns the compact project records for all projects in the team.          Parameters         ----------         team : {Id} The team to find projects in.         [params] : {Object} Parameters for the request           - [archived] : {Boolean} Only return projects whose `archived` field takes on the value of           this parameter.
Returns the compact task records for all tasks within the given project,         ordered by their priority within the project. Tasks can exist in more than one project at a time.          Parameters         ----------         project : {Id} The project in which to search for tasks.         [params] : {Object} Parameters for the request
Adds the specified list of users as followers to the project. Followers are a subset of members, therefore if         the users are not already members of the project they will also become members as a result of this operation.         Returns the updated project record.          Parameters         ----------         project : {Id} The project to add followers to.         [data] : {Object} Data for the request           - followers : {Array} An array of followers to add to the project.
Removes the specified list of users from following the project, this will not affect project membership status.         Returns the updated project record.          Parameters         ----------         project : {Id} The project to remove followers from.         [data] : {Object} Data for the request           - followers : {Array} An array of followers to remove from the project.
Adds the specified list of users as members of the project. Returns the updated project record.          Parameters         ----------         project : {Id} The project to add members to.         [data] : {Object} Data for the request           - members : {Array} An array of members to add to the project.
Removes the specified list of members from the project. Returns the updated project record.          Parameters         ----------         project : {Id} The project to remove members from.         [data] : {Object} Data for the request           - members : {Array} An array of members to remove from the project.
Create a new custom field setting on the project.          Parameters         ----------         project : {Id} The project to associate the custom field with         [data] : {Object} Data for the request           - custom_field : {Id} The id of the custom field to associate with this project.           - [is_important] : {Boolean} Whether this field should be considered important to this project.           - [insert_before] : {Id} An id of a Custom Field Settings on this project, before which the new Custom Field Settings will be added.           `insert_before` and `insert_after` parameters cannot both be specified.           - [insert_after] : {Id} An id of a Custom Field Settings on this project, after which the new Custom Field Settings will be added.           `insert_before` and `insert_after` parameters cannot both be specified.
Remove a custom field setting on the project.          Parameters         ----------         project : {Id} The project to associate the custom field with         [data] : {Object} Data for the request           - [custom_field] : {Id} The id of the custom field to remove from this project.
Returns the full user record for the single user with the provided ID.          Parameters         ----------         user : {String} An identifier for the user. Can be one of an email address,         the globally unique identifier for the user, or the keyword `me`         to indicate the current user making the request.         [params] : {Object} Parameters for the request
Returns the shipping cost for a given country     If the shipping cost for the given country has not been set, it will     fallback to the default shipping cost if it has been enabled in the app     settings
Get all shipping countries
Get the shipping options for a given country
Return the shipping cost for a given country code and shipping option (shipping rate name)
Add an item to the basket
Put multiple items in the basket,         removing anything that already exists
Remove an item from the basket
Get total number of items in the basket
Get quantity of a single item in the basket
Compile the front end assets
Table display of each request for a given product.      Allows the given Page pk to refer to a direct parent of     the ProductVariant model or be the ProductVariant model itself.     This allows for the standard longclaw product modelling philosophy where     ProductVariant refers to the actual product (in the case where there is     only 1 variant) or to be variants of the product page.
Refund the order specified by the pk
Mark the order specified by pk as fulfilled
Renders a 'requests' button on the page index showing the number     of times the product has been requested.      Attempts to only show such a button for valid product/variant pages
Template tag which provides a `script` tag for each javascript item     required by the payment gateway
Get all items in the basket
Delete all items in the basket
Return the shipping rate for a country & shipping option name.
Calculate the price range of the products variants
Create a new django project using the longclaw template
Build the longclaw assets
Setup the parser and call the command function
Get all sales for a given time period
Generic function for creating a payment token from the     payment backend. Some payment backends (e.g. braintree) support creating a payment     token, which should be imported from the backend as 'get_token'
Create an order using an existing transaction ID.     This is useful for capturing the payment outside of     longclaw - e.g. using paypals' express checkout or     similar
Capture the payment for a basket and create an order      request.data should contain:      'address': Dict with the following fields:         shipping_name         shipping_address_line1         shipping_address_city         shipping_address_zip         shipping_address_country         billing_name         billing_address_line1         billing_address_city         billing_address_zip         billing_address_country      'email': Email address of the customer     'shipping': The shipping rate (in the sites' currency)
Create an order from a basket and customer infomation
Create a new product request
Get all the requests for a single variant
Dummy function for creating a payment through a payment gateway.         Should be overridden in gateway implementations.         Can be used for testing - to simulate a failed payment/error,         pass `error: true` in the request data.
Check user has cookies enabled
Instantiates a class-based view to provide 'inspect' functionality for         the assigned model. The view class used can be overridden by changing         the 'inspect_view_class' attribute.
Utilised by Wagtail's 'register_admin_urls' hook to register urls for         our the views that class offers.
Create a stripe token for a card
Total cost of the order
Issue a full refund for this order
Cancel this order, optionally refunding it
Sets the attribute_name of this CatalogQueryRange.         The name of the attribute to be searched.          :param attribute_name: The attribute_name of this CatalogQueryRange.         :type: str
Sets the idempotency_key of this BatchUpsertCatalogObjectsRequest.         A value you specify that uniquely identifies this request among all your requests. A common way to create a valid idempotency key is to use a Universally unique identifier (UUID).  If you're unsure whether a particular request was successful, you can reattempt it with the same idempotency key without worrying about creating duplicate objects.  See [Idempotency](/basics/api101/idempotency) for more information.          :param idempotency_key: The idempotency_key of this BatchUpsertCatalogObjectsRequest.         :type: str
Sets the start_of_day_local_time of this WorkweekConfig.         The local time at which a business week cuts over. Represented as a string in `HH:MM` format (`HH:MM:SS` is also accepted, but seconds are truncated).          :param start_of_day_local_time: The start_of_day_local_time of this WorkweekConfig.         :type: str
:param method: http request method         :param url: http request url         :param query_params: query parameters in the url         :param headers: http request headers         :param body: request json body, for `application/json`         :param post_params: request post parameters,                             `application/x-www-form-urlencode`                             and `multipart/form-data`
Sets the catalog_object_id of this CreateOrderRequestModifier.         The catalog object ID of a [CatalogModifier](#type-catalogmodifier).          :param catalog_object_id: The catalog_object_id of this CreateOrderRequestModifier.         :type: str
Sets the domain_name of this RegisterDomainRequest.         A domain name as described in RFC-1034 that will be registered with ApplePay          :param domain_name: The domain_name of this RegisterDomainRequest.         :type: str
Sets the attribute_prefix of this CatalogQueryPrefix.         The desired prefix of the search attribute value.          :param attribute_prefix: The attribute_prefix of this CatalogQueryPrefix.         :type: str
Sets the id of this Shift.         UUID for this object          :param id: The id of this Shift.         :type: str
Sets the employee_id of this Shift.         The ID of the employee this shift belongs to.          :param employee_id: The employee_id of this Shift.         :type: str
Sets the start_at of this Shift.         RFC 3339; shifted to location timezone + offset. Precision up to the minute is respected; seconds are truncated.          :param start_at: The start_at of this Shift.         :type: str
Sets the percentage of this OrderLineItemTax.         The percentage of the tax, as a string representation of a decimal number.  A value of `7.25` corresponds to a percentage of 7.25%.          :param percentage: The percentage of this OrderLineItemTax.         :type: str
Sets the modifier_list_id of this CatalogItemModifierListInfo.         The ID of the [CatalogModifierList](#type-catalogmodifierlist) controlled by this [CatalogModifierListInfo](#type-catalogmodifierlistinfo).          :param modifier_list_id: The modifier_list_id of this CatalogItemModifierListInfo.         :type: str
Sets the location_id of this Order.         The ID of the merchant location this order is associated with.          :param location_id: The location_id of this Order.         :type: str
Sets the reference_id of this Order.         A client specified identifier to associate an entity in another system with this order.          :param reference_id: The reference_id of this Order.         :type: str
Sets the note of this OrderFulfillmentPickupDetails.         A general note about the pickup fulfillment.  Notes are useful for providing additional instructions and are displayed in Square apps.          :param note: The note of this OrderFulfillmentPickupDetails.         :type: str
Sets the cancel_reason of this OrderFulfillmentPickupDetails.         A description of why the pickup was canceled. Max length is 100 characters.          :param cancel_reason: The cancel_reason of this OrderFulfillmentPickupDetails.         :type: str
Sets the modifier_id of this CatalogModifierOverride.         The ID of the [CatalogModifier](#type-catalogmodifier) whose default behavior is being overridden.          :param modifier_id: The modifier_id of this CatalogModifierOverride.         :type: str
Sets the merchant_support_email of this CreateCheckoutRequest.         The email address to display on the Square Checkout confirmation page and confirmation email that the buyer can use to contact the merchant.  If this value is not set, the confirmation page and email will display the primary email address associated with the merchant's Square account.  Default: none; only exists if explicitly set.          :param merchant_support_email: The merchant_support_email of this CreateCheckoutRequest.         :type: str
Sets the pre_populate_buyer_email of this CreateCheckoutRequest.         If provided, the buyer's email is pre-populated on the checkout page as an editable text field.  Default: none; only exists if explicitly set.          :param pre_populate_buyer_email: The pre_populate_buyer_email of this CreateCheckoutRequest.         :type: str
Sets the redirect_url of this CreateCheckoutRequest.         The URL to redirect to after checkout is completed with `checkoutId`, Square's `orderId`, `transactionId`, and `referenceId` appended as URL parameters. For example, if the provided redirect_url is `http://www.example.com/order-complete`, a successful transaction redirects the customer to:  `http://www.example.com/order-complete?checkoutId=xxxxxx&orderId=xxxxxx&referenceId=xxxxxx&transactionId=xxxxxx`  If you do not provide a redirect URL, Square Checkout will display an order confirmation page on your behalf; however Square strongly recommends that you provide a redirect URL so you can verify the transaction results and finalize the order through your existing/normal confirmation workflow.  Default: none; only exists if explicitly set.          :param redirect_url: The redirect_url of this CreateCheckoutRequest.         :type: str
Sets the description of this AdditionalRecipient.         The description of the additional recipient.          :param description: The description of this AdditionalRecipient.         :type: str
Sets the break_type_id of this ModelBreak.         The `BreakType` this `Break` was templated on.          :param break_type_id: The break_type_id of this ModelBreak.         :type: str
Sets the expected_duration of this ModelBreak.         Format: RFC-3339 P[n]Y[n]M[n]DT[n]H[n]M[n]S. The expected length of the break.          :param expected_duration: The expected_duration of this ModelBreak.         :type: str
Sets the limit of this ListEmployeeWagesRequest.         Maximum number of Employee Wages to return per page. Can range between 1 and 200. The default is the maximum at 200.          :param limit: The limit of this ListEmployeeWagesRequest.         :type: int
Sets the receivable_id of this AdditionalRecipientReceivableRefund.         The ID of the receivable that the refund was applied to.          :param receivable_id: The receivable_id of this AdditionalRecipientReceivableRefund.         :type: str
Sets the refund_id of this AdditionalRecipientReceivableRefund.         The ID of the refund that is associated to this receivable refund.          :param refund_id: The refund_id of this AdditionalRecipientReceivableRefund.         :type: str
Sets the transaction_location_id of this AdditionalRecipientReceivableRefund.         The ID of the location that created the receivable. This is the location ID on the associated transaction.          :param transaction_location_id: The transaction_location_id of this AdditionalRecipientReceivableRefund.         :type: str
Sets the card_nonce of this ChargeRequest.         A nonce generated from the `SqPaymentForm` that represents the card to charge.  The application that provides a nonce to this endpoint must be the _same application_ that generated the nonce with the `SqPaymentForm`. Otherwise, the nonce is invalid.  Do not provide a value for this field if you provide a value for `customer_card_id`.          :param card_nonce: The card_nonce of this ChargeRequest.         :type: str
Sets the customer_card_id of this ChargeRequest.         The ID of the customer card on file to charge. Do not provide a value for this field if you provide a value for `card_nonce`.  If you provide this value, you _must_ also provide a value for `customer_id`.          :param customer_card_id: The customer_card_id of this ChargeRequest.         :type: str
Sets the customer_id of this ChargeRequest.         The ID of the customer to associate this transaction with. This field is required if you provide a value for `customer_card_id`, and optional otherwise.          :param customer_id: The customer_id of this ChargeRequest.         :type: str
Sets the order_id of this ChargeRequest.         The ID of the order to associate with this transaction.  If you provide this value, the `amount_money` value of your request must __exactly match__ the value of the order's `total_money` field.          :param order_id: The order_id of this ChargeRequest.         :type: str
Sets the quantity of this OrderLineItem.         The quantity purchased, as a string representation of a number.  This string must have a positive integer value.          :param quantity: The quantity of this OrderLineItem.         :type: str
Sets the variation_name of this OrderLineItem.         The name of the variation applied to this line item.          :param variation_name: The variation_name of this OrderLineItem.         :type: str
Sets the transaction_id of this AdditionalRecipientReceivable.         The ID of the transaction that the additional recipient receivable was applied to.          :param transaction_id: The transaction_id of this AdditionalRecipientReceivable.         :type: str
Sets the page_index of this V1Page.         The page's position in the merchant's list of pages. Always an integer between 0 and 6, inclusive.          :param page_index: The page_index of this V1Page.         :type: int
Sets the break_name of this BreakType.         A human-readable name for this type of break. Will be displayed to employees in Square products.          :param break_name: The break_name of this BreakType.         :type: str
Sets the display_name of this OrderFulfillmentRecipient.         The display name of the fulfillment recipient.  If provided, overrides the value from customer profile indicated by customer_id.          :param display_name: The display_name of this OrderFulfillmentRecipient.         :type: str
Sets the email_address of this OrderFulfillmentRecipient.         The email address of the fulfillment recipient.  If provided, overrides the value from customer profile indicated by customer_id.          :param email_address: The email_address of this OrderFulfillmentRecipient.         :type: str
Sets the phone_number of this OrderFulfillmentRecipient.         The phone number of the fulfillment recipient.  If provided, overrides the value from customer profile indicated by customer_id.          :param phone_number: The phone_number of this OrderFulfillmentRecipient.         :type: str
Sets the amount of this Money.         The amount of money, in the smallest denomination of the currency indicated by `currency`. For example, when `currency` is `USD`, `amount` is in cents.          :param amount: The amount of this Money.         :type: int
Sets the tender_id of this CreateRefundRequest.         The ID of the tender to refund.  A [`Transaction`](#type-transaction) has one or more `tenders` (i.e., methods of payment) associated with it, and you refund each tender separately with the Connect API.          :param tender_id: The tender_id of this CreateRefundRequest.         :type: str
Sets the reason of this CreateRefundRequest.         A description of the reason for the refund.  Default value: `Refund via API`          :param reason: The reason of this CreateRefundRequest.         :type: str
Takes value and turn it into a string suitable for inclusion in         the path, by url-encoding.          :param obj: object or string value.          :return string: quoted value.
Builds a JSON POST object.          If obj is None, return None.         If obj is str, int, float, bool, return directly.         If obj is datetime.datetime, datetime.date             convert to string in iso8601 format.         If obj is list, sanitize each element in the list.         If obj is dict, return the dict.         If obj is swagger model, return the properties dict.          :param obj: The data to serialize.         :return: The serialized form of data.
Deserializes response into an object.          :param response: RESTResponse object to be deserialized.         :param response_type: class literal for             deserialzied object, or string of class name.          :return: deserialized object.
Deserializes dict, list, str into an object.          :param data: dict, list or str.         :param klass: class literal, or string of class name.          :return: object.
Returns `Accept` based on an array of accepts provided.          :param accepts: List of headers.         :return: Accept (e.g. application/json).
Deserializes string to primitive type.          :param data: str.         :param klass: class literal.          :return: int, float, str, bool.
Aligns the AST so that the argument with the highest cardinality is on the left.          :return: a new AST.
This function processes the list of truisms and finds bounds for ASTs.
Checks whether we can handle this truism. The truism should already be aligned.
Swap the operands of the truism if the unknown variable is on the right side and the concrete value is on the         left side.
Given a constraint, _get_assumptions() returns a set of constraints that are implicitly         assumed to be true. For example, `x <= 10` would return `x >= 0`.
Given a constraint, _unpack_truisms() returns a set of constraints that must be True         this constraint to be True.
Handles all comparisons.
Parses a list of expressions from the tokens
Make a copy of self and return.          :return: A new ValueSet object.         :rtype: ValueSet
Apply a new annotation onto self, and return a new ValueSet object.          :param RegionAnnotation annotation: The annotation to apply.         :return: A new ValueSet object         :rtype: ValueSet
The minimum integer value of a value-set. It is only defined when there is exactly one region.          :return: A integer that represents the minimum integer value of this value-set.         :rtype:  int
The maximum integer value of a value-set. It is only defined when there is exactly one region.          :return: A integer that represents the maximum integer value of this value-set.         :rtype:  int
Operation extract          - A cheap hack is implemented: a copy of self is returned if (high_bit - low_bit + 1 == self.bits), which is a             ValueSet instance. Otherwise a StridedInterval is returned.          :param high_bit:         :param low_bit:         :return: A ValueSet or a StridedInterval
Used to make exact comparisons between two ValueSets.          :param o:   The other ValueSet to compare with.         :return:    True if they are exactly same, False otherwise.
Evaluates expression e, returning the results in the form of concrete ASTs.
Returns independent constraints, split from this Frontend's `constraints`.
Creates a boolean symbol (i.e., a variable).      :param name:            The name of the symbol     :param explicit_name:   If False, an identifier is appended to the name to ensure uniqueness.      :return:                A Bool object representing this symbol.
Convert a constraint to SI if possible.      :param expr:     :return:
Fill up `self._op_expr` dict.          :param op_list:     A list of operation names.         :param op_dict:     A dictionary of operation methods.         :param op_class:    Where the operation method comes from.         :return:
Clears all caches associated with this backend.
Resolves a claripy.ast.Base into something usable by the backend.          :param expr:    The expression.         :param save:    Save the result in the expression's object cache         :return:        A backend object.
Calls operation `op` on args `args` with this backend.          :return:   A backend object representing the result.
_call          :param op:         :param args:         :return:
Should return True if `e` can be easily found to be True.          :param e:                   The AST.         :param extra_constraints:   Extra constraints (as ASTs) to add to the solver for this solve.         :param solver:              A solver, for backends that require it.         :param model_callback:      a function that will be executed with recovered models (if any)         :returns:                   A boolean.
Should return True if e can be easily found to be False.          :param e:                   The AST         :param extra_constraints:   Extra constraints (as ASTs) to add to the solver for this solve.         :param solver:              A solver, for backends that require it         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                   A boolean.
Should return True if `e` can possible be True.          :param e:                   The AST.         :param extra_constraints:   Extra constraints (as ASTs) to add to the solver for this solve.         :param solver:              A solver, for backends that require it.         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                   A boolean
Should return False if `e` can possibly be False.          :param e:                   The AST.         :param extra_constraints:   Extra constraints (as ASTs) to add to the solver for this solve.         :param solver:              A solver, for backends that require it.         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                   A boolean.
This function adds constraints to the backend solver.          :param c: A sequence of ASTs         :param s: A backend solver object         :param bool track: True to enable constraint tracking, which is used in unsat_core()
This function returns the unsat core from the backend solver.          :param s: A backend solver object.         :return: The unsat core.
This function returns up to `n` possible solutions for expression `expr`.          :param expr: expression (an AST) to evaluate         :param n: number of results to return         :param solver: a solver object, native to the backend, to assist in                        the evaluation (for example, a z3.Solver)         :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve         :param model_callback:      a function that will be executed with recovered models (if any)         :return:              A sequence of up to n results (backend objects)
Evaluate one or multiple expressions.          :param exprs:               A list of expressions to evaluate.         :param n:                   Number of different solutions to return.         :param extra_constraints:   Extra constraints (as ASTs) to add to the solver for this solve.         :param solver:              A solver object, native to the backend, to assist in the evaluation.         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                    A list of up to n tuples, where each tuple is a solution for all expressions.
Return the minimum value of `expr`.          :param expr: expression (an AST) to evaluate         :param solver: a solver object, native to the backend, to assist in                        the evaluation (for example, a z3.Solver)         :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve         :param model_callback:      a function that will be executed with recovered models (if any)         :return: the minimum possible value of expr (backend object)
Return the maximum value of expr.          :param expr: expression (an AST) to evaluate         :param solver: a solver object, native to the backend, to assist in                        the evaluation (for example, a z3.Solver)         :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve         :param model_callback:      a function that will be executed with recovered models (if any)         :return: the maximum possible value of expr (backend object)
This function does a constraint check and returns the solvers state          :param solver:              The backend solver object.         :param extra_constraints:   Extra constraints (as ASTs) to add to s for this solve         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                    'SAT', 'UNSAT', or 'UNKNOWN'
This function does a constraint check and returns the solvers state          :param solver:              The backend solver object.         :param extra_constraints:   Extra constraints (as ASTs) to add to s for this solve         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                    'SAT', 'UNSAT', or 'UNKNOWN'
This function does a constraint check and checks if the solver is in a sat state.          :param solver:              The backend solver object.         :param extra_constraints:   Extra constraints (as ASTs) to add to s for this solve         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                    True if sat, otherwise false
Return True if `v` is a solution of `expr` with the extra constraints, False otherwise.          :param expr:                An expression (an AST) to evaluate         :param v:                   The proposed solution (an AST)         :param solver:              A solver object, native to the backend, to assist in the evaluation (for example,                                     a z3.Solver).         :param extra_constraints:   Extra constraints (as ASTs) to add to the solver for this solve.         :param model_callback:      a function that will be executed with recovered models (if any)         :return:                    True if `v` is a solution of `expr`, False otherwise
This should return whether `a` is identical to `b`. Of course, this isn't always clear. True should mean that it         is definitely identical. False eans that, conservatively, it might not be.          :param a: an AST         :param b: another AST
:param name:     :param bits:     :param stride:     :param lower_bound:     :param upper_bound:     :param to_conv:     :param bool discrete_set:     :param int discrete_set_max_cardinality:     :return:
Lower bound of result of ORing 2-intervals.          :param a: Lower bound of first interval         :param b: Upper bound of first interval         :param c: Lower bound of second interval         :param d: Upper bound of second interval         :param w: bit width         :return: Lower bound of ORing 2-intervals
Upper bound of result of ORing 2-intervals.          :param a: Lower bound of first interval         :param b: Upper bound of first interval         :param c: Lower bound of second interval         :param d: Upper bound of second interval         :param w: bit width         :return: Upper bound of ORing 2-intervals
Lower bound of result of ANDing 2-intervals.          :param a: Lower bound of first interval         :param b: Upper bound of first interval         :param c: Lower bound of second interval         :param d: Upper bound of second interval         :param w: bit width         :return: Lower bound of ANDing 2-intervals
Upper bound of result of ANDing 2-intervals.          :param a: Lower bound of first interval         :param b: Upper bound of first interval         :param c: Lower bound of second interval         :param d: Upper bound of second interval         :param w: bit width         :return: Upper bound of ANDing 2-intervals
Lower bound of result of XORing 2-intervals.          :param a: Lower bound of first interval         :param b: Upper bound of first interval         :param c: Lower bound of second interval         :param d: Upper bound of second interval         :param w: bit width         :return: Lower bound of XORing 2-intervals
Upper bound of result of XORing 2-intervals.          :param a: Lower bound of first interval         :param b: Upper bound of first interval         :param c: Lower bound of second interval         :param d: Upper bound of second interval         :param w: bit width         :return: Upper bound of XORing 2-intervals
Evaluate this StridedInterval to obtain a list of concrete integers.          :param n: Upper bound for the number of concrete integers         :param signed: Treat this StridedInterval as signed or unsigned         :return: A list of at most `n` concrete integers
Checks whether an integer is solution of the current strided Interval         :param b: integer to check         :return: True if b belongs to the current Strided Interval, False otherwhise
Split `self` at the south pole, which is the same as in unsigned arithmetic.         When returning two StridedIntervals (which means a splitting occurred), it is guaranteed that the first         StridedInterval is on the right side of the south pole.          :return: a list of split StridedIntervals, that contains either one or two StridedIntervals
Split `self` at the north pole, which is the same as in signed arithmetic.          :return: A list of split StridedIntervals
Split `self` at both north and south poles.          :return: A list of split StridedIntervals
Get lower bound and upper bound for `self` in signed arithmetic.          :return: a list of (lower_bound, upper_bound) tuples
Get lower bound and upper bound for `self` in unsigned arithmetic.          :return: a list of (lower_bound, upper_bound) tuples.
Logical shift right with a concrete shift amount          :param int shift_amount: Number of bits to shift right.         :return: The new StridedInterval after right shifting         :rtype: StridedInterval
Arithmetic shift right with a concrete shift amount          :param int shift_amount: Number of bits to shift right.         :return: The new StridedInterval after right shifting         :rtype: StridedInterval
Used to make exact comparisons between two StridedIntervals. Usually it is only used in test cases.          :param o: The other StridedInterval to compare with.         :return: True if they are exactly same, False otherwise.
Signed less than          :param o: The other operand         :return: TrueResult(), FalseResult(), or MaybeResult()
Unsigned less than.          :param o: The other operand         :return: TrueResult(), FalseResult(), or MaybeResult()
Equal          :param o: The ohter operand         :return: TrueResult(), FalseResult(), or MaybeResult()
Return the complement of the interval         Refer section 3.1 augmented for managing strides          :return:
If this is a TOP value.          :return: True if this is a TOP
Refer section 3.1; gap function.          :param src_interval: first argument or interval 1         :param tar_interval: second argument or interval 2         :return: Interval representing gap between two intervals
Get a TOP StridedInterval.          :return:
Return the cardinality for a set of number (| x, y |) on the wrapped-interval domain.          :param x: The first operand (an integer)         :param y: The second operand (an integer)         :return: The cardinality
Convert an unsigned integer to a signed integer.          :param v: The unsigned integer         :param bits: How many bits this integer should be         :return: The converted signed integer
Determines if an overflow happens during the addition of `a` and `b`.          :param a: The first operand (StridedInterval)         :param b: The other operand (StridedInterval)         :return: True if overflows, False otherwise
Perform wrapped unsigned multiplication on two StridedIntervals.          :param a: The first operand (StridedInterval)         :param b: The second operand (StridedInterval)         :return: The multiplication result
Perform wrapped signed multiplication on two StridedIntervals.          :param a: The first operand (StridedInterval)         :param b: The second operand (StridedInterval)         :return: The product
Perform wrapped unsigned division on two StridedIntervals.          :param a: The dividend (StridedInterval)         :param b: The divisor (StridedInterval)         :return: The quotient
Perform wrapped unsigned division on two StridedIntervals.          :param a: The dividend (StridedInterval)         :param b: The divisor (StridedInterval)         :return: The quotient
Perform a wrapped LTE comparison only considering the SI bounds          :param a: The first operand         :param b: The second operand         :return: True if a <= b, False otherwise
Unary operation: neg          :return: 0 - self
Binary operation: add          :param b: The other operand         :return: self + b
Binary operation: sub          :param b: The other operand         :return: self - b
Binary operation: multiplication          :param o: The other operand         :return: self * o
Binary operation: signed division          :param o: The divisor         :return: (self / o) in signed arithmetic
Binary operation: unsigned division          :param o: The divisor         :return: (self / o) in unsigned arithmetic
Unary operation: bitwise not          :return: ~self
Binary operation: logical or          :param b: The other operand         :return: self | b
Binary operation: logical and          :param b: The other operand         :return:
Operation xor          :param t:   The other operand.
Logical shift right.          :param StridedInterval shift_amount: The amount of shifting         :return: The shifted StridedInterval         :rtype: StridedInterval
Arithmetic shift right.          :param StridedInterval shift_amount: The amount of shifting         :return: The shifted StridedInterval         :rtype: StridedInterval
Unary operation: SignExtend          :param new_length: New length after sign-extension         :return: A new StridedInterval
Unary operation: ZeroExtend          :param new_length: New length after zero-extension         :return: A new StridedInterval
Unary operation: SignExtend          :param new_length: New length after sign-extension         :return: A new StridedInterval
The union operation. It might return a DiscreteStridedIntervalSet to allow for better precision in analysis.          :param b: Operand         :return: A new DiscreteStridedIntervalSet, or a new StridedInterval.
Return interval with bigger cardinality         Refer Section 3.1          :param interval1: first interval         :param interval2: second interval         :return: Interval or interval2 whichever has greater cardinality
Get the number of consecutive zeros         :param x:         :return:
Pseudo least upper bound.         Join the given set of intervals into a big interval. The resulting strided interval is the one which in         all the possible joins of the presented SI, presented the least number of values.          The number of joins to compute is linear with the number of intervals to join.          Draft of proof:         Considering  three generic SI (a,b, and c) ordered from their lower bounds, such that         a.lower_bund <= b.lower_bound <= c.lower_bound, where <= is the lexicographic less or equal.         The only joins which have sense to compute are:         * a U b U c         * b U c U a         * c U a U b          All the other combinations fall in either one of these cases. For example: b U a U c does not make make sense         to be calculated. In fact, if one draws this union, the result is exactly either (b U c U a) or (a U b U c) or         (c U a U b).         :param intervals_to_join: Intervals to join         :return: Interval that contains all intervals
It two intervals in a way that the resulting SI is the one that has the least         SI cardinality (i.e., which represents the least number of elements) possible if the smart_join flag is enabled,         otherwise it just joins the SI according the order they are passed to the function.          The pseudo-join operation is not associative in wrapping intervals (please refer to section 3.1 paper         'Signedness-Agnostic Program Analysis: Precise Integer Bounds for Low-Level Code'), Therefore the join of three         WI may  give us different results according on the order we join them. All of the results will be sound, though.          Please use the function least_upper_bound as a stub.          :param s:           The first SI         :param b:           The other SI.         :param smart_join:  Enable the smart join behavior. If this flag is set, this function joins the two SI in a way                             that the resulting Si has least number of elements (more precise). If it is unset, this                             function will join the two SI according on the order they are passed to the function.         :return:            A new StridedInterval
Calculates the minimal integer that appears in both StridedIntervals.         As a wrapper method of _minimal_common_integer_splitted(), this method takes arbitrary StridedIntervals.         For more information, please refer to the comment of _minimal_common_integer_splitted().          :param si_0:   the first StridedInterval         :type  si_0:   StridedInterval         :param si_1:   the second StridedInterval         :type  si_1:   StridedInterval          :return: the minimal common integer, or None if there is no common integer
It calculates the GCD of a and b, and two values x and y such that:         a*x + b*y = GCD(a,b).         This code has been taken from the project sympy.          :param a: first integer         :param b: second integer         :return: x,y and the GCD of a and b
:param a: First integer         :param b: Second integer         :return: the integer GCD between a and b
It finds the fist natural solution of the diophantine equation         a*x + b*y = c. Some lines of this code are taken from the project         sympy.          :param c: constant         :param a: quotient of x         :param b: quotient of y         :return: the first natural solution of the diophatine equation
Calculates the minimal integer that appears in both StridedIntervals.         It's equivalent to finding an integral solution for equation `ax + b = cy + d` that makes `ax + b` minimal         si_0.stride, si_1.stride being a and c, and si_0.lower_bound, si_1.lower_bound being b and d, respectively.         Upper bounds are used to check whether the minimal common integer exceeds the bound or not. None is returned         if no minimal common integers can be found within the range.          Some assumptions:         # - None of the StridedIntervals straddles the south pole. Consequently, we have x <= max_int(si.bits) and y <=         #   max_int(si.bits)         # - a, b, c, d are all positive integers         # - x >= 0, y >= 0          :param StridedInterval si_0: the first StridedInterval         :param StridedInterval si_1: the second StrideInterval         :return: the minimal common integer, or None if there is no common integer
This is a delayed reversing function. All it really does is to invert the _reversed property of this         StridedInterval object.          :return: None
This method reverses the StridedInterval object for real. Do expect loss of precision for most cases!          :return: A new reversed StridedInterval instance
This method reverses the StridedInterval object for real. Do expect loss of precision for most cases!          :return: A new reversed StridedInterval instance
Return an smt-lib script that check the satisfiability of the current constraints          :return string: smt-lib script
Apply an annotation on the backend object.          :param BackendObject bo: The backend object.         :param Annotation annotation: The annotation to be applied         :return: A new BackendObject         :rtype: BackendObject
Converts a Z3 model to a name->primitive dict.
This function is the deserializer for ASTs.     It exists to work around the fact that pickle will (normally) call __new__() with no arguments during deserialization.     For ASTs, this does not work.
Calculates the hash of an AST, given the operation, args, and kwargs.          :param op:          The operation.         :param args:        The arguments to the operation.         :param keywords:    A dict including the 'symbolic', 'variables', and 'length' items.         :returns:           a hash.          We do it using md5 to avoid hash collisions.         (hash(-1) == hash(-2), for example)
Removes an annotation from this AST.          :param a: the annotation to remove         :returns: a new AST, with the annotation removed
Removes several annotations from this AST.          :param remove_sequence: a sequence/set of the annotations to remove         :returns: a new AST, with the annotations removed
Returns a string representation of this AST, but with a maximum depth to         prevent floods of text being printed.          :param max_depth:           The maximum depth to print.         :param explicit_length:     Print lengths of BVV arguments.         :param details:             An integer value specifying how detailed the output should be:                                         LITE_REPR - print short repr for both operations and BVs,                                         MID_REPR  - print full repr for operations and short for BVs,                                         FULL_REPR - print full repr of both operations and BVs.         :return:                    A string representing the AST
Return an iterator over the nested children ASTs.
Return an iterator over the leaf ASTs.
This returns the same AST, with the arguments swapped out for new_args.
Splits the AST if its operation is `split_on` (i.e., return all the arguments). Otherwise, return a list with         just the AST.
Structurally compares two A objects, and check if their corresponding leaves are definitely the same A object         (name-wise or hash-identity wise).          :param o: the other claripy A object         :return: True/False
Returns this AST with subexpressions replaced by those that can be found in `replacements` dict.          :param variable_set:    For optimization, ast's without these variables are not checked for replacing.         :param replacements:    A dictionary of hashes to their replacements.         :param leaf_operation:  An operation that should be applied to the leaf nodes.         :return:                An AST with all instances of ast's in replacements.
Returns this AST but with the AST 'old' replaced with AST 'new' in its subexpressions.
Returns an equivalent AST that "burrows" the ITE expressions as deep as possible into the ast, for simpler         printing.
Returns an equivalent AST that "excavates" the ITE expressions out as far as possible toward the root of the         AST, for processing in static analyses.
Handles the following case:             ((A << a) | (A >> (_N - a))) & mask, where                 A being a BVS,                 a being a integer that is less than _N,                 _N is either 32 or 64, and                 mask can be evaluated to 0xffffffff (64-bit) or 0xffff (32-bit) after reversing the rotate-shift                 operation.          It will be simplified to:             (A & (mask >>> a)) <<< a
Creates a floating-point symbol.      :param name:            The name of the symbol     :param sort:            The sort of the floating point     :param explicit_name:   If False, an identifier is appended to the name to ensure uniqueness.     :return:                An FP AST.
Convert this float to a different sort          :param sort:    The sort to convert to         :param rm:      Optional: The rounding mode to use         :return:        An FP AST
Convert this floating point value to an integer.          :param size:    The size of the bitvector to return         :param signed:  Optional: Whether the target integer is signed         :param rm:      Optional: The rounding mode to use         :return:        A bitvector whose value is the rounded version of this FP's value
This is an over-approximation of the cardinality of this DSIS.          :return:
Collapse into a StridedInterval instance.          :return: A new StridedInterval instance.
Return the collapsed object if ``should_collapse()`` is True, otherwise return self.          :return: A DiscreteStridedIntervalSet object.
Operation extract          :param high_bit:    The highest bit to begin extraction.         :param low_bit:     The lowest bit to end extraction.         :return:            Extracted bits.
Union with another StridedInterval.          :param si:         :return:
Union with another DiscreteStridedIntervalSet.          :param dsis:         :return:
Intersection with another :class:`StridedInterval`.          :param si: The other operand         :return:
Intersection with another :class:`DiscreteStridedIntervalSet`.          :param dsis:    The other operand.         :return:
Dump the symbol in its smt-format depending on its type      :param e: symbol to dump     :param daggify: The daggify parameter can be used to switch from a linear-size representation that uses ‘let’                     operators to represent the formula as a dag or a simpler (but possibly exponential) representation                     that expands the formula as a tree      :return string: smt-lib representation of the symbol
Since we decided to emulate integer with bitvector, this method transform their     concrete value (if any) in the corresponding integer
Returns a SMT script that declare all the symbols and constraint and checks         their satisfiability (check-sat)          :param extra-constraints: list of extra constraints that we want to evaluate only                                  in the scope of this call                                          :return string: smt-lib representation of the script that checks the satisfiability
Returns a SMT script that declare all the symbols and constraint and checks         their satisfiability (check-sat)          :param extra-constraints: list of extra constraints that we want to evaluate only                                  in the scope of this call          :return string: smt-lib representation of the script that checks the satisfiability
Create a new symbolic string (analogous to z3.String())      :param name:                 The name of the symbolic string (i. e. the name of the variable)     :param size:                 The size in bytes of the string (i. e. the length of the string)     :param uninitialized:        Whether this value should be counted as an "uninitialized" value in the course of an                                  analysis.     :param bool explicit_name:   If False, an identifier is appended to the name to ensure uniqueness.      :returns:                    The String object representing the symbolic string
Create a new Concrete string (analogous to z3.StringVal())      :param value: The constant value of the concrete string      :returns:                    The String object representing the concrete string
Return the start index of the pattern inside the input string in a         Bitvector representation, otherwise it returns -1 (always using a BitVector)          :param bitlength: size of the biitvector holding the result
A counterpart to FP.raw_to_bv - does nothing and returns itself.
Create a concrete version of the concatenated string     :param args: List of string that has to be concatenated      :return : a concrete version of the concatenated string
Create a concrete version of the substring     :param start_idx : starting index of the substring     :param end_idx : last index of the substring     :param initial_string      :return : a concrete version of the substring
Create a concrete version of the replaced string     (replace ONLY th efirst occurrence of the pattern)      :param initial_string: string in which the pattern needs to be replaced     :param pattern_to_be_replaced: substring that has to be replaced inside initial_string     :param replacement_poattern: pattern that has to be inserted in initial_string t replace                                  pattern_to_be_replaced     :return: a concrete representation of the replaced string
Return True if the concrete value of the input_string starts with prefix     otherwise false.      :param prefix: prefix we want to check     :param input_string: the string we want to check      :return: True if the input_string starts with prefix else false
Return True if the concrete value of the input_string ends with suffix     otherwise false.      :param suffix: suffix we want to check     :param input_string: the string we want to check      :return : True if the input_string ends with suffix else false
Return True if the concrete value of the input_string ends with suffix     otherwise false.      :param input_string: the string we want to check     :param substring: the substring we want to find the index     :param startIndex: the index to start searching at     :param bitlength: bitlength of the bitvector representing the index of the substring      :return BVV: index of the substring in bit-vector representation or -1 in bitvector representation
Return True if the concrete value of the input_string ends with suffix     otherwise false.      :param input_string: the string we want to transform in an integer     :param bitlength: bitlength of the bitvector representing the index of the substring      :return BVV: bit-vector representation of the integer resulting from ythe string or -1 in bitvector representation                  if the string cannot be transformed into an integer
Returns a sequence of the solvers that self and others share.
Creates a bit-vector symbol (i.e., a variable).      If you want to specify the maximum or minimum value of a normal symbol that is not part of value-set analysis, you     should manually add constraints to that effect. **Do not use ``min`` and ``max`` for symbolic execution.**      :param name:            The name of the symbol.     :param size:            The size (in bits) of the bit-vector.     :param min:             The minimum value of the symbol, used only for value-set analysis     :param max:             The maximum value of the symbol, used only for value-set analysis     :param stride:          The stride of the symbol, used only for value-set analysis     :param uninitialized:   Whether this value should be counted as an "uninitialized" value in the course of an                             analysis.     :param bool explicit_name:   If False, an identifier is appended to the name to ensure uniqueness.     :param bool discrete_set: If True, a DiscreteStridedIntervalSet will be used instead of a normal StridedInterval.     :param int discrete_set_max_card: The maximum cardinality of the discrete set. It is ignored if discrete_set is set                                       to False or None.      :returns:               a BV object representing this symbol.
Creates a bit-vector value (i.e., a concrete value).      :param value:   The value. Either an integer or a string. If it's a string, it will be interpreted as the bytes of                     a big-endian constant.     :param size:    The size (in bits) of the bit-vector. Optional if you provide a string, required for an integer.      :returns:       A BV object representing this value.
Chops a BV into consecutive sub-slices. Obviously, the length of this BV must be a multiple of bits.          :returns:   A list of smaller bitvectors, each ``bits`` in length. The first one will be the left-most (i.e.                     most significant) bits.
Extracts a byte from a BV, where the index refers to the byte in a big-endian order          :param index: the byte to extract         :return: An 8-bit BV
Extracts several bytes from a bitvector, where the index refers to the byte in a big-endian order          :param index: the byte index at which to start extracting         :param size: the number of bytes to extract         :return: A BV of size ``size * 8``
Interpret this bitvector as an integer, and return the floating-point representation of that integer.          :param sort:    The sort of floating point value to return         :param signed:  Optional: whether this value is a signed integer         :param rm:      Optional: the rounding mode to use         :return:        An FP AST whose value is the same as this BV
Interpret the bits of this bitvector as an IEEE754 floating point number.         The inverse of this function is raw_to_bv.          :return:        An FP AST whose bit-pattern is the same as this BV
Override Backend.convert() to add fast paths for BVVs and BoolVs.
Eval the ast, replacing symbols by their last value in the model.
Returns whether the constraints is satisfied trivially by using the         last model.
Updates this cache mixin with results discovered by the other split off one.
This function returns up to `n` possible solutions for expression `expr`.          :param expr: expression (an AST) to evaluate         :param n: number of results to return         :param solver: a solver object, native to the backend, to assist in                        the evaluation (for example, a z3.Solver)         :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve         :param model_callback:      a function that will be executed with recovered models (if any)         :return:              A sequence of up to n results (backend objects)
r"""Convert seekpath-formatted kpoints path to sumo-preferred format.          If 'GAMMA' is used as a label this will be replaced by '\Gamma'.          Args:             seekpath (list): A :obj:`list` of 2-tuples containing the labels at                 each side of each segment of the k-point path::                      [(A, B), (B, C), (C, D), ...]                  where a break in the sequence is indicated by a non-repeating                 label. E.g.::                      [(A, B), (B, C), (D, E), ...]                  for a break between C and D.             point_coords (dict): Dict of coordinates corresponding to k-point                 labels::                      {'GAMMA': [0., 0., 0.], ...}         Returns:             dict: The path and k-points as::                  {                     'path', [[l1, l2, l3], [l4, l5], ...],                     'kpoints', {l1: [a1, b1, c1], l2: [a2, b2, c2], ...}                 }
r"""Read Bradley--Cracknell k-points path from data file          Args:             bravais (str): Lattice code including orientation e.g. 'trig_p_c'          Returns:             dict: kpoint path and special point locations, formatted as e.g.::                {'kpoints': {'\Gamma': [0., 0., 0.], 'X': [0., 0.5, 0.], ...},                'path': [['\Gamma', 'X', ..., 'P'], ['H', 'N', ...]]}
Get Bravais lattice symbol from symmetry data
Get a colour for a particular elemental and orbital combination.      If the element is not specified in the colours dictionary, the cache is     checked. If this element-orbital combination has not been chached before,     a new colour is drawn from the current matplotlib colour cycle and cached.      The default cache is sumo.plotting.colour_cache. To reset this cache, use     ``sumo.plotting.colour_cache.clear()``.      Args:         element (:obj:`str`): The element.         orbital (:obj:`str`): The orbital.         colours (:obj:`dict`, optional): Use custom colours for specific             element and orbital combinations. Specified as a :obj:`dict` of             :obj:`dict` of the colours. For example::                  {                     'Sn': {'s': 'r', 'p': 'b'},                     'O': {'s': '#000000'}                 }              The colour can be a hex code, series of rgb value, or any other             format supported by matplotlib.         cache (:obj:`dict`, optional): Cache of colour values already             assigned. The format is the same as the custom colours dict. If             None, the module-level cache ``sumo.plotting.colour_cache`` is             used.      Returns:         tuple: (colour, cache)
Get the plotting data.          Args:             yscale (:obj:`float`, optional): Scaling factor for the y-axis.             xmin (:obj:`float`, optional): The minimum energy to mask the                 energy and density of states data (reduces plotting load).             xmax (:obj:`float`, optional): The maximum energy to mask the                 energy and density of states data (reduces plotting load).             colours (:obj:`dict`, optional): Use custom colours for specific                 element and orbital combinations. Specified as a :obj:`dict` of                 :obj:`dict` of the colours. For example::                      {                         'Sn': {'s': 'r', 'p': 'b'},                         'O': {'s': '#000000'}                     }                  The colour can be a hex code, series of rgb value, or any other                 format supported by matplotlib.             plot_total (:obj:`bool`, optional): Plot the total density of                 states. Defaults to ``True``.             legend_cutoff (:obj:`float`, optional): The cut-off (in % of the                 maximum density of states within the plotting range) for an                 elemental orbital to be labelled in the legend. This prevents                 the legend from containing labels for orbitals that have very                 little contribution in the plotting range.             subplot (:obj:`bool`, optional): Plot the density of states for                 each element on separate subplots. Defaults to ``False``.             zero_to_efermi (:obj:`bool`, optional): Normalise the plot such                 that the Fermi level is set as 0 eV.             cache (:obj:`dict`, optional): Cache object tracking how colours                 have been assigned to orbitals. The format is the same as the                 "colours" dict. This defaults to the module-level                 sumo.plotting.colour_cache object, but an empty dict can be                 used as a fresh cache. This object will be modified in-place.          Returns:             dict: The plotting data. Formatted with the following keys:                  "energies" (:obj:`numpy.ndarray`)                     The energies.                  "mask" (:obj:`numpy.ndarray`)                     A mask used to trim the density of states data and                     prevent unwanted data being included in the output file.                  "lines" (:obj:`list`)                     A :obj:`list` of :obj:`dict` containing the density data                     and some metadata. Each line :obj:`dict` contains the keys:                          "label" (:obj:`str`)                             The label for the legend.                          "dens" (:obj:`numpy.ndarray`)                             The density of states data.                          "colour" (:obj:`str`)                             The colour of the line.                          "alpha" (:obj:`float`)                             The alpha value for line fill.                  "ymin" (:obj:`float`)                     The minimum y-axis limit.                  "ymax" (:obj:`float`)                     The maximum y-axis limit.
Get a :obj:`matplotlib.pyplot` object of the density of states.          Args:             subplot (:obj:`bool`, optional): Plot the density of states for                 each element on separate subplots. Defaults to ``False``.             width (:obj:`float`, optional): The width of the plot.             height (:obj:`float`, optional): The height of the plot.             xmin (:obj:`float`, optional): The minimum energy on the x-axis.             xmax (:obj:`float`, optional): The maximum energy on the x-axis.             yscale (:obj:`float`, optional): Scaling factor for the y-axis.             colours (:obj:`dict`, optional): Use custom colours for specific                 element and orbital combinations. Specified as a :obj:`dict` of                 :obj:`dict` of the colours. For example::                      {                         'Sn': {'s': 'r', 'p': 'b'},                         'O': {'s': '#000000'}                     }                  The colour can be a hex code, series of rgb value, or any other                 format supported by matplotlib.             plot_total (:obj:`bool`, optional): Plot the total density of                 states. Defaults to ``True``.             legend_on (:obj:`bool`, optional): Plot the graph legend. Defaults                 to ``True``.             num_columns (:obj:`int`, optional): The number of columns in the                 legend.             legend_frame_on (:obj:`bool`, optional): Plot a frame around the                 graph legend. Defaults to ``False``.             legend_cutoff (:obj:`float`, optional): The cut-off (in % of the                 maximum density of states within the plotting range) for an                 elemental orbital to be labelled in the legend. This prevents                 the legend from containing labels for orbitals that have very                 little contribution in the plotting range.             xlabel (:obj:`str`, optional): Label/units for x-axis (i.e. energy)             ylabel (:obj:`str`, optional): Label/units for y-axis (i.e. DOS)             zero_to_efermi (:obj:`bool`, optional): Normalise the plot such                 that the Fermi level is set as 0 eV.             dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for                 the image.             fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a                 a single font, specified as a :obj:`str`, or several fonts,                 specified as a :obj:`list` of :obj:`str`.             plt (:obj:`matplotlib.pyplot`, optional): A                 :obj:`matplotlib.pyplot` object to use for plotting.             style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib                 style specifications, to be composed on top of Sumo base                 style.             no_base_style (:obj:`bool`, optional): Prevent use of sumo base                 style. This can make alternative styles behave more                 predictably.          Returns:             :obj:`matplotlib.pyplot`: The density of states plot.
Returns orbital projections for each branch in a band structure.      Args:         bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`):             The band structure.         selection (list): A list of :obj:`tuple` or :obj:`string`             identifying which projections to return. Projections can be             specified by both element and orbital, for example::                  [('Sn', 's'), ('Bi', 'p'), ('S', 'p')]              If just the element is specified then all the orbitals of that             element are combined. For example, the following will combine             all the S orbitals into a single projection::                  [('Bi', 's'), ('Bi', 'p'), 'S']              Particular orbitals can also be combined, for example::                  [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))]          normalise (:obj:`str`, optional): Normalisation the projections.             Options are:                * ``'all'``: Projections normalised against the sum of all                    other projections.               * ``'select'``: Projections normalised against the sum of the                    selected projections.               * ``None``: No normalisation performed.              Defaults to ``None``.      Returns:         list: A ``list`` of orbital projections for each branch of the band         structure, in the same order as specified in ``selection``, with         the format::              [ [ {spin: projections} ], [ {spin: projections} ], ... ]          Where spin is a :obj:`pymatgen.electronic_structure.core.Spin`         object and projections is a :obj:`numpy.array` of::              projections[band_index][kpoint_index]          If there are no projections in the band structure, then an array of         zeros is returned for each spin.
Returns orbital projections from a band structure.      Args:         bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`):             The band structure.         selection (list): A list of :obj:`tuple` or :obj:`string`             identifying which projections to return. Projections can be             specified by both element and orbital, for example::                  [('Bi', 's'), ('Bi', 'p'), ('S', 'p')]              If just the element is specified then all the orbitals of             that element are combined. For example, the following will combine             all the S orbitals into a single projection::                  [('Bi', 's'), ('Bi', 'p'), 'S']              Particular orbitals can also be combined, for example::                  [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))]          normalise (:obj:`str`, optional): Normalisation the projections.             Options are:                * ``'all'``: Projections normalised against the sum of all                    other projections.               * ``'select'``: Projections normalised against the sum of the                    selected projections.               * ``None``: No normalisation performed.              Defaults to ``None``.      Returns:         list: A ``list`` of orbital projections, in the same order as specified         in ``selection``, with the format::              [ {spin: projections}, {spin: projections} ... ]          Where spin is a :obj:`pymatgen.electronic_structure.core.Spin`         object and projections is a :obj:`numpy.array` of::              projections[band_index][kpoint_index]          If there are no projections in the band structure, then an array of         zeros is returned for each spin.
Get a :obj:`matplotlib.pyplot` object of the band structure.          If the system is spin polarised, orange lines are spin up, dashed         blue lines are spin down. For metals, all bands are coloured blue. For         semiconductors, blue lines indicate valence bands and orange lines         indicates conduction bands.          Args:             zero_to_efermi (:obj:`bool`): Normalise the plot such that the                 valence band maximum is set as 0 eV.             ymin (:obj:`float`, optional): The minimum energy on the y-axis.             ymax (:obj:`float`, optional): The maximum energy on the y-axis.             width (:obj:`float`, optional): The width of the plot.             height (:obj:`float`, optional): The height of the plot.             vbm_cbm_marker (:obj:`bool`, optional): Plot markers to indicate                 the VBM and CBM locations.             ylabel (:obj:`str`, optional): y-axis (i.e. energy) label/units             dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for                 the image.             plt (:obj:`matplotlib.pyplot`, optional): A                 :obj:`matplotlib.pyplot` object to use for plotting.             dos_plotter (:obj:`~sumo.plotting.dos_plotter.SDOSPlotter`, \                 optional): Plot the density of states alongside the band                 structure. This should be a                 :obj:`~sumo.plotting.dos_plotter.SDOSPlotter` object                 initialised with the data to plot.             dos_options (:obj:`dict`, optional): The options for density of                 states plotting. This should be formatted as a :obj:`dict`                 containing any of the following keys:                      "yscale" (:obj:`float`)                         Scaling factor for the y-axis.                     "xmin" (:obj:`float`)                         The minimum energy to mask the energy and density of                         states data (reduces plotting load).                     "xmax" (:obj:`float`)                         The maximum energy to mask the energy and density of                         states data (reduces plotting load).                     "colours" (:obj:`dict`)                         Use custom colours for specific element and orbital                         combinations. Specified as a :obj:`dict` of                         :obj:`dict` of the colours. For example::                              {                                 'Sn': {'s': 'r', 'p': 'b'},                                 'O': {'s': '#000000'}                             }                          The colour can be a hex code, series of rgb value, or                         any other format supported by matplotlib.                     "plot_total" (:obj:`bool`)                         Plot the total density of states. Defaults to ``True``.                     "legend_cutoff" (:obj:`float`)                         The cut-off (in % of the maximum density of states                         within the plotting range) for an elemental orbital to                         be labelled in the legend. This prevents the legend                         from containing labels for orbitals that have very                         little contribution in the plotting range.                     "subplot" (:obj:`bool`)                         Plot the density of states for each element on separate                         subplots. Defaults to ``False``.              dos_label (:obj:`str`, optional): DOS axis label/units             dos_aspect (:obj:`float`, optional): Aspect ratio for the band                 structure and density of states subplot. For example,                 ``dos_aspect = 3``, results in a ratio of 3:1, for the band                 structure:dos plots.             aspect (:obj:`float`, optional): The aspect ratio of the band                 structure plot. By default the dimensions of the figure size                 are used to determine the aspect ratio. Set to ``1`` to force                 the plot to be square.             fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a                 a single font, specified as a :obj:`str`, or several fonts,                 specified as a :obj:`list` of :obj:`str`.             style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib                 style specifications, to be composed on top of Sumo base                 style.             no_base_style (:obj:`bool`, optional): Prevent use of sumo base                 style. This can make alternative styles behave more                 predictably.          Returns:             :obj:`matplotlib.pyplot`: The electronic band structure plot.
Get a :obj:`matplotlib.pyplot` of the projected band structure.          If the system is spin polarised and ``mode = 'rgb'`` spin up and spin         down bands are differentiated by solid and dashed lines, respectively.         For the other modes, spin up and spin down are plotted separately.          Args:             selection (list): A list of :obj:`tuple` or :obj:`string`                 identifying which elements and orbitals to project on to the                 band structure. These can be specified by both element and                 orbital, for example, the following will project the Bi s, p                 and S p orbitals::                      [('Bi', 's'), ('Bi', 'p'), ('S', 'p')]                  If just the element is specified then all the orbitals of                 that element are combined. For example, to sum all the S                 orbitals::                      [('Bi', 's'), ('Bi', 'p'), 'S']                  You can also choose to sum particular orbitals by supplying a                 :obj:`tuple` of orbitals. For example, to sum the S s, p, and                 d orbitals into a single projection::                    [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))]                  If ``mode = 'rgb'``, a maximum of 3 orbital/element                 combinations can be plotted simultaneously (one for red, green                 and blue), otherwise an unlimited number of elements/orbitals                 can be selected.             mode (:obj:`str`, optional): Type of projected band structure to                 plot. Options are:                      "rgb"                         The band structure line color depends on the character                         of the band. Each element/orbital contributes either                         red, green or blue with the corresponding line colour a                         mixture of all three colours. This mode only supports                         up to 3 elements/orbitals combinations. The order of                         the ``selection`` :obj:`tuple` determines which colour                         is used for each selection.                     "stacked"                         The element/orbital contributions are drawn as a                         series of stacked circles, with the colour depending on                         the composition of the band. The size of the circles                         can be scaled using the ``circle_size`` option.              interpolate_factor (:obj:`int`, optional): The factor by which to                 interpolate the band structure (necessary to make smooth                 lines). A larger number indicates greater interpolation.             circle_size (:obj:`float`, optional): The area of the circles used                 when ``mode = 'stacked'``.             projection_cutoff (:obj:`float`): Don't plot projections with                 intensities below this number. This option is useful for                 stacked plots, where small projections clutter the plot.             zero_to_efermi (:obj:`bool`): Normalise the plot such that the                 valence band maximum is set as 0 eV.             ymin (:obj:`float`, optional): The minimum energy on the y-axis.             ymax (:obj:`float`, optional): The maximum energy on the y-axis.             width (:obj:`float`, optional): The width of the plot.             height (:obj:`float`, optional): The height of the plot.             vbm_cbm_marker (:obj:`bool`, optional): Plot markers to indicate                 the VBM and CBM locations.             ylabel (:obj:`str`, optional): y-axis (i.e. energy) label/units             dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for                 the image.             plt (:obj:`matplotlib.pyplot`, optional): A                 :obj:`matplotlib.pyplot` object to use for plotting.             dos_plotter (:obj:`~sumo.plotting.dos_plotter.SDOSPlotter`, \                 optional): Plot the density of states alongside the band                 structure. This should be a                 :obj:`~sumo.plotting.dos_plotter.SDOSPlotter` object                 initialised with the data to plot.             dos_options (:obj:`dict`, optional): The options for density of                 states plotting. This should be formatted as a :obj:`dict`                 containing any of the following keys:                      "yscale" (:obj:`float`)                         Scaling factor for the y-axis.                     "xmin" (:obj:`float`)                         The minimum energy to mask the energy and density of                         states data (reduces plotting load).                     "xmax" (:obj:`float`)                         The maximum energy to mask the energy and density of                         states data (reduces plotting load).                     "colours" (:obj:`dict`)                         Use custom colours for specific element and orbital                         combinations. Specified as a :obj:`dict` of                         :obj:`dict` of the colours. For example::                             {                                 'Sn': {'s': 'r', 'p': 'b'},                                 'O': {'s': '#000000'}                             }                          The colour can be a hex code, series of rgb value, or                         any other format supported by matplotlib.                     "plot_total" (:obj:`bool`)                         Plot the total density of states. Defaults to ``True``.                     "legend_cutoff" (:obj:`float`)                         The cut-off (in % of the maximum density of states                         within the plotting range) for an elemental orbital to                         be labelled in the legend. This prevents the legend                         from containing labels for orbitals that have very                         little contribution in the plotting range.                     "subplot" (:obj:`bool`)                         Plot the density of states for each element on separate                         subplots. Defaults to ``False``.              dos_label (:obj:`str`, optional): DOS axis label/units             dos_aspect (:obj:`float`, optional): Aspect ratio for the band                 structure and density of states subplot. For example,                 ``dos_aspect = 3``, results in a ratio of 3:1, for the band                 structure:dos plots.             aspect (:obj:`float`, optional): The aspect ratio of the band                 structure plot. By default the dimensions of the figure size                 are used to determine the aspect ratio. Set to ``1`` to force                 the plot to be square.             fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a                 a single font, specified as a :obj:`str`, or several fonts,                 specified as a :obj:`list` of :obj:`str`.             style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib                 style specifications, to be composed on top of Sumo base                 style.             no_base_style (:obj:`bool`, optional): Prevent use of sumo base                 style. This can make alternative styles behave more                 predictably.          Returns:             :obj:`matplotlib.pyplot`: The projected electronic band structure             plot.
Tidy the band structure & add the density of states if required.
This is basically the same as the SDOSPlotter get_plot function.
Implement label hacks: Hide trailing @, remove label with leading @          Labels split with $\mid$ symbol will be treated for each part.
Utility method to add tick marks to a band structure.
Get a :obj:`matplotlib.pyplot` object of the optical spectra.          Args:             width (:obj:`float`, optional): The width of the plot.             height (:obj:`float`, optional): The height of the plot.             xmin (:obj:`float`, optional): The minimum energy on the x-axis.             xmax (:obj:`float`, optional): The maximum energy on the x-axis.             ymin (:obj:`float`, optional): The minimum absorption intensity on                 the y-axis.             ymax (:obj:`float`, optional): The maximum absorption intensity on                 the y-axis.             colours (:obj:`list`, optional): A :obj:`list` of colours to use in                 the plot. The colours can be specified as a hex code, set of                 rgb values, or any other format supported by matplotlib.             dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for                 the image.             plt (:obj:`matplotlib.pyplot`, optional): A                 :obj:`matplotlib.pyplot` object to use for plotting.             fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a                 a single font, specified as a :obj:`str`, or several fonts,                 specified as a :obj:`list` of :obj:`str`.             style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib                 style specifications, to be composed on top of Sumo base                 style.             no_base_style (:obj:`bool`, optional): Prevent use of sumo base                 style. This can make alternative styles behave more                 predictably.          Returns:             :obj:`matplotlib.pyplot`: The plot of optical spectra.
Determine if the structure matches the standard primitive structure.          The standard primitive will be different between seekpath and pymatgen         high-symmetry paths, but this is handled by the specific subclasses.          Args:             atol (:obj:`float`, optional): Absolute tolerance used to compare                 the input structure with the primitive standard structure.          Returns:             bool: ``True`` if the structure is the same as the standard             primitive, otherwise ``False``.
r"""Return a list of k-points and labels along the high-symmetry path.          The format of the returned data will be different if phonopy is         ``True`` or ``False``. This is because phonopy requires the labels and         kpoints to be provided in a different format than kgen.          Adapted from         :obj:`pymatgen.symmetry.bandstructure.HighSymmKpath.get_kpoints`.          Args:             line_density (:obj:`int`, optional): Density of k-points along the                 path.             cart_coords (:obj:`bool`, optional): Whether the k-points are                 returned in cartesian or reciprocal coordinates. Defaults to                 ``False`` (fractional coordinates).             phonopy (:obj:`bool`, optional): Format the k-points and labels for                 use with phonopy. Defaults to ``False``.          Returns:             tuple: A :obj:`tuple` of the k-points along the high-symmetry path,             and k-point labels. Returned as ``(kpoints, labels)``.              If ``phonopy == False``, then:                  * ``kpoints`` is a :obj:`numpy.ndarray` of the k-point                   coordinates along the high-symmetry path. For example::                        [[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0], [0.5, 0, 0.25],                        [0.5, 0, 0.5]]                  * ``labels`` is a :obj:`list` of the high symmetry labels for                   each k-point (will be an empty :obj:`str` if the k-point has                   no label). For example::                        ['\Gamma', '', 'X', '', 'Y']              If ``phonopy == True``, then:                  * ``kpoints`` is a :obj:`list` of :obj:`numpy.ndarray`                   containing the k-points for each branch of the band                   structure. This means that the first and last k-points of a                   particular branch may be repeated. For example::                        [[[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0]],                        [[0.5, 0, 0], [0.5, 0, 0.25], [0.5, 0, 0.5]]]                  * ``labels`` is a :obj:`list` of the high symmetry labels.                   For example::                        ['\Gamma', 'X', 'Y']
Return the lattice crystal system.          Hexagonal cells are differentiated into rhombohedral and hexagonal         lattices.          Args:             number (int): The international space group number.          Returns:             str: The lattice crystal system.
Extract fitting data for band extrema based on spin, kpoint and band.      Searches forward and backward from the extrema point, but will only sample     there data if there are enough points in that direction.      Args:         bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`):             The band structure.         spin (:obj:`~pymatgen.electronic_structure.core.Spin`): Which spin             channel to sample.         band_id (int): Index of the band to sample.         kpoint_id (int): Index of the kpoint to sample.      Returns:         list: The data necessary to calculate the effective mass, along with         some metadata. Formatted as a :obj:`list` of :obj:`dict`, each with the         keys:          'energies' (:obj:`numpy.ndarray`)             Band eigenvalues in eV.          'distances' (:obj:`numpy.ndarray`)             Distances of the k-points in reciprocal space.          'band_id' (:obj:`int`)             The index of the band,          'spin' (:obj:`~pymatgen.electronic_structure.core.Spin`)             The spin channel          'start_kpoint' (:obj:`int`)             The index of the k-point at which the band extrema occurs          'end_kpoint' (:obj:`int`)             The k-point towards which the data has been sampled.
Fit the effective masses using either a parabolic or nonparabolic fit.      Args:         distances (:obj:`numpy.ndarray`): The x-distances between k-points in             reciprocal Angstroms, normalised to the band extrema.         energies (:obj:`numpy.ndarray`): The band eigenvalues normalised to the             eigenvalue of the band extrema.         parabolic (:obj:`bool`, optional): Use a parabolic fit of the band             edges. If ``False`` then nonparabolic fitting will be attempted.             Defaults to ``True``.      Returns:         float: The effective mass in units of electron rest mass, :math:`m_0`.
r"""Get the k-point path, coordinates and symmetry labels for a structure.      If a manual :obj:`list` of kpoints is supplied using the ``kpt_list``     variable, the ``mode`` option will be ignored.      The format of the returned data will be different if phonopy is ``True`` or     ``False``. This is because phonopy requires the labels and kpoints to be     provided in a different format than kgen.      Args:         structure (:obj:`~pymatgen.core.structure.Structure`): The structure.         mode (:obj:`str`, optional): Method used for calculating the             high-symmetry path. The options are:              bradcrack                 Use the paths from Bradley and Cracknell. See [brad]_.              pymatgen                 Use the paths from pymatgen. See [curt]_.              seekpath                 Use the paths from SeeK-path. See [seek]_.          symprec (:obj:`float`, optional): The tolerance for determining the             crystal symmetry.         spg (:obj:`~pymatgen.symmetry.groups.SpaceGroup`, optional): Space             group used to override the symmetry determined by spglib. This is             not recommended and only provided for testing purposes.             This option will only take effect when ``mode = 'bradcrack'``.         line_density (:obj:`int`, optional): Density of k-points along the             path.         cart_coords (:obj:`bool`, optional): Whether the k-points are returned             in cartesian or reciprocal coordinates. Defaults to ``False``             (fractional coordinates).         kpt_list (:obj:`list`, optional): List of k-points to use, formatted as             a list of subpaths, each containing a list of fractional k-points.             For example::                  [ [[0., 0., 0.], [0., 0., 0.5]],                   [[0.5, 0., 0.], [0.5, 0.5, 0.]] ]              Will return points along ``0 0 0 -> 0 0 1/2 | 1/2 0 0             -> 1/2 1/2 0``         path_labels (:obj:`list`, optional): The k-point labels. These should             be provided as a :obj:`list` of :obj:`str` for each subpath of the             overall path. For example::                  [ ['Gamma', 'Z'], ['X', 'M'] ]              combined with the above example for ``kpt_list`` would indicate the             path: Gamma -> Z | X -> M. If no labels are provided, letters from             A -> Z will be used instead.         phonopy (:obj:`bool`, optional): Format the k-points and labels for             use with phonopy. Defaults to ``False``.      Returns:         tuple: A tuple of a :obj:`~sumo.symmetry.kpath` object, the k-points         along the high-symmetry path, and the k-point labels. Returned as         ``(kpath, kpoints, labels)``.          The type of ``kpath`` object will depend on the value of ``mode`` and         whether ``kpt_list`` is set.          If ``phonopy == False``, then:              * ``kpoints`` is a :obj:`numpy.ndarray` of the k-point                 coordinates along the high-symmetry path. For example::                      [[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0], [0.5, 0, 0.25],                     [0.5, 0, 0.5]]              * ``labels`` is a :obj:`list` of the high symmetry labels for                 each k-point (will be an empty :obj:`str` if the k-point has                 no label). For example::                      ['\Gamma', '', 'X', '', 'Y']          If ``phonopy == True``, then:              * ``kpoints`` is a :obj:`list` of :obj:`numpy.ndarray`                 containing the k-points for each branch of the band                 structure. This means that the first and last k-points of a                 particular branch may be repeated. For example::                      [[[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0]],                     [[0.5, 0, 0], [0.5, 0, 0.25], [0.5, 0, 0.5]]]              * ``labels`` is a :obj:`list` of the high symmetry labels.                 For example::                      ['\Gamma', 'X', 'Y']
r"""Write the k-points data to VASP KPOINTS files.      Folders are named as 'split-01', 'split-02', etc ...     KPOINTS files are named KPOINTS_band_split_01 etc ...      Args:         filename (:obj:`str`): Path to VASP structure file.         kpoints (:obj:`numpy.ndarray`): The k-point coordinates along the             high-symmetry path. For example::                  [[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0], [0.5, 0, 0.25],                 [0.5, 0, 0.5]]          labels (:obj:`list`) The high symmetry labels for each k-point (will be             an empty :obj:`str` if the k-point has no label). For example::                  ['\Gamma', '', 'X', '', 'Y']          make_folders (:obj:`bool`, optional): Generate folders and copy in             required files (INCAR, POTCAR, POSCAR, and possibly CHGCAR) from             the current directory.         ibzkpt (:obj:`str`, optional): Path to IBZKPT file. If set, the             generated k-points will be appended to the k-points in this file             and given a weight of 0. This is necessary for hybrid band             structure calculations.         kpts_per_split (:obj:`int`, optional): If set, the k-points are split             into separate k-point files (or folders) each containing the number             of k-points specified. This is useful for hybrid band structure             calculations where it is often intractable to calculate all             k-points in the same calculation.         directory (:obj:`str`, optional): The output file directory.         cart_coords (:obj:`bool`, optional): Whether the k-points are returned             in cartesian or reciprocal coordinates. Defaults to ``False``             (fractional coordinates).
Return a decorator that will apply matplotlib style sheets to a plot.      ``style_sheets`` is a base set of styles, which will be ignored if     ``no_base_style`` is set in the decorated function arguments.      The style will further be overwritten by any styles in the ``style``     optional argument of the decorated function.      Args:         style_sheets (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib             supported definition of a style sheet. Can be a list of style of             style sheets.
Get a :obj:`matplotlib.pyplot` object with publication ready defaults.      Args:         width (:obj:`float`, optional): The width of the plot.         height (:obj:`float`, optional): The height of the plot.         plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot`             object to use for plotting.         dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for             the plot.      Returns:         :obj:`matplotlib.pyplot`: A :obj:`matplotlib.pyplot` object with         publication ready defaults set.
Get a :obj:`matplotlib.pyplot` subplot object with pretty defaults.      Args:         nrows (int): The number of rows in the subplot.         ncols (int): The number of columns in the subplot.         width (:obj:`float`, optional): The width of the plot.         height (:obj:`float`, optional): The height of the plot.         sharex (:obj:`bool`, optional): All subplots share the same x-axis.             Defaults to ``True``.         sharey (:obj:`bool`, optional): All subplots share the same y-axis.             Defaults to ``True``.         dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for             the plot.         plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot`             object to use for plotting.         gridspec_kw (:obj:`dict`, optional): Gridspec parameters. Please see:             :obj:`matplotlib.pyplot.subplot` for more information. Defaults             to ``None``.      Returns:         :obj:`matplotlib.pyplot`: A :obj:`matplotlib.pyplot` subplot object         with publication ready defaults set.
Custom power ticker function.
Get a RGB coloured line for plotting.      Args:         x (list): x-axis data.         y (list): y-axis data (can be multidimensional array).         red (list): Red data (must have same shape as ``y``).         green (list): Green data (must have same shape as ``y``).         blue (list): blue data (must have same shape as ``y``).         alpha (:obj:`list` or :obj:`int`, optional): Alpha (transparency)             data (must have same shape as ``y`` or be an :obj:`int`).         linestyles (:obj:`str`, optional): Linestyle for plot. Options are             ``"solid"`` or ``"dotted"``.
Apply gaussian broadening to the dielectric response.      Args:         dielectric_data (tuple): The high-frequency dielectric data, following             the same format as             :attr:`pymatgen.io.vasp.outputs.Vasprun.dielectric`.             This is a :obj:`tuple` containing the energy, the real part of the             dielectric tensor, and the imaginary part of the tensor, as a             :obj:`list` of :obj:`floats`. E.g.::                  (                     [energies],                     [[real_xx, real_yy, real_zz, real_xy, real_yz, real_xz]],                     [[imag_xx, imag_yy, imag_zz, imag_xy, imag_yz, imag_xz]]                 )          sigma (float): Standard deviation for gaussian broadening.      Returns:         :obj:`tuple` of :obj:`list` of :obj:`list` of :obj:`float`: The         broadened dielectric response. Returned as a tuple containing the         energy, the real part of the dielectric tensor, and the imaginary         part of the tensor. E.g.::              (                 [energies],                 [[real_xx, real_yy, real_zz, real_xy, real_yz, real_xz]],                 [[imag_xx, imag_yy, imag_zz, imag_xy, imag_yz, imag_xz]]             )
r"""Calculate optical properties from the dielectric function      Supported properties:      Absorption     ~~~~~~~~~~      The unit of alpha is :math:`\mathrm{cm}^{-1}`.      Refractive index :math:`n` has real and imaginary parts:      .. math::          n = [(e^\prime + ie^{\prime\prime} / e_0]^{1/2}           = n^\prime + in^{\prime\prime}      Relationship between :math:`a` and imaginary :math:`n^{\prime\prime}`:      .. math::          a = 4 \pi n^{\prime\prime} / \lambda      Where:      .. math:: \lambda = hc/E      Args:         dielectric_data (tuple): The high-frequency dielectric data, following             the same format as :obj:`pymatgen.io.vasp.Vasprun.dielectric`.             This is a :obj:`tuple` containing the energy, the real part of the             dielectric tensor, and the imaginary part of the tensor, as a             :obj:`list` of :obj:`floats`. E.g.::                  (                     [energies],                     [[real_xx, real_yy, real_zz, real_xy, real_yz, real_xz]],                     [[imag_xx, imag_yy, imag_zz, imag_xy, imag_yz, imag_xz]]                 )          properties (set):             The set of properties to return. Intermediate properties will be             calculated as needed. Accepted values: 'eps_real', 'eps_im',             'absorption', 'loss', 'n_real', 'n_imag'          average (:obj:`bool`, optional): Average the dielectric response across             the xx, yy, zz directions and calculate properties with scalar             maths. Defaults to ``True``. If False, solve dielectric matrix to             obtain directional properties, returning xx, yy, zz components.             This may be significantly slower!      Returns:         :obj:`tuple` of :obj:`list` of :obj:`float`: The optical absorption in         :math:`\mathrm{cm}^{-1}`. If ``average`` is ``True``, the data will be         returned as::              ([energies], [property]).          If ``average`` is ``False``, the data will be returned as::              ([energies], [property_xx, property_yy, property_zz]).
Write the absorption or loss spectra to a file.      Note that this function expects to receive an iterable series of spectra.      Args:         abs_data (tuple): Series (either :obj:`list` or :obj:`tuple`) of             optical absorption or loss spectra. Each spectrum should be             formatted as a :obj:`tuple` of :obj:`list` of :obj:`float`. If the             data has been averaged, each spectrum should be::                  ([energies], [alpha])              Else, if the data has not been averaged, each spectrum should be::                  ([energies], [alpha_xx, alpha_yy, alpha_zz]).          prefix (:obj:`str`, optional): Prefix for file names.         directory (:obj:`str`, optional): The directory in which to save files.
A script to plot optical absorption spectra from VASP calculations.      Args:         modes (:obj:`list` or :obj:`tuple`):             Ordered list of :obj:`str` determining properties to plot.             Accepted options are 'absorption' (default), 'eps', 'eps-real',                 'eps-im', 'n', 'n-real', 'n-im', 'loss' (equivalent to n-im).         filenames (:obj:`str` or :obj:`list`, optional): Path to vasprun.xml             file (can be gzipped). Alternatively, a list of paths can be             provided, in which case the absorption spectra for each will be             plotted concurrently.         prefix (:obj:`str`, optional): Prefix for file names.         directory (:obj:`str`, optional): The directory in which to save files.         gaussian (:obj:`float`): Standard deviation for gaussian broadening.         band_gaps (:obj:`float` or :obj:`list`, optional): The band gap as a             :obj:`float`, plotted as a dashed line. If plotting multiple             spectra then a :obj:`list` of band gaps can be provided.         labels (:obj:`str` or :obj:`list`): A label to identify the spectra.             If plotting multiple spectra then a :obj:`list` of labels can             be provided.         average (:obj:`bool`, optional): Average the dielectric response across             all lattice directions. Defaults to ``True``.         height (:obj:`float`, optional): The height of the plot.         width (:obj:`float`, optional): The width of the plot.         xmin (:obj:`float`, optional): The minimum energy on the x-axis.         xmax (:obj:`float`, optional): The maximum energy on the x-axis.         ymin (:obj:`float`, optional): The minimum absorption intensity on the             y-axis.         ymax (:obj:`float`, optional): The maximum absorption intensity on the             y-axis.         colours (:obj:`list`, optional): A :obj:`list` of colours to use in the             plot. The colours can be specified as a hex code, set of rgb             values, or any other format supported by matplotlib.         style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style             specifications, to be composed on top of Sumo base style.         no_base_style (:obj:`bool`, optional): Prevent use of sumo base style.             This can make alternative styles behave more predictably.         image_format (:obj:`str`, optional): The image file format. Can be any             format supported by matplotlib, including: png, jpg, pdf, and svg.             Defaults to pdf.         dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for             the image.         plt (:obj:`matplotlib.pyplot`, optional): A             :obj:`matplotlib.pyplot` object to use for plotting.         fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a             a single font, specified as a :obj:`str`, or several fonts,             specified as a :obj:`list` of :obj:`str`.      Returns:         A matplotlib pyplot object.
Load phonopy output and return an ``phonopy.Phonopy`` object.      Args:         filename (str): Path to phonopy output. Can be any of ``FORCE_SETS``,             ``FORCE_CONSTANTS``, or ``force_constants.hdf5``.         structure (:obj:`~pymatgen.core.structure.Structure`): The unitcell             structure.         dim (list): The supercell size, as a :obj:`list` of :obj:`float`.         symprec (:obj:`float`, optional): The tolerance for determining the             crystal symmetry.         primitive_matrix (:obj:`list`, optional): The transformation matrix             from the conventional to primitive cell. Only required when the             conventional cell was used as the starting structure. Should be             provided as a 3x3 :obj:`list` of :obj:`float`.         factor (:obj:`float`, optional): The conversion factor for phonon             frequency. Defaults to :obj:`phonopy.units.VaspToTHz`.         symmetrise (:obj:`bool`, optional): Symmetrise the force constants.             Defaults to ``True``.         born (:obj:`str`, optional): Path to file containing Born effective             charges. Should be in the same format as the file produced by the             ``phonopy-vasp-born`` script provided by phonopy.         write_fc (:obj:`bool` or :obj:`str`,  optional): Write the force             constants to disk. If ``True``, a ``FORCE_CONSTANTS`` file will be             written. Alternatively, if set to ``"hdf5"``, a             ``force_constants.hdf5`` file will be written. Defaults to             ``False`` (force constants not written).
Get a default set of labels (1), (2), (3)... for a k-point path          Repeated points will be identified and the labels re-used.          Args:             kpt_list (list): Nested list representing k-point path segments,                 e.g.::                    [[[0., 0., 0.], [0., 0., 0.5], [0., 0.5, 0.5]],                    [[0.5, 0.5, 0.], [0., 0., 0.]]]          Returns:             list: Corresponding nested list of labels, e.g.::                [['(1)', '(2)', '(3)'], ['(4)', '(1)']]
Load a vasprun and extract the total and projected density of states.      Args:         vasprun (str): Path to a vasprun.xml or vasprun.xml.gz file or             a :obj:`pymatgen.io.vasp.outputs.Vasprun` object.         elements (:obj:`dict`, optional): The elements and orbitals to extract             from the projected density of states. Should be provided as a             :obj:`dict` with the keys as the element names and corresponding             values as a :obj:`tuple` of orbitals. For example, the following             would extract the Bi s, px, py and d orbitals::                  {'Bi': ('s', 'px', 'py', 'd')}              If an element is included with an empty :obj:`tuple`, all orbitals             for that species will be extracted. If ``elements`` is not set or             set to ``None``, all elements for all species will be extracted.         lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into             their lm contributions (e.g. p -> px, py, pz). Should be provided             as a :obj:`dict`, with the elements names as keys and a             :obj:`tuple` of orbitals as the corresponding values. For example,             the following would be used to decompose the oxygen p and d             orbitals::                  {'O': ('p', 'd')}          atoms (:obj:`dict`, optional): Which atomic sites to use when             calculating the projected density of states. Should be provided as             a :obj:`dict`, with the element names as keys and a :obj:`tuple` of             :obj:`int` specifying the atomic indices as the corresponding             values. The elemental projected density of states will be summed             only over the atom indices specified. If an element is included             with an empty :obj:`tuple`, then all sites for that element will             be included. The indices are 0 based for each element specified in             the POSCAR. For example, the following will calculate the density             of states for the first 4 Sn atoms and all O atoms in the             structure::                  {'Sn': (1, 2, 3, 4), 'O': (, )}              If ``atoms`` is not set or set to ``None`` then all atomic sites             for all elements will be considered.         gaussian (:obj:`float`, optional): Broaden the density of states using             convolution with a gaussian function. This parameter controls the             sigma or standard deviation of the gaussian distribution.         total_only (:obj:`bool`, optional): Only extract the total density of             states. Defaults to ``False``.         log (:obj:`bool`): Print logging messages. Defaults to ``False``.         adjust_fermi (:obj:`bool`, optional): Shift the Fermi level to sit at             the valence band maximum (does not affect metals).      Returns:         dict: The total and projected density of states. Formatted as a         :obj:`tuple` of ``(dos, pdos)``, where ``dos`` is a         :obj:`~pymatgen.electronic_structure.dos.Dos` object containing the         total density of states and ``pdos`` is a :obj:`dict` of         :obj:`dict` mapping the elements and their orbitals to         :obj:`~pymatgen.electronic_structure.dos.Dos` objects. For example::              {                 'Bi': {'s': Dos, 'p': Dos ... },                 'S': {'s': Dos}             }
Extract the projected density of states from a CompleteDos object.      Args:         dos (:obj:`~pymatgen.electronic_structure.dos.CompleteDos`): The             density of states.         elements (:obj:`dict`, optional): The elements and orbitals to extract             from the projected density of states. Should be provided as a             :obj:`dict` with the keys as the element names and corresponding             values as a :obj:`tuple` of orbitals. For example, the following             would extract the Bi s, px, py and d orbitals::                  {'Bi': ('s', 'px', 'py', 'd')}              If an element is included with an empty :obj:`tuple`, all orbitals             for that species will be extracted. If ``elements`` is not set or             set to ``None``, all elements for all species will be extracted.         lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into             their lm contributions (e.g. p -> px, py, pz). Should be provided             as a :obj:`dict`, with the elements names as keys and a             :obj:`tuple` of orbitals as the corresponding values. For example,             the following would be used to decompose the oxygen p and d             orbitals::                  {'O': ('p', 'd')}          atoms (:obj:`dict`, optional): Which atomic sites to use when             calculating the projected density of states. Should be provided as             a :obj:`dict`, with the element names as keys and a :obj:`tuple` of             :obj:`int` specifying the atomic indices as the corresponding             values. The elemental projected density of states will be summed             only over the atom indices specified. If an element is included             with an empty :obj:`tuple`, then all sites for that element will             be included. The indices are 0 based for each element specified in             the POSCAR. For example, the following will calculate the density             of states for the first 4 Sn atoms and all O atoms in the             structure::                  {'Sn': (1, 2, 3, 4), 'O': (, )}              If ``atoms`` is not set or set to ``None`` then all atomic sites             for all elements will be considered.      Returns:         dict: The projected density of states. Formatted as a :obj:`dict` of         :obj:`dict` mapping the elements and their orbitals to         :obj:`~pymatgen.electronic_structure.dos.Dos` objects. For example::              {                 'Bi': {'s': Dos, 'p': Dos ... },                 'S': {'s': Dos}             }
Get the projected density of states for an element.      Args:         dos (:obj:`~pymatgen.electronic_structure.dos.CompleteDos`): The             density of states.         element (str): Element symbol. E.g. 'Zn'.         sites (tuple): The atomic indices over which to sum the density of             states, as a :obj:`tuple`. Indices are zero based for each             element. For example, ``(0, 1, 2)`` will sum the density of states             for the 1st, 2nd and 3rd sites of the element specified.         lm_orbitals (:obj:`tuple`, optional): The orbitals to decompose into             their lm contributions (e.g. p -> px, py, pz). Should be provided             as a :obj:`tuple` of :obj:`str`. For example, ``('p')``, will             extract the projected density of states for the px, py, and pz             orbitals. Defaults to ``None``.         orbitals (:obj:`tuple`, optional): The orbitals to extract from the             projected density of states. Should be provided as a :obj:`tuple`             of :obj:`str`. For example, ``('s', 'px', 'dx2')`` will extract the             s, px, and dx2 orbitals, only. If ``None``, all orbitals will be             extracted. Defaults to ``None``.      Returns:         dict: The projected density of states. Formatted as a :obj:`dict`         mapping the orbitals to :obj:`~pymatgen.electronic_structure.dos.Dos`         objects. For example::              {                 's': Dos,                 'p': Dos             }
Write the density of states data to disk.      Args:         dos (:obj:`~pymatgen.electronic_structure.dos.Dos` or \              :obj:`~pymatgen.electronic_structure.dos.CompleteDos`): The total             density of states.         pdos (dict): The projected density of states. Formatted as a             :obj:`dict` of :obj:`dict` mapping the elements and their orbitals             to :obj:`~pymatgen.electronic_structure.dos.Dos` objects. For             example::                  {                     'Bi': {'s': Dos, 'p': Dos},                     'S': {'s': Dos}                 }          prefix (:obj:`str`, optional): A prefix for file names.         directory (:obj:`str`, optional): The directory in which to save files.         zero_to_efermi (:obj:`bool`, optional): Normalise the energy such              that the Fermi level is set as 0 eV.
Sort the orbitals of an element's projected density of states.      Sorts the orbitals based on a standard format. E.g. s < p < d.     Will also sort lm decomposed orbitals. This is useful for plotting/saving.      Args:         element_pdos (dict): An element's pdos. Should be formatted as a             :obj:`dict` of ``{orbital: dos}``. Where dos is a             :obj:`~pymatgen.electronic_structure.dos.Dos` object. For example::                  {'s': dos, 'px': dos}      Returns:         list: The sorted orbitals.
Calculate the effective masses of the bands of a semiconductor.      Args:         filenames (:obj:`str` or :obj:`list`, optional): Path to vasprun.xml             or vasprun.xml.gz file. If no filenames are provided, the code             will search for vasprun.xml or vasprun.xml.gz files in folders             named 'split-0*'. Failing that, the code will look for a vasprun in             the current directory. If a :obj:`list` of vasprun files is             provided, these will be combined into a single band structure.         num_sample_points (:obj:`int`, optional): Number of k-points to sample             when fitting the effective masses.         temperature (:obj:`int`, optional): Find band edges within kB * T of             the valence band maximum and conduction band minimum. Not currently             implemented.         degeneracy_tol (:obj:`float`, optional): Tolerance for determining the             degeneracy of the valence band maximum and conduction band minimum.         parabolic (:obj:`bool`, optional): Use a parabolic fit of the band             edges. If ``False`` then nonparabolic fitting will be attempted.             Defaults to ``True``.      Returns:         dict: The hole and electron effective masses. Formatted as a         :obj:`dict` with keys: ``'hole_data'`` and ``'electron_data'``. The         data is a :obj:`list` of :obj:`dict` with the keys:          'effective_mass' (:obj:`float`)             The effective mass in units of electron rest mass, :math:`m_0`.          'energies' (:obj:`numpy.ndarray`)             Band eigenvalues in eV.          'distances' (:obj:`numpy.ndarray`)             Distances of the k-points in reciprocal space.          'band_id' (:obj:`int`)             The index of the band,          'spin' (:obj:`~pymatgen.electronic_structure.core.Spin`)             The spin channel          'start_kpoint' (:obj:`int`)             The index of the k-point at which the band extrema occurs          'end_kpoint' (:obj:`int`)
Log data about the direct and indirect band gaps.      Args:         bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`):
Log data about the valence band maximum or conduction band minimum.      Args:         bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`):             The band structure.         edge_data (dict): The :obj:`dict` from ``bs.get_vbm()`` or             ``bs.get_cbm()``
Log data about the effective masses and their directions.      Args:         data (dict): The effective mass data. Formatted as a :obj:`dict` with             the keys:              'effective_mass' (:obj:`float`)                 The effective mass in units of electron rest mass, :math:`m_0`.              'energies' (:obj:`numpy.ndarray`)                 Band eigenvalues in eV.              'band_id' (:obj:`int`)                 The index of the band,              'spin' (:obj:`~pymatgen.electronic_structure.core.Spin`)                 The spin channel              'start_kpoint' (:obj:`int`)                 The index of the k-point at which the band extrema occurs              'end_kpoint' (:obj:`int`)                 The k-point towards which the data has been sampled.          is_spin_polarized (bool): Whether the system is spin polarized.
Get a :obj:`matplotlib.pyplot` object of the phonon band structure.          Args:             units (:obj:`str`, optional): Units of phonon frequency. Accepted                 (case-insensitive) values are Thz, cm-1, eV, meV.             ymin (:obj:`float`, optional): The minimum energy on the y-axis.             ymax (:obj:`float`, optional): The maximum energy on the y-axis.             width (:obj:`float`, optional): The width of the plot.             height (:obj:`float`, optional): The height of the plot.             dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for                 the image.             fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a                 a single font, specified as a :obj:`str`, or several fonts,                 specified as a :obj:`list` of :obj:`str`.             plt (:obj:`matplotlib.pyplot`, optional): A                 :obj:`matplotlib.pyplot` object to use for plotting.             dos (:obj:`np.ndarray`): 2D Numpy array of total DOS data             dos_aspect (float): Width division for vertical DOS             color (:obj:`str` or :obj:`tuple`, optional): Line/fill colour in                 any matplotlib-accepted format             style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib                 style specifications, to be composed on top of Sumo base                 style.             no_base_style (:obj:`bool`, optional): Prevent use of sumo base                 style. This can make alternative styles behave more                 predictably.          Returns:             :obj:`matplotlib.pyplot`: The phonon band structure plot.
Utility method to tidy phonon band structure diagrams.
Utility method to add tick marks to a band structure.
Plot electronic band structure diagrams from vasprun.xml files.      Args:         filenames (:obj:`str` or :obj:`list`, optional): Path to vasprun.xml             or vasprun.xml.gz file. If no filenames are provided, the code             will search for vasprun.xml or vasprun.xml.gz files in folders             named 'split-0*'. Failing that, the code will look for a vasprun in             the current directory. If a :obj:`list` of vasprun files is             provided, these will be combined into a single band structure.         prefix (:obj:`str`, optional): Prefix for file names.         directory (:obj:`str`, optional): The directory in which to save files.         vbm_cbm_marker (:obj:`bool`, optional): Plot markers to indicate the             VBM and CBM locations.         projection_selection (list): A list of :obj:`tuple` or :obj:`string`             identifying which elements and orbitals to project on to the             band structure. These can be specified by both element and             orbital, for example, the following will project the Bi s, p             and S p orbitals::                  [('Bi', 's'), ('Bi', 'p'), ('S', 'p')]              If just the element is specified then all the orbitals of             that element are combined. For example, to sum all the S             orbitals::                  [('Bi', 's'), ('Bi', 'p'), 'S']              You can also choose to sum particular orbitals by supplying a             :obj:`tuple` of orbitals. For example, to sum the S s, p, and             d orbitals into a single projection::                  [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))]              If ``mode = 'rgb'``, a maximum of 3 orbital/element             combinations can be plotted simultaneously (one for red, green             and blue), otherwise an unlimited number of elements/orbitals             can be selected.         mode (:obj:`str`, optional): Type of projected band structure to             plot. Options are:                  "rgb"                     The band structure line color depends on the character                     of the band. Each element/orbital contributes either                     red, green or blue with the corresponding line colour a                     mixture of all three colours. This mode only supports                     up to 3 elements/orbitals combinations. The order of                     the ``selection`` :obj:`tuple` determines which colour                     is used for each selection.                 "stacked"                     The element/orbital contributions are drawn as a                     series of stacked circles, with the colour depending on                     the composition of the band. The size of the circles                     can be scaled using the ``circle_size`` option.         circle_size (:obj:`float`, optional): The area of the circles used             when ``mode = 'stacked'``.         dos_file (:obj:'str', optional): Path to vasprun.xml file from which to             read the density of states information. If set, the density of             states will be plotted alongside the bandstructure.         elements (:obj:`dict`, optional): The elements and orbitals to extract             from the projected density of states. Should be provided as a             :obj:`dict` with the keys as the element names and corresponding             values as a :obj:`tuple` of orbitals. For example, the following             would extract the Bi s, px, py and d orbitals::                  {'Bi': ('s', 'px', 'py', 'd')}              If an element is included with an empty :obj:`tuple`, all orbitals             for that species will be extracted. If ``elements`` is not set or             set to ``None``, all elements for all species will be extracted.         lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into             their lm contributions (e.g. p -> px, py, pz). Should be provided             as a :obj:`dict`, with the elements names as keys and a             :obj:`tuple` of orbitals as the corresponding values. For example,             the following would be used to decompose the oxygen p and d             orbitals::                  {'O': ('p', 'd')}          atoms (:obj:`dict`, optional): Which atomic sites to use when             calculating the projected density of states. Should be provided as             a :obj:`dict`, with the element names as keys and a :obj:`tuple` of             :obj:`int` specifying the atomic indices as the corresponding             values. The elemental projected density of states will be summed             only over the atom indices specified. If an element is included             with an empty :obj:`tuple`, then all sites for that element will             be included. The indices are 0 based for each element specified in             the POSCAR. For example, the following will calculate the density             of states for the first 4 Sn atoms and all O atoms in the             structure::                  {'Sn': (1, 2, 3, 4), 'O': (, )}              If ``atoms`` is not set or set to ``None`` then all atomic sites             for all elements will be considered.         total_only (:obj:`bool`, optional): Only extract the total density of             states. Defaults to ``False``.         plot_total (:obj:`bool`, optional): Plot the total density of states.             Defaults to ``True``.         legend_cutoff (:obj:`float`, optional): The cut-off (in % of the             maximum density of states within the plotting range) for an             elemental orbital to be labelled in the legend. This prevents             the legend from containing labels for orbitals that have very             little contribution in the plotting range.         gaussian (:obj:`float`, optional): Broaden the density of states using             convolution with a gaussian function. This parameter controls the             sigma or standard deviation of the gaussian distribution.         height (:obj:`float`, optional): The height of the plot.         width (:obj:`float`, optional): The width of the plot.         ymin (:obj:`float`, optional): The minimum energy on the y-axis.         ymax (:obj:`float`, optional): The maximum energy on the y-axis.         style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style             specifications, to be composed on top of Sumo base style.         no_base_style (:obj:`bool`, optional): Prevent use of sumo base style.             This can make alternative styles behave more predictably.         colours (:obj:`dict`, optional): Use custom colours for specific             element and orbital combinations. Specified as a :obj:`dict` of             :obj:`dict` of the colours. For example::                  {                     'Sn': {'s': 'r', 'p': 'b'},                     'O': {'s': '#000000'}                 }              The colour can be a hex code, series of rgb value, or any other             format supported by matplotlib.         yscale (:obj:`float`, optional): Scaling factor for the y-axis.         image_format (:obj:`str`, optional): The image file format. Can be any             format supported by matplotlib, including: png, jpg, pdf, and svg.             Defaults to pdf.         dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for             the image.         plt (:obj:`matplotlib.pyplot`, optional): A             :obj:`matplotlib.pyplot` object to use for plotting.         fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a             a single font, specified as a :obj:`str`, or several fonts,             specified as a :obj:`list` of :obj:`str`.      Returns:         If ``plt`` set then the ``plt`` object will be returned. Otherwise, the         method will return a :obj:`list` of filenames written to disk.
Search for vasprun files from the current directory.      The precedence order for file locations is:        1. First search for folders named: 'split-0*'       2. Else, look in the current directory.      The split folder names should always be zero based, therefore easily     sortable.
Write the band structure data files to disk.      Args:         vs (`Vasprun`): Pymatgen `Vasprun` object.         bs (`BandStructureSymmLine`): Calculated band structure.         prefix (`str`, optional): Prefix for data file.         directory (`str`, optional): Directory in which to save the data.      Returns:         The filename of the written data file.
Parse the element and orbital argument strings.      The presence of an element without any orbitals means that we want to plot     all of its orbitals.      Args:         string (`str`): The selected elements and orbitals in in the form:             `"Sn.s.p,O"`.      Returns:         A list of tuples specifying which elements/orbitals to plot. The output         for the above example would be:              `[('Sn', ('s', 'p')), 'O']`
A script to plot phonon band structure diagrams.      Args:         filename (str): Path to phonopy output. Can be a band structure yaml             file, ``FORCE_SETS``, ``FORCE_CONSTANTS``, or             ``force_constants.hdf5``.         poscar (:obj:`str`, optional): Path to POSCAR file of unitcell. Not             required if plotting the phonon band structure from a yaml file. If             not specified, the script will search for a POSCAR file in the             current directory.         prefix (:obj:`str`, optional): Prefix for file names.         directory (:obj:`str`, optional): The directory in which to save files.         born (:obj:`str`, optional): Path to file containing Born effective             charges. Should be in the same format as the file produced by the             ``phonopy-vasp-born`` script provided by phonopy.         qmesh (:obj:`list` of :obj:`int`, optional): Q-point mesh to use for             calculating the density of state. Formatted as a 3x1 :obj:`list` of             :obj:`int`.         spg (:obj:`str` or :obj:`int`, optional): The space group international             number or symbol to override the symmetry determined by spglib.             This is not recommended and only provided for testing purposes.             This option will only take effect when ``mode = 'bradcrack'``.         primitive_matrix (:obj:`list`, optional): The transformation matrix             from the conventional to primitive cell. Only required when the             conventional cell was used as the starting structure. Should be             provided as a 3x3 :obj:`list` of :obj:`float`.         line_density (:obj:`int`, optional): Density of k-points along the             path.         units (:obj:`str`, optional): Units of phonon frequency. Accepted             (case-insensitive) values are Thz, cm-1, eV, meV.         symprec (:obj:`float`, optional): Tolerance for space-group-finding             operations         mode (:obj:`str`, optional): Method used for calculating the             high-symmetry path. The options are:              bradcrack                 Use the paths from Bradley and Cracknell. See [brad]_.              pymatgen                 Use the paths from pymatgen. See [curt]_.              seekpath                 Use the paths from SeeK-path. See [seek]_.          kpt_list (:obj:`list`, optional): List of k-points to use, formatted as             a list of subpaths, each containing a list of fractional k-points.             For example::                  [ [[0., 0., 0.], [0., 0., 0.5]],                   [[0.5, 0., 0.], [0.5, 0.5, 0.]] ]              Will return points along ``0 0 0 -> 0 0 1/2 | 1/2 0 0             -> 1/2 1/2 0``         path_labels (:obj:`list`, optional): The k-point labels. These should             be provided as a :obj:`list` of :obj:`str` for each subpath of the             overall path. For example::                  [ ['Gamma', 'Z'], ['X', 'M'] ]              combined with the above example for ``kpt_list`` would indicate the             path: Gamma -> Z | X -> M. If no labels are provided, letters from             A -> Z will be used instead.         eigenvectors (:obj:`bool`, optional): Write the eigenvectors to the             yaml file.         dos (str): Path to Phonopy total dos .dat file         height (:obj:`float`, optional): The height of the plot.         width (:obj:`float`, optional): The width of the plot.         ymin (:obj:`float`, optional): The minimum energy on the y-axis.         ymax (:obj:`float`, optional): The maximum energy on the y-axis.         style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style             specifications, to be composed on top of Sumo base style.         no_base_style (:obj:`bool`, optional): Prevent use of sumo base style.             This can make alternative styles behave more predictably.         image_format (:obj:`str`, optional): The image file format. Can be any             format supported by matplotlib, including: png, jpg, pdf, and svg.             Defaults to pdf.         dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for             the image.         plt (:obj:`matplotlib.pyplot`, optional): A             :obj:`matplotlib.pyplot` object to use for plotting.         fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a             a single font, specified as a :obj:`str`, or several fonts,             specified as a :obj:`list` of :obj:`str`.      Returns:         A matplotlib pyplot object.
Write the phonon band structure data files to disk.      Args:         bs (:obj:`~pymatgen.phonon.bandstructure.PhononBandStructureSymmLine`):             The phonon band structure.         prefix (:obj:`str`, optional): Prefix for data file.         directory (:obj:`str`, optional): Directory in which to save the data.      Returns:         str: The filename of the written data file.
Generate KPOINTS files for VASP band structure calculations.      This script provides a wrapper around several frameworks used to generate     k-points along a high-symmetry path. The paths found in Bradley and     Cracknell, SeeK-path, and pymatgen are all supported.      It is important to note that the standard primitive cell symmetry is     different between SeeK-path and pymatgen. If the correct the structure     is not used, the high-symmetry points (and band path) may be invalid.      Args:         filename (:obj:`str`, optional): Path to VASP structure file. Default             is ``POSCAR``.         directory (:obj:`str`, optional): The output file directory.         make_folders (:obj:`bool`, optional): Generate folders and copy in             required files (INCAR, POTCAR, POSCAR, and possibly CHGCAR) from             the current directory.         symprec (:obj:`float`, optional): The precision used for determining             the cell symmetry.         kpts_per_split (:obj:`int`, optional): If set, the k-points are split             into separate k-point files (or folders) each containing the number             of k-points specified. This is useful for hybrid band structure             calculations where it is often intractable to calculate all             k-points in the same calculation.         ibzkpt (:obj:`str`, optional): Path to IBZKPT file. If set, the             generated k-points will be appended to the k-points in this file             and given a weight of 0. This is necessary for hybrid band             structure calculations.         spg (:obj:`str` or :obj:`int`, optional): The space group international             number or symbol to override the symmetry determined by spglib.             This is not recommended and only provided for testing purposes.             This option will only take effect when ``mode = 'bradcrack'``.         line_density (:obj:`int`, optional): Density of k-points along the             path.         mode (:obj:`str`, optional): Method used for calculating the             high-symmetry path. The options are:              bradcrack                 Use the paths from Bradley and Cracknell. See [brad]_.              pymatgen                 Use the paths from pymatgen. See [curt]_.              seekpath                 Use the paths from SeeK-path. See [seek]_.          cart_coords (:obj:`bool`, optional): Whether the k-points are returned             in cartesian or reciprocal coordinates. Defaults to ``False``             (fractional coordinates).         kpt_list (:obj:`list`, optional): List of k-points to use, formatted as             a list of subpaths, each containing a list of fractional k-points.             For example::                  [ [[0., 0., 0.], [0., 0., 0.5]],                   [[0.5, 0., 0.], [0.5, 0.5, 0.]] ]              Will return points along ``0 0 0 -> 0 0 1/2 | 1/2 0 0             -> 1/2 1/2 0``         path_labels (:obj:`list`, optional): The k-point labels. These should             be provided as a :obj:`list` of :obj:`str` for each subpath of the             overall path. For example::                  [ ['Gamma', 'Z'], ['X', 'M'] ]              combined with the above example for ``kpt_list`` would indicate the             path: Gamma -> Z | X -> M. If no labels are provided, letters from             A -> Z will be used instead. If a label begins with '@' it will be             concealed when plotting with sumo-bandplot.
A script to plot the density of states from a vasprun.xml file.      Args:         filename (:obj:`str`, optional): Path to a vasprun.xml file (can be             gzipped).         prefix (:obj:`str`, optional): Prefix for file names.         directory (:obj:`str`, optional): The directory in which to save files.         elements (:obj:`dict`, optional): The elements and orbitals to extract             from the projected density of states. Should be provided as a             :obj:`dict` with the keys as the element names and corresponding             values as a :obj:`tuple` of orbitals. For example, the following             would extract the Bi s, px, py and d orbitals::                  {'Bi': ('s', 'px', 'py', 'd')}              If an element is included with an empty :obj:`tuple`, all orbitals             for that species will be extracted. If ``elements`` is not set or             set to ``None``, all elements for all species will be extracted.         lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into             their lm contributions (e.g. p -> px, py, pz). Should be provided             as a :obj:`dict`, with the elements names as keys and a             :obj:`tuple` of orbitals as the corresponding values. For example,             the following would be used to decompose the oxygen p and d             orbitals::                  {'O': ('p', 'd')}          atoms (:obj:`dict`, optional): Which atomic sites to use when             calculating the projected density of states. Should be provided as             a :obj:`dict`, with the element names as keys and a :obj:`tuple` of             :obj:`int` specifying the atomic indices as the corresponding             values. The elemental projected density of states will be summed             only over the atom indices specified. If an element is included             with an empty :obj:`tuple`, then all sites for that element will             be included. The indices are 0 based for each element specified in             the POSCAR. For example, the following will calculate the density             of states for the first 4 Sn atoms and all O atoms in the             structure::                  {'Sn': (1, 2, 3, 4), 'O': (, )}              If ``atoms`` is not set or set to ``None`` then all atomic sites             for all elements will be considered.         subplot (:obj:`bool`, optional): Plot the density of states for each             element on separate subplots. Defaults to ``False``.         shift (:obj:`bool`, optional): Shift the energies such that the valence             band maximum (or Fermi level for metals) is at 0 eV. Defaults to             ``True``.         total_only (:obj:`bool`, optional): Only extract the total density of             states. Defaults to ``False``.         plot_total (:obj:`bool`, optional): Plot the total density of states.             Defaults to ``True``.         legend_on (:obj:`bool`, optional): Plot the graph legend. Defaults             to ``True``.         legend_frame_on (:obj:`bool`, optional): Plot a frame around the             graph legend. Defaults to ``False``.         legend_cutoff (:obj:`float`, optional): The cut-off (in % of the             maximum density of states within the plotting range) for an             elemental orbital to be labelled in the legend. This prevents             the legend from containing labels for orbitals that have very             little contribution in the plotting range.         gaussian (:obj:`float`, optional): Broaden the density of states using             convolution with a gaussian function. This parameter controls the             sigma or standard deviation of the gaussian distribution.         height (:obj:`float`, optional): The height of the plot.         width (:obj:`float`, optional): The width of the plot.         xmin (:obj:`float`, optional): The minimum energy on the x-axis.         xmax (:obj:`float`, optional): The maximum energy on the x-axis.         num_columns (:obj:`int`, optional): The number of columns in the             legend.         colours (:obj:`dict`, optional): Use custom colours for specific             element and orbital combinations. Specified as a :obj:`dict` of             :obj:`dict` of the colours. For example::                  {                     'Sn': {'s': 'r', 'p': 'b'},                     'O': {'s': '#000000'}                 }              The colour can be a hex code, series of rgb value, or any other             format supported by matplotlib.         xlabel (:obj:`str`, optional): Label/units for x-axis (i.e. energy)         ylabel (:obj:`str`, optional): Label/units for y-axis (i.e. DOS)         yscale (:obj:`float`, optional): Scaling factor for the y-axis.         style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style             specifications, to be composed on top of Sumo base style.         no_base_style (:obj:`bool`, optional): Prevent use of sumo base style.             This can make alternative styles behave more predictably.         image_format (:obj:`str`, optional): The image file format. Can be any             format supported by matplotlib, including: png, jpg, pdf, and svg.             Defaults to pdf.         dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for             the image.         plt (:obj:`matplotlib.pyplot`, optional): A             :obj:`matplotlib.pyplot` object to use for plotting.         fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a             a single font, specified as a :obj:`str`, or several fonts,             specified as a :obj:`list` of :obj:`str`.      Returns:         A matplotlib pyplot object.
Parse the element and orbital argument strings.      The presence of an element without any orbitals means that we want to plot     all of its orbitals.      Args:         string (str): The element and orbitals as a string, in the form             ``"C.s.p,O"``.      Returns:         dict: The elements and orbitals as a :obj:`dict`. For example::              {'Bi': ['s', 'px', 'py', 'd']}.          If an element symbol is included with an empty list, then all orbitals         for that species are considered.
Parse the atom string.      Args:         atoms_string (str): The atoms to plot, in the form ``"C.1.2.3,"``.      Returns:         dict: The atomic indices over which to sum the DOS. Formatted as::              {Element: [atom_indices]}.          Indices are zero indexed for each atomic species. If an element symbol         is included with an empty list, then all sites for that species are         considered.
Basic support for 3.6's f-strings, in 3.5!      Formats "s" using appropriate globals and locals     dictionaries.  This f-string:         f"hello a is {a}"     simply becomes         f("hello a is {a}")     In other words, just throw parentheses around the     string, and you're done!      Implemented internally using str.format_map().     This means it doesn't support expressions:         f("two minus three is {2-3}")     And it doesn't support function calls:         f("how many elements? {len(my_list)}")     But most other f-string features work.
Accepts either a string or an iterable of strings.     (Iterable is assumed to be individual lines.)     Returns a string.
Find cmd on PATH.
Print help for subcommands.  Prints the help text for the specified subcommand. If subcommand is not specified, prints one-line summaries for every command.
Add a blurb (a Misc/NEWS entry) to the current CPython repo.
Move all new blurbs to a single blurb file for the release.  This is used by the release manager when cutting a new release.
Merge all blurbs together into a single Misc/NEWS file.  Optional output argument specifies where to write to. Default is <cpython-root>/Misc/NEWS.  If overwriting, blurb merge will prompt you to make sure it's okay. To force it to overwrite, use -f.
Creates and populates the Misc/NEWS.d directory tree.
Split the current Misc/NEWS into a zillion little blurb files.  Assumes that the newest version section in Misc/NEWS is under development, and splits those entries into the "next" subdirectory. If the current version has actually been released, use the --released flag.  Also runs "blurb populate" for you.
Parses a string.  Appends a list of blurb ENTRIES to self, as tuples:           (metadata, body)         metadata is a dict.  body is a string.
Read a blurb file.  Broadly equivalent to blurb.parse(open(filename).read()).
Parses a "next" filename into its equivalent blurb metadata. Returns a dict.
changes metadata!
Save out blurbs created from "blurb split".         They don't have dates, so we have to get creative.
cherry-pick COMMIT_SHA1 into target BRANCHES.
return '2.7' from 'backport-sha-2.7'      raises ValueError if the specified branch name is not of a form that         cherry_picker would have created
Validate that a hexdigest sha is a valid commit in the repo      raises ValueError if the sha does not reference a commit within the repo
return version information from a git branch name
Return the current branch
Return a tuple of title and body from the commit message
Check whether the current folder is a Git repo.
Locate and return the default config for current revison.
Choose and return the config path and it's contents as dict.
Save a set of options into Git config.
Remove a set of options from Git config.
Retrieve one option from Git config.
Retrieve given file path contents of certain Git revision.
Save paused progress state into Git config.
Get the remote name to use for upstream branches         Uses "upstream" if it exists, "origin" otherwise
git fetch <upstream>
git checkout -b <branch_name>
Return the commit message for the current commit hash,         replace #<PRID> with GH-<PRID>
git checkout default branch
git cherry-pick -x <commit_sha1>
prefix the commit message with (X.Y)
git push <origin> <branchname>
Create PR in GitHub
open url in the web browser
Remove the temporary backport branch.          Switch to the default branch before that.
run `git cherry-pick --abort` and then clean up the branch
git push origin <current_branch>         open the PR         clean up branch
Return the run progress state stored in the Git config.          Raises ValueError if the retrieved state is not of a form that                           cherry_picker would have stored in the config.
Open a file-like object using a pkg relative path.      Example:          fd = openDatFile('foopkg.barpkg/wootwoot.bin')
Scrape types from a blob of text and return node tuples.      Args:         text (str): Text to scrape.         ptype (str): Optional ptype to scrape. If present, only scrape rules which match the provided type.      Returns:         (str, str): Yield tuples of type, valu strings.
Use msgpack to serialize a compatible python object.      Args:         item (obj): The object to serialize      Notes:         String objects are encoded using utf8 encoding.  In order to handle         potentially malformed input, ``unicode_errors='surrogatepass'`` is set         to allow encoding bad input strings.      Returns:         bytes: The serialized bytes in msgpack format.
Use msgpack to de-serialize a python object.      Args:         byts (bytes): The bytes to de-serialize      Notes:         String objects are decoded using utf8 encoding.  In order to handle         potentially malformed input, ``unicode_errors='surrogatepass'`` is set         to allow decoding bad input strings.      Returns:         obj: The de-serialized object
Generator which unpacks a file object of msgpacked content.      Args:         fd: File object to consume data from.      Notes:         String objects are decoded using utf8 encoding.  In order to handle         potentially malformed input, ``unicode_errors='surrogatepass'`` is set         to allow decoding bad input strings.      Yields:         Objects from a msgpack stream.
Generator which yields msgpack objects from a file path.      Args:         path: File path to open and consume data from.      Notes:         String objects are decoded using utf8 encoding.  In order to handle         potentially malformed input, ``unicode_errors='surrogatepass'`` is set         to allow decoding bad input strings.      Yields:         Objects from a msgpack stream.
Dump an object to a file by path.      Args:         item (object): The object to serialize.         path (str): The file path to save.      Returns:         None
Feed bytes to the unpacker and return completed objects.          Args:             byts (bytes): Bytes to unpack.          Notes:             It is intended that this function is called multiple times with             bytes from some sort of a stream, as it will unpack and return             objects as they are available.          Returns:             list: List of tuples containing the item size and the unpacked item.
Clear the cached model rows and rebuild them only if they have been loaded already.
Get byts for a message
Calculate the imei check byte.
Execute a function in an executor thread.      Args:         todo ((func,args,kwargs)): A todo tuple.
Initializes (or re-initializes for testing purposes) all of a task's task-local variables      Precondition:         If task is None, this must be called from task context
Note: No locking is provided.  Under normal circumstances, like the other task is not running (e.g. this is running     from the same event loop as the task) or task is the current task, this is fine.
Access a task local variable by name      Precondition:         If task is None, this must be called from task context
Walk the phone info tree to find the best-match info for the given number.      Example:          info = getPhoneInfo(17035551212)         country = info.get('cc')
Open a HiveDict at the given full path.
Load a node from storage into the tree.         ( used by initialization routines to build the tree)
A set operation at the hive level (full path).
Atomically increments a node's value.
Remove and return the value for the given node.
Add the structured data from items to the CryoTank.          Args:             items (list):  A list of objects to store in the CryoTank.             seqn (iden, offs): An iden / offset pair to record.          Returns:             int: The ending offset of the items or seqn.
Yield metrics rows starting at offset.          Args:             offs (int): The index offset.             size (int): The maximum number of records to yield.          Yields:             ((int, dict)): An index offset, info tuple for metrics.
Yield a number of items from the CryoTank starting at a given offset.          Args:             offs (int): The index of the desired datum (starts at 0)             size (int): The max number of items to yield.          Yields:             ((index, object)): Index and item values.
Yield a number of raw items from the CryoTank starting at a given offset.          Args:             offs (int): The index of the desired datum (starts at 0)             size (int): The max number of items to yield.          Yields:             ((indx, bytes)): Index and msgpacked bytes.
Returns information about the CryoTank instance.          Returns:             dict: A dict containing items and metrics indexes.
Generate a new CryoTank with a given name or get an reference to an existing CryoTank.          Args:             name (str): Name of the CryoTank.          Returns:             CryoTank: A CryoTank instance.
Get a list of (name, info) tuples for the CryoTanks.          Returns:             list: A list of tufos.
Argparse expects exit() to be a terminal function and not return.         As such, this function must raise an exception which will be caught         by Cmd.hasValidOpts.
Note:  this overrides an existing method in ArgumentParser
Generate a uniq hash for the JSON compatible primitive data structure.
Argparse expects exit() to be a terminal function and not return.         As such, this function must raise an exception instead.
Retrieve volume usage info for the given path.
Consume chars in set from the string and return (subtxt,offset).      Example:          text = "foo(bar)"         chars = set('abcdefghijklmnopqrstuvwxyz')          name,off = nom(text,0,chars)
Parse a list (likely for comp type) coming from a command line input.      The string elements within the list may optionally be quoted.
Parse in a command line string which may be quoted.
Special syntax for the right side of equals in a macro
Parse a foo:bar=<valu> kwarg into (prop,valu),off
Parse a foo:bar=<valu>[,...] kwarg list into (prop,valu),off
A storm sub-query aware command line splitter.         ( not for storm commands, but for commands which may take storm )
Consume and return one command argument, stopping when it hits a character (not in a quotation) in `until`.
foo:bar = hehe
:foo=10
.foo = bar
<- * / <- prop
<+- * / <+- prop
-> *         -> #tag.match         -> form:prop         -> form
Ignore whitespace as well as comment syntax ``//`` and ``/* ... */``
:foo:bar -> baz:faz
:foo:bar -+> baz:faz
foo:bar
:foo:bar
.foo
$foo         $foo.bar         $foo.bar()         $foo[0]         $foo.bar(10)
cmdargv *must* have leading whitespace to prevent         foo@bar from becoming cmdname foo with argv=[@bar]
Parse a time string into an epoch millis value.
Return a date string for an epoch-millis timestamp.      Args:         tick (int): The timestamp in milliseconds since the epoch.      Returns:         (str):  A date time string
Parse a simple time delta string and return the delta.
Encrypt the given bytes and return an envelope dict in msgpack form.          Args:             byts (bytes): The message to be encrypted.             asscd (bytes): Extra data that needs to be authenticated (but not encrypted).          Returns:             bytes: The encrypted message. This is a msgpacked dictionary             containing the IV, ciphertext, and associated data.
Decode an envelope dict and decrypt the given bytes.          Args:             byts (bytes): Bytes to decrypt.          Returns:             bytes: Decrypted message.
Wrap a message with a sequence number and encrypt it.          Args:             mesg: The mesg to encrypt.          Returns:             bytes: The encrypted message.
Decrypt a message, validating its sequence number is as we expect.          Args:             ciphertext (bytes): The message to decrypt and verify.          Returns:             mesg: A mesg.          Raises:             s_exc.CryptoErr: If the message decryption fails or the sequence number was unexpected.
Parse a Semantic Version string into is component parts.      Args:         text (str): A text string to parse into semver components. This string has whitespace and leading 'v'         characters stripped off of it.      Examples:         Parse a string into it semvar parts::              parts = parseSemver('v1.2.3')      Returns:         dict: The dictionary will contain the keys 'major', 'minor' and 'patch' pointing to integer values.         The dictionary may also contain keys for 'build' and 'pre' information if that data is parsed out         of a semver string. None is returned if the string is not a valid Semver string.
Pack a set of major/minor/patch integers into a single integer for storage.      Args:         major (int): Major version level integer.         minor (int): Minor version level integer.         patch (int): Patch version level integer.      Returns:         int:  System normalized integer value to represent a software version.
Unpack a system normalized integer representing a softare version into its component parts.      Args:         ver (int): System normalized integer value to unpack into a tuple.      Returns:         (int, int, int): A tuple containing the major, minor and patch values shifted out of the integer.
Join a string of parts together with a . separator.      Args:         *vsnparts:      Returns:
Extract a list of major/minor/version integer strings from a string.      Args:         text (str): String to parse         seps (tuple): A tuple or list of separators to use when parsing the version string.      Examples:         Parse a simple version string into a major and minor parts::              parts = parseVersionParts('1.2')          Parse a complex version string into a major and minor parts::              parts = parseVersionParts('wowsoft_1.2')          Parse a simple version string into a major, minor and patch parts.  Parts after the "3." are dropped from the         results::              parts = parseVersionParts('1.2.3.4.5')      Notes:         This attempts to brute force out integers from the version string by stripping any leading ascii letters and         part separators, and then regexing out numeric parts optionally followed by part separators.  It will stop at         the first mixed-character part encountered.  For example, "1.2-3a" would only parse out the "1" and "2" from         the string.      Returns:         dict: Either a empty dictionary or dictionary containing up to three keys, 'major', 'minor' and 'patch'.
Set a name in the SlabDict.          Args:             name (str): The key name.             valu (obj): A msgpack compatible value.          Returns:             None
Pop a name from the SlabDict.          Args:             name (str): The name to remove.             defval (obj): The default value to return if the name is not present.          Returns:             object: The object stored in the SlabDict, or defval if the object was not present.
Note:             This method may raise a MapFullError
Deletes an **entire database** (i.e. a table), losing all data.
Return the last key/value pair from the given db.
Returns:             Tuple of number of items consumed, number of items added
Copy an entire database in this slab to a new database in potentially another slab.          Args:             sourcedb (LmdbDatabase): which database in this slab to copy rows from             destslab (LmdbSlab): which slab to copy rows to             destdbname (str): the name of the database to copy rows to in destslab             progresscb (Callable[int]):  if not None, this function will be periodically called with the number of rows                                          completed          Returns:             (int): the number of rows copied          Note:             If any rows already exist in the target database, this method returns an error.  This means that one cannot             use destdbname=None unless there are no explicit databases in the destination slab.
Like put, but returns the previous value if existed
Note:             This method may raise a MapFullError
Retrieve the ipv4 address for this host ( optionally as seen from dest ).     Example:         addr = s_socket.hostaddr()
Returns the iden that starts with prefix.  Prints out error and returns None if it doesn't match         exactly one.
Decode the given byts with the named decoder.     If name is a comma separated list of decoders,     loop through and do them all.      Example:          byts = s_encoding.decode('base64',byts)      Note: Decoder names may also be prefixed with +           to *encode* for that name/layer.
Add an additional ingest file format
Iterate through the data provided by a file like object.      Optional parameters may be used to control how the data     is deserialized.      Examples:         The following example show use of the iterdata function.::              with open('foo.csv','rb') as fd:                 for row in iterdata(fd, format='csv', encoding='utf8'):                     dostuff(row)      Args:         fd (file) : File like object to iterate over.         close_fd (bool) : Default behavior is to close the fd object.                           If this is not true, the fd will not be closed.         **opts (dict): Ingest open directive.  Causes the data in the fd                        to be parsed according to the 'format' key and any                        additional arguments.      Yields:         An item to process. The type of the item is dependent on the format         parameters.
Remove any v0 (i.e. pre-010) rules from storage and replace them with v1 rules.          Notes:             v0 had two differences user was a username.  Replaced with iden of user as 'iden' field.             Also 'iden' was storage as binary.  Now it is stored as hex string.
Initialize a new (min,max) tuple interval from values.      Args:         *vals ([int,...]):  A list of values (or Nones)      Returns:         ((int,int)):    A (min,max) interval tuple or None
Determine if two interval tuples have overlap.      Args:         iv0 ((int,int)):    An interval tuple         iv1 ((int,int));    An interval tuple      Returns:         (bool): True if the intervals overlap, otherwise False
Parse an interval time string and return a (min,max) tuple.      Args:         text (str): A time interval string      Returns:         ((int,int)):    A epoch millis epoch time string
Get a proxy to a cortex backed by a temporary directory.      Args:         mods (list): A list of modules which are loaded into the cortex.      Notes:         The cortex and temporary directory are town down on exit.         This should only be called from synchronous code.      Returns:         Proxy to the cortex.
Set the view layers from a list of idens.         NOTE: view layers are stored "top down" ( write is layers[0] )
Yield Node.pack() tuples which match the query.
Adds a trigger to the cortex
Check that, as a non-admin, may only manipulate resources created by you.
Deletes a trigger from the cortex
Change an existing trigger's query
Lists all the triggers that the current user is authorized to access
Add a cron job to the cortex          A cron job is a persistently-stored item that causes storm queries to be run in the future.  The specification         for the times that the queries run can be one-shot or recurring.          Args:             query (str):  The storm query to execute in the future             reqs (Union[Dict[str, Union[int, List[int]]], List[Dict[...]]]):                 Either a dict of the fixed time fields or a list of such dicts.  The keys are in the set ('year',                 'month', 'dayofmonth', 'dayofweek', 'hour', 'minute'.  The values must be positive integers, except for                 the key of 'dayofmonth' in which it may also be a negative integer which represents the number of days                 from the end of the month with -1 representing the last day of the month.  All values may also be lists                 of valid values.             incunit (Optional[str]):                 A member of the same set as above, with an additional member 'day'.  If is None (default), then the                 appointment is one-shot and will not recur.             incval (Union[int, List[int]):                 A integer or a list of integers of the number of units          Returns (bytes):             An iden that can be used to later modify, query, and delete the job.          Notes:             reqs must have fields present or incunit must not be None (or both)             The incunit if not None it must be larger in unit size than all the keys in all reqs elements.
Delete a cron job          Args:             iden (bytes):  The iden of the cron job to be deleted
Change an existing cron job's query          Args:             iden (bytes):  The iden of the cron job to be changed
Get information about all the cron jobs accessible to the current user
Add a tag to a node specified by iden.          Args:             iden (str): A hex encoded node BUID.             tag (str):  A tag string.             valu (tuple):  A time interval tuple or (None, None).
Add a list of packed nodes to the cortex.          Args:             nodes (list): [ ( (form, valu), {'props':{}, 'tags':{}}), ... ]          Yields:             (tuple): Packed node tuples ((form,valu), {'props': {}, 'tags':{}})
Count the number of nodes which result from a storm query.          Args:             text (str): Storm query text.             opts (dict): Storm query options.          Returns:             (int): The number of nodes resulting from the query.
Evalute a storm query and yield packed nodes.
Evaluate a storm query and yield result messages.         Yields:             ((str,dict)): Storm messages.
Return the list of splices at the given offset.
Return stream of (iden, provenance stack) tuples at the given offset.
Return the providence stack associated with the given iden.          Args:             iden (str):  the iden from splice          Note: the iden appears on each splice entry as the 'prov' property
Registration for built-in Storm commands.
Registration for built-in Storm Libraries
Registration for splice handlers.
Registration for built-in Layer ctors
Registration for built-in Cortex feed functions.
Registration for built-in Cortex httpapi endpoints
Recalculate form counts from scratch.
Register a callback for tag addition.          Args:             name (str): The name of the tag or tag glob.             func (function): The callback func(node, tagname, tagval).
Unregister a callback for tag addition.          Args:             name (str): The name of the tag or tag glob.             func (function): The callback func(node, tagname, tagval).
Register a callback for tag deletion.          Args:             name (str): The name of the tag or tag glob.             func (function): The callback func(node, tagname, tagval).
Unregister a callback for tag deletion.          Args:             name (str): The name of the tag or tag glob.             func (function): The callback func(node, tagname, tagval).
Execute a runt lift function.          Args:             full (str): Property to lift by.             valu:             cmpr:          Returns:             bytes, list: Yields bytes, list tuples where the list contains a series of                 key/value pairs which are used to construct a Node object.
Delete a cortex view by iden.
Args:             layers ([str]): A top-down list of of layer guids             iden (str): The view iden ( defaults to default view ).
Add a Layer to the cortex.          Notes:              The addLayer ``**info`` arg is expected to be shaped like the following::                  info = {                     'iden': <str>, ( optional iden. default guid() )                     'type': <str>, ( optional type. default lmdb )                     'owner': <str>, ( optional owner. default root )                     'config': {}, # type specific config options.                 }
Convenience function to join a remote telepath layer         into this cortex and default view.
Add a synapse.lib.storm.Cmd class to the cortex.
feeds:             - cryotank: tcp://cryo.vertex.link/cryo00/tank01               type: syn.splice
Get a list of packed nodes from a ingest definition.
Evaluate a storm query and yield Nodes only.
Evaluate a storm query and yield (node, path) tuples.         Yields:             (Node, Path) tuples
A simple non-streaming way to return a list of nodes.
Evaluate a storm query and yield result messages.         Yields:             ((str,dict)): Storm messages.
Parse storm query text and return a Query object.
Log a storm query.
Return a single Node() instance by (form,valu) tuple.
Get nodes by a property value or lift syntax.          Args:             full (str): The full name of a property <form>:<prop>.             valu (obj): A value that the type knows how to lift by.             cmpr (str): The comparison operator you are lifting by.          Some node property types allow special syntax here.          Examples:              # simple lift by property equality             core.getNodesBy('file:bytes:size', 20)              # The inet:ipv4 type knows about cidr syntax             core.getNodesBy('inet:ipv4', '1.2.3.0/24')
Quickly add/modify a list of nodes from node definition tuples.         This API is the simplest/fastest way to add nodes, set node props,         and add tags to nodes remotely.          Args:              nodedefs (list): A list of node definition tuples. See below.          A node definition tuple is defined as:              ( (form, valu), {'props':{}, 'tags':{})          The "props" or "tags" keys may be omitted.
Add data using a feed/parser function.          Args:             name (str): The name of the feed record format.             items (list): A list of items to ingest.             seqn ((str,int)): An (iden, offs) tuple for this feed chunk.          Returns:             (int): The next expected offset (or None) if seqn is None.
Return a transaction object for the default view.          Args:             write (bool): Set to True for a write transaction.          Returns:             (synapse.lib.snap.Snap)          NOTE: This must be used in a with block.
Load a single cortex module with the given ctor and conf.          Args:             ctor (str): The python module class path             conf (dict):Config dictionary for the module
Get the normalized property value based on the Cortex data model.          Args:             prop (str): The property to normalize.             valu: The value to normalize.          Returns:             (tuple): A two item tuple, containing the normed value and the info dictionary.          Raises:             s_exc.NoSuchProp: If the prop does not exist.             s_exc.BadTypeValu: If the value fails to normalize.
Get the normalized type value based on the Cortex data model.          Args:             name (str): The type to normalize.             valu: The value to normalize.          Returns:             (tuple): A two item tuple, containing the normed value and the info dictionary.          Raises:             s_exc.NoSuchType: If the type does not exist.             s_exc.BadTypeValu: If the value fails to normalize.
Execute a series of storage operations.          Overrides implementation in layer.py to avoid unnecessary async calls.
Check for any pre-010 entries in 'dbname' in my slab and migrate those to the new slab.          Once complete, drop the database from me with the name 'dbname'          Returns (bool): True if a migration occurred, else False
Check for any pre-010 provstacks and migrate those to the new slab.
Migration-only method          Notes:             Precondition: buid cache must be disabled
Migration-only method          Notes:             Precondition: buid cache must be disabled
Migration-only function
Iterate (buid, valu) rows for the given form in this layer.
Iterate (buid, valu) rows for the given form:prop in this layer.
Iterate (buid, valu) rows for the given universal prop
Resolve a telepath alias via ~/.syn/aliases.yaml      Args:         name (str): Name of the alias to resolve.      Notes:         An exact match against the aliases will always be returned first.         If no exact match is found and the name contains a '/' in it, the         value before the slash is looked up and the remainder of the path         is joined to any result. This is done to support dynamic Telepath         share names.      Returns:         str: The url string, if present in the alias.  None will be returned         if there are no matches.
Open a URL to a remote telepath object.      Args:         url (str): A telepath URL.         **opts (dict): Telepath connect options.      Returns:         (synapse.telepath.Proxy): A telepath proxy object.      The telepath proxy may then be used for sync or async calls:          proxy = openurl(url)         value = proxy.getFooThing()      ... or ...          proxy = await openurl(url)         valu = await proxy.getFooThing()      ... or ...          async with await openurl(url) as proxy:             valu = await proxy.getFooThing()
Call a remote method by name.          Args:             methname (str): The name of the remote method.             *args: Arguments to the method call.             **kwargs: Keyword arguments to the method call.          Most use cases will likely use the proxy methods directly:          The following two are effectively the same:              valu = proxy.getFooBar(x, y)             valu = proxy.call('getFooBar', x, y)
Returns True if the rate limit has not been reached.          Example:              if not rlimit.allows():                 rasie RateExceeded()              # ok to go...
Disable and invalidate the layer buid cache for migration
Returns:             Iterable[Tuple[bytes, Dict[str, Any]]]:  yield a stream of tuple (buid, propdict)
Return a list of "fully qualified" class names for an instance.      Example:          for name in getClsNames(foo):             print(name)
Return a fully qualified string for the <mod>.<class>.<func> name     of a given method.
Iterate the locals of an item and yield (name,valu) pairs.      Example:          for name,valu in getItemLocals(item):             dostuff()
Get a dictionary of special annotations for a Telepath Proxy.      Args:         item:  Item to inspect.      Notes:         This will set the ``_syn_telemeth`` attribute on the item         and the items class, so this data is only computed once.      Returns:         dict: A dictionary of methods requiring special handling by the proxy.
Switch to another user (admin only).          This API allows remote admin/service accounts         to impersonate a user.  Used mostly by services         that manage their own authentication/sessions.
Get the value of a key in the cell default hive
Set or change the value of a key in the cell default hive
Remove and return the value of a key in the cell default hive
Set the admin status of the given user/role.
An admin only API endpoint for getting user info.
A signal handler used to print asyncio task stacks and thread stacks.
Schedule a coroutine to run on the global loop and return it's result.      Args:         coro (coroutine): The coroutine instance.      Notes:         This API is thread safe and should only be called by non-loop threads.
The synchelp decorator allows the transparent execution of     a coroutine using the global loop from a thread other than     the event loop.  In both use cases, teh actual work is done     by the global event loop.      Examples:          Use as a decorator::              @s_glob.synchelp             async def stuff(x, y):                 await dostuff()          Calling the stuff function as regular async code using the standard await syntax::              valu = await stuff(x, y)          Calling the stuff function as regular sync code outside of the event loop thread::              valu = stuff(x, y)
Try to match a day-of-week abbreviation, then try a day-of-week full name
Parse a non-day increment value. Should be an integer or a comma-separated integer list.
Parse a non-day fixed value
Parse a --day argument
Prints details about a particular cron job. Not actually a different API call
Promote the currently running task.
Create a synapse task from the given coroutine.
Add a function/coroutine/Base to be called on fini().
Remove a callback function previously added with link()          Example:              base.unlink( callback )
Add an base function callback for a specific event with optional filtering.  If the function returns a         coroutine, it will be awaited.          Args:             evnt (str):         An event name             func (function):    A callback function to receive event tufo          Examples:              Add a callback function and fire it:                  async def baz(event):                     x = event[1].get('x')                     y = event[1].get('y')                     return x + y                  d.on('foo', baz)                  # this fire triggers baz...                 await d.fire('foo', x=10, y=20)          Returns:             None:
Remove a previously registered event handler function.          Example:              base.off( 'foo', onFooFunc )
Fire the given event name on the Base.         Returns a list of the return values of each callback.          Example:              for ret in d.fire('woot',foo='asdf'):                 print('got: %r' % (ret,))
Distribute an existing event tuple.          Args:             mesg ((str,dict)):  An event tuple.          Example:              await base.dist( ('foo',{'bar':'baz'}) )
Shut down the object and notify any onfini() coroutines.          Returns:             Remaining ref count
A context manager which can be used to add a callback and remove it when         using a ``with`` statement.          Args:             evnt (str):         An event name             func (function):    A callback function to receive event tufo
Wait for the base to fini()          Returns:             None if timed out, True if fini happened          Example:              base.waitfini(timeout=30)
Schedules a free-running coroutine to run on this base's event loop.  Kills the coroutine if Base is fini'd.         It does not pend on coroutine completion.          Precondition:             This function is *not* threadsafe and must be run on the Base's event loop          Returns:             asyncio.Task: An asyncio.Task object.
Schedules a coroutine to run as soon as possible on the same event loop that this Base is running on          Note:             This method may *not* be run inside an event loop
Register SIGTERM/SIGINT signal handlers with the ioloop to fini this object.
Construct and return a new Waiter for events on this base.          Example:              # wait up to 3 seconds for 10 foo:bar events...              waiter = base.waiter(10,'foo:bar')              # .. fire thread that will cause foo:bar events              events = waiter.wait(timeout=3)              if events == None:                 # handle the timout case...              for event in events:                 # parse the events if you need...          NOTE: use with caution... it's easy to accidentally construct               race conditions with this mechanism ;)
Wait for the required number of events and return them or None on timeout.          Example:              evnts = waiter.wait(timeout=30)              if evnts == None:                 handleTimedOut()                 return              for evnt in evnts:                 doStuff(evnt)
Add a Base (or sub-class) to the BaseRef by name.          Args:             name (str): The name/iden of the Base             base (Base): The Base instance          Returns:             (None)
Atomically get/gen a Base and incref.         (requires ctor during BaseRef init)          Args:             name (str): The name/iden of the Base instance.
Get a 16 byte guid value.      By default, this is a random guid value.      Args:         valu: Object used to construct the guid valu from.  This must be able             to be msgpack'd.      Returns:         str: 32 character, lowercase ascii string.
A binary GUID like sequence of 32 bytes.      Args:         valu (object): Optional, if provided, the hash of the msgpack         encoded form of the object is returned. This can be used to         create stable buids.      Notes:         By default, this returns a random 32 byte value.      Returns:         bytes: A 32 byte value.
Ensure ( or coerce ) a value into being an integer or None.      Args:         x (obj):    An object to intify      Returns:         (int):  The int value ( or None )
Create or open ( for read/write ) a file path join.      Args:         *paths: A list of paths to join together to make the file.      Notes:         If the file already exists, the fd returned is opened in ``r+b`` mode.         Otherwise, the fd is opened in ``w+b`` mode.      Returns:         io.BufferedRandom: A file-object which can be read/written too.
A file lock with-block helper.      Args:         path (str): A path to a lock file.      Examples:          Get the lock on a file and dostuff while having the lock::              path = '/hehe/haha.lock'             with lockfile(path):                 dostuff()      Notes:         This is curently based on fcntl.lockf(), and as such, it is purely         advisory locking. If multiple processes are attempting to obtain a         lock on the same file, this will block until the process which has         the current lock releases it.      Yields:         None
List the (optionally glob filtered) full paths from a dir.      Args:         *paths ([str,...]): A list of path elements         glob (str): An optional fnmatch glob str
Combines/creates a yaml file and combines with obj.  obj and file must be maps/dict or empty.
Get an err tufo from an exception.      Args:         e (Exception): An Exception (or Exception subclass).      Notes:         This can be called outside of the context of an exception handler,         however details such as file, line, function name and source may be         missing.      Returns:         ((str, dict)):
Populate err,errmsg,errtrace info from exc.
Divide an iterable into chunks.      Args:         item: Item to slice         size (int): Maximum chunk size.      Notes:         This supports Generator objects and objects which support calling         the __getitem__() method with a slice object.      Yields:         Slices of the item containing up to "size" number of items.
Generator which yields bytes from a file descriptor.      Args:         fd (file): A file-like object to read bytes from.         size (int): Size, in bytes, of the number of bytes to read from the         fd at a given time.      Notes:         If the first read call on the file descriptor is a empty bytestring,         that zero length bytestring will be yielded and the generator will         then be exhuasted. This behavior is intended to allow the yielding of         contents of a zero byte file.      Yields:         bytes: Bytes from the file descriptor.
A decorator for making a function fire a thread.
Configure synapse logging.      Args:         mlogger (logging.Logger): Reference to a logging.Logger()         defval (str): Default log level      Notes:         This calls logging.basicConfig and should only be called once per process.      Returns:         None
Return a value or raise an exception from a retn tuple.
Initialize a config dict using the given confdef tuples.
Migrate old cell Auth() data into a HiveAuth().
Fire the onset() handlers for this property.          Args:             node (synapse.lib.node.Node): The node whose property was set.             oldv (obj): The previous value of the property.
Get a list of storage operations to delete this property from the buid.          Args:             buid (bytes): The node buid.          Returns:             (tuple): The storage operations
Fire the onAdd() callbacks for node creation.
Fire the onDel() callbacks for node deletion.
Get a set of lift operations for use with an Xact.
Adds a model definition (same format as input to Model.addDataModels and output of Model.getModelDef).
Add a list of (name, mdef) tuples.          A model definition (mdef) is structured as follows::              {                 "ctors":(                     ('name', 'class.path.ctor', {}, {'doc': 'The foo thing.'}),                 ),                  "types":(                     ('name', ('basetype', {typeopts}), {info}),                 ),                  "forms":(                     (formname, (typename, typeopts), {info}, (                         (propname, (typename, typeopts), {info}),                     )),                 ),                 "univs":(                     (propname, (typename, typeopts), {info}),                 )             }          Args:             mods (list):  The list of tuples.          Returns:             None
Add a Type instance to the data model.
Bind and listen on the given host/port with possible SSL.          Args:             host (str): A hostname or IP address.             port (int): The TCP port to bind.
Share an object via the telepath protocol.          Args:             name (str): Name of the shared object             item (object): The object to share over telepath.
Returns a valid day of the month given the desired value.      Negative values are interpreted as offset backwards from the last day of the month, with -1 representing the     last day of the month.  Out-of-range values are clamped to the first or last day of the month.
Make ApptRec json/msgpack-friendly
Convert from json/msgpack-friendly
Returns next timestamp that meets requirements, incrementing by (self.incunit * incval) if not increasing, or         0.0 if there are no future matches
Return a datetime incremented by incunit * incval
Find the next time this appointment should be scheduled.          Delete any nonrecurring record that just happened.
Enable cron jobs to start running, start the scheduler loop          Go through all the appointments, making sure the query is valid, and remove the ones that aren't.  (We can't         evaluate queries until enabled because not all the modules are loaded yet.)
Load all the appointments from persistent storage
Updates the data structures to add an appointment
Store a single appointment
Yields a series of dicts that cover the combination of all multiple-value (e.g. lists or tuples) values, with         non-multiple-value values remaining the same.
Persistently adds an appointment          Args:             query (str):                 storm query to run             reqs (Union[None, Dict[TimeUnit, Union[int, Tuple[int]], List[...]):                 one or more dicts of the fixed aspects of the appointment.  dict value may be a single or multiple.                 May be an empty dict or None.             incunit (Union[None, TimeUnit]):                 the unit that changes for recurring, or None for non-recurring.  It is an error for this value to match                 a key in reqdict.             incvals (Union[None, int, Iterable[int]): count of units of incunit or explicit day of week or day of month.                 Not allowed for incunit == None, required for others (1 would be a typical                 value)          Notes:             For values in reqs that are lists and incvals if a list, all combinations of all values (the product) are             used          Returns:             iden of new appointment
Change the query of an appointment
Delete an appointment
Task loop to issue query tasks at the right times.
Fire off the task to make the storm query
Actually run the storm query, updating the appropriate statistics and results
Return an object from the embedded synapse data folder.      Example:          for tld in syanpse.data.get('iana.tlds'):             dostuff(tld)      NOTE: Files are named synapse/data/<name>.mpk
Return the valu of a given property on the node.      Args:         pode (tuple): A packed node.         prop (str): Property to retrieve.      Notes:         The prop argument may be the full property name (foo:bar:baz), relative property name (:baz) , or the unadorned         property name (baz).      Returns:
Get all the tags for a given node.      Args:         pode (tuple): A packed node.         leaf (bool): If True, only return the full tags.      Returns:         list: A list of tag strings.
Check if a packed node has a given tag.      Args:         pode (tuple): A packed node.         tag (str): The tag to check.      Examples:         Check if a node is tagged with "woot" and dostuff if it is.              if s_node.tagged(node,'woot'):                 dostuff()      Notes:         If the tag starts with `#`, this is removed prior to checking.      Returns:         bool: True if the tag is present. False otherwise.
Args:             path (Path):                 If set, then vars from path are copied into the new runtime, and vars are copied back out into path                 at the end          Note:             If opts is not None and opts['vars'] is set and path is not None, then values of path vars take precedent
Return the serializable/packed version of the node.          Returns:             (tuple): An (iden, info) node tuple.
Update the .seen interval and optionally a source specific seen node.
Return a list of (prop, (form, valu)) refs out for the node.
Set a property on the node.          Args:             name (str): The name of the property.             valu (obj): The value of the property.             init (bool): Set to True to disable read-only enforcement          Returns:             (bool): True if the property was changed.
Generate operations to set a property on a node.
Return a secondary property value from the Node.          Args:             name (str): The name of a secondary property.          Returns:             (obj): The secondary property value or None.
Remove a property from a node and return the value
Delete a tag from the node.
Delete a node from the cortex.          The following tear-down operations occur in order:              * validate that you have permissions to delete the node             * validate that you have permissions to delete all tags             * validate that there are no remaining references to the node.              * delete all the tags (bottom up)                 * fire onDelTag() handlers                 * delete tag properties from storage                 * log tag:del splices              * delete all secondary properties                 * fire onDelProp handler                 * delete secondary property from storage                 * log prop:del splices              * delete the primary property                 * fire onDel handlers for the node                 * delete primary property from storage                 * log node:del splices
Returns:         a regular expression string with ** and * interpreted as tag globs      Precondition:         tag is a valid tagmatch      Notes:         A single asterisk will replace exactly one dot-delimited component of a tag         A double asterisk will replace one or more of any character.          The returned string does not contain a starting '^' or trailing '$'.
Note:  we override default impl from parent to avoid costly KeyError
A helper method for Type subclasses to use for a simple way to truncate         indx bytes.
Compare the two values using the given type specific comparator.
Normalize the value for a given type.          Args:             valu (obj): The value to normalize.          Returns:             ((obj,dict)): The normalized valu, info tuple.          Notes:             The info dictionary uses the following key conventions:                 subs (dict): The normalized sub-fields as name: valu entries.
Return the property index bytes for the given *normalized* value.
Extend this type to construct a sub-type.          Args:             name (str): The name of the new sub-type.             opts (dict): The type options for the sub-type.             info (dict): The type info for the sub-type.          Returns:             (synapse.types.Type): A new sub-type instance.
Create a new instance of this type with the specified options.          Args:             opts (dict): The type specific options for the new instance.
Return a list of index operation tuples to lift values in a table.          Valid index operations include:             ('eq', <indx>)             ('pref', <indx>)             ('range', (<minindx>, <maxindx>))
Get a tick, tock time pair.          Args:             vals (list): A pair of values to norm.          Returns:             (int, int): A ordered pair of integers.
Override default ``*range=`` handler to account for relative computation.
Override default *range= handler to account for relative computation.
Async connect and return a Link().
Listen on the given host/port and fire onlink(Link).      Returns a server object that contains the listening sockets
Start an PF_UNIX server listening on the given path.
Connect to a PF_UNIX server listening on the given path.
Async transmit routine which will wait for writer drain().
Return sibling node by relative offset from self.
Yield "rightward" siblings until None.
Execute a non-coroutine function in the ioloop executor pool.      Args:         func: Function to execute.         *args: Args for the function.         **kwargs: Kwargs for the function.      Examples:          Execute a blocking API call in the executor pool::              import requests              def block(url, params=None):                 return requests.get(url, params=params).json()              fut = s_coro.executor(block, 'http://some.tld/thign')             resp = await fut      Returns:         asyncio.Future: An asyncio future.
Wait on an an asyncio event with an optional timeout      Returns:         true if the event got set, None if timed out
Calls func and awaits it if a returns a coroutine.      Note:         This is useful for implementing a function that might take a telepath proxy object or a local object, and you         must call a non-async method on that object.          This is also useful when calling a callback that might either be a coroutine function or a regular function.     Usage:         ok = await s_coro.ornot(maybeproxy.allowed, 'path')
Ensure a string is valid hex.      Args:         text (str): String to normalize.      Examples:         Norm a few strings:              hexstr('0xff00')             hexstr('ff00')      Notes:         Will accept strings prefixed by '0x' or '0X' and remove them.      Returns:         str: Normalized hex string.
Divide a normalized tag string into hierarchical layers.
Dynamically import a python module and return a local.      Example:          cls = getDynLocal('foopkg.barmod.BlahClass')         blah = cls()
Retrieve and return an unbound method by python path.
Dynamically import a python module or exception.
Dynamically import a module and return a module local or raise an exception.
Run a dynamic task and return the result.      Example:          foo = runDynTask( ('baz.faz.Foo', (), {} ) )
Hold this during a series of renames to delay ndef         secondary property processing until the end....
Rename a form within all the layers.
Change all props as a result of an ndef change.
Produce a deconflicted list of form values across layers         as a *copy* to avoid iter vs edit issues in the indexes.
Yield packed node tuples for the given storm query text.
Execute a storm query and yield (Node(), Path()) tuples.
Run a storm query and yield Node() objects.
Retrieve a node tuple by binary id.          Args:             buid (bytes): The binary ID for the node.          Returns:             Optional[s_node.Node]: The node object or None.
Return a single Node by (form,valu) tuple.          Args:             ndef ((str,obj)): A (form,valu) ndef tuple.  valu must be             normalized.          Returns:             (synapse.lib.node.Node): The Node or None.
The main function for retrieving nodes by prop.          Args:             full (str): The property/tag name.             valu (obj): A lift compatible value for the type.             cmpr (str): An optional alternate comparator.          Yields:             (synapse.lib.node.Node): Node instances.
Add a node by form name and value with optional props.          Args:             name (str): The form of node to add.             valu (obj): The value for the node.             props (dict): Optional secondary properties for the node.
Call a feed function and return what it returns (typically yields Node()s).          Args:             name (str): The name of the feed record type.             items (list): A list of records of the given feed type.          Returns:             (object): The return value from the feed function. Typically Node() generator.
Add a node via (form, norm, info, buid) and add ops to editatom
return a form, norm, info, buid tuple
Add/merge nodes in bulk.          The addNodes API is designed for bulk adds which will         also set properties and add tags to existing nodes.         Nodes are specified as a list of the following tuples:              ( (form, valu), {'props':{}, 'tags':{}})          Args:             nodedefs (list): A list of nodedef tuples.          Returns:             (list): A list of xact messages.
Yield row tuples from a series of lift operations.          Row tuples only requirement is that the first element         be the binary id of a node.          Args:             lops (list): A list of lift operations.          Yields:             (tuple): (layer_indx, (buid, ...)) rows.
Join a row generator into (row, Node()) tuples.          A row generator yields tuples of node buid, rawprop dict          Args:             rows: A generator of (layer_idx, (buid, ...)) tuples.             rawprop(str):  "raw" propname e.g. if a tag, starts with "#".  Used                 for filtering so that we skip the props for a buid if we're                 asking from a higher layer than the row was from (and hence,                 we'll presumable get/have gotten the row when that layer is                 lifted.             cmpf (func): A comparison function used to filter nodes.         Yields:             (tuple): (row, node)
Return the lowercased name of this module.          Notes:             This pulls the ``mod_name`` attribute on the class. This allows             an implementer to set a arbitrary name for the module.  If this             attribute is not set, it defaults to             ``self.__class__.__name__.lower()`` and sets ``mod_name`` to             that value.          Returns:             (str): The module name.
Construct a path relative to this module's working directory.          Args:             *paths: A list of path strings          Notes:             This creates the module specific directory if it does not exist.          Returns:             (str): The full path (or None if no cortex dir is configured).
Add a ctor callback to the global scope.
Retrieve a value from the closest scope frame.
Add values as iter() compatible items in the current scope frame.
Add a constructor to be called when a specific property is not present.          Example:              scope.ctor('foo',FooThing)             ...             foo = scope.get('foo')
Iterate through values added with add() from each scope frame.
Add an item to the queue.
Return a node if it is currently being made, mark as a dependency, else None if none found
Update the shared map with my in-construction node
Allow any other editatoms waiting on me to complete to resume
Wait on the other editatoms who are constructing nodes my new nodes refer to
Push the recorded changes to disk, notify all the listeners
Construct and return a cmdr for the given remote cell.      Example:          cmdr = await getItemCmdr(foo)
Create a cmdr for the given item and run the cmd loop.      Example:          runItemCmdr(foo)
Helper for getting a documentation data file paths.      Args:         fn (str): Name of the file to retrieve the full path for.         root (str): Optional root path to look for a docdata in.      Notes:         Defaults to looking for the ``docdata`` directory in the current         working directory. This behavior works fine for notebooks nested         in the docs directory of synapse; but this root directory that         is looked for may be overridden by providing an alternative root.      Returns:         str: A file path.      Raises:         ValueError if the file does not exist or directory traversal attempted..
Get a temporary cortex proxy.
Get a Cmdr instance with prepopulated locs
Get a Telepath Proxt to a Cortex instance which is backed by a temporary Cortex.      Args:         mods (list): A list of additional CoreModules to load in the Cortex.      Notes:         The Proxy returned by this should be fini()'d to tear down the temporary Cortex.      Returns:         s_telepath.Proxy
Get a CmdrCore instance which is backed by a temporary Cortex.      Args:         mods (list): A list of additional CoreModules to load in the Cortex.         outp: A output helper.  Will be used for the Cmdr instance.      Notes:         The CmdrCore returned by this should be fini()'d to tear down the temporary Cortex.      Returns:         CmdrCore: A CmdrCore instance.
Add feed data to the cortex.
A helper for executing a storm command and getting a list of storm messages.          Args:             text (str): Storm command to execute.             opts (dict): Opt to pass to the cortex during execution.             num (int): Number of nodes to expect in the output query. Checks that with an assert statement.             cmdr (bool): If True, executes the line via the Cmdr CLI and will send output to outp.          Notes:             The opts dictionary will not be used if cmdr=True.          Returns:             list: A list of storm messages.
Emulate a small bit of readline behavior.      Returns:         (bool) True if current user enabled vi mode ("set editing-mode vi") in .inputrc
Run a line of command input for this command.          Args:             line (str): Line to execute          Examples:             Run the foo command with some arguments:                  await foo.runCmdLine('foo --opt baz woot.com')
Use the _cmd_syntax def to split/parse/normalize the cmd line.          Args:             text (str): Command to process.          Notes:             This is implemented independent of argparse (et al) due to the             need for syntax aware argument splitting. Also, allows different             split per command type          Returns:             dict: An opts dictionary.
Register SIGINT signal handler with the ioloop to cancel the currently running cmdloop task.
Prompt for user input from stdin.
Add a Cmd subclass to this cli.
Run commands from a user in an interactive fashion until fini() or EOFError is raised.
Run a single command line.          Args:             line (str): Line to execute.          Examples:             Execute the 'woot' command with the 'help' switch:                  await cli.runCmdLine('woot --help')          Returns:             object: Arbitrary data from the cmd class.
Generates a CA keypair.          Args:             name (str): The name of the CA keypair.             signas (str): The CA keypair to sign the new CA with.             outp (synapse.lib.output.Output): The output buffer.          Examples:             Make a CA named "myca":                  mycakey, mycacert = cdir.genCaCert('myca')          Returns:             ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the private key and certificate objects.
Generates a host keypair.          Args:             name (str): The name of the host keypair.             signas (str): The CA keypair to sign the new host keypair with.             outp (synapse.lib.output.Output): The output buffer.             csr (OpenSSL.crypto.PKey): The CSR public key when generating the keypair from a CSR.             sans (list): List of subject alternative names.          Examples:             Make a host keypair named "myhost":                  myhostkey, myhostcert = cdir.genHostCert('myhost')          Returns:             ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the private key and certificate objects.
Generates a user keypair.          Args:             name (str): The name of the user keypair.             signas (str): The CA keypair to sign the new user keypair with.             outp (synapse.lib.output.Output): The output buffer.             csr (OpenSSL.crypto.PKey): The CSR public key when generating the keypair from a CSR.          Examples:             Generate a user cert for the user "myuser":                  myuserkey, myusercert = cdir.genUserCert('myuser')          Returns:             ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the key and certificate objects.
Generates a user PKCS #12 archive.         Please note that the resulting file will contain private key material.          Args:             name (str): The name of the user keypair.             outp (synapse.lib.output.Output): The output buffer.          Examples:             Make the PKC12 object for user "myuser":                  myuserpkcs12 = cdir.genClientCert('myuser')          Returns:             OpenSSL.crypto.PKCS12: The PKCS #12 archive.
Validate the PEM encoded x509 user certificate bytes and return it.          Args:             byts (bytes): The bytes for the User Certificate.             cacerts (tuple): A tuple of OpenSSL.crypto.X509 CA Certificates.          Raises:             OpenSSL.crypto.X509StoreContextError: If the certificate is not valid.          Returns:             OpenSSL.crypto.X509: The certificate, if it is valid.
Return a list of CA certs from the CertDir.          Returns:             [OpenSSL.crypto.X509]: List of CA certificates.
Gets the path to the CA certificate that issued a given host keypair.          Args:             name (str): The name of the host keypair.          Examples:             Get the path to the CA cert which issue the cert for "myhost":                  mypath = cdir.getHostCaPath('myhost')          Returns:             str: The path if exists.
Gets the path to a host certificate.          Args:             name (str): The name of the host keypair.          Examples:             Get the path to the host certificate for the host "myhost":                  mypath = cdir.getHostCertPath('myhost')          Returns:             str: The path if exists.
Gets the path to the CA certificate that issued a given user keypair.          Args:             name (str): The name of the user keypair.          Examples:             Get the path to the CA cert which issue the cert for "myuser":                  mypath = cdir.getUserCaPath('myuser')          Returns:             str: The path if exists.
Gets the name of the first existing user cert for a given user and host.          Args:             user (str): The name of the user.             host (str): The name of the host.          Examples:             Get the name for the "myuser" user cert at "cool.vertex.link":                  usercertname = cdir.getUserForHost('myuser', 'cool.vertex.link')          Returns:             str: The cert name, if exists.
Imports certs and keys into the Synapse cert directory          Args:             path (str): The path of the file to be imported.             mode (str): The certdir subdirectory to import the file into.          Examples:             Import CA certifciate 'mycoolca.crt' to the 'cas' directory.                  certdir.importFile('mycoolca.crt', 'cas')          Notes:             importFile does not perform any validation on the files it imports.          Returns:             None
Checks if a CA certificate exists.          Args:             name (str): The name of the CA keypair.          Examples:             Check if the CA certificate for "myca" exists:                  exists = cdir.isCaCert('myca')          Returns:             bool: True if the certificate is present, False otherwise.
Checks if a user client certificate (PKCS12) exists.          Args:             name (str): The name of the user keypair.          Examples:             Check if the client certificate "myuser" exists:                  exists = cdir.isClientCert('myuser')          Returns:             bool: True if the certificate is present, False otherwise.
Checks if a host certificate exists.          Args:             name (str): The name of the host keypair.          Examples:             Check if the host cert "myhost" exists:                  exists = cdir.isUserCert('myhost')          Returns:             bool: True if the certificate is present, False otherwise.
Checks if a user certificate exists.          Args:             name (str): The name of the user keypair.          Examples:             Check if the user cert "myuser" exists:                  exists = cdir.isUserCert('myuser')          Returns:             bool: True if the certificate is present, False otherwise.
Signs a certificate with a CA keypair.          Args:             cert (OpenSSL.crypto.X509): The certificate to sign.             signas (str): The CA keypair name to sign the new keypair with.          Examples:             Sign a certificate with the CA "myca":                  cdir.signCertAs(mycert, 'myca')          Returns:             None
Signs a host CSR with a CA keypair.          Args:             cert (OpenSSL.crypto.X509Req): The certificate signing request.             signas (str): The CA keypair name to sign the CSR with.             outp (synapse.lib.output.Output): The output buffer.             sans (list): List of subject alternative names.          Examples:             Sign a host key with the CA "myca":                  cdir.signHostCsr(mycsr, 'myca')          Returns:             ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)):  Tuple containing the public key and certificate objects.
Self-sign a certificate.          Args:             cert (OpenSSL.crypto.X509): The certificate to sign.             pkey (OpenSSL.crypto.PKey): The PKey with which to sign the certificate.          Examples:             Sign a given certificate with a given private key:                  cdir.selfSignCert(mycert, myotherprivatekey)          Returns:             None
Signs a user CSR with a CA keypair.          Args:             cert (OpenSSL.crypto.X509Req): The certificate signing request.             signas (str): The CA keypair name to sign the CSR with.             outp (synapse.lib.output.Output): The output buffer.          Examples:             cdir.signUserCsr(mycsr, 'myca')          Returns:             ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the public key and certificate objects.
Returns an ssl.SSLContext appropriate for initiating a TLS session
Returns an ssl.SSLContext appropriate to listen on a socket          Args:             hostname:  if None, the value from socket.gethostname is used to find the key in the servers directory.             This name should match the not-suffixed part of two files ending in .key and .crt in the hosts subdirectory
Save a certificate in PEM format to a file outside the certdir.
Save a private key in PEM format to a file outside the certdir.
Perform one side of an Ecliptic Curve Diffie Hellman Ephemeral key exchange.      Args:         statprv_u (PriKey): Static Private Key for U         statpub_v (PubKey: Static Public Key for V         ephmprv_u (PriKey): Ephemeral Private Key for U         ephmpub_v (PubKey): Ephemeral Public Key for V         length (int): Number of bytes to return         salt (bytes): Salt to use when computing the key.         info (bytes): Additional information to use when computing the key.      Notes:         This makes no assumption about the reuse of the Ephemeral keys passed         to the function. It is the caller's responsibility to destroy the keys         after they are used for doing key generation. This implementation is         the dhHybrid1 scheme described in NIST 800-56A Revision 2.      Returns:         bytes: The derived key.
Compute the ECC signature for the given bytestream.          Args:             byts (bytes): The bytes to sign.          Returns:             bytes: The RSA Signature bytes.
Perform a ECDH key exchange with a public key.          Args:             pubkey (PubKey): A PubKey to perform the ECDH with.          Returns:             bytes: The ECDH bytes. This is deterministic for a given pubkey             and private key.
Get the private key bytes in DER/PKCS8 format.          Returns:             bytes: The DER/PKCS8 encoded private key.
Get the public key bytes in DER/SubjectPublicKeyInfo format.          Returns:             bytes: The DER/SubjectPublicKeyInfo encoded public key.
Verify the signature for the given bytes using the ECC         public key.          Args:             byts (bytes): The data bytes.             sign (bytes): The signature bytes.          Returns:             bool: True if the data was verified, False otherwise.
Brute force the version out of a string.          Args:             valu (str): String to attempt to get version information for.          Notes:             This first attempts to parse strings using the it:semver normalization             before attempting to extract version parts out of the string.          Returns:             int, dict: The system normalized version integer and a subs dictionary.
A sane "stand alone" url parser.      Example:          info = chopurl(url)
Use elements from this hash set to create a unique         (re)identifier.
Consume all the bytes from a file like object.          Example:              hset = HashSet()             hset.eatfd(fd)
Update all the hashes in the set with the given bytes.
Add an entry to the provenance stack for the duration of the context
Duplicate the current provenance stack onto another task
Returns the provenance stack given the iden to it
Returns a stream of provenance stacks at the given offset
Returns the iden corresponding to a provenance stack and stores if it hasn't seen it before
Writes the current provenance stack to storage if it wasn't already there and returns it          Returns (Tuple[bool, str, List[]]):             Whether the stack was not cached, the iden of the prov stack, and the provstack
Save a series of items to a sequence.          Args:             items (tuple): The series of items to save into the sequence.          Returns:             The index of the first item
Determine the next insert offset according to storage.          Returns:             int: The next insert offset.
Iterate over items in a sequence from a given offset.          Args:             offs (int): The offset to begin iterating from.          Yields:             (indx, valu): The index and valu of the item.
Iterate over raw indx, bytes tuples from a given offset.
Chop a latlong string and return (float,float).     Does not perform validation on the coordinates.      Args:         text (str):  A longitude,latitude string.      Returns:         (float,float): A longitude, latitude float tuple.
Determine if the given point is within dist of any of points.      Args:         point ((float,float)): A latitude, longitude float tuple.         dist (int): A distance in mm ( base units )         points (list): A list of latitude, longitude float tuples to compare against.
Calculate the haversine distance between two points     defined by (lat,lon) tuples.      Args:         px ((float,float)): lat/long position 1         py ((float,float)): lat/long position 2         r (float): Radius of sphere      Returns:         (int):  Distance in mm.
Calculate a min/max bounding box for the circle defined by lalo/dist.      Args:         lat (float): The latitude in degrees         lon (float): The longitude in degrees         dist (int): A distance in geo:dist base units (mm)      Returns:         (float,float,float,float): (latmin, latmax, lonmin, lonmax)
SumoLogic REST API endpoint changes based on the geo location of the client.         For example, If the client geolocation is Australia then the REST end point is         https://api.au.sumologic.com/api/v1          When the default REST endpoint (https://api.sumologic.com/api/v1) is used the server         responds with a 401 and causes the SumoLogic class instantiation to fail and this very         unhelpful message is shown 'Full authentication is required to access this resource'          This method makes a request to the default REST endpoint and resolves the 401 to learn         the right endpoint
Perform a single Sumo metrics query
call inner component connect
Translate CardConnection protocol mask into PCSC protocol mask.
Translate protocol into PCSC protocol header.
Connect to the card.          If protocol is not specified, connect with the default         connection protocol.          If mode is not specified, connect with SCARD_SHARE_SHARED.
Disconnect from the card.
Return card ATR
Transmit an apdu to the card and return response apdu.          @param bytes:    command apdu to transmit (list of bytes)          @param protocol: the transmission protocol, from             CardConnection.T0_protocol, CardConnection.T1_protocol, or             CardConnection.RAW_protocol          @return:     a tuple (response, sw1, sw2) where                     sw1 is status word 1, e.g. 0x90                     sw2 is status word 2, e.g. 0x1A                     response are the response bytes excluding status words
Transmit a control command to the reader and return response.          controlCode: control command          bytes:       command data to transmit (list of bytes)          return:      response are the response bytes (if any)
get an attribute          attribId: Identifier for the attribute to get          return:   response are the attribute byte array
Return the ATR of the card inserted into the reader.
expand all nodes
recursivly walk tree control
This will get us the program's directory,     even if we are frozen using py2exe. From WhereAmI page on py2exe wiki.
Remove a reader group
Connect to card.         @param protocol: a bit mask of the protocols to use, from         L{CardConnection.T0_protocol}, L{CardConnection.T1_protocol},         L{CardConnection.RAW_protocol}, L{CardConnection.T15_protocol}          @param mode: SCARD_SHARE_SHARED (default), SCARD_SHARE_EXCLUSIVE or         SCARD_SHARE_DIRECT          @param disposition: SCARD_LEAVE_CARD (default), SCARD_RESET_CARD,         SCARD_UNPOWER_CARD or SCARD_EJECT_CARD
Transmit an apdu. Internally calls doTransmit() class method         and notify observers upon command/response APDU events.         Subclasses must override the doTransmit() class method.          @param bytes:      list of bytes to transmit          @param protocol:   the transmission protocol, from                     CardConnection.T0_protocol,                     CardConnection.T1_protocol, or                     CardConnection.RAW_protocol
Send a control command and buffer.  Internally calls doControl()         class method and notify observers upon command/response events.         Subclasses must override the doControl() class method.          @param controlCode: command code          @param bytes:       list of bytes to transmit
return the requested attribute          @param attribId: attribute id like SCARD_ATTR_VENDOR_NAME
Return a card connection thru a remote reader.
Called when a card is activated by double-clicking         on the card or reader tree control or toolbar.         In this sample, we just connect to the card on the first activation.
Called when a card is selected by clicking on the         card or reader tree control or toolbar.
Called when a card is selected by clicking on the         card or reader tree control or toolbar.
Called when a reader is selected by clicking on the         reader tree control or toolbar.
Create and display application frame.
return command line arguments for shutting down the         server; this command line is built from the name server         startup arguments.
Starts Pyro naming server with command line arguments         (see pyro documentation)
Shutdown pyro naming server.
wait until name server is started.
Return a CardConnection to the Card object.
Returns the list of smartcard reader groups.
Add a reader group
Remove a reader group
Get the list of Part10 features supported by the reader.      @param response: result of CM_IOCTL_GET_FEATURE_REQUEST commmand      @rtype: list     @return: a list of list [[tag1, value1], [tag2, value2]]
return the controlCode for a feature or None      @param feature:     feature to look for     @param featureList: feature list as returned by L{getFeatureRequest()}      @return: feature value or None
return the PIN_PROPERTIES structure      @param cardConnection: L{CardConnection} object     @param featureList: feature list as returned by L{getFeatureRequest()}     @param controlCode: control code for L{FEATURE_IFD_PIN_PROPERTIES}      @rtype: dict     @return: a dict
return the GET_TLV_PROPERTIES structure      @param cardConnection: L{CardConnection} object     @param featureList: feature list as returned by L{getFeatureRequest()}     @param controlCode: control code for L{FEATURE_GET_TLV_PROPERTIES}      @rtype: dict     @return: a dict
return the GET_TLV_PROPERTIES structure      @param response: result of  L{FEATURE_GET_TLV_PROPERTIES}      @rtype: dict     @return: a dict
Return a card connection thru the reader.
Called when a local reader is added or removed.         Create remote pyro reader objects for added readers.         Delete remote pyro reader objects for removed readers.
Add an observer.
Remove an observer.
Runs until stopEvent is notified, and notify         observers of all reader insertion/removal.
Remove from other items already in list.
CardConnectionObserver callback.
Starts Pyro naming server with command line arguments (see         pyro documentation)
wait until name server is started.
Synchronize methods in the given class.     Only synchronize the methods whose names are     given, or all methods if names=None.
Lock card with SCardBeginTransaction.
Unlock card with SCardEndTransaction.
Gain exclusive access to card during APDU transmission for if this         decorator decorates a PCSCCardConnection.
Static method to create a reader from a reader clazz.          @param clazz:      the reader class name         @param readername: the reader name
Toolbar ReaderObserver callback that is notified when         readers are added or removed.
Send an APDU command to the connected smartcard.          @param command: list of APDU bytes, e.g. [0xA0, 0xA4, 0x00, 0x00, 0x02]          @return: a tuple (response, sw1, sw2) where                 response is the APDU response                 sw1, sw2 are the two status words
Returns next error checking strategy.
Add an exception filter to the error checking chain.          @param exClass:    the exception to exclude, e.g.         L{smartcard.sw.SWExceptions.WarningProcessingException} A filtered         exception will not be raised when the sw1,sw2 conditions that         would raise the excption are met.
Add reader to a reader group.
Remove a reader from a reader group
Activate a card.
Deactivate a card.
Called when the user activates a reader in the tree.
Called when user right-clicks a node in the card tree control.
Called when the user selects a card in the tree.
Called when the user selects a reader in the tree.
Called when the user activates a reader in the toolbar combo box.
Retrieve a function object from a full dotted-package name.
Load a module and retrieve a class (NOT an instance).      If the parentClass is supplied, className must be of parentClass     or a subclass of parentClass (or None is returned).
Returns the list or PCSC readers on which to wait for cards.
Wait for card insertion and returns a card service.
Wait for card insertion or removal.
Converts a GUID string into a list of bytes.      >>> strToGUID('{AD4F1667-EA75-4124-84D4-641B3B197C65}')     [103, 22, 79, 173, 117, 234, 36, 65, 132, 212, 100, 27, 59, 25, 124, 101]
Converts a GUID sequence of bytes into a string.      >>> GUIDToStr([103,22,79,173,  117,234,  36,65,     ...            132, 212, 100, 27, 59, 25, 124, 101])     '{AD4F1667-EA75-4124-84D4-641B3B197C65}'
If 'changed' indicates that this object         has changed, notify all its observers, then         call clearChanged(). Each observer has its         update() called with two arguments: this         observable object and the generic 'arg'.
Returns true if the atr matches the masked CardType atr.          @param atr:    the atr to chek for matching         @param reader: the reader (optional); default is None          When atr is compared to the CardType ATR, matches returns true if         and only if CardType.atr & CardType.mask = atr & CardType.mask,         where & is the bitwise logical AND.
Cypher/uncypher APDUs before transmission
Called when a reader is activated by double-clicking on the         reader tree control or toolbar.
Called when a card is deactivated in the reader tree control         or toolbar.
Called when a card is selected by clicking on the card or         reader tree control or toolbar.
Called when a reader is selected by clicking on the reader         tree control or toolbar.
Returns a dictionnary of supported protocols.
Dump the details of an ATR.
Called when a card is inserted.         Adds a smart card to the smartcards tree.
Called when a card is removed.         Removes a card from the tree.
Add an ATR to a reader node.
Return the ATR of the card inserted into the reader.
Called when a card is inserted.         Adds the smart card child to the reader node.
Called when a reader is inserted.         Adds the smart card reader to the smartcard readers tree.
Called when a card is removed.         Removes the card from the tree.
Called when a reader is removed.         Removes the reader from the smartcard readers tree.
Called on panel destruction.
Disconnect and reconnect in exclusive mode PCSCCardconnections.
Create a module from imported objects.      Parameters     ----------     name : str       New module name.     objs : dict       Dictionary of the objects (or their name) to import into the module,       keyed by the name they will take in the created module.     doc : str       Docstring of the new module.     source : Module object       Module where objects are defined if not explicitly given.     mode : {'raise', 'warn', 'ignore'}       How to deal with missing objects.       Returns     -------     ModuleType       A module built from a list of objects' name.
r"""Base flow index      Return the base flow index, defined as the minimum 7-day average flow divided by the mean flow.      Parameters     ----------     q : xarray.DataArray       Rate of river discharge [m³/s]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArrray       Base flow index.      Notes     -----     Let :math:`\mathbf{q}=q_0, q_1, \ldots, q_n` be the sequence of daily discharge and :math:`\overline{\mathbf{q}}`     the mean flow over the period. The base flow index is given by:      .. math::         \frac{\min(\mathrm{CMA}_7(\mathbf{q}))}{\overline{\mathbf{q}}}       where :math:`\mathrm{CMA}_7` is the seven days moving average of the daily flow:      .. math::         \mathrm{CMA}_7(q_i) = \frac{\sum_{j=i-3}^{i+3} q_j}{7}
r"""Cold spell duration index      Number of days with at least six consecutive days where the daily minimum temperature is below the 10th     percentile.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature.     tn10 : float       10th percentile of daily minimum temperature.     window : int       Minimum number of days with temperature below threshold to qualify as a cold spell. Default: 6.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Count of days with at least six consecutive days where the daily minimum temperature is below the 10th       percentile [days].      Notes     -----     Let :math:`TN_i` be the minimum daily temperature for the day of the year :math:`i` and :math:`TN10_i` the 10th     percentile of the minimum daily temperature over the 1961-1990 period for day of the year :math:`i`, the cold spell     duration index over period :math:`\phi` is defined as:      .. math::         \sum_{i \in \phi} \prod_{j=i}^{i+6} \left[ TN_j < TN10_j \right]      where :math:`[P]` is 1 if :math:`P` is true, and 0 if false.      References     ----------     From the Expert Team on Climate Change Detection, Monitoring and Indices (ETCCDMI).      Example     -------     >>> tn10 = percentile_doy(historical_tasmin, per=.1)     >>> cold_spell_duration_index(reference_tasmin, tn10)
r"""Cold spell days      The number of days that are part of a cold spell, defined as five or more consecutive days with mean daily     temperature below a threshold in °C.      Parameters     ----------     tas : xarrray.DataArray       Mean daily temperature [℃] or [K]     thresh : str       Threshold temperature below which a cold spell begins [℃] or [K]. Default : '-10 degC'     window : int       Minimum number of days with temperature below threshold to qualify as a cold spell.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Cold spell days.      Notes     -----     Let :math:`T_i` be the mean daily temperature on day :math:`i`, the number of cold spell days during     period :math:`\phi` is given by      .. math::         \sum_{i \in \phi} \prod_{j=i}^{i+5} [T_j < thresh]      where :math:`[P]` is 1 if :math:`P` is true, and 0 if false.
r"""Average daily precipitation intensity      Return the average precipitation over wet days.      Parameters     ----------     pr : xarray.DataArray       Daily precipitation [mm/d or kg/m²/s]     thresh : str       precipitation value over which a day is considered wet. Default : '1 mm/day'     freq : str, optional       Resampling frequency defining the periods       defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling. Default : '1 mm/day'      Returns     -------     xarray.DataArray       The average precipitation over wet days for each period      Notes     -----     Let :math:`\mathbf{p} = p_0, p_1, \ldots, p_n` be the daily precipitation and :math:`thresh` be the precipitation     threshold defining wet days. Then the daily precipitation intensity is defined as      .. math::         \frac{\sum_{i=0}^n p_i [p_i \leq thresh]}{\sum_{i=0}^n [p_i \leq thresh]}      where :math:`[P]` is 1 if :math:`P` is true, and 0 if false.      Examples     --------     The following would compute for each grid cell of file `pr.day.nc` the average     precipitation fallen over days with precipitation >= 5 mm at seasonal     frequency, ie DJF, MAM, JJA, SON, DJF, etc.:      >>> pr = xr.open_dataset('pr.day.nc')     >>> daily_int = daily_pr_intensity(pr, thresh='5 mm/day', freq="QS-DEC")
r"""Maximum number of consecutive dry days      Return the maximum number of consecutive days within the period where precipitation     is below a certain threshold.      Parameters     ----------     pr : xarray.DataArray       Mean daily precipitation flux [mm]     thresh : str       Threshold precipitation on which to base evaluation [mm]. Default : '1 mm/day'     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       The maximum number of consecutive dry days.      Notes     -----     Let :math:`\mathbf{p}=p_0, p_1, \ldots, p_n` be a daily precipitation series and :math:`thresh` the threshold     under which a day is considered dry. Then let :math:`\mathbf{s}` be the sorted vector of indices :math:`i` where     :math:`[p_i < thresh] \neq [p_{i+1} < thresh]`, that is, the days when the temperature crosses the threshold.     Then the maximum number of consecutive dry days is given by      .. math::         \max(\mathbf{d}) \quad \mathrm{where} \quad d_j = (s_j - s_{j-1}) [p_{s_j} > thresh]      where :math:`[P]` is 1 if :math:`P` is true, and 0 if false. Note that this formula does not handle sequences at     the start and end of the series, but the numerical algorithm does.
r"""Maximum number of consecutive frost days (Tmin < 0℃).      Resample the daily minimum temperature series by computing the maximum number     of days below the freezing point over each period.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature values [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       The maximum number of consecutive days below the freezing point.      Notes     -----     Let :math:`\mathbf{x}=x_0, x_1, \ldots, x_n` be a daily minimum temperature series and     :math:`\mathbf{s}` be the sorted vector of indices :math:`i` where :math:`[p_i < 0\celsius] \neq [p_{i+1} <     0\celsius]`, that is, the days when the temperature crosses the freezing point.     Then the maximum number of consecutive frost days is given by      .. math::         \max(\mathbf{d}) \quad \mathrm{where} \quad d_j = (s_j - s_{j-1}) [x_{s_j} > 0\celsius]      where :math:`[P]` is 1 if :math:`P` is true, and 0 if false. Note that this formula does not handle sequences at     the start and end of the series, but the numerical algorithm does.
r"""Number of days with a diurnal freeze-thaw cycle      The number of days where Tmax > 0℃ and Tmin < 0℃.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature [℃] or [K]     tasmin : xarray.DataArray       Minimum daily temperature values [℃] or [K]     freq : str       Resampling frequency      Returns     -------     xarray.DataArray       Number of days with a diurnal freeze-thaw cycle      Notes     -----     Let :math:`TX_{i}` be the maximum temperature at day :math:`i` and :math:`TN_{i}` be     the daily minimum temperature at day :math:`i`. Then the number of freeze thaw cycles     during period :math:`\phi` is given by :      .. math::          \sum_{i \in \phi} [ TX_{i} > 0℃ ] [ TN_{i} <  0℃ ]      where :math:`[P]` is 1 if :math:`P` is true, and 0 if false.
r"""Mean of daily temperature range.      The mean difference between the daily maximum temperature and the daily minimum temperature.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature values [℃] or [K]     tasmin : xarray.DataArray       Minimum daily temperature values [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       The average variation in daily temperature range for the given time period.      Notes     -----     Let :math:`TX_{ij}` and :math:`TN_{ij}` be the daily maximum and minimum temperature at day :math:`i`     of period :math:`j`. Then the mean diurnal temperature range in period :math:`j` is:      .. math::          DTR_j = \frac{ \sum_{i=1}^I (TX_{ij} - TN_{ij}) }{I}
r"""Mean absolute day-to-day variation in daily temperature range.      Mean absolute day-to-day variation in daily temperature range.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature values [℃] or [K]     tasmin : xarray.DataArray       Minimum daily temperature values [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       The average day-to-day variation in daily temperature range for the given time period.      Notes     -----     Let :math:`TX_{ij}` and :math:`TN_{ij}` be the daily maximum and minimum temperature at     day :math:`i` of period :math:`j`. Then calculated is the absolute day-to-day differences in     period :math:`j` is:      .. math::         vDTR_j = \frac{ \sum_{i=2}^{I} |(TX_{ij}-TN_{ij})-(TX_{i-1,j}-TN_{i-1,j})| }{I}
r"""Extreme intra-period temperature range.      The maximum of max temperature (TXx) minus the minimum of min temperature (TNn) for the given time period.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature values [℃] or [K]     tasmin : xarray.DataArray       Minimum daily temperature values [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Extreme intra-period temperature range for the given time period.      Notes     -----     Let :math:`TX_{ij}` and :math:`TN_{ij}` be the daily maximum and minimum temperature at day :math:`i`     of period :math:`j`. Then the extreme temperature range in period :math:`j` is:      .. math::          ETR_j = max(TX_{ij}) - min(TN_{ij})
r"""First day consistently exceeding threshold temperature.      Returns first day of period where a temperature threshold is exceeded     over a given number of days.      Parameters     ----------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K]. Default '0 degC'     window : int       Minimum number of days with temperature above threshold needed for evaluation     freq : str, optional       Resampling frequency      Returns     -------     float       Day of the year when temperature exceeds threshold over a given number of days for the first time. If there are       no such day, return np.nan.      Notes     -----     Let :math:`x_i` be the daily mean temperature at day of the year :math:`i` for values of :math:`i` going from 1     to 365 or 366. The start date of the freshet is given by the smallest index :math:`i` for which      .. math::         \prod_{j=i}^{i+w} [x_j > thresh]      is true, where :math:`w` is the number of days the temperature threshold should be exceeded,  and :math:`[P]` is     1 if :math:`P` is true, and 0 if false.
r"""Frost days index      Number of days where daily minimum temperatures are below 0℃.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Frost days index.      Notes     -----     Let :math:`TN_{ij}` be the daily minimum temperature at day :math:`i` of period :math:`j`. Then     counted is the number of days where:      .. math::          TN_{ij} < 0℃
r"""Growing season length.      The number of days between the first occurrence of at least     six consecutive days with mean daily temperature over 5℃ and     the first occurrence of at least six consecutive days with     mean daily temperature below 5℃ after July 1st in the northern     hemisphere and January 1st in the southern hemisphere.      Parameters     ---------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K]. Default: '5.0 degC'.     window : int       Minimum number of days with temperature above threshold to mark the beginning and end of growing season.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Growing season length.      Notes     -----     Let :math:`TG_{ij}` be the mean temperature at day :math:`i` of period :math:`j`. Then counted is     the number of days between the first occurrence of at least 6 consecutive days with:      .. math::          TG_{ij} > 5 ℃      and the first occurrence after 1 July of at least 6 consecutive days with:      .. math::          TG_{ij} < 5 ℃
r"""Heat wave frequency      Number of heat waves over a given period. A heat wave is defined as an event     where the minimum and maximum daily temperature both exceeds specific thresholds     over a minimum number of days.      Parameters     ----------      tasmin : xarrray.DataArray       Minimum daily temperature [℃] or [K]     tasmax : xarrray.DataArray       Maximum daily temperature [℃] or [K]     thresh_tasmin : str       The minimum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '22 degC'     thresh_tasmax : str       The maximum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '30 degC'     window : int       Minimum number of days with temperatures above thresholds to qualify as a heatwave.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Number of heatwave at the wanted frequency      Notes     -----     The thresholds of 22° and 25°C for night temperatures and 30° and 35°C for day temperatures were selected by     Health Canada professionals, following a temperature–mortality analysis. These absolute temperature thresholds     characterize the occurrence of hot weather events that can result in adverse health outcomes for Canadian     communities (Casati et al., 2013).      In Robinson (2001), the parameters would be `thresh_tasmin=27.22, thresh_tasmax=39.44, window=2` (81F, 103F).      References     ----------     Casati, B., A. Yagouti, and D. Chaumont, 2013: Regional Climate Projections of Extreme Heat Events in Nine Pilot     Canadian Communities for Public Health Planning. J. Appl. Meteor. Climatol., 52, 2669–2698,     https://doi.org/10.1175/JAMC-D-12-0341.1      Robinson, P.J., 2001: On the Definition of a Heat Wave. J. Appl. Meteor., 40, 762–775,     https://doi.org/10.1175/1520-0450(2001)040<0762:OTDOAH>2.0.CO;2
r"""Heat wave index.      Number of days that are part of a heatwave, defined as five or more consecutive days over 25℃.      Parameters     ----------     tasmax : xarrray.DataArray       Maximum daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to designate a heatwave [℃] or [K]. Default: '25.0 degC'.     window : int       Minimum number of days with temperature above threshold to qualify as a heatwave.     freq : str, optional       Resampling frequency      Returns     -------     DataArray       Heat wave index.
r"""Heat wave max length      Maximum length of heat waves over a given period. A heat wave is defined as an event     where the minimum and maximum daily temperature both exceeds specific thresholds     over a minimum number of days.      By definition heat_wave_max_length must be >= window.      Parameters     ----------      tasmin : xarrray.DataArray       Minimum daily temperature [℃] or [K]     tasmax : xarrray.DataArray       Maximum daily temperature [℃] or [K]     thresh_tasmin : str       The minimum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '22 degC'     thresh_tasmax : str       The maximum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '30 degC'     window : int       Minimum number of days with temperatures above thresholds to qualify as a heatwave.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Maximum length of heatwave at the wanted frequency      Notes     -----     The thresholds of 22° and 25°C for night temperatures and 30° and 35°C for day temperatures were selected by     Health Canada professionals, following a temperature–mortality analysis. These absolute temperature thresholds     characterize the occurrence of hot weather events that can result in adverse health outcomes for Canadian     communities (Casati et al., 2013).      In Robinson (2001), the parameters would be `thresh_tasmin=27.22, thresh_tasmax=39.44, window=2` (81F, 103F).      References     ----------     Casati, B., A. Yagouti, and D. Chaumont, 2013: Regional Climate Projections of Extreme Heat Events in Nine Pilot     Canadian Communities for Public Health Planning. J. Appl. Meteor. Climatol., 52, 2669–2698,     https://doi.org/10.1175/JAMC-D-12-0341.1      Robinson, P.J., 2001: On the Definition of a Heat Wave. J. Appl. Meteor., 40, 762–775,     https://doi.org/10.1175/1520-0450(2001)040<0762:OTDOAH>2.0.CO;2
r"""Heating degree days      Sum of degree days below the temperature threshold at which spaces are heated.      Parameters     ----------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K]. Default: '17.0 degC'.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Heating degree days index.      Notes     -----     Let :math:`TG_{ij}` be the daily mean temperature at day :math:`i` of period :math:`j`. Then the     heating degree days are:      .. math::          HD17_j = \sum_{i=1}^{I} (17℃ - TG_{ij})
r"""Number of ice/freezing days      Number of days where daily maximum temperatures are below 0℃.      Parameters     ----------     tasmax : xarrray.DataArray       Maximum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Number of ice/freezing days.      Notes     -----     Let :math:`TX_{ij}` be the daily maximum temperature at day :math:`i` of period :math:`j`. Then     counted is the number of days where:      .. math::          TX_{ij} < 0℃
r"""Ratio of rainfall to total precipitation      The ratio of total liquid precipitation over the total precipitation. If solid precipitation is not provided,     then precipitation is assumed solid if the temperature is below 0°C.      Parameters     ----------     pr : xarray.DataArray       Mean daily precipitation flux [Kg m-2 s-1] or [mm].     prsn : xarray.DataArray       Mean daily solid precipitation flux [Kg m-2 s-1] or [mm].     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     freq : str       Resampling frequency      Returns     -------     xarray.DataArray       Ratio of rainfall to total precipitation      Notes     -----     Let :math:`PR_i` be the mean daily precipitation of day :math:`i`, then for a period :math:`j` starting at     day :math:`a` and finishing on day :math:`b`:      .. math::          PR_{ij} = \sum_{i=a}^{b} PR_i           PRwet_{ij}      See also     --------     winter_rain_ratio
r"""Number of days with tmin below a threshold in      Number of days where daily minimum temperature is below a threshold.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K] . Default: '-10 degC'.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Number of days Tmin < threshold.      Notes     -----     Let :math:`TN_{ij}` be the daily minimum temperature at day :math:`i` of period :math:`j`. Then     counted is the number of days where:      .. math::          TX_{ij} < Threshold [℃]
r"""Number of summer days      Number of days where daily maximum temperature exceed a threshold.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K]. Default: '25 degC'.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Number of summer days.      Notes     -----     Let :math:`TX_{ij}` be the daily maximum temperature at day :math:`i` of period :math:`j`. Then     counted is the number of days where:      .. math::          TX_{ij} > Threshold [℃]
r"""Highest precipitation amount cumulated over a n-day moving window.      Calculate the n-day rolling sum of the original daily total precipitation series     and determine the maximum value over each period.      Parameters     ----------     da : xarray.DataArray       Daily precipitation values [Kg m-2 s-1] or [mm]     window : int       Window size in days.     freq : str, optional       Resampling frequency : default 'YS' (yearly)      Returns     -------     xarray.DataArray       The highest cumulated n-day precipitation value at the given time frequency.      Examples     --------     The following would compute for each grid cell of file `pr.day.nc` the highest 5-day total precipitation     at an annual frequency:      >>> da = xr.open_dataset('pr.day.nc').pr     >>> window = 5     >>> output = max_n_day_precipitation_amount(da, window, freq="YS")
r"""Highest 1-day precipitation amount for a period (frequency).      Resample the original daily total precipitation temperature series by taking the max over each period.      Parameters     ----------     pr : xarray.DataArray       Daily precipitation values [Kg m-2 s-1] or [mm]     freq : str, optional       Resampling frequency one of : 'YS' (yearly) ,'M' (monthly), or 'QS-DEC' (seasonal - quarters starting in december)      Returns     -------     xarray.DataArray       The highest 1-day precipitation value at the given time frequency.      Notes     -----     Let :math:`PR_i` be the mean daily precipitation of day `i`, then for a period `j`:      .. math::         PRx_{ij} = max(PR_{ij})      Examples     --------     The following would compute for each grid cell of file `pr.day.nc` the highest 1-day total     at an annual frequency:      >>> pr = xr.open_dataset('pr.day.nc').pr     >>> rx1day = max_1day_precipitation_amount(pr, freq="YS")
r"""Accumulated total (liquid + solid) precipitation.      Resample the original daily mean precipitation flux and accumulate over each period.      Parameters     ----------     pr : xarray.DataArray       Mean daily precipitation flux [Kg m-2 s-1] or [mm].     freq : str, optional       Resampling frequency as defined in       http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling.      Returns     -------     xarray.DataArray       The total daily precipitation at the given time frequency.      Notes     -----     Let :math:`PR_i` be the mean daily precipitation of day :math:`i`, then for a period :math:`j` starting at     day :math:`a` and finishing on day :math:`b`:      .. math::         PR_{ij} = \sum_{i=a}^{b} PR_i      Examples     --------     The following would compute for each grid cell of file `pr_day.nc` the total     precipitation at the seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.:      >>> pr_day = xr.open_dataset('pr_day.nc').pr     >>> prcp_tot_seasonal = precip_accumulation(pr_day, freq="QS-DEC")
Number of rain on frozen ground events      Number of days with rain above a threshold after a series of seven days below freezing temperature.     Precipitation is assumed to be rain when the temperature is above 0℃.      Parameters     ----------     pr : xarray.DataArray       Mean daily precipitation flux [Kg m-2 s-1] or [mm]     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     thresh : str       Precipitation threshold to consider a day as a rain event. Default : '1 mm/d'     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       The number of rain on frozen ground events per period [days]      Notes     -----     Let :math:`PR_i` be the mean daily precipitation and :math:`TG_i` be the mean daily temperature of day :math:`i`.     Then for a period :math:`j`, rain on frozen grounds days are counted where:      .. math::          PR_{i} > Threshold [mm]      and where      .. math::          TG_{i} ≤ 0℃      is true for continuous periods where :math:`i ≥ 7`
r"""Number of days with daily mean temperature over the 90th percentile.      Number of days with daily mean temperature over the 90th percentile.      Parameters     ----------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     t90 : xarray.DataArray       90th percentile of daily mean temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Count of days with daily mean temperature below the 10th percentile [days]      Notes     -----     The 90th percentile should be computed for a 5 day window centered on each calendar day for a reference period.      Example     -------     >>> t90 = percentile_doy(historical_tas, per=0.9)     >>> hot_days = tg90p(tas, t90)
r"""Number of days with daily mean temperature below the 10th percentile.      Number of days with daily mean temperature below the 10th percentile.      Parameters     ----------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     t10 : xarray.DataArray       10th percentile of daily mean temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Count of days with daily mean temperature below the 10th percentile [days]      Notes     -----     The 10th percentile should be computed for a 5 day window centered on each calendar day for a reference period.      Example     -------     >>> t10 = percentile_doy(historical_tas, per=0.1)     >>> cold_days = tg10p(tas, t10)
r"""Highest mean temperature.      The maximum of daily mean temperature.      Parameters     ----------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Maximum of daily minimum temperature.      Notes     -----     Let :math:`TN_{ij}` be the mean temperature at day :math:`i` of period :math:`j`. Then the maximum     daily mean temperature for period :math:`j` is:      .. math::          TNx_j = max(TN_{ij})
r"""Mean of daily average temperature.      Resample the original daily mean temperature series by taking the mean over each period.      Parameters     ----------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       The mean daily temperature at the given time frequency      Notes     -----     Let :math:`TN_i` be the mean daily temperature of day :math:`i`, then for a period :math:`p` starting at     day :math:`a` and finishing on day :math:`b`:      .. math::         TG_p = \frac{\sum_{i=a}^{b} TN_i}{b - a + 1}       Examples     --------     The following would compute for each grid cell of file `tas.day.nc` the mean temperature     at the seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.:      >>> t = xr.open_dataset('tas.day.nc')     >>> tg = tm_mean(t, freq="QS-DEC")
r"""Lowest mean temperature      Minimum of daily mean temperature.      Parameters     ----------     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Minimum of daily minimum temperature.      Notes     -----     Let :math:`TG_{ij}` be the mean temperature at day :math:`i` of period :math:`j`. Then the minimum     daily mean temperature for period :math:`j` is:      .. math::          TGn_j = min(TG_{ij})
r"""Highest minimum temperature.      The maximum of daily minimum temperature.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Maximum of daily minimum temperature.      Notes     -----     Let :math:`TN_{ij}` be the minimum temperature at day :math:`i` of period :math:`j`. Then the maximum     daily minimum temperature for period :math:`j` is:      .. math::          TNx_j = max(TN_{ij})
r"""Mean minimum temperature.      Mean of daily minimum temperature.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Mean of daily minimum temperature.      Notes     -----     Let :math:`TN_{ij}` be the minimum temperature at day :math:`i` of period :math:`j`. Then mean     values in period :math:`j` are given by:      .. math::          TN_{ij} = \frac{ \sum_{i=1}^{I} TN_{ij} }{I}
r"""Lowest minimum temperature      Minimum of daily minimum temperature.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Minimum of daily minimum temperature.      Notes     -----     Let :math:`TN_{ij}` be the minimum temperature at day :math:`i` of period :math:`j`. Then the minimum     daily minimum temperature for period :math:`j` is:      .. math::          TNn_j = min(TN_{ij})
r"""Tropical nights      The number of days with minimum daily temperature above threshold.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K]. Default: '20 degC'.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Number of days with minimum daily temperature above threshold.      Notes     -----     Let :math:`TN_{ij}` be the daily minimum temperature at day :math:`i` of period :math:`j`. Then     counted is the number of days where:      .. math::          TN_{ij} > Threshold [℃]
r"""Highest max temperature      The maximum value of daily maximum temperature.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Maximum value of daily maximum temperature.      Notes     -----     Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`. Then the maximum     daily maximum temperature for period :math:`j` is:      .. math::          TXx_j = max(TX_{ij})
r"""Mean max temperature      The mean of daily maximum temperature.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Mean of daily maximum temperature.      Notes     -----     Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`. Then mean     values in period :math:`j` are given by:      .. math::          TX_{ij} = \frac{ \sum_{i=1}^{I} TX_{ij} }{I}
r"""Lowest max temperature      The minimum of daily maximum temperature.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature [℃] or [K]     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Minimum of daily maximum temperature.      Notes     -----     Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`. Then the minimum     daily maximum temperature for period :math:`j` is:      .. math::          TXn_j = min(TX_{ij})
r"""Frequency of extreme warm days      Return the number of days with tasmax > thresh per period      Parameters     ----------     tasmax : xarray.DataArray       Mean daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K]. Default : '30 degC'     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Number of days exceeding threshold.      Notes:     Let :math:`TX_{ij}` be the daily maximum temperature at day :math:`i` of period :math:`j`. Then     counted is the number of days where:      .. math::          TN_{ij} > Threshold [℃]
r"""Number of days with both hot maximum and minimum daily temperatures.      The number of days per period with tasmin above a threshold and tasmax above another threshold.      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     tasmax : xarray.DataArray       Maximum daily temperature [℃] or [K]     thresh_tasmin : str       Threshold temperature for tasmin on which to base evaluation [℃] or [K]. Default : '22 degC'     thresh_tasmax : str       Threshold temperature for tasmax on which to base evaluation [℃] or [K]. Default : '30 degC'     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       the number of days with tasmin > thresh_tasmin and       tasmax > thresh_tasamax per period       Notes     -----     Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`, :math:`TN_{ij}`     the daily minimum temperature at day :math:`i` of period :math:`j`, :math:`TX_{thresh}` the threshold for maximum     daily temperature, and :math:`TN_{thresh}` the threshold for minimum daily temperature. Then counted is the number     of days where:      .. math::          TX_{ij} > TX_{thresh} [℃]      and where:      .. math::          TN_{ij} > TN_{thresh} [℃]
r"""Frequency of extreme warm nights      Return the number of days with tasmin > thresh per period      Parameters     ----------     tasmin : xarray.DataArray       Minimum daily temperature [℃] or [K]     thresh : str       Threshold temperature on which to base evaluation [℃] or [K]. Default : '22 degC'     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       The number of days with tasmin > thresh per period
r"""Warm spell duration index      Number of days with at least six consecutive days where the daily maximum temperature is above the 90th     percentile. The 90th percentile should be computed for a 5-day window centred on each calendar day in the     1961-1990 period.      Parameters     ----------     tasmax : xarray.DataArray       Maximum daily temperature [℃] or [K]     tx90 : float       90th percentile of daily maximum temperature [℃] or [K]     window : int       Minimum number of days with temperature below threshold to qualify as a warm spell.     freq : str, optional       Resampling frequency      Returns     -------     xarray.DataArray       Count of days with at least six consecutive days where the daily maximum temperature is above the 90th       percentile [days].      References     ----------     From the Expert Team on Climate Change Detection, Monitoring and Indices (ETCCDMI).     Used in Alexander, L. V., et al. (2006), Global observed changes in daily climate extremes of temperature and     precipitation, J. Geophys. Res., 111, D05109, doi: 10.1029/2005JD006290.
r"""Wet days      Return the total number of days during period with precipitation over threshold.      Parameters     ----------     pr : xarray.DataArray       Daily precipitation [mm]     thresh : str       Precipitation value over which a day is considered wet. Default: '1 mm/day'.     freq : str, optional       Resampling frequency defining the periods       defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling.      Returns     -------     xarray.DataArray       The number of wet days for each period [day]      Examples     --------     The following would compute for each grid cell of file `pr.day.nc` the number days     with precipitation over 5 mm at the seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.:      >>> pr = xr.open_dataset('pr.day.nc')     >>> wd = wetdays(pr, pr_min = 5., freq="QS-DEC")
Ratio of rainfall to total precipitation during winter      The ratio of total liquid precipitation over the total precipitation over the winter months (DJF. If solid     precipitation is not provided, then precipitation is assumed solid if the temperature is below 0°C.      Parameters     ----------     pr : xarray.DataArray       Mean daily precipitation flux [Kg m-2 s-1] or [mm].     prsn : xarray.DataArray       Mean daily solid precipitation flux [Kg m-2 s-1] or [mm].     tas : xarray.DataArray       Mean daily temperature [℃] or [K]     freq : str       Resampling frequency      Returns     -------     xarray.DataArray       Ratio of rainfall to total precipitation during winter months (DJF)
Select entries according to a time period.      Parameters     ----------     da : xarray.DataArray       Input data.     **indexer : {dim: indexer, }, optional       Time attribute and values over which to subset the array. For example, use season='DJF' to select winter values,       month=1 to select January, or month=[6,7,8] to select summer months. If not indexer is given, all values are       considered.      Returns     -------     xr.DataArray       Selected input values.
Apply operation over each period that is part of the index selection.      Parameters     ----------     da : xarray.DataArray       Input data.     op : str {'min', 'max', 'mean', 'std', 'var', 'count', 'sum', 'argmax', 'argmin'} or func       Reduce operation. Can either be a DataArray method or a function that can be applied to a DataArray.     freq : str       Resampling frequency defining the periods       defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling.     **indexer : {dim: indexer, }, optional       Time attribute and values over which to subset the array. For example, use season='DJF' to select winter values,       month=1 to select January, or month=[6,7,8] to select summer months. If not indexer is given, all values are       considered.      Returns     -------     xarray.DataArray       The maximum value for each period.
Return the day of year of the maximum value.
Fit an array to a univariate distribution along the time dimension.      Parameters     ----------     arr : xarray.DataArray       Time series to be fitted along the time dimension.     dist : str       Name of the univariate distribution, such as beta, expon, genextreme, gamma, gumbel_r, lognorm, norm       (see scipy.stats).      Returns     -------     xarray.DataArray       An array of distribution parameters fitted using the method of Maximum Likelihood.
Return the value corresponding to the given return period.      Parameters     ----------     arr : xarray.DataArray       Maximized/minimized input data with a `time` dimension.     t : int or sequence       Return period. The period depends on the resolution of the input data. If the input array's resolution is       yearly, then the return period is in years.     dist : str       Name of the univariate distribution, such as beta, expon, genextreme, gamma, gumbel_r, lognorm, norm       (see scipy.stats).     mode : {'min', 'max}       Whether we are looking for a probability of exceedance (max) or a probability of non-exceedance (min).      Returns     -------     xarray.DataArray       An array of values with a 1/t probability of exceedance (if mode=='max').
Return the value corresponding to a return period.      Parameters     ----------     da : xarray.DataArray       Input data.     t : int or sequence       Return period. The period depends on the resolution of the input data. If the input array's resolution is       yearly, then the return period is in years.     dist : str       Name of the univariate distribution, such as beta, expon, genextreme, gamma, gumbel_r, lognorm, norm       (see scipy.stats).     mode : {'min', 'max'}       Whether we are looking for a probability of exceedance (high) or a probability of non-exceedance (low).     window : int       Averaging window length (days).     freq : str       Resampling frequency. If None, the frequency is assumed to be 'YS' unless the indexer is season='DJF',       in which case `freq` would be set to `YS-DEC`.     **indexer : {dim: indexer, }, optional       Time attribute and values over which to subset the array. For example, use season='DJF' to select winter values,       month=1 to select January, or month=[6,7,8] to select summer months. If not indexer is given, all values are       considered.      Returns     -------     xarray.DataArray       An array of values with a 1/t probability of exceedance or non-exceedance when mode is high or low respectively.
Return the default frequency.
Return a distribution object from scipy.stats.
Return whether an output is considered missing or not.
For all modules or classes listed, return the children that are instances of xclim.utils.Indicator.      modules : sequence       Sequence of modules to inspect.
Return a sequence of dicts storing metadata about all available indices.
Return the length of the longest consecutive run of True values.          Parameters         ----------         arr : N-dimensional array (boolean)           Input array         dim : Xarray dimension (default = 'time')           Dimension along which to calculate consecutive run          Returns         -------         N-dimensional array (int)           Length of longest run of True values along dimension
Return the number of runs of a minimum length.          Parameters         ----------         da: N-dimensional Xarray data array  (boolean)           Input data array         window : int           Minimum run length.         dim : Xarray dimension (default = 'time')           Dimension along which to calculate consecutive run          Returns         -------         out : N-dimensional xarray data array (int)           Number of distinct runs of a minimum length.
Return the number of consecutive true values in array for runs at least as long as given duration.          Parameters         ----------         da: N-dimensional Xarray data array  (boolean)           Input data array         window : int           Minimum run length.         dim : Xarray dimension (default = 'time')           Dimension along which to calculate consecutive run           Returns         -------         out : N-dimensional xarray data array (int)           Total number of true values part of a consecutive runs of at least `window` long.
Return the index of the first item of a run of at least a given length.          Parameters         ----------         ----------         arr : N-dimensional Xarray data array  (boolean)           Input array         window : int           Minimum duration of consecutive run to accumulate values.         dim : Xarray dimension (default = 'time')           Dimension along which to calculate consecutive run          Returns         -------         out : N-dimensional xarray data array (int)           Index of first item in first valid run. Returns np.nan if there are no valid run.
Return the length, starting position and value of consecutive identical values.      Parameters     ----------     arr : sequence       Array of values to be parsed.      Returns     -------     (values, run lengths, start positions)     values : np.array       The values taken by arr over each run     run lengths : np.array       The length of each run     start position : np.array       The starting index of each run      Examples     --------     >>> a = [1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]     >>> rle_1d(a)     (array([1, 2, 3]), array([2, 4, 6]), array([0, 2, 6]))
Return the number of consecutive true values in array for runs at least as long as given duration.      Parameters     ----------     arr : bool array       Input array     window : int       Minimum duration of consecutive run to accumulate values.      Returns     -------     int       Total number of true values part of a consecutive run at least `window` long.
Return the index of the first item of a run of at least a given length.      Parameters     ----------     ----------     arr : bool array       Input array     window : int       Minimum duration of consecutive run to accumulate values.      Returns     -------     int       Index of first item in first valid run. Returns np.nan if there are no valid run.
Return the length of the longest consecutive run of identical values.      Parameters     ----------     arr : bool array       Input array      Returns     -------     int       Length of longest run.
Return the number of runs of a minimum length.      Parameters     ----------     arr : bool array       Input array      window : int       Minimum run length.      Returns     -------     out : func       Number of distinct runs of a minimum length.
Dask-parallel version of windowed_run_count_1d, ie the number of consecutive true values in     array for runs at least as long as given duration.      Parameters     ----------     x : bool array       Input array     window : int       Minimum duration of consecutive run to accumulate values.      Returns     -------     out : func       A function operating along the time dimension of a dask-array.
Dask-parallel version of windowed_run_events_1d, ie the number of runs at least as long as given duration.      Parameters     ----------     x : bool array       Input array     window : int       Minimum run length      Returns     -------     out : func       A function operating along the time dimension of a dask-array.
Dask-parallel version of longest_run_1d, ie the maximum number of consecutive true values in     array.      Parameters     ----------     x : bool array       Input array      Returns     -------     out : func       A function operating along the time dimension of a dask-array.
r"""Check that a variable's attribute has the expected value. Warn user otherwise.
r"""Assert that the series is daily and monotonic (no jumps in time index).      A ValueError is raised otherwise.
r"""Check that variable is air temperature.
r"""Decorator to check that a computation runs on a valid temperature dataset.
r"""Decorator to check that a computation runs on a valid temperature dataset.
r"""Decorator to check that a computation runs on a valid temperature dataset.
r"""Decorator to check that a computation runs on valid min and max temperature datasets.
r"""Decorator to check that a computation runs on valid discharge data.
r"""Decorator to check that a computation has an instance of xarray.DataArray      as first argument.
r"""Return a boolean DataArray indicating whether there are missing days in the resampled array.      Parameters     ----------     da : DataArray       Input array at daily frequency.     freq : str       Resampling frequency.      Returns     -------     out : DataArray       A boolean array set to True if any month or year has missing values.
Render our pages as a jinja template for fancy templating goodness.
Return the pint Unit for the DataArray units.      Parameters     ----------     value : xr.DataArray or string       Input data array or expression.      Returns     -------     pint.Unit       Units of the data array.
Return a CF-Convention unit string from a `pint` unit.      Parameters     ----------     value : pint.Unit       Input unit.      Returns     -------     out : str       Units following CF-Convention.
Multiply xarray.DataArray by pint.Quantity.      Parameters     ----------     da : xr.DataArray       Input array.     q : pint.Quantity       Multiplicating factor.     out_units : str       Units the output array should be converted into.
Convert a mathematical expression into a value with the same units as a DataArray.      Parameters     ----------     source : str, pint.Quantity or xr.DataArray       The value to be converted, e.g. '4C' or '1 mm/d'.     target : str, pint.Unit or DataArray       Target array of values to which units must conform.     context : str       Returns     -------     out       The source value converted to target's units.
Create a decorator to check units of function arguments.
Create an xarray datset of ensemble of climate simulation from a list of netcdf files. Input data is     concatenated along a newly created data dimension ('realization')      Returns a xarray dataset object containing input data from the list of netcdf files concatenated along     a new dimension (name:'realization'). In the case where input files have unequal time dimensions output     ensemble dataset is created for overlapping time-steps common to all input files      Parameters     ----------     ncfiles : sequence       List of netcdf file paths. If mf_flag is true ncfiles should be a list of lists where     each sublist contains input .nc files of a multifile dataset      mf_flag : Boolean . If true climate simulations are treated as multifile datasets before concatenation      Returns     -------     xarray dataset containing concatenated data from all input files      Notes     -----     Input netcdf files require equal spatial dimension size (e.g. lon, lat dimensions)     If input data contains multiple cftime calendar types they must be at monthly or coarser frequency      Examples     --------     >>> from xclim import utils     >>> import glob     >>> ncfiles = glob.glob('/*.nc')     >>> ens = utils.create_ensemble(ncfiles)     >>> print(ens)     Using multifile datasets:     simulation 1 is a list of .nc files (e.g. separated by time)     >>> ncfiles = glob.glob('dir/*.nc')     simulation 2 is also a list of .nc files     >>> ens = utils.create_ensemble(ncfiles)
Calculate ensemble statistics between a results from an ensemble of climate simulations      Returns a dataset containing ensemble statistics for input climate simulations.     Alternatively calculate ensemble percentiles (default) or ensemble mean and standard deviation      Parameters     ----------     ens : Ensemble dataset (see xclim.utils.create_ensemble)     values : tuple of integers - percentile values to calculate  : default : (10, 50, 90)     time_block : integer       for large ensembles iteratively calculate percentiles in time-step blocks (n==time_block).        If not defined the function tries to estimate an appropriate value      Returns     -------     xarray dataset with containing data variables of requested ensemble statistics      Examples     --------     >>> from xclim import utils     >>> import glob     >>> ncfiles = glob.glob('/*tas*.nc')     Create ensemble dataset     >>> ens = utils.create_ensemble(ncfiles)     Calculate default ensemble percentiles     >>> ens_percs = utils.ensemble_statistics(ens)     >>> print(ens_percs['tas_p10'])     Calculate non-default percentiles (25th and 75th)     >>> ens_percs = utils.ensemble_statistics(ens, values=(25,75))     >>> print(ens_percs['tas_p25'])     Calculate by time blocks (n=10) if ensemble size is too large to load in memory     >>> ens_percs = utils.ensemble_statistics(ens, time_block=10)     >>> print(ens_percs['tas_p25'])
Calculate ensemble statistics between a results from an ensemble of climate simulations      Returns a dataset containing ensemble mean, standard-deviation,     minimum and maximum for input climate simulations.      Parameters     ----------     ens : Ensemble dataset (see xclim.utils.create_ensemble)      Returns     -------     xarray dataset with containing data variables of ensemble statistics      Examples     --------     >>> from xclim import utils     >>> import glob     >>> ncfiles = glob.glob('/*tas*.nc')     Create ensemble dataset     >>> ens = utils.create_ensemble(ncfiles)     Calculate ensemble statistics     >>> ens_means_std = utils.ensemble_mean_std_max_min(ens)     >>> print(ens_mean_std['tas_mean'])
Count number of days above or below threshold.      Parameters     ----------     da : xarray.DataArray       Input data.     op : {>, <, >=, <=, gt, lt, ge, le }       Logical operator, e.g. arr > thresh.     thresh : float       Threshold value.     freq : str       Resampling frequency defining the periods       defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling.      Returns     -------     xarray.DataArray       The number of days meeting the constraints for each period.
Percentile value for each day of the year      Return the climatological percentile over a moving window around each day of the year.      Parameters     ----------     arr : xarray.DataArray       Input data.     window : int       Number of days around each day of the year to include in the calculation.     per : float       Percentile between [0,1]      Returns     -------     xarray.DataArray       The percentiles indexed by the day of the year.
Return the largest doy allowed by calendar.      Parameters     ----------     arr : xarray.DataArray       Array with `time` coordinate.      Returns     -------     int       The largest day of the year found in calendar.
Interpolate from one set of dayofyear range to another      Interpolate an array defined over a `dayofyear` range (say 1 to 360) to another `dayofyear` range (say 1     to 365).      Parameters     ----------     source : xarray.DataArray       Array with `dayofyear` coordinates.     doy_max : int       Largest day of the year allowed by calendar.      Returns     -------     xarray.DataArray       Interpolated source array over coordinates spanning the target `dayofyear` range.
Interpolate from one set of dayofyear range to another calendar.      Interpolate an array defined over a `dayofyear` range (say 1 to 360) to another `dayofyear` range (say 1     to 365).      Parameters     ----------     source : xarray.DataArray       Array with `dayofyear` coordinates.     target : xarray.DataArray       Array with `time` coordinate.      Returns     -------     xarray.DataArray       Interpolated source array over coordinates spanning the target `dayofyear` range.
Subset a datarray or dataset spatially (and temporally) using a lat lon bounding box and years selection.      Return a subsetted data array for grid points falling within a spatial bounding box     defined by longitude and latitudinal bounds and for years falling within provided year bounds.      Parameters     ----------     arr : xarray.DataArray or xarray.Dataset       Input data.     lon_bnds : list of floats       List of maximum and minimum longitudinal bounds. Optional. Defaults to all longitudes in original data-array.     lat_bnds :  list of floats       List maximum and minimum latitudinal bounds. Optional. Defaults to all latitudes in original data-array.     start_yr : int       First year of the subset. Defaults to first year of input.     end_yr : int       Last year of the subset. Defaults to last year of input.      Returns     -------     xarray.DataArray or xarray.DataSet       subsetted data array or dataset      Examples     --------     >>> from xclim import utils     >>> ds = xr.open_dataset('pr.day.nc')     Subset lat lon and years     >>> prSub = utils.subset_bbox(ds.pr, lon_bnds=[-75,-70],lat_bnds=[40,45],start_yr=1990,end_yr=1999)     Subset data array lat, lon and single year     >>> prSub = utils.subset_bbox(ds.pr, lon_bnds=[-75,-70],lat_bnds=[40,45],start_yr=1990,end_yr=1990)     Subset dataarray single year keep entire lon, lat grid     >>> prSub = utils.subset_bbox(ds.pr,start_yr=1990,end_yr=1990) # one year only entire grid     Subset multiple variables in a single dataset     >>> ds = xr.open_mfdataset(['pr.day.nc','tas.day.nc'])     >>> dsSub = utils.subset_bbox(ds,lon_bnds=[-75,-70],lat_bnds=[40,45],start_yr=1990,end_yr=1999)
Extract a nearest gridpoint from datarray based on lat lon coordinate.     Time series can optionally be subsetted by year(s)      Return a subsetted data array (or dataset) for the grid point falling nearest the input     longitude and latitudecoordinates. Optionally subset the data array for years falling     within provided year bounds      Parameters     ----------     da : xarray.DataArray or xarray.DataSet       Input data.     lon : float       Longitude coordinate.     lat:  float       Latitude coordinate.     start_yr : int       First year of the subset. Defaults to first year of input.     end_yr : int       Last year of the subset. Defaults to last year of input.      Returns     -------     xarray.DataArray or xarray.DataSet       Subsetted data array or dataset      Examples     --------     >>> from xclim import utils     >>> ds = xr.open_dataset('pr.day.nc')     Subset lat lon point and multiple years     >>> prSub = utils.subset_gridpoint(ds.pr, lon=-75,lat=45,start_yr=1990,end_yr=1999)     Subset lat, lon point and single year     >>> prSub = utils.subset_gridpoint(ds.pr, lon=-75,lat=45,start_yr=1990,end_yr=1990)      Subset multiple variables in a single dataset     >>> ds = xr.open_mfdataset(['pr.day.nc','tas.day.nc'])     >>> dsSub = utils.subset_gridpoint(ds, lon=-75,lat=45,start_yr=1990,end_yr=1999)
r"""     function that returns a 0/1 mask when a condition is True or False      the function returns 1 where operator(da, da_value) is True                          0 where operator(da, da_value) is False                          nan where da is nan      Parameters     ----------     da : xarray.DataArray     da_value : float     operator : string       Returns     -------     xarray.DataArray
r"""Daily climate data downsampler      Parameters     ----------     da : xarray.DataArray     freq : string      Returns     -------     xarray.DataArray       Note     ----          Usage Example              grouper = daily_downsampler(da_std, freq='YS')             x2 = grouper.mean()              # add time coords to x2 and change dimension tags to time             time1 = daily_downsampler(da_std.time, freq=freq).first()             x2.coords['time'] = ('tags', time1.values)             x2 = x2.swap_dims({'tags': 'time'})             x2 = x2.sortby('time')
Apply a function recursively to values of dictionary.      Parameters     ----------     d : dict       Input dictionary, possibly nested.     func : function       Function to apply to dictionary values.      Returns     -------     dict       Dictionary whose values are the output of the given function.
Crude regex parsing.
Modify attribute with argument values.      Parameters     ----------     attrs : dict       Attributes to be assigned to function output. The values of the attributes in braces will be replaced the       the corresponding args values.     params : dict       A BoundArguments.arguments dictionary storing a function's arguments.
Return a dictionary representation of the class.          Notes         -----         This is meant to be used by a third-party library wanting to wrap this class into another interface.
Format attributes including {} tags with arguments.
Return whether an output is considered missing or not.
Create a subclass from the attributes dictionary.
provide a wrapper for python string     map byte to str (python 3)     all string in utf-8 encoding     normalize string to NFC
:type token: object
extract features in a sentence      :type sentence: list of token, each token is a list of tag
tokenize text for word segmentation      :param text: raw text input     :return: tokenize text
Text classification      Parameters     ==========      X: {unicode, str}         raw sentence     domain: {None, 'bank'}         domain of text             * None: general domain             * bank: bank domain     Returns     =======     tokens: list         categories of sentence      Examples     --------      >>> # -*- coding: utf-8 -*-     >>> from underthesea import classify     >>> sentence = "HLV ngoại đòi gần tỷ mỗi tháng dẫn dắt tuyển Việt Nam"     >>> classify(sentence)     ['The thao']      >>> sentence = "Tôi rất thích cách phục vụ của nhân viên BIDV"     >>> classify(sentence, domain='bank')     ('CUSTOMER SUPPORT',)
Vietnamese POS tagging      Parameters     ==========      sentence: {unicode, str}         Raw sentence      Returns     =======     tokens: list of tuple with word, pos tag         tagged sentence     Examples     --------     >>> # -*- coding: utf-8 -*-     >>> from underthesea import pos_tag     >>> sentence = "Chợ thịt chó nổi tiếng ở TPHCM bị truy quét"     >>> pos_tag(sentence)     [('Chợ', 'N'),     ('thịt', 'N'),     ('chó', 'N'),     ('nổi tiếng', 'A'),     ('ở', 'E'),     ('TPHCM', 'Np'),     ('bị', 'V'),     ('truy quét', 'V')]
save wscorpus to files          :param str folder: path to directory         :type folder: string         :param str format: either TEXT or COLUMN         :type format: str
Location and classify named entities in text      Parameters     ==========      sentence: {unicode, str}         raw sentence      Returns     =======     tokens: list of tuple with word, pos tag, chunking tag, ner tag         tagged sentence      Examples     --------      >>> # -*- coding: utf-8 -*-     >>> from underthesea import ner     >>> sentence = "Ông Putin ca ngợi những thành tựu vĩ đại của Liên Xô"     >>> ner(sentence)     [('Ông', 'Nc', 'B-NP', 'O'),     ('Putin', 'Np', 'B-NP', 'B-PER'),     ('ca ngợi', 'V', 'B-VP', 'O'),     ('những', 'L', 'B-NP', 'O'),     ('thành tựu', 'N', 'B-NP', 'O'),     ('vĩ đại', 'A', 'B-AP', 'O'),     ('của', 'E', 'B-PP', 'O'),     ('Liên Xô', 'Np', 'B-NP', 'B-LOC')]
Load public RSA key, with work-around for keys using     incorrect header/footer format.      Read more about RSA encryption with cryptography:     https://cryptography.io/latest/hazmat/primitives/asymmetric/rsa/
Encrypt password using given RSA public key and encode it with base64.      The encrypted password can only be decrypted by someone with the     private key (in this case, only Travis).
Rewrite a file adding a line to its beginning.
Returns the singleton instance. Upon its first call, it creates a         new instance of the decorated class and calls its `__init__` method.         On all subsequent calls, the already created instance is returned.
Vietnamese chunking      Parameters     ==========      sentence: {unicode, str}         raw sentence      Returns     =======     tokens: 	list of tuple with word, pos tag, chunking tag         tagged sentence      Examples     --------      >>> # -*- coding: utf-8 -*-     >>> from underthesea import chunk     >>> sentence = "Nghi vấn 4 thi thể Triều Tiên trôi dạt bờ biển Nhật Bản"     >>> chunk(sentence)     [('Nghi vấn', 'N', 'B-NP'),     ('4', 'M', 'B-NP'),     ('thi thể', 'N', 'B-NP'),     ('Triều Tiên', 'Np', 'B-NP'),     ('trôi dạt', 'V', 'B-VP'),     ('bờ biển', 'N', 'B-NP'),     ('Nhật Bản', 'Np', 'B-NP')]
Vietnamese word segmentation      Parameters     ==========      sentence: {unicode, str}         raw sentence      Returns     =======     tokens: list of text         tagged sentence      Examples     --------      >>> # -*- coding: utf-8 -*-     >>> from underthesea import word_tokenize     >>> sentence = "Bác sĩ bây giờ có thể thản nhiên báo tin bệnh nhân bị ung thư"      >>> word_tokenize(sentence)     ['Bác sĩ', 'bây giờ', 'có thể', 'thản nhiên', 'báo tin', 'bệnh nhân', 'bị', 'ung thư']      >>> word_tokenize(sentence, format="text")     'Bác_sĩ bây_giờ có_thể thản_nhiên báo_tin bệnh_nhân bị ung_thư'
:param unicode|str text: input text         :type text: unicode|str          :return: transformed text         :rtype: unicode
Fit FastText according to X, y          Parameters:         ----------         X : list of text             each item is a text         y: list            each item is either a label (in multi class problem) or list of            labels (in multi label problem)
Create a selenium driver using specified config properties          :returns: a new selenium driver         :rtype: selenium.webdriver.remote.webdriver.WebDriver
Create a driver in a remote server         View valid capabilities in https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities          :returns: a new remote selenium driver
Create a driver in local machine          :returns: a new local selenium driver
Create initial driver capabilities          :params driver_name: name of selected driver         :returns: capabilities dictionary
Add capabilities from properties file          :param capabilities: capabilities object         :param section: properties section
Setup Firefox webdriver          :param capabilities: capabilities object         :returns: a new local Firefox driver
Create and configure a firefox profile          :returns: firefox profile
Converts the string value in a boolean, integer or string          :param value: string value         :returns: boolean, integer or string value
Setup Chrome webdriver          :param capabilities: capabilities object         :returns: a new local Chrome driver
Create and configure a chrome options object          :returns: chrome options object
Add Chrome options from properties file          :param options: chrome options object         :param option_name: chrome option name
Add Chrome arguments from properties file          :param options: chrome options object
Setup Opera webdriver          :param capabilities: capabilities object         :returns: a new local Opera driver
Setup Internet Explorer webdriver          :param capabilities: capabilities object         :returns: a new local Internet Explorer driver
Setup Edge webdriver          :param capabilities: capabilities object         :returns: a new local Edge driver
Setup phantomjs webdriver          :param capabilities: capabilities object         :returns: a new local phantomjs driver
Setup Appium webdriver          :returns: a new remote Appium driver
Read implicitly timeout from configuration properties and configure driver implicitly wait
Capture screenshot and save it in screenshots folder          :param name: screenshot name suffix         :returns: screenshot path
Get webdriver logs and write them to log files          :param test_name: test that has generated these logs
Get webdriver logs of the specified type and write them to a log file          :param log_type: browser, client, driver, performance, server, syslog, crashlog or logcat         :param test_name: test that has generated these logs
Discard previous logcat logs
Tries to find the element, but does not thrown an exception if the element is not found          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :returns: the web element if it has been found or False         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement
Tries to find the element and checks that it is visible, but does not thrown an exception if the element is             not found          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :returns: the web element if it is visible or False         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement
Tries to find the element and checks that it is visible, but does not thrown an exception if the element is             not found          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :returns: True if the web element is not found or it is not visible
Try to find sequentially the elements of the list and return the first element found          :param elements: list of PageElements or element locators as a tuple (locator_type, locator_value) to be found                          sequentially         :returns: first element found or None         :rtype: toolium.pageelements.PageElement or tuple
Tries to find the element and checks that it is clickable, but does not thrown an exception if the element             is not found          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :returns: the web element if it is clickable or False         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement
Tries to find the element and checks that it has stopped moving, but does not thrown an exception if the element             is not found          :param element_times: Tuple with 2 items where:             [0] element: PageElement or element locator as a tuple (locator_type, locator_value) to be found             [1] times: number of iterations checking the element's location that must be the same for all of them             in order to considering the element has stopped         :returns: the web element if it is clickable or False         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement
Tries to find the element and checks that it contains the specified text, but does not thrown an exception if the element is             not found          :param element_text_pair: Tuple with 2 items where:             [0] element: PageElement or element locator as a tuple (locator_type, locator_value) to be found             [1] text: text to be contained into the element         :returns: the web element if it contains the text or False         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement
Tries to find the element and checks that it contains the requested attribute with the expected value,            but does not thrown an exception if the element is not found          :param element_attribute_value: Tuple with 3 items where:             [0] element: PageElement or element locator as a tuple (locator_type, locator_value) to be found             [1] attribute: element's attribute where to check its value             [2] value: expected value for the element's attribute         :returns: the web element if it contains the expected value for the requested attribute or False         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement
Common method to wait until condition met          :param condition_method: method to check the condition         :param condition_input: parameter that will be passed to the condition method         :param timeout: max time to wait         :returns: condition method response
Search element and wait until it is found          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param timeout: max time to wait         :returns: the web element if it is present         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element is not found after the timeout
Search element and wait until it is visible          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param timeout: max time to wait         :returns: the web element if it is visible         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element is still not visible after the timeout
Search element and wait until it is not visible          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param timeout: max time to wait         :returns: the web element if it exists but is not visible         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element is still visible after the timeout
Search list of elements and wait until one of them is found          :param elements: list of PageElements or element locators as a tuple (locator_type, locator_value) to be found                          sequentially         :param timeout: max time to wait         :returns: first element found         :rtype: toolium.pageelements.PageElement or tuple         :raises TimeoutException: If no element in the list is found after the timeout
Search element and wait until it is clickable          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param timeout: max time to wait         :returns: the web element if it is clickable         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element is not clickable after the timeout
Search element and wait until it has stopped moving          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param times: number of iterations checking the element's location that must be the same for all of them         in order to considering the element has stopped         :returns: the web element if the element is stopped         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element does not stop after the timeout
Search element and wait until it contains the expected text          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param text: text expected to be contained into the element         :param timeout: max time to wait         :returns: the web element if it contains the expected text         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element does not contain the expected text after the timeout
Search element and wait until it does not contain the expected text          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param text: text expected to be contained into the element         :param timeout: max time to wait         :returns: the web element if it does not contain the given text         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element contains the expected text after the timeout
Search element and wait until the requested attribute contains the expected value          :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found         :param attribute: attribute belonging to the element         :param value: expected value for the attribute of the element         :param timeout: max time to wait         :returns: the web element if the element's attribute contains the expected value         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement         :raises TimeoutException: If the element's attribute does not contain the expected value after the timeout
Return the remote node that it's executing the actual test session          :returns: tuple with server type (local, grid, ggr, selenium) and remote node name
Return the configured server url          :returns: server url
Download the video recorded in the remote node during the specified test session and save it in videos folder          :param remote_node: remote node name         :param session_id: test session id         :param video_name: video name
Get grid-extras url of a node          :param remote_node: remote node name         :returns: grid-extras url
Get grid-extras url to download videos          :param remote_node: remote node name         :param session_id: test session id         :returns: grid-extras url to download videos
Download a video from the remote node          :param video_url: video url         :param video_name: video name
Check if the remote node has the video recorder enabled          :param remote_node: remote node name         :returns: true if it has the video recorder enabled
Get center coordinates of an element          :param element: either a WebElement, PageElement or element locator as a tuple (locator_type, locator_value)         :returns: dict with center coordinates
Get the height of Safari navigation bar          :returns: height of navigation bar
Generic method to get window size using a javascript workaround for Android web tests          :returns: dict with window width and height
Convert web coords into native coords. Assumes that the initial context is WEBVIEW and switches to          NATIVE_APP context.          :param coords: dict with web coords, e.g. {'x': 10, 'y': 10}         :returns: dict with native coords
Swipe over an element          :param element: either a WebElement, PageElement or element locator as a tuple (locator_type, locator_value)         :param x: horizontal movement         :param y: vertical movement         :param duration: time to take the swipe, in ms
Return the web element from a page element or its locator          :param element: either a WebElement, PageElement or element locator as a tuple (locator_type, locator_value)         :returns: WebElement object
Return the first WEBVIEW context or raise an exception if it is not found          :returns: first WEBVIEW context
Returns the default (first) driver wrapper          :returns: default driver wrapper         :rtype: toolium.driver_wrapper.DriverWrapper
Capture a screenshot in each driver          :param name: screenshot name suffix
Get default driver wrapper, configure it and connect driver          :param config_files: driver wrapper specific config files         :returns: default driver wrapper         :rtype: toolium.driver_wrapper.DriverWrapper
Stop all drivers, capture screenshots, copy webdriver and GGR logs and download saved videos          :param scope: execution scope (function, module, class or session)         :param test_name: executed test name         :param test_passed: True if the test has passed         :param context: behave context
Stop all drivers except default if it should be reused          :param maintain_default: True if the default driver should not be closed
Download saved videos if video is enabled or if test fails          :param name: destination file name         :param test_passed: True if the test has passed         :param maintain_default: True if the default driver should not be closed
Get all webdriver logs of each driver and write them to log files          :param test_name: test that has generated these logs         :param test_passed: True if the test has passed
Get all GGR logs of each driver and write them to log files          :param test_name: test that has generated these logs         :param test_passed: True if the test has passed
Get configured value from system properties, method parameters or default value          :param system_property_name: system property name         :param specific_value: test case specific value         :param default_value: default value         :returns: configured value
Configure common config and output folders for all tests          :param tc_config_files: test case specific config files
Return default config directory, based in the actual test path          :returns: default config directory
Find a directory in parent tree with a specific filename          :param directory: directory name to find         :param filename: filename to find         :returns: absolute directory path
Configure screenshots, videos and visual directories          :param driver_info: driver property value to name folders
Initialize config files and update config files names with the environment          :param tc_config_files: test case specific config files         :returns: initialized config files object
Initialization method that will be executed before the test execution      :param context: behave context
Feature initialization      :param context: behave context     :param feature: running feature
Scenario initialization      :param context: behave context     :param scenario: running scenario
Common scenario initialization in behave or lettuce      :param context_or_world: behave context or lettuce world     :param scenario: running scenario     :param no_driver: True if this is an api test and driver should not be started
Create and configure driver wrapper in behave or lettuce tests      :param context_or_world: behave context or lettuce world
Connect driver in behave or lettuce tests      :param context_or_world: behave context or lettuce world
Add assert screenshot methods to behave or lettuce object      :param context_or_world: behave context or lettuce world     :param scenario: running scenario
Clean method that will be executed after each scenario in behave or lettuce      :param context_or_world: behave context or lettuce world     :param scenario: running scenario     :param status: scenario status (passed, failed or skipped)
Extract Jira Test Case key from scenario tags.     Two tag formats are allowed:     @jira('PROJECT-32')     @jira=PROJECT-32      :param scenario: behave scenario     :returns: Jira test case key
Clean method that will be executed after each feature      :param context: behave context     :param feature: running feature
Common after all method in behave or lettuce      :param context_or_world: behave context or lettuce world
Configure selenium instance logger          :param tc_config_log_filename: test case specific logging config file         :param tc_output_log_filename: test case specific output logger file
Configure selenium instance properties          :param tc_config_prop_filenames: test case specific properties filenames         :param behave_properties: dict with behave user data properties
Configure baseline directory
Configure baseline directory after driver is created
Configure initial selenium instance using logging and properties files for Selenium or Appium tests          :param tc_config_files: test case specific config files         :param is_selenium_test: true if test is a selenium or appium test case         :param behave_properties: dict with behave user data properties
Set up the selenium driver and connect to the server          :param maximize: True if the driver should be maximized         :returns: selenium driver
Reads bounds from config and, if monitor is specified, modify the values to match with the specified monitor          :return: coords X and Y where set the browser window.
Check if the driver should be reused          :param scope: execution scope (function, module, class or session)         :param test_passed: True if the test has passed         :param context: behave context         :returns: True if the driver should be reused
Get driver platform where tests are running         :return: platform name
Returns the given string converted to a string that can be used for a clean filename.     Removes leading and trailing spaces; converts anything that is not an alphanumeric,     dash or underscore to underscore; converts behave examples separator ` -- @` to underscore.     It also cuts the resulting name to `max_length`.      @see https://github.com/django/django/blob/master/django/utils/text.py
Get README content and update rst urls      :returns: long description
Decorator to update test status in Jira      :param test_key: test case key in Jira     :returns: jira test
Read Jira configuration from properties file and save it
Save test status and comments to update Jira later      :param test_key: test case key in Jira     :param test_status: test case status     :param test_comment: test case comments
Update test status in Jira      :param test_key: test case key in Jira     :param test_status: test case status     :param test_comment: test case comments     :param test_attachments: test case attachments
Extract error message from the HTTP response      :param response_content: HTTP response from test case execution API     :returns: error message
Click the element          :returns: page element instance
Reset each page element object          :param driver_wrapper: driver wrapper instance
Return page elements and page objects of this page object          :returns: list of page elements and page objects
Wait until page object is loaded         Search all page elements configured with wait=True          :param timeout: max time to wait         :returns: this page object instance
Get an option value for a given section         If the section or the option are not found, the default value is returned          :param section: config section         :param option: config option         :param default: default value         :returns: config value
Get an option boolean value for a given section         If the section or the option are not found, the default value is returned          :param section: config section         :param option: config option         :param default: default value         :returns: boolean config value
Returns a deep copy of config object          :returns: a copy of the config object
Update config properties values         Property name must be equal to 'Section_option' of config property          :param new_properties: dict with new properties values
Update a config property value with a new property value         Property name must be equal to 'Section_option' of config property          :param section: config section         :param option: config option         :param new_properties: dict with new properties values
Reads properties files and saves them to a config object          :param conf_properties_files: comma-separated list of properties files         :returns: config object
Scenario initialization      :param scenario: running scenario
Reset each page element object          :param driver_wrapper: driver wrapper instance
Find multiple WebElements using element locator          :returns: list of web element objects         :rtype: list of selenium.webdriver.remote.webelement.WebElement                 or list of appium.webdriver.webelement.WebElement
Find multiple PageElement using element locator          :returns: list of page element objects         :rtype: list of toolium.pageelements.PageElement
Find WebElement using element locator          :returns: web element object         :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement
Find WebElement using element locator and save it in _web_element attribute
Scroll element into view          :returns: page element instance
Search element and wait until it is visible          :param timeout: max time to wait         :returns: page element instance
Search element and wait until it is not visible          :param timeout: max time to wait         :returns: page element instance
Search element and wait until it is clickable          :param timeout: max time to wait         :returns: page element instance
Assert that a screenshot of the element is the same as a screenshot on disk, within a given threshold.          :param filename: the filename for the screenshot, which will be appended with ``.png``         :param threshold: percentage threshold for triggering a test failure (value between 0 and 1)         :param exclude_elements: list of WebElements, PageElements or element locators as a tuple (locator_type,                                  locator_value) that must be excluded from the assertion         :param force: if True, the screenshot is compared even if visual testing is disabled by configuration
download a file from the server using a request with retries policy         :param url: server url to request         :param path_file: path and file where to download         :param timeout: threshold until the video file is downloaded         :return boolean
retrieve the current selenoid host info         request: http://<username>:<password>@<ggr_host>:<ggr_port>/host/<ggr_session_id>         :return: dict
Is the GGR session still active? Associated to a browser and the sessionId         Example of GGR status:         {"browsers":{"MicrosoftEdge":{"latest":{}},"android":{"8.1":{}},"chrome":{"70.0":{},"latest":{"test_tef":{"count":1,"sessions":[{"caps":{"browserName":"chrome","enableVNC":true,"enableVideo":true,"platform":"ANY","screenResolution":"1280x1024x24","version":"latest","videoName":"selenoide952e551bb9395e16d060f28c54e5d31.mp4","videoScreenSize":"1280x1024"},"container":"8489205e28c9781472e99c3921a6240de3894a3603ed9e187ad6360b6b013b8b","containerInfo":{"id":"8489205e28c9781472e99c3921a6240de3894a3603ed9e187ad6360b6b013b8b","ip":"172.17.0.4"},"id":"1345506093dfed8dbcef610da476911a228ca315978e5464ae49fb1142bbc49b","screen":"1280x1024x24","vnc":true}]}}},"firefox":{"59.0":{},"63.0":{},"64.0":{},"latest":{}},"internet explorer":{"11":{}},"opera":{"56.0":{},"latest":{}},"safari":{"latest":{}}},"pending":0,"queued":0,"total":30,"used":1}         :return boolean (although in case of error in the request will be returned None)
download the execution video file if the scenario fails or the video is enabled,         renaming the file to scenario name and removing the video file in the server.              GGR request: http://<username>:<password>@<ggr_host>:<ggr_port>/video/<session_id>         selenoid request: http://<username>:<password>@<ggr_host>:<ggr_port>/video/<session_id>.mp4         :param scenario_name: scenario name         :param timeout: threshold until the video file is downloaded
download the session log file from remote selenoid,         renaming the file to scenario name and removing the video file in the server.              GGR request: http://<username>:<password>@<ggr_host>:<ggr_port>/logs/<ggr_session_id>         selenoid request: http://<username>:<password>@<ggr_host>:<ggr_port>/logs/<ggr_session_id>.log         :param scenario_name: scenario name         :param timeout: threshold until the video file is downloaded
download a file from remote selenoid and removing the file in the server.         request: http://<username>:<password>@<ggr_host>:<ggr_port>/download/<ggr_session_id>/<filename>         :param filename: file name with extension to download         :param timeout: threshold until the video file is downloaded         :return: downloaded file path or None
log a warning message:         :param exc: exception message
log an error message:         :param exc: exception message
print in console avoiding output buffering         :param text_to_print: Text to print by console
get all steps defined in the feature description associated to each action         :param description: feature description
remove the step prefix to will be replaced by Given         :param step: step text
print the step by console if the show variable is enabled         :param step: step text
execute a steps set by action         :param context: It’s a clever place where you and behave can store information to share around, automatically managed by behave.         :param action: action executed: see labels allowed above.
Check if the dyn_env has got any exception when executing the steps and restore the value of status to False.         :return: True if any exception has been raised when executing steps
actions before the feature         :param context: It’s a clever place where you and behave can store information to share around, automatically managed by behave.
actions before each scenario         :param context: It’s a clever place where you and behave can store information to share around, automatically managed by behave.
actions after each scenario         :param context: It’s a clever place where you and behave can store information to share around, automatically managed by behave.
actions after the feature         :param context: It’s a clever place where you and behave can store information to share around, automatically managed by behave.
Fail first step in the given Scenario and add exception message for the output.         This is needed because xUnit exporter in Behave fails if there are not failed steps.         :param scenario: Behave's Scenario
Set value on the element          :param value: value to be set
Reset each page element object          :param driver_wrapper: driver wrapper instance
Install the example notebooks for litho1pt0 in the given location      WARNING: If the path exists, the Notebook files will be written into the path     and will overwrite any existing files with which they collide. The default     path ("./Litho1pt0-Notebooks") is chosen to make collision less likely / problematic      The documentation for litho1pt0 is in the form of jupyter notebooks.      Some dependencies exist for the notebooks to be useful:         - matplotlib: for some diagrams        - cartopy: for plotting map examples      litho1pt0 dependencies are explicitly imported into the notebooks including:         - stripy (for interpolating on the sphere)        - numpy        - scipy (for k-d tree point location)
Remove duplicates rows from N equally-sized arrays
Checks if first three points are collinear
Permute field
Return to original ordering
Return the gradient of an n-dimensional array.          The method consists of minimizing a quadratic functional Q(G) over         gradient vectors (in x and y directions), where Q is an approximation         to the linearized curvature over the triangulation of a C-1 bivariate         function F(x,y) which interpolates the nodal values and gradients.          Parameters         ----------          f : array of floats, shape (n,)             field over which to evaluate the gradient          nit: int (default: 3)             number of iterations to reach a convergence tolerance, tol             nit >= 1          tol: float (default: 1e-3)             maximum change in gradient between iterations.             convergence is reached when this condition is met.          Returns         -------          dfdx : array of floats, shape (n,)             derivative of f in the x direction          dfdy : array of floats, shape (n,)             derivative of f in the y direction          Notes         -----          For SIGMA = 0, optimal efficiency was achieved in testing with          tol = 0, and nit = 3 or 4.           The restriction of F to an arc of the triangulation is taken to be          the Hermite interpolatory tension spline defined by the data values          and tangential gradient components at the endpoints of the arc, and          Q is the sum over the triangulation arcs, excluding interior          constraint arcs, of the linearized curvatures of F along the arcs --          the integrals over the arcs of D2F(T)**2, where D2F(T) is the second          derivative of F with respect to distance T along the arc.
Return the gradient at a specified node.          This routine employs a local method, in which values depend only on nearby         data points, to compute an estimated gradient at a node.          gradient_local() is more efficient than gradient() only if it is unnecessary         to compute gradients at all of the nodes. Both routines have similar accuracy.          Parameters         ----------
Smooths a surface f by choosing nodal function values and gradients to         minimize the linearized curvature of F subject to a bound on the         deviation from the data values. This is more appropriate than interpolation         when significant errors are present in the data.          Parameters         ----------          f : array of floats, shape (n,)             field to apply smoothing on          w : array of floats, shape (n,)             weights associated with data value in f             w[i] = 1/sigma_f^2 is a good rule of thumb.          sm : float             positive parameter specifying an upper bound on Q2(f).             generally n-sqrt(2n) <= sm <= n+sqrt(2n)          smtol : float             specifies relative error in satisfying the constraint             sm(1-smtol) <= Q2 <= sm(1+smtol) between 0 and 1.          gstol : float             tolerance for convergence.             gstol = 0.05*mean(sigma_f)^2 is a good rule of thumb.          Returns         -------          f_smooth : array of floats, shape (n,)             smoothed version of f          (dfdx, dfdy) : tuple of floats, tuple of 2 shape (n,) arrays             first derivatives of f_smooth in the x and y directions
Base class to handle nearest neighbour, linear, and cubic interpolation.         Given a triangulation of a set of nodes and values at the nodes,         this method interpolates the value at the given xi,yi coordinates.          Parameters         ----------          xi    : float / array of floats, shape (l,)                  x Cartesian coordinate(s)          yi    : float / array of floats, shape (l,)                  y Cartesian coordinate(s)          zdata : array of floats, shape (n,)                  value at each point in the triangulation                  must be the same size of the mesh          order : int (default=1)                  order of the interpolatory function used                   0 = nearest-neighbour                   1 = linear                   3 = cubic          Returns         -------          zi    : float / array of floats, shape (l,)                 interpolates value(s) at (xi, yi)          err   : int / array of ints, shape (l,)                 whether interpolation (0), extrapolation (1) or error (other)
Nearest-neighbour interpolation.         Calls nearnd to find the index of the closest neighbours to xi,yi          Parameters         ----------          xi : float / array of floats, shape (l,)             x coordinates on the Cartesian plane          yi : float / array of floats, shape (l,)             y coordinates on the Cartesian plane          Returns         -------          zi : float / array of floats, shape (l,)             nearest-neighbour interpolated value(s) of (xi,yi)
Piecewise linear interpolation/extrapolation to arbitrary point(s).         The method is fast, but has only C^0 continuity.          Parameters         ----------          xi : float / array of floats, shape (l,)             x coordinates on the Cartesian plane          yi : float / array of floats, shape (l,)             y coordinates on the Cartesian plane          zdata : array of floats, shape (n,)             value at each point in the triangulation             must be the same size of the mesh          Returns         -------          zi : float / array of floats, shape (l,)             interpolated value(s) of (xi,yi)          err : int / array of ints, shape (l,)             whether interpolation (0), extrapolation (1) or error (other)
Cubic spline interpolation/extrapolation to arbirary point(s).         This method has C^1 continuity.          Parameters         ----------          xi : float / array of floats, shape (l,)             x coordinates on the Cartesian plane          yi : float / array of floats, shape (l,)             y coordinates on the Cartesian plane          zdata : array of floats, shape (n,)             value at each point in the triangulation             must be the same size of the mesh          gradz (optional) : array of floats, shape (2,n)             derivative at each point in the triangulation in the             x-direction (first row), y-direction (second row)             if not supplied it is evaluated using self.gradient          derivatives (optional) : bool (default: False)             optionally returns the first derivatives at point(s) (xi,yi)          Returns         -------          zi : float / array of floats, shape (l,)             interpolated value(s) of (xi,yi)          err : int / array of ints, shape (l,)             whether interpolation (0), extrapolation (1) or error (other)          dzx, dzy (optional) : float, array of floats, shape(l,)             first partial derivatives in x and y direction at (xi,yi)
Get indices of neighbour simplices for each simplex and arc indices.         Identical to get_neighbour_simplices() but also returns an array         of indices that reside on boundary hull, -1 denotes no neighbour.
Locate the index of the nearest vertex to points (xi,yi)         and return the squared distance between (xi,yi) and         each nearest neighbour.          Parameters         ----------          xi : array of floats, shape (l,)             Cartesian coordinates in the x direction          yi : array of floats, shape (l,)             Cartesian coordinates in the y direction          Returns         -------          index : array of ints             the nearest vertex to each of the supplied points          dist : array of floats             squared distance to the closest vertex identified          Notes         -----          Faster searches can be obtained using a KDTree.          Store all x and y coordinates in a (c)KDTree, then query          a set of points to find their nearest neighbours.
Returns indices of the triangles containing xi yi          Parameters         ----------          xi : float / array of floats, shape (l,)             Cartesian coordinates in the x direction          yi : float / array of floats, shape (l,)             Cartesian coordinates in the y direction          Returns         -------          tri_indices: array of ints, shape (l,)          Notes         -----           The simplices are found as cartesian.Triangulation.simplices[tri_indices]
Returns the simplices containing (xi,yi)         and the local barycentric, normalised coordinates.          Parameters         ----------          xi : float / array of floats, shape (l,)             Cartesian coordinates in the x direction          yi : float / array of floats, shape (l,)             Cartesian coordinates in the y direction          Returns         -------          bcc : normalised barycentric coordinates          tri : simplices containing (xi,yi)          Notes         -----          The ordering of the vertices may differ from that stored in          self.simplices array but will still be a loop around the simplex.
Find the neighbour-vertices in the triangulation for the given vertex         Searches self.simplices for vertex entries and sorts neighbours
Find all triangles which own any of the vertices in the list provided
Find all the segments in the triangulation and return an         array of vertices (n1,n2) where n1 < n2
Identify the centroid of every simplex in the triangulation. If an array of         simplices is given then the centroids of only those simplices is returned.
Identify the midpoints of every line segment in the triangulation.         If an array of segments of shape (no_of_segments,2) is given,         then the midpoints of only those segments is returned.          Notes         -----          Segments in the array must not be duplicates or the re-triangulation          will fail. Take care not to miss that (n1,n2) is equivalent to (n2,n1).
Identify the trisection points of every line segment in the triangulation
Find the Convex Hull of the internal set of x,y points.          Returns         -------          bnodes : array of ints             indices corresponding to points on the convex hull
Compute the area of each triangle within the triangulation of points.          Returns         -------          area : array of floats, shape (nt,)             area of each triangle in self.simplices where nt             is the number of triangles.
Compute the edge-lengths of each triangle in the triangulation.
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation connected to any of the vertices in the list provided
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation that are associated with the triangles in the list         of indices provided.          Notes         -----          The triangles are here represented as a single index.          The vertices of triangle i are given by self.simplices[i].
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation connected to any of the vertices in the list provided
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation that are associated with the triangles in the list provided.          Notes         -----          The triangles are here represented as a single index.          The vertices of triangle i are given by self.simplices[i].
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation connected to any of the vertices in the list provided
Join this triangulation with another. If the points are known to have no duplicates, then         set unique=False to skip the testing and duplicate removal
Query the cKDtree for the nearest neighbours and Euclidean         distance from x,y points.          Returns 0, 0 if a cKDtree has not been constructed         (switch tree=True if you need this routine)          Parameters         ----------          x : 1D array of Cartesian x coordinates          y : 1D array of Cartesian y coordinates          k : number of nearest neighbours to return              (default: 1)          max_distance : maximum Euclidean distance to search                         for neighbours (default: inf)          Returns         -------          d    : Euclidean distance between each point and their                 nearest neighbour(s)          vert : vertices of the nearest neighbour(s)
Returns layer depth at lat / lon (degrees)     where lat/lon may be arrays (of equal size).     Depths are returned in metres.
lat, lon (degrees)
Lat / Lon are in degrees     Depth in km     quantity_ID needs to match those in the litho1 model      Points that are not found are given the out-of-range value of -99999
Lat / Lon are in degrees     Depth in km     quantity_ID needs to match those in the litho1 model      Points that are not found are given the out-of-range value of -99999
Don't forget to strip the model data first     truncate_raw_litho1_data(model path, truncated_model_path)
Ensures that the data is stored in a format which is valid for initialising the class
Weighted average of scattered data to the nodal points     of a triangulation using the barycentric coordinates as     weightings.      Parameters     ----------      x1, x2 : 1D arrays arrays of x,y or lon, lat (radians)      data :   1D array of data to be lumped to the node locations      interpolator : a stripy.Triangulation or stripy.sTriangulation object      which defines the node locations and their triangulation      Returns     -------      grid  : 1D array containing the results of the weighted average      norm  : 1D array of the normalisation used to compute `grid`      count : 1D int array of number of points that contribute anything to a given node
remove duplicates from an array of lon / lat points
Convert lon / lat (radians) for the spherical triangulation into x,y,z     on the unit sphere
Convert x,y,z representation of points *on the unit sphere* of the     spherical triangulation to lon / lat (radians).      Note - no check is made here that (x,y,z) are unit vectors
Take stripack df/dx, df/dy, df/dz format and convert to     surface gradients df/dlon, df/dlat      Notes
N points along the line joining lonlat1 and lonlat2
Compute the angles between lon / lat points p1 and p2 given in radians.     On the unit sphere, this also corresponds to the great circle distance.     p1 and p2 can be numpy arrays of the same length.
Create shuffle and deshuffle vectors
Checks if first three points are collinear - in the spherical         case this corresponds to all points lying on a great circle         and, hence, all coordinate vectors being in a single plane.
Return the lon / lat components of the gradient         of a scalar field on the surface of the sphere.           The method consists of minimizing a quadratic functional Q(G) over         gradient vectors, where Q is an approximation to the linearized         curvature over the triangulation of a C-1 bivariate function F(x,y)         which interpolates the nodal values and gradients.          Parameters         ----------          data : array of floats, shape (n,)             field over which to evaluate the gradient          nit: int (default: 3)             number of iterations to reach a convergence tolerance, tol             nit >= 1          tol: float (default: 1e-3)             maximum change in gradient between iterations.             convergence is reached when this condition is met.          Returns         -------          dfdlon : array of floats, shape (n,)             derivative of f in the longitudinal direction          dfdlat : array of floats, shape (n,)             derivative of f in the lattitudinal direction          Notes         -----          The gradient is computed via the Cartesian components using         spherical.sTriangulation.gradient_xyz and the iteration parameters         controling the spline interpolation are passed directly to this         routine (See notes for gradient_xyz for more details).          The gradient operator in this geometry is not well defined at the poles         even if the scalar field is smooth and the Cartesian gradient is well defined.          The routine spherical.dxyz2dlonlat is available to convert the Cartesian         to lon/lat coordinates at any point on the unit sphere. This is helpful         to avoid recalculation if you need both forms.
Return the cartesian components of the gradient         of a scalar field on the surface of the sphere.          The method consists of minimizing a quadratic functional Q(G) over         gradient vectors, where Q is an approximation to the linearized         curvature over the triangulation of a C-1 bivariate function F(x,y)         which interpolates the nodal values and gradients.          Parameters         ----------          f : array of floats, shape (n,)             field over which to evaluate the gradient          nit: int (default: 3)             number of iterations to reach a convergence tolerance, tol             nit >= 1          tol: float (default: 1e-3)             maximum change in gradient between iterations.             convergence is reached when this condition is met.          Returns         -------          dfdx : array of floats, shape (n,)             derivative of f in the x direction          dfdy : array of floats, shape (n,)             derivative of f in the y direction          dfdz : array of floats, shape (n,)             derivative of f in the z direction          Notes         -----          For SIGMA = 0, optimal efficiency was achieved in testing with          tol = 0, and nit = 3 or 4.           The restriction of F to an arc of the triangulation is taken to be          the Hermite interpolatory tension spline defined by the data values          and tangential gradient components at the endpoints of the arc, and          Q is the sum over the triangulation arcs, excluding interior          constraint arcs, of the linearized curvatures of F along the arcs --          the integrals over the arcs of D2F(T)**2, where D2F(T) is the second          derivative of F with respect to distance T along the arc.
Smooths a surface f by choosing nodal function values and gradients to         minimize the linearized curvature of F subject to a bound on the         deviation from the data values. This is more appropriate than interpolation         when significant errors are present in the data.          Parameters         ----------          f : array of floats, shape (n,)             field to apply smoothing on          w : array of floats, shape (n,)             weights associated with data value in f             w[i] = 1/sigma_f^2 is a good rule of thumb.          sm : float             positive parameter specifying an upper bound on Q2(f).             generally n-sqrt(2n) <= sm <= n+sqrt(2n)          smtol : float             specifies relative error in satisfying the constraint             sm(1-smtol) <= Q2 <= sm(1+smtol) between 0 and 1.          gstol : float             tolerance for convergence.             gstol = 0.05*mean(sigma_f)^2 is a good rule of thumb.          Returns         -------          f_smooth : array of floats, shape (n,)             smoothed version of f          (dfdx, dfdy, dfdz) : tuple of floats, tuple of 3 shape (n,) arrays             first derivatives of f_smooth in the x, y, and z directions
Ensure lons and lats are:          - 1D numpy arrays          - equal size          - within the appropriate range in radians
Base class to handle nearest neighbour, linear, and cubic interpolation.         Given a triangulation of a set of nodes on the unit sphere, along with data         values at the nodes, this method interpolates (or extrapolates) the value         at a given longitude and latitude.          Parameters         ----------          lons : float / array of floats, shape (l,)                 longitudinal coordinate(s) on the sphere          lats : float / array of floats, shape (l,)                 latitudinal coordinate(s) on the sphere          zdata : array of floats, shape (n,)                 value at each point in the triangulation                 must be the same size of the mesh          order : int (default=1)                 order of the interpolatory function used                  0 = nearest-neighbour                  1 = linear                  3 = cubic          Returns         -------          zi : float / array of floats, shape (l,)             interpolated value(s) at (lons, lats)          err : int / array of ints, shape (l,)             whether interpolation (0), extrapolation (1) or error (other)
Interpolate using nearest-neighbour approximation         Returns the same as interpolate(lons,lats,data,order=0)
Interpolate using linear approximation         Returns the same as interpolate(lons,lats,data,order=1)
Interpolate using cubic spline approximation         Returns the same as interpolate(lons,lats,data,order=3)
Locate the index of the nearest vertex to points (lons,lats)         and return the squared great circle distance between (lons,lats) and         each nearest neighbour.          Parameters         ----------          lons : float / array of floats, shape (l,)             longitudinal coordinate(s) on the sphere          lats : float / array of floats, shape (l,)             latitudinal coordinate(s) on the sphere          Returns         -------          index : array of ints             the nearest vertex to each of the supplied points          dist : array of floats             great circle distance (angle) on the unit sphere to the closest             vertex identified.           Notes         -----          Faster searches can be obtained using a k-d tree.          See sTriangulation.nearest_vertices() for details.          There is an additional overhead associated with building and storing the k-d tree.
Returns indices of the triangles containing lons / lats.          Parameters         ----------          lons : float / array of floats, shape (l,)             longitudinal coordinate(s) on the sphere          lats : float / array of floats, shape (l,)             latitudinal coordinate(s) on the sphere          Returns         -------          tri_indices : array of ints, shape (l,)           Notes         -----           The simplices are found as spherical.sTriangulation.simplices[tri_indices]
Returns the simplices containing (lons,lats)         and the local barycentric, normalised coordinates.          Parameters         ----------          lons : 1D array of longitudinal coordinates in radians          lats : 1D array of latitudinal coordinates in radians          Returns         -------          bcc  : normalised barycentric coordinates          tri  : simplicies containing (lons,lats)          Notes         -----          That the ordering of the vertices may differ from          that stored in the self.simplices array but will          still be a loop around the simplex.
Find the neighbour-vertices in the triangulation for the given vertex         (from the data structures of the triangulation)
Find all the segments in the triangulation and return an         array of vertices (n1,n2) where n1 < n2
Add midpoints to any segment connected to the vertices in the         list / array provided.
Identify the centroid of every simplex in the triangulation. If an array of         simplices is given then the centroids of only those simplices is returned.
Identify the midpoints of every line segment in the triangulation.         If an array of segments of shape (no_of_segments,2) is given,         then the midpoints of only those segments is returned. Note,         segments in the array must not be duplicates or the re-triangulation         will fail. Take care not to miss that (n1,n2) is equivalent to (n2,n1).
Identify the trisection points of every line segment in the triangulation
Calculate the area enclosed by 3 points on the unit sphere.          Parameters         ----------          lons : array of floats, shape (3)             longitudinal coordinates in radians          lats : array of floats, shape (3)             latitudinal coordinates in radians          Returns         -------          area : float             area of triangle on the unit sphere
Compute the area each triangle within the triangulation of points         on the unit sphere.          Returns         -------          area : array of floats, shape (nt,)             area of each triangle in self.simplices where nt             is the number of triangles.          Notes         -----          This uses a Fortran 90 subroutine that wraps the AREA function          to iterate over many points.
Compute the edge-lengths of each triangle in the triangulation.
Compute the angles between lon / lat points p1 and p2 given in radians.         On the unit sphere, this also corresponds to the great circle distance.         p1 and p2 can be numpy arrays of the same length.          This method simply calls the module-level function of the same name.         Consider using the module function instead, as this method may be         deprecated in favor of that function. For now, this method is         retained to avoid issues with the Jupyter notebooks.
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation connected to any of the vertices in the list provided
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation that are associated with the triangles in the list         of indices provided.          Notes         -----          The triangles are here represented as a single index.          The vertices of triangle i are given by self.simplices[i].
return points defining a refined triangulation obtained by bisection of all edges         in the triangulation that are associated with the triangles in the list provided.          Notes         -----          The triangles are here represented as a single index.          The vertices of triangle i are given by self.simplices[i].
Join this triangulation with another. If the points are known to have no duplicates, then         set unique=True to skip the testing and duplicate removal
Query the cKDtree for the nearest neighbours and Euclidean         distance from x,y points.          Returns 0, 0 if a cKDtree has not been constructed         (switch tree=True if you need this routine)          Parameters         ----------          lon : 1D array of longitudinal coordinates in radians          lat : 1D array of latitudinal coordinates in radians          k   : number of nearest neighbours to return              (default: 1)          max_distance              : maximum Euclidean distance to search                for neighbours (default: 2.0)          Returns         -------          d    : Euclidean distance between each point and their                 nearest neighbour(s)          vert : vertices of the nearest neighbour(s)
Fetch the request sent for the batch. Note should only used for query batches
Abort a given bulk job
Gets result ids and generates each result set from the batch and returns it         as an generator fetching the next result set when needed          Args:             batch_id: id of batch             job_id: id of job, if not provided, it will be looked up
Get matchlist for ranked games played on given account ID and platform ID         and filtered using given filter parameters, if any          A number of optional parameters are provided for filtering. It is up to the caller to         ensure that the combination of filter parameters provided is valid for the requested         account, otherwise, no matches may be returned.          Note that if either beginIndex or endIndex are specified, then both must be specified and         endIndex must be greater than beginIndex.          If endTime is specified, but not beginTime, then beginTime is effectively the start of the         account's match history.          If beginTime is specified, but not endTime, then endTime is effectively the current time.          Note that endTime should be greater than beginTime if both are specified, although there is         no maximum limit on their range.          :param string region:               The region to execute this request on         :param string encrypted_account_id: The account ID.         :param Set[int] queue:              Set of queue IDs for which to filtering matchlist.         :param long begin_time:             The begin time to use for filtering matchlist specified as                                             epoch milliseconds.         :param long end_time:               The end time to use for filtering matchlist specified as epoch                                             milliseconds.         :param int begin_index:             The begin index to use for filtering matchlist.         :param int end_index:               The end index to use for filtering matchlist.         :param Set[int] season:             Set of season IDs for which to filtering matchlist.         :param Set[int] champion:           Set of champion IDs for which to filtering matchlist.          :returns: MatchlistDto
Get match timeline by match ID.          Not all matches have timeline data.          :param string region: The region to execute this request on         :param long match_id: The match ID.          :returns: MatchTimelineDto
called before a request is processed.          :param string region: the region of this request         :param string endpoint_name: the name of the endpoint being requested         :param string method_name: the name of the method being requested         :param url: the URL that is being requested.         :param query_params: dict: the parameters to the url that is being queried,                                    e.g. ?key1=val&key2=val2
Called after a response is received and before it is returned to the user.          :param string region: the region of this request         :param string endpoint_name: the name of the endpoint that was requested         :param string method_name: the name of the method that was requested         :param url: The url that was requested         :param response: the response received. This is a response from the Requests library
Get League of Legends status for the given shard.          Requests to this API are not counted against the application Rate Limits.          :param string region: the region to execute this request on          :returns: ShardStatus
called before a request is processed.          :param string endpoint_name: the name of the endpoint being requested         :param string method_name: the name of the method being requested         :param url: the URL that is being requested.         :param query_params: dict: the parameters to the url that is being queried,                                    e.g. ?key1=val&key2=val2
Get a champion mastery by player ID and champion ID.          :param string                           region: the region to execute this request on         :param string encrypted_summoner_id:    Summoner ID associated with the player         :param long champion_id:                Champion ID to retrieve Champion Mastery for          :returns: ChampionMasteryDTO: This object contains single Champion Mastery information for                                       player and champion combination.
Get a summoner by account ID.          :param string region:               The region to execute this request on         :param string encrypted_account_id: The account ID.          :returns: SummonerDTO: represents a summoner
Get a summoner by summoner name          :param string region:           The region to execute this request on         :param string summoner_name:    Summoner Name          :returns: SummonerDTO: represents a summoner
Get a summoner by PUUID.          :param string region:           The region to execute this request on         :param string encrypted_puuid:  PUUID          :returns: SummonerDTO: represents a summoner
Get a summoner by summoner ID.          :param string region:                   The region to execute this request on         :param string encrypted_summoner_id:    Summoner ID          :returns: SummonerDTO: represents a summoner
Get list of featured games.          :param string region: The region to execute this request on          :returns: FeaturedGames
FOR KR SUMMONERS, A 404 WILL ALWAYS BE RETURNED.          Valid codes must be no longer than 256 characters and only use         valid characters: 0-9, a-z, A-Z, and -          :param string region:                   the region to execute this request on         :param string encrypted_summoner_id:    Summoner ID          :returns: string
Sends a request through the BaseApi instance provided, injecting the provided endpoint_name         into the method call, so the caller doesn't have to.          :param string method_name:  The name of the calling method         :param string region:       The region to execute this request on         :param string url:          The full URL to the method being requested.         :param dict query_params:   Query parameters to be provided in the HTTP request
Get the challenger league for a given queue.          :param string region:   the region to execute this request on         :param string queue:    the queue to get the challenger players for          :returns: LeagueListDTO
Get the master league for a given queue.          :param string region:   the region to execute this request on         :param string queue:    the queue to get the master players for          :returns: LeagueListDTO
Get league with given ID, including inactive entries          :param string region:       the region to execute this request on         :param string league_id:    the league ID to query          :returns: LeagueListDTO
Get all the league entries          :param string region:   the region to execute this request on         :param string queue:    the queue to query, i.e. RANKED_SOLO_5x5         :param string tier:     the tier to query, i.e. DIAMOND         :param string division: the division to query, i.e. III          :returns: Set[LeagueEntryDTO]
Returns champion rotations, including free-to-play and low-level free-to-play rotations.          :returns: ChampionInfo
Get the response Body          :returns Body: A Body object containing the response.
Cancel any request.
Read the last-modified header as a datetime, if present.
Write the contents of the body to the optionally provided file and         providing progress to the optional callback. The callback will be         invoked 3 different ways:          * First as ``callback(start=self)``         * For each chunk of data written as           ``callback(wrote=chunk_size_in_bytes, total=all_byte_cnt)``         * Upon completion as ``callback(finish=self)``          :param file: file name or file-like object         :param callback: optional progress callback
Get an iterator of pages.          :param int pages: optional limit to number of pages         :return: iter of this and subsequent pages
Encode the results of this paged response as JSON writing to the         provided file-like `out` object. This function will iteratively read         as many pages as present, streaming the contents out as JSON.          :param file-like out: an object with a `write` function         :param int limit: optional maximum number of items to write         :param bool sort_keys: if True, output keys sorted, default is False         :param bool indent: if True, indent output, default is False
Get an iterator of the 'items' in each page. Instead of a feature         collection from each page, the iterator yields the features.          :param int limit: The number of 'items' to limit to.         :return: iter of items in page
Create a Downloader with the provided client.      :param mosaic bool: If True, the Downloader will fetch mosaic quads.     :returns: :py:Class:`planet.api.downloader.Downloader`
Execute a quick search.
Create a saved search
Execute a saved search
List searches
Activate and download
Get quad IDs and information for a mosaic
Get information for a specific mosaic
Get information for a specific mosaic quad
Get contributing scenes for a mosaic quad
Download quads from a mosaic
configure logging via verbosity level of between 0 and 2 corresponding     to log levels warning, info and debug respectfully.
Planet API Client
Get command help
Login using email/password
try to find a geometry in the provided JSON object
check the status of the response and if needed raise an APIException
Derive a filename from the given response.      >>> import requests     >>> from planet.api import utils     >>> response = requests.Response()     >>> response.headers = {     ...     'date': 'Thu, 14 Feb 2019 16:13:26 GMT',     ...     'last-modified': 'Wed, 22 Nov 2017 17:22:31 GMT',     ...     'accept-ranges': 'bytes',     ...     'content-type': 'image/tiff',     ...     'content-length': '57350256',     ...     'content-disposition': 'attachment; filename="open_california.tif"'     ... }     >>> response.url = 'https://planet.com/path/to/example.tif?foo=f6f1'     >>> print(utils.get_filename(response))     open_california.tif     >>> del response     >>> response = requests.Response()     >>> response.headers = {     ...     'date': 'Thu, 14 Feb 2019 16:13:26 GMT',     ...     'last-modified': 'Wed, 22 Nov 2017 17:22:31 GMT',     ...     'accept-ranges': 'bytes',     ...     'content-type': 'image/tiff',     ...     'content-length': '57350256'     ... }     >>> response.url = 'https://planet.com/path/to/example.tif?foo=f6f1'     >>> print(utils.get_filename(response))     example.tif     >>> del response     >>> response = requests.Response()     >>> response.headers = {     ...     'date': 'Thu, 14 Feb 2019 16:13:26 GMT',     ...     'last-modified': 'Wed, 22 Nov 2017 17:22:31 GMT',     ...     'accept-ranges': 'bytes',     ...     'content-type': 'image/tiff',     ...     'content-length': '57350256'     ... }     >>> response.url = 'https://planet.com/path/to/oops/'     >>> print(utils.get_filename(response)) #doctest:+SKIP     planet-bFL6pwki.tif     >>>      :param response: An HTTP response.     :type response: :py:class:`requests.Response`     :returns: a filename (i.e. ``basename``)     :rtype: str
Get a filename from the Content-Disposition header, if available.      >>> from planet.api import utils     >>> headers = {     ...     'date': 'Thu, 14 Feb 2019 16:13:26 GMT',     ...     'last-modified': 'Wed, 22 Nov 2017 17:22:31 GMT',     ...     'accept-ranges': 'bytes',     ...     'content-type': 'image/tiff',     ...     'content-length': '57350256',     ...     'content-disposition': 'attachment; filename="open_california.tif"'     ... }     >>> name = utils.get_filename_from_headers(headers)     >>> print(name)     open_california.tif     >>>     >>> headers.pop('content-disposition', None)     'attachment; filename="open_california.tif"'     >>> name = utils.get_filename_from_headers(headers)     >>> print(name)     None     >>>      :param headers dict: a ``dict`` of response headers     :returns: a filename (i.e. ``basename``)     :rtype: str or None
Get a filename from a URL.      >>> from planet.api import utils     >>> urls = [     ...     'https://planet.com/',     ...     'https://planet.com/path/to/',     ...     'https://planet.com/path/to/example.tif',     ...     'https://planet.com/path/to/example.tif?foo=f6f1&bar=baz',     ...     'https://planet.com/path/to/example.tif?foo=f6f1&bar=baz#quux'     ... ]     >>> for url in urls:     ...     print('{} -> {}'.format(url, utils.get_filename_from_url(url)))     ...     https://planet.com/ -> None     https://planet.com/path/to/ -> None     https://planet.com/path/to/example.tif -> example.tif     https://planet.com/path/to/example.tif?foo=f6f1&bar=baz -> example.tif     https://planet.com/path/to/example.tif?foo=f6f1&bar=baz#quux -> example.tif     >>>      :returns: a filename (i.e. ``basename``)     :rtype: str or None
Get a pseudo-random, Planet-looking filename.      >>> from planet.api import utils     >>> print(utils.get_random_filename()) #doctest:+SKIP     planet-61FPnh7K     >>> print(utils.get_random_filename('image/tiff')) #doctest:+SKIP     planet-V8ELYxy5.tif     >>>      :returns: a filename (i.e. ``basename``)     :rtype: str
Create a callback handler for asynchronous Body handling.      If provided, the callback will be invoked as described in     :py:meth:`planet.api.models.Body.write`. In addition, if the download     is skipped because the destination exists, the callback will be invoked     with ``callback(skip=body)``.      The name of the file written to will be determined from the Body.name     property.      :param directory str: The optional directory to write to.     :param callback func: An optional callback to receive notification of                           write progress.     :param overwrite bool: Overwrite any existing files. Defaults to True.
Quick check to determine if the provided text looks like WKT
A quick check to see if this input looks like GeoJSON. If not a dict     JSON-like object, attempt to parse input as JSON. If the resulting object     has a type property that looks like GeoJSON, return that object or None
Execute a function f(*a, **kw) listening for KeyboardInterrupt and if     handled, invoke the cancel function. Blocks until f is complete or the     interrupt is handled.
If the request has no filter config, add one that should do what is     expected (include all items)     see: PE-11813
Login using email identity and credentials. Returns a JSON         object containing an `api_key` property with the user's API_KEY.         :param str identity: email         :param str credentials: password         :returns: JSON object (Python dict)
Execute a quick search with the specified request.          :param request: see :ref:`api-search-request`         :param **kw: See Options below         :returns: :py:class:`planet.api.models.Items`         :raises planet.api.exceptions.APIException: On API error.          :Options:          * page_size (int): Size of response pages         * sort (string): Sorting order in the form `field (asc|desc)`
Execute a saved search by search id.          :param sid string: The id of the search         :returns: :py:class:`planet.api.models.Items`         :raises planet.api.exceptions.APIException: On API error.          :Options:          * page_size (int): Size of response pages         * sort (string): Sorting order in the form `field (asc|desc)`
Get searches listing.          :param quick bool: Include quick searches (default False)         :param quick saved: Include saved searches (default True)         :returns: :py:class:`planet.api.models.Searches`         :raises planet.api.exceptions.APIException: On API error.
Get stats for the provided request.          :param request dict: A search request that also contains the 'interval'                              property.         :returns: :py:class:`planet.api.models.JSON`         :raises planet.api.exceptions.APIException: On API error.
Request activation of the specified asset representation.          Asset representations are obtained from :py:meth:`get_assets`.          :param request dict: An asset representation from the API.         :returns: :py:class:`planet.api.models.Body` with no response content         :raises planet.api.exceptions.APIException: On API error.
Download the specified asset. If provided, the callback will be         invoked asynchronously. Otherwise it is up to the caller to handle the         response Body.          :param asset dict: An asset representation from the API         :param callback: An optional function to aysnchronsously handle the                          download. See :py:func:`planet.api.write_to_file`         :returns: :py:Class:`planet.api.models.Response` containing a                   :py:Class:`planet.api.models.Body` of the asset.         :raises planet.api.exceptions.APIException: On API error.
Get the an item response for the given item_type and id          :param item_type str: A valid item-type         :param id str: The id of the item         :returns: :py:Class:`planet.api.models.JSON`         :raises planet.api.exceptions.APIException: On API error.
Get an item's asset response for the given item_type and id          :param item_type str: A valid item-type         :param id str: The id of the item         :returns: :py:Class:`planet.api.models.JSON`         :raises planet.api.exceptions.APIException: On API error.
Get information for all mosaics accessible by the current user.          :returns: :py:Class:`planet.api.models.Mosaics`
Get the API representation of a mosaic by name.          :param name str: The name of the mosaic         :returns: :py:Class:`planet.api.models.Mosaics`         :raises planet.api.exceptions.APIException: On API error.
Search for quads from a mosaic that are inside the specified         bounding box.  Will yield all quads if no bounding box is specified.          :param mosaic dict: A mosaic representation from the API         :param bbox tuple: A lon_min, lat_min, lon_max, lat_max area to search         :returns: :py:Class:`planet.api.models.MosaicQuads`         :raises planet.api.exceptions.APIException: On API error.
Get a quad response for a specific mosaic and quad.          :param mosaic dict: A mosaic representation from the API         :param quad_id str: A quad id (typically <xcoord>-<ycoord>)         :returns: :py:Class:`planet.api.models.JSON`         :raises planet.api.exceptions.APIException: On API error.
Download the specified mosaic quad. If provided, the callback will         be invoked asynchronously.  Otherwise it is up to the caller to handle         the response Body.          :param asset dict: A mosaic quad representation from the API         :param callback: An optional function to aysnchronsously handle the                          download. See :py:func:`planet.api.write_to_file`         :returns: :py:Class:`planet.api.models.Response` containing a                   :py:Class:`planet.api.models.Body` of the asset.         :raises planet.api.exceptions.APIException: On API error.
build an AND filter from the provided opts dict as passed to a command     from the filter_options decorator. Assumes all dict values are lists of     filter dict constructs.
Build a AND filter from the provided kwargs defaulting to an     empty 'and' filter (@todo: API workaround) if nothing is provided.      If the 'filter_json' argument is provided, this will be assumed to contain     a filter specification and will be anded with other filters. If the     'filter_json' is a search, the search filter value will be used.      All kw values should be tuple or list
call the provided function and wrap any API exception with a click     exception. this means no stack trace is visible to the user but instead     a (hopefully) nice message is provided.     note: could be a decorator but didn't play well with click
Wrapper to echo JSON with optional 'pretty' printing. If pretty is not     provided explicity and stdout is a terminal (and not redirected or piped),     the default will be to indent and sort keys
Get the value of an option interpreting as a file implicitly or     explicitly and falling back to the value if not explicitly specified.     If the value is '@name', then a file must exist with name and the returned     value will be the contents of that file. If the value is '@-' or '-', then     stdin will be read and returned as the value. Finally, if a file exists     with the provided value, that file will be read. Otherwise, the value     will be returned.
Build a data-api search request body for the specified item_types.     If 'filter_like' is a request, item_types will be merged and, if name or     interval is provided, will replace any existing values.      :param dict filter_like: a filter or request with a filter     :param sequence(str) item_types: item-types to specify in the request     :param str name: optional name     :param str interval: optional interval [year, month, week, day]
Build a DateRangeFilter.      Predicate arguments accept a value str that in ISO-8601 format or a value     that has a `isoformat` callable that returns an ISO-8601 str.      :raises: ValueError if predicate value does not parse      >>> date_range('acquired', gt='2017') == \     {'config': {'gt': '2017-01-01T00:00:00Z'}, \     'field_name': 'acquired', 'type': 'DateRangeFilter'}     True
Returns the VRF configuration as a resource dict.          Args:             value (string): The vrf name to retrieve from the                 running configuration.          Returns:             A Python dict object containing the VRF attributes as                 key/value pairs.
_parse_rd scans the provided configuration block and extracts         the vrf rd. The return dict is intended to be merged into the response         dict.          Args:             config (str): The vrf configuration block from the nodes running                 configuration          Returns:             dict: resource dict attribute
_parse_description scans the provided configuration block and         extracts the vrf description value. The return dict is intended to         be merged into the response dict.          Args:             config (str): The vrf configuration block from the nodes                 running configuration          Returns:             dict: resource dict attribute
Returns a dict object of all VRFs in the running-config          Returns:             A dict object of VRF attributes
Creates a new VRF resource          Note: A valid RD has the following format admin_ID:local_assignment.             The admin_ID can be an AS number or globally assigned IPv4 address.             The local_assignment can be an integer between 0-65,535 if the             admin_ID is an IPv4 address and can be between 0-4,294,967,295 if             the admin_ID is an AS number. If the admin_ID is an AS number the             local_assignment could also be in the form of an IPv4 address.          Args:             vrf_name (str): The VRF name to create             rd (str): The value to configure the vrf rd          Returns:             True if create was successful otherwise False
Configures the specified VRF using commands          Args:             vrf_name (str): The VRF name to configure             commands: The list of commands to configure          Returns:             True if the commands completed successfully
Configures the VRF rd (route distinguisher)          Note: A valid RD has the following format admin_ID:local_assignment.             The admin_ID can be an AS number or globally assigned IPv4 address.             The local_assignment can be an integer between 0-65,535 if the             admin_ID is an IPv4 address and can be between 0-4,294,967,295 if             the admin_ID is an AS number. If the admin_ID is an AS number the             local_assignment could also be in the form of an IPv4 address.          Args:             vrf_name (str): The VRF name to set rd for             rd (str): The value to configure the vrf rd          Returns:             True if the operation was successful otherwise False
Configures the VRF description          Args:             vrf_name (str): The VRF name to configure             description(str): The string to set the vrf description to             default (bool): Configures the vrf description to its default value             disable (bool): Negates the vrf description          Returns:             True if the operation was successful otherwise False
Configures ipv4 routing for the vrf          Args:             vrf_name (str): The VRF name to configure             default (bool): Configures ipv4 routing for the vrf value to                 default if this value is true             disable (bool): Negates the ipv4 routing for the vrf if set to true          Returns:             True if the operation was successful otherwise False
Adds a VRF to an interface          Notes:             Requires interface to be in routed mode. Must apply ip address             after VRF has been applied. This feature can also be accessed             through the interfaces api.          Args:             vrf_name (str): The VRF name to configure             interface (str): The interface to add the VRF too             default (bool): Set interface VRF forwarding to default             disable (bool): Negate interface VRF forwarding          Returns:             True if the operation was successful otherwise False
Get the vrrp configurations for a single node interface          Args:             name (string): The name of the interface for which vrrp                 configurations will be retrieved.          Returns:             A dictionary containing the vrrp configurations on the interface.             Returns None if no vrrp configurations are defined or             if the interface is not configured.
Get the vrrp configurations for all interfaces on a node          Returns:             A dictionary containing the vrrp configurations on the node,             keyed by interface.
Creates a vrrp instance from an interface          Note:             This method will attempt to create a vrrp in the node's             operational config. If the vrrp already exists on the             interface, then this method will set the properties of             the existing vrrp to those that have been passed in, if             possible.          Args:             interface (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be created.             kwargs (dict): A dictionary specifying the properties to                 be applied to the new vrrp instance. See library                 documentation for available keys and values.          Returns:             True if the vrrp could be created otherwise False (see Node)
Deletes a vrrp instance from an interface          Note:             This method will attempt to delete the vrrp from the node's             operational config. If the vrrp does not exist on the             interface then this method will not perform any changes             but still return True          Args:             interface (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be deleted.          Returns:             True if the vrrp could be deleted otherwise False (see Node)
Defaults a vrrp instance from an interface          Note:             This method will attempt to default the vrrp on the node's             operational config. Default results in the deletion of the             specified vrrp . If the vrrp does not exist on the             interface then this method will not perform any changes             but still return True          Args:             interface (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be defaulted.          Returns:             True if the vrrp could be defaulted otherwise False (see Node)
Set the enable property of the vrrp          Args:             name (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be managed.             value (boolean): True to enable the vrrp, False to disable.             run (boolean): True to execute the command, False to                 return a string with the formatted command.          Returns:             If run is True, returns True if the command executed successfully,             error if failure              If run is False, returns the formatted command string which can             be passed to the node
Set the primary_ip property of the vrrp          Args:             name (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be managed.             value (string): IP address to be set.             disable (boolean): Unset primary ip if True.             default (boolean): Set primary ip to default if True.             run (boolean): Set to True to execute the command, False to                 return a string with the formatted command.          Returns:             If run is True, returns True if the command executed successfully,             error if failure.              If run is False, returns the formatted command string which can             be passed to the node
Set the primary_ip property of the vrrp          Args:             name (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be managed.             value (integer): Priority to assign to the vrrp.             disable (boolean): Unset priority if True.             default (boolean): Set priority to default if True.             run (boolean): Set to True to execute the command, False to                 return a string with the formatted command.          Returns:             If run is True, returns True if the command executed successfully,             error if failure.              If run is False, returns the formatted command string which can             be passed to the node
Configure the secondary_ip property of the vrrp          Notes:             set_secondary_ips takes a list of secondary ip addresses             which are to be set on the virtal router. An empty list will             remove any existing secondary ip addresses from the vrrp.             A list containing addresses will configure the virtual router             with only the addresses specified in the list - any existing             addresses not included in the list will be removed.          Args:             name (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be managed.             secondary_ips (list): A list of secondary ip addresses to                 be assigned to the virtual router.             run (boolean): Set to True to execute the command, False to                 return a string with the formatted command.          Returns:             If run is True, returns True if the command executed successfully,             error if failure.              If run is False, returns the formatted command string which can             be passed to the node
Set the mac_addr_adv_interval property of the vrrp          Args:             name (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be managed.             value (integer): mac-address advertisement-interval value to                 assign to the vrrp.             disable (boolean): Unset mac-address advertisement-interval                 if True.             default (boolean): Set mac-address advertisement-interval to                 default if True.             run (boolean): Set to True to execute the command, False to                 return a string with the formatted command.          Returns:             If run is True, returns True if the command executed successfully,             error if failure.              If run is False, returns the formatted command string which can             be passed to the node
Configure the track property of the vrrp          Notes:             set_tracks takes a list of tracked objects which are             to be set on the virtual router. An empty list will remove             any existing tracked objects from the vrrp. A list containing             track entries configures the virtual router to track only the             objects specified in the list - any existing tracked objects             on the vrrp not included in the list will be removed.          Args:             name (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be managed.             tracks (list): A list of track definition dictionaries. Each                 dictionary is a definition of a tracked object in one                 of the two formats::                      {'name': tracked_object_name,                      'action': 'shutdown'}                     {'name': tracked_object_name,                      'action': 'decrement',                      'amount': amount_of_decrement}              run (boolean): Set to True to execute the command, False to                 return a string with the formatted command.          Returns:             If run is True, returns True if the command executed successfully,             error if failure.              If run is False, returns the formatted command string which can             be passed to the node
Set the bfd_ip property of the vrrp          Args:             name (string): The interface to configure.             vrid (integer): The vrid number for the vrrp to be managed.             value (string): The bfd ip address to be set.             disable (boolean): Unset bfd ip if True.             default (boolean): Set bfd ip to default if True.             run (boolean): Set to True to execute the command, False to                 return a string with the formatted command.          Returns:             If run is True, returns True if the command executed successfully,             error if failure.              If run is False, returns the formatted command string which can             be passed to the node
Formats a vrrp configuration dictionary to match the         information as presented from the get and getall methods.         vrrp configuration dictionaries passed to the create         method may contain data for setting properties which results         in a default value on the node. In these instances, the data         for setting or changing the property is replaced with the         value that would be returned from the get and getall methods.          Intended for validating updated vrrp configurations.
Converts a prefix length to a dotted decimal subnet mask      Args:         prefixlen (str): The prefix length value to convert      Returns:         str: The subt mask as a dotted decimal string
Returns all ACLs in a dict object.          Returns:             A Python dictionary object containing all ACL             configuration indexed by ACL name::                  {                     "<ACL1 name>": {...},                     "<ACL2 name>": {...}                 }
Returns the Mlag configuration as a resource dict          Returns:             dict: A dict ojbect containing the Mlag resource attributes.
Parses the mlag global configuration          Returns:             dict: A dict object that is intended to be merged into the                 resource dict
Scans the config block and parses the domain-id value          Args:             config (str): The config block to scan          Returns:             dict: A dict object that is intended to be merged into the                 resource dict
Scans the config block and parses the local-interface value          Args:             config (str): The config block to scan          Returns:             dict: A dict object that is intended to be merged into the                 resource dict
Scans the config block and parses the peer-address value          Args:             config (str): The config block to scan          Returns:             dict: A dict object that is intended to be merged into the                 resource dict
Scans the config block and parses the peer-link value          Args:             config (str): The config block to scan          Returns:             dict: A dict object that is intended to be merged into the                 resource dict
Scans the global config and returns the configured interfaces          Returns:             dict: A dict object that is intended to be merged into the                 resource dict.
Configures the mlag domain-id value          Args:             value (str): The value to configure the domain-id             default (bool): Configures the domain-id using the default keyword             disable (bool): Negates the domain-id using the no keyword          Returns:             bool: Returns True if the commands complete successfully
Configures the mlag local-interface value          Args:             value (str): The value to configure the local-interface             default (bool): Configures the local-interface using the                 default keyword             disable (bool): Negates the local-interface using the no keyword          Returns:             bool: Returns True if the commands complete successfully
Configures the mlag peer-address value          Args:             value (str): The value to configure the peer-address             default (bool): Configures the peer-address using the                 default keyword             disable (bool): Negates the peer-address using the no keyword          Returns:             bool: Returns True if the commands complete successfully
Configures the mlag peer-link value          Args:             value (str): The value to configure the peer-link             default (bool): Configures the peer-link using the                 default keyword             disable (bool): Negates the peer-link using the no keyword          Returns:             bool: Returns True if the commands complete successfully
Configures the mlag shutdown value          Default setting for set_shutdown is disable=True, meaning         'no shutdown'. Setting both default and disable to False will         effectively enable shutdown.          Args:             default (bool): Configures the shutdown using the                 default keyword             disable (bool): Negates shutdown using the no keyword          Returns:             bool: Returns True if the commands complete successfully
Configures the interface mlag value for the specified interface          Args:             name (str): The interface to configure.  Valid values for the                 name arg include Port-Channel*             value (str): The mlag identifier to cofigure on the interface             default (bool): Configures the interface mlag value using the                 default keyword             disable (bool): Negates the interface mlag value using the                 no keyword          Returns:             bool: Returns True if the commands complete successfully
Returns the bgp routing configuration as a dict object
Returns the OSPF routing configuration             Args:                 vrf (str): VRF name to return OSPF routing config for            Returns:                dict:                     keys: router_id (int): OSPF router-id                           vrf (str): VRF of the OSPF process                           networks (dict): All networks that                                            are advertised in OSPF                           ospf_process_id (int): OSPF proc id                           redistribution (dict): All protocols that                                                  are configured to be                                                  redistributed in OSPF                           shutdown (bool): Gives the current shutdown                                            off the process
Parses config file for the OSPF proc ID             Args:                config(str):  Running configuration            Returns:                dict: key: ospf_process_id (int)
Parses config file for the OSPF vrf name             Args:                config(str):  Running configuration            Returns:                dict: key: ospf_vrf (str)
Parses config file for the networks advertised            by the OSPF process             Args:                config(str):  Running configuration            Returns:                list: dict:                          keys: network (str)                                netmask (str)                                area (str)
Parses config file for the OSPF router ID             Args:                config (str):  Running configuration            Returns:                list: dict:                          keys: protocol (str)                                route-map (optional) (str)
Removes the entire ospf process from the running configuration             Args:                None            Returns:                bool: True if the command completed succssfully
Creates a OSPF process in the specified VRF or the default VRF.             Args:                 ospf_process_id (str): The OSPF process Id value                 vrf (str): The VRF to apply this OSPF process to            Returns:                 bool: True if the command completed successfully            Exception:                 ValueError: If the ospf_process_id passed in less                             than 0 or greater than 65536
Allows for a list of OSPF subcommands to be configured"             Args:                cmd: (list or str): Subcommand to be entered            Returns:                bool: True if all the commands completed successfully
Controls the router id property for the OSPF Proccess             Args:                value (str): The router-id value                default (bool): Controls the use of the default keyword                disable (bool): Controls the use of the no keyword            Returns:                bool: True if the commands are completed successfully
Adds a network to be advertised by OSPF             Args:                network (str):  The network to be advertised in dotted decimal                                notation                netmask (str):  The netmask to configure                area (str):  The area the network belongs to.                             By default this value is 0            Returns:                bool: True if the command completes successfully            Exception:                ValueError: This will get raised if network or netmask                            are not passed to the method
Adds a protocol redistribution to OSPF             Args:                protocol (str):  protocol to redistribute                route_map_name (str): route-map to be used to                                      filter the protocols            Returns:                bool: True if the command completes successfully            Exception:                ValueError:  This will be raised if the protocol pass is not one                             of the following: [rip, bgp, static, connected]
Removes a protocol redistribution to OSPF             Args:                protocol (str):  protocol to redistribute                route_map_name (str): route-map to be used to                                      filter the protocols            Returns:                bool: True if the command completes successfully            Exception:                ValueError:  This will be raised if the protocol pass is not one                             of the following: [rip, bgp, static, connected]
Return all ip routes configured on the switch as a resource dict          Returns:             dict: An dict object of static route entries in the form::                  { ip_dest:                     { next_hop:                         { next_hop_ip:                             { distance:                                 { 'tag': tag,                                   'route_name': route_name                                 }                             }                         }                     }                 }              If the ip address specified does not have any associated             static routes, then None is returned.          Notes:             The keys ip_dest, next_hop, next_hop_ip, and distance in             the returned dictionary are the values of those components             of the ip route specification. If a route does not contain             a next_hop_ip, then that key value will be set as 'None'.
Create a static route          Args:             ip_dest (string): The ip address of the destination in the                 form of A.B.C.D/E             next_hop (string): The next hop interface or ip address             **kwargs['next_hop_ip'] (string): The next hop address on                 destination interface             **kwargs['distance'] (string): Administrative distance for this                 route             **kwargs['tag'] (string): Route tag             **kwargs['route_name'] (string): Route name          Returns:             True if the operation succeeds, otherwise False.
Delete a static route          Args:             ip_dest (string): The ip address of the destination in the                 form of A.B.C.D/E             next_hop (string): The next hop interface or ip address             **kwargs['next_hop_ip'] (string): The next hop address on                 destination interface             **kwargs['distance'] (string): Administrative distance for this                 route             **kwargs['tag'] (string): Route tag             **kwargs['route_name'] (string): Route name          Returns:             True if the operation succeeds, otherwise False.
Set a static route to default (i.e. delete the matching route)          Args:             ip_dest (string): The ip address of the destination in the                 form of A.B.C.D/E             next_hop (string): The next hop interface or ip address             **kwargs['next_hop_ip'] (string): The next hop address on                 destination interface             **kwargs['distance'] (string): Administrative distance for this                 route             **kwargs['tag'] (string): Route tag             **kwargs['route_name'] (string): Route name          Returns:             True if the operation succeeds, otherwise False.
Set the tag value for the specified route          Args:             ip_dest (string): The ip address of the destination in the                 form of A.B.C.D/E             next_hop (string): The next hop interface or ip address             **kwargs['next_hop_ip'] (string): The next hop address on                 destination interface             **kwargs['distance'] (string): Administrative distance for this                 route             **kwargs['tag'] (string): Route tag             **kwargs['route_name'] (string): Route name          Returns:             True if the operation succeeds, otherwise False.          Notes:             Any existing route_name value must be included in call to                 set_tag, otherwise the tag will be reset                 by the call to EOS.
Set the route_name value for the specified route          Args:             ip_dest (string): The ip address of the destination in the                 form of A.B.C.D/E             next_hop (string): The next hop interface or ip address             **kwargs['next_hop_ip'] (string): The next hop address on                 destination interface             **kwargs['distance'] (string): Administrative distance for this                 route             **kwargs['tag'] (string): Route tag             **kwargs['route_name'] (string): Route name          Returns:             True if the operation succeeds, otherwise False.          Notes:             Any existing tag value must be included in call to                 set_route_name, otherwise the tag will be reset                 by the call to EOS.
Build the EOS command string for ip route interactions.          Args:             ip_dest (string): The ip address of the destination in the                 form of A.B.C.D/E             next_hop (string): The next hop interface or ip address             **kwargs['next_hop_ip'] (string): The next hop address on                 destination interface             **kwargs['distance'] (string): Administrative distance for this                 route             **kwargs['tag'] (string): Route tag             **kwargs['route_name'] (string): Route name          Returns the ip route command string to be sent to the switch for         the given set of parameters.
Configure a static route          Args:             ip_dest (string): The ip address of the destination in the                 form of A.B.C.D/E             next_hop (string): The next hop interface or ip address             **kwargs['next_hop_ip'] (string): The next hop address on                 destination interface             **kwargs['distance'] (string): Administrative distance for this                 route             **kwargs['tag'] (string): Route tag             **kwargs['route_name'] (string): Route name             **kwargs['delete'] (boolean): If true, deletes the specified route                 instead of creating or setting values for the route             **kwargs['default'] (boolean): If true, defaults the specified                 route instead of creating or setting values for the route          Returns:             True if the operation succeeds, otherwise False.
Returns the VLAN configuration as a resource dict.          Args:             vid (string): The vlan identifier to retrieve from the                 running configuration.  Valid values are in the range                 of 1 to 4095          Returns:             A Python dict object containing the VLAN attributes as                 key/value pairs.
_parse_name scans the provided configuration block and extracts         the vlan name.  The config block is expected to always return the         vlan name.  The return dict is intended to be merged into the response         dict.          Args:             config (str): The vlan configuration block from the nodes running                 configuration          Returns:             dict: resource dict attribute
_parse_state scans the provided configuration block and extracts         the vlan state value.  The config block is expected to always return         the vlan state config.  The return dict is inteded to be merged into         the response dict.          Args:             config (str): The vlan configuration block from the nodes                 running configuration          Returns:             dict: resource dict attribute
_parse_trunk_groups scans the provided configuration block and         extracts all the vlan trunk groups.  If no trunk groups are configured         an empty List is returned as the vlaue.  The return dict is intended         to be merged into the response dict.          Args:             config (str): The vlan configuration block form the node's                 running configuration          Returns:             dict: resource dict attribute
Returns a dict object of all Vlans in the running-config          Returns:             A dict object of Vlan attributes
Creates a new VLAN resource          Args:             vid (str): The VLAN ID to create          Returns:             True if create was successful otherwise False
Deletes a VLAN from the running configuration          Args:             vid (str): The VLAN ID to delete          Returns:             True if the operation was successful otherwise False
Defaults the VLAN configuration          .. code-block:: none              default vlan <vlanid>          Args:             vid (str): The VLAN ID to default          Returns:             True if the operation was successful otherwise False
Configures the specified Vlan using commands          Args:             vid (str): The VLAN ID to configure             commands: The list of commands to configure          Returns:             True if the commands completed successfully
Configures the VLAN name          EosVersion:             4.13.7M          Args:             vid (str): The VLAN ID to Configures             name (str): The value to configure the vlan name             default (bool): Defaults the VLAN ID name             disable (bool): Negates the VLAN ID name          Returns:             True if the operation was successful otherwise False
Configures the VLAN state          EosVersion:             4.13.7M          Args:             vid (str): The VLAN ID to configure             value (str): The value to set the vlan state to             default (bool): Configures the vlan state to its default value             disable (bool): Negates the vlan state          Returns:             True if the operation was successful otherwise False
Configures the list of trunk groups support on a vlan          This method handles configuring the vlan trunk group value to default         if the default flag is set to True.  If the default flag is set         to False, then this method will calculate the set of trunk         group names to be added and to be removed.          EosVersion:             4.13.7M          Args:             vid (str): The VLAN ID to configure             value (str): The list of trunk groups that should be configured                 for this vlan id.             default (bool): Configures the trunk group value to default if                 this value is true             disable (bool): Negates the trunk group value if set to true          Returns:             True if the operation was successful otherwise False
Configures the user authentication for eAPI          This method configures the username and password combination to use         for authenticating to eAPI.          Args:             username (str): The username to use to authenticate the eAPI                 connection with             password (str): The password in clear text to use to authenticate                 the eAPI connection with
Generates an eAPI request object          This method will take a list of EOS commands and generate a valid         eAPI request object form them.  The eAPI request object is then         JSON encoding and returned to the caller.          eAPI Request Object          .. code-block:: json              {                 "jsonrpc": "2.0",                 "method": "runCmds",                 "params": {                     "version": 1,                     "cmds": [                         <commands>                     ],                     "format": [json, text],                 }                 "id": <reqid>             }          Args:             commands (list): A list of commands to include in the eAPI                 request object             encoding (string): The encoding method passed as the `format`                 parameter in the eAPI request             reqid (string): A custom value to assign to the request ID                 field.  This value is automatically generated if not passed             **kwargs: Additional keyword arguments for expanded eAPI                 functionality. Only supported eAPI params are used in building                 the request          Returns:             A JSON encoding request structure that can be send over eAPI
Sends the eAPI request to the destination node          This method is responsible for sending an eAPI request to the         destination node and returning a response based on the eAPI response         object.  eAPI responds to request messages with either a success         message or failure message.          eAPI Response - success          .. code-block:: json              {                 "jsonrpc": "2.0",                 "result": [                     {},                     {}                     {                         "warnings": [                             <message>                         ]                     },                 ],                 "id": <reqid>             }          eAPI Response - failure          .. code-block:: json              {                 "jsonrpc": "2.0",                 "error": {                     "code": <int>,                     "message": <string>                     "data": [                         {},                         {},                         {                             "errors": [                                 <message>                             ]                         }                     ]                 }                 "id": <reqid>             }          Args:             data (string): The data to be included in the body of the eAPI                 request object          Returns:             A decoded response.  The response object is deserialized from                 JSON and returned as a standard Python dictionary object          Raises:             CommandError if an eAPI failure response object is returned from                 the node.   The CommandError exception includes the error                 code and error message from the eAPI response.
Parses the eAPI failure response message          This method accepts an eAPI failure message and parses the necesary         parts in order to generate a CommandError.          Args:             message (str): The error message to parse          Returns:             tuple: A tuple that consists of the following:                 * code: The error code specified in the failure message                 * message: The error text specified in the failure message                 * error: The error text from the command that generated the                     error (the last command that ran)                 * output: A list of all output from all commands
Executes the list of commands on the destination node          This method takes a list of commands and sends them to the         destination node, returning the results.  The execute method handles         putting the destination node in enable mode and will pass the         enable password, if required.          Args:             commands (list): A list of commands to execute on the remote node             encoding (string): The encoding to send along with the request                 message to the destination node.  Valid values include 'json'                 or 'text'.  This argument will influence the response object                 encoding             **kwargs: Arbitrary keyword arguments          Returns:             A decoded response message as a native Python dictionary object             that has been deserialized from JSON.          Raises:             CommandError:  A CommandError is raised that includes the error                 code, error message along with the list of commands that were                 sent to the node.  The exception instance is also stored in                 the error property and is availble until the next request is                 sent
Returns the current VARP configuration          The Varp resource returns the following:              * mac_address (str): The virtual-router mac address             * interfaces (dict): A list of the interfaces that have a                                  virtual-router address configured.          Return:             A Python dictionary object of key/value pairs that represents             the current configuration of the node.  If the specified             interface does not exist then None is returned::                  {                     "mac_address": "aa:bb:cc:dd:ee:ff",                     "interfaces": {                         "Vlan100": {                             "addresses": [ "1.1.1.1", "2.2.2.2"]                         },                         "Vlan200": [...]                     }                 }
Sets the virtual-router mac address          This method will set the switch virtual-router mac address. If a         virtual-router mac address already exists it will be overwritten.          Args:             mac_address (string): The mac address that will be assigned as                 the virtual-router mac address. This should be in the format,                 aa:bb:cc:dd:ee:ff.             default (bool): Sets the virtual-router mac address to the system                 default (which is to remove the configuration line).             disable (bool): Negates the virtual-router mac address using                 the system no configuration command          Returns:             True if the set operation succeeds otherwise False.
Scans the config and returns a block of code          Args:             parent (str): The parent string to search the config for and                 return the block             config (str): A text config string to be searched. Default                 is to search the running-config of the Node.          Returns:             A string object that represents the block from the config.  If             the parent string is not found, then this method will             return None.
Sends the commands list to the node in config mode          This method performs configuration the node using the array of         commands specified.   This method wraps the configuration commands         in a try/except block and stores any exceptions in the error         property.          Note:             If the return from this method is False, use the error property             to investigate the exception          Args:             commands (list): A list of commands to be sent to the node in                 config mode          Returns:             True if the commands are executed without exception otherwise                 False is returned
Builds a command with keywords          Notes:             Negating a command string by overriding 'value' with None or an                 assigned value that evalutates to false has been deprecated.                 Please use 'disable' to negate a command.              Parameters are evaluated in the order 'default', 'disable', 'value'          Args:             string (str): The command string             value (str): The configuration setting to subsititue into the                 command string. If value is a boolean and True, just the                 command string is used             default (bool): Specifies the command should use the default                 keyword argument. Default preempts disable and value.             disable (bool): Specifies the command should use the no                 keyword argument. Disable preempts value.          Returns:             A command string that can be used to configure the node
Configures the specified interface with the commands          Args:             name (str): The interface name to configure             commands: The commands to configure in the interface          Returns:             True if the commands completed successfully
Returns the current NTP configuration          The Ntp resource returns the following:              * source_interface (str): The interface port that specifies                                       NTP server             * servers (list): A list of the NTP servers that have been                               assigned to the node. Each entry in the                               list is a key/value pair of the name of                               the server as the key and None or 'prefer'                               as the value if the server is preferred.          Returns:             A Python dictionary object of key/value pairs that represents             the current NTP configuration of the node::                  {                     "source_interface": 'Loopback0',                     'servers': [                         { '1.1.1.1': None },                         { '1.1.1.2': 'prefer' },                         { '1.1.1.3': 'prefer' },                         { '1.1.1.4': None },                     ]                 }
Delete the NTP source entry from the node.          Returns:             True if the operation succeeds, otherwise False.
Default the NTP source entry from the node.          Returns:             True if the operation succeeds, otherwise False.
Assign the NTP source on the node          Args:             name (string): The interface port that specifies the NTP source.          Returns:             True if the operation succeeds, otherwise False.
Add or update an NTP server entry to the node config          Args:             name (string): The IP address or FQDN of the NTP server.             prefer (bool): Sets the NTP server entry as preferred if True.          Returns:             True if the operation succeeds, otherwise False.
Remove an NTP server entry from the node config          Args:             name (string): The IP address or FQDN of the NTP server.          Returns:             True if the operation succeeds, otherwise False.
Remove all NTP server entries from the node config          Returns:             True if the operation succeeds, otherwise False.
Returns the system configuration abstraction          The System resource returns the following:              * hostname (str): The hostname value          Returns:             dict: Represents the node's system configuration
Parses the global config and returns the hostname value          Returns:             dict: The configured value for hostname.  The returned dict                 object is intended to be merged into the resource dict
Parses the global config and returns the value for both motd             and login banners.          Returns:            dict: The configure value for modtd and login banners. If the                   banner is not set it will return a value of None for that                   key. The returned dict object is intendd to be merged                   into the resource dict
Configures the global system hostname setting          EosVersion:             4.13.7M          Args:             value (str): The hostname value             default (bool): Controls use of the default keyword             disable (bool): Controls the use of the no keyword          Returns:             bool: True if the commands are completed successfully
Configures the state of global ip routing          EosVersion:             4.13.7M          Args:             value(bool): True if ip routing should be enabled or False if                 ip routing should be disabled             default (bool): Controls the use of the default keyword             disable (bool): Controls the use of the no keyword          Returns:             bool: True if the commands completed successfully otherwise False
Configures system banners          Args:             banner_type(str): banner to be changed (likely login or motd)             value(str): value to set for the banner             default (bool): Controls the use of the default keyword             disable (bool): Controls the use of the no keyword`          Returns:             bool: True if the commands completed successfully otherwise False
Returns the specific IP interface properties          The Ipinterface resource returns the following:              * name (str): The name of the interface             * address (str): The IP address of the interface in the form                 of A.B.C.D/E             * mtu (int): The configured value for IP MTU.           Args:             name (string): The interface identifier to retrieve the                 configuration for          Return:             A Python dictionary object of key/value pairs that represents                 the current configuration of the node.  If the specified                 interface does not exist then None is returned.
Parses the config block and returns the ip address value          The provided configuration block is scaned and the configured value         for the IP address is returned as a dict object.  If the IP address         value is not configured, then None is returned for the value          Args:             config (str): The interface configuration block to parse          Return:             dict: A dict object intended to be merged into the resource dict
Parses the config block and returns the configured IP MTU value          The provided configuration block is scanned and the configured value         for the IP MTU is returned as a dict object.  The IP MTU value is         expected to always be present in the provided config block          Args:             config (str): The interface configuration block to parse          Return:             dict: A dict object intended to be merged into the resource dict
Configures the interface IP address          Args:             name (string): The interface identifier to apply the interface                 config to              value (string): The IP address and mask to set the interface to.                 The value should be in the format of A.B.C.D/E              default (bool): Configures the address parameter to its default                 value using the EOS CLI default command              disable (bool): Negates the address parameter value using the                 EOS CLI no command          Returns:             True if the operation succeeds otherwise False.
Configures the interface IP MTU          Args:             name (string): The interface identifier to apply the interface                 config to              value (integer): The MTU value to set the interface to.  Accepted                 values include 68 to 65535              default (bool): Configures the mtu parameter to its default                 value using the EOS CLI default command              disable (bool); Negate the mtu parameter value using the EOS                 CLI no command          Returns:             True if the operation succeeds otherwise False.          Raises:             ValueError: If the value for MTU is not an integer value or                 outside of the allowable range
Returns the spanning-tree configuration as a dict object          The dictionary object represents the entire spanning-tree         configuration derived from the nodes running config.  This         includes both globally configuration attributes as well as         interfaces and instances.  See the StpInterfaces and StpInstances         classes for the key/value pair definitions.          Note:             See the individual classes for detailed message structures          Returns:             A Python dictionary object of key/value pairs the represent             the entire supported spanning-tree configuration::                  {                     "mode": [mstp, none],                     "interfaces": {...},                     "instances": {...}                 }
Configures the global spanning-tree mode          Note:             This configuration parameter is not defaultable          Args:             value (string): The value to configure the global spanning-tree                 mode of operation.  Valid values include 'mstp', 'none'             default (bool): Set the global spanning-tree mode to default.             disable (bool): Negate the global spanning-tree mode.          Returns:             True if the configuration operation succeeds otherwise False          Raises:             ValueError if the value is not in the accepted range
Returns the specified interfaces STP configuration resource          The STP interface resource contains the following              * name (str): The interface name             * portfast (bool): The spanning-tree portfast admin state             * bpduguard (bool): The spanning-tree bpduguard admin state             * portfast_type (str): The spanning-tree portfast <type> value.                 Valid values include "edge", "network", "normal"          Args:             name (string): The interface identifier to retrieve the config                 for.  Note: Spanning-tree interfaces are only supported on                 Ethernet and Port-Channel interfaces          Returns:             dict: A resource dict object that represents the interface                 configuration.              None: If the specified interace is not a STP port
Configures the portfast value for the specified interface          Args:             name (string): The interface identifier to configure.  The name                 must be the full interface name (eg Ethernet1, not Et1).              value (string): The value to configure the portfast setting to.                 Valid values include "edge", "network", "normal".  The                 default value is "normal"          Returns:             True if the command succeeds, otherwise False          Raises:             ValueError: Raised if an invalid interface name or value is                 specified
Configures the portfast value for the specified interface          Args:             name (string): The interface identifier to configure.  The name                 must be the full interface name (eg Ethernet1, not Et1)              value (bool): True if portfast is enabled otherwise False              default (bool): Configures the portfast parameter to its default                 value using the EOS CLI default config command              disable (bool): Negates the portfast parameter using the EOS                 CLI no config command          Returns:             True if the command succeeds, otherwise False          Raises:             ValueError: Rasied if an invalid interface name is specified              TypeError: Raised if the value keyword argument does not evaluate                 to a valid boolean
Configures the bpduguard value for the specified interface          Args:             name (string): The interface identifier to configure.  The name                 must be the full interface name (eg Ethernet1, not Et1)              value (bool): True if bpduguard is enabled otherwise False              default (bool): Configures the bpduguard parameter to its default                 value using the EOS CLI default config command              disable (bool): Negates the bpduguard parameter using the EOS                 CLI no config command          Returns:             True if the command succeeds, otherwise False          Raises:             ValueError: Rasied if an invalid interface name is specified              TypeError: Raised if the value keyword argument does not evaluate                 to a valid boolean
Accepts a path to search for modules. The method will filter on files     that end in .pyc or files that start with __.      Arguments:         p (string): The path to search     Returns:         list of file names
Accepts dictionary of 'client' and 'api' modules and creates     the corresponding files.
This takes a dict of modules and created the RST index file.
Creates an RST file for the module name passed in. It places it in the     path defined
Imports a module into the current runtime environment      This function emulates the Python import system that allows for     importing full path modules.  It will break down the module and     import each part (or skip if it is already loaded in cache).      Args:         name (str): The name of the module to import.  This should be             the full path of the module      Returns:         The module that was imported
Attempts to load a module into the current environment      This function will load a module specified by name.  The module     name is first checked to see if it is already loaded and will return     the module if it is.   If the module hasn't been previously loaded     it will attempt to import it      Args:         name (str): Specifies the full name of the module.  For instance             pyeapi.api.vlans      Returns:         The module that has been imported or retrieved from the sys modules
Log a message to syslog and stderr      Args:         text (str): The string object to print
Converts the supplied value to a list object      This function will inspect the supplied value and return an     iterable in the form of a list.      Args:         value (object): An valid Python object      Returns:         An iterable object of type list
Expands a delimited string of ranged integers into a list of strings      :param arg: The string range to expand     :param value_delimiter: The delimiter that separates values     :param range_delimiter: The delimiter that signifies a range of values      :return: An array of expanded string values     :rtype: list
Collapses a list of values into a range set      :param arg: The list of values to collapse     :param value_delimiter: The delimiter that separates values     :param range_delimiter: The delimiter that separates a value range      :return: An array of collapsed string values     :rtype: list
Returns a dictionary object that represents a switchport          The Switchport resource returns the following:              * name (str): The name of the interface             * mode (str): The switchport mode value             * access_vlan (str): The switchport access vlan value             * trunk_native_vlan (str): The switchport trunk native vlan vlaue             * trunk_allowed_vlans (str): The trunk allowed vlans value             * trunk_groups (list): The list of trunk groups configured          Args:             name (string): The interface identifier to get.  Note: Switchports                 are only supported on Ethernet and Port-Channel interfaces          Returns:             dict: A Python dictionary object of key/value pairs that represent                 the switchport configuration for the interface specified  If                 the specified argument is not a switchport then None                 is returned
Scans the specified config and parses the switchport mode value          Args:             config (str): The interface configuration block to scan          Returns:             dict: A Python dict object with the value of switchport mode.                 The dict returned is intended to be merged into the resource                 dict
Scans the specified config and parses the trunk group values          Args:             config (str): The interface configuraiton blcok          Returns:             A dict object with the trunk group values that can be merged                 into the resource dict
Scans the specified config and parse the access-vlan value         Args:             config (str): The interface configuration block to scan          Returns:             dict: A Python dict object with the value of switchport access                 value.  The dict returned is intended to be merged into the                 resource dict
Scans the specified config and parse the trunk native vlan value          Args:             config (str): The interface configuration block to scan          Returns:             dict: A Python dict object with the value of switchport trunk                 native vlan value.  The dict returned is intended to be                 merged into the resource dict
Scans the specified config and parse the trunk allowed vlans value          Args:             config (str): The interface configuration block to scan          Returns:             dict: A Python dict object with the value of switchport trunk                 allowed vlans value.  The dict returned is intended to be                 merged into the resource dict
Returns a dict object to all Switchports          This method will return all of the configured switchports as a         dictionary object keyed by the interface identifier.          Returns:             A Python dictionary object that represents all configured                 switchports in the current running configuration
Configures the switchport mode          Args:             name (string): The interface identifier to create the logical                 layer 2 switchport for.  The name must be the full interface                 name and not an abbreviated interface name (eg Ethernet1, not                 Et1)              value (string): The value to set the mode to.  Accepted values                 for this argument are access or trunk              default (bool): Configures the mode parameter to its default                 value using the EOS CLI              disable (bool): Negate the mode parameter using the EOS CLI          Returns:             True if the create operation succeeds otherwise False.
Configures the switchport trunk group value          Args:             intf (str): The interface identifier to configure.             value (str): The set of values to configure the trunk group             default (bool): Configures the trunk group default value             disable (bool): Negates all trunk group settings          Returns:             True if the config operation succeeds otherwise False
Adds the specified trunk group to the interface          Args:             intf (str): The interface name to apply the trunk group to             value (str): The trunk group value to apply to the interface          Returns:             True if the operation as successfully applied otherwise false
Removes a specified trunk group to the interface          Args:             intf (str): The interface name to remove the trunk group from             value (str): The trunk group value          Returns:             True if the operation as successfully applied otherwise false
Returns a generic interface as a set of key/value pairs          This class is should normally serve as a  base class for building more         specific interface resources.  The attributes of this resource are         common to all interfaces regardless of type in EOS.          The generic interface resource returns the following:              * name (str): The name of the interface             * type (str): Always returns 'generic'             * shutdown (bool): True if the interface is shutdown             * description (str): The interface description value          Args:             name (str): The interface identifier to retrieve from the                 running-configuration          Returns:             A Python dictionary object of key/value pairs that represents                 the interface configuration.  If the specified interface                 does not exist, then None is returned
Scans the specified config block and returns the description value          Args:             config (str): The interface config block to scan          Returns:             dict: Returns a dict object with the description value retrieved                 from the config block.  If the description value is not                 configured, None is returned as the value.  The returned dict                 is intended to be merged into the interface resource dict.
Configures the subinterface encapsulation value          Args:             name (string): The interface identifier.  It must be a full                 interface name (ie Ethernet, not Et)             vid (int): The vlan id number             default (boolean): Specifies to default the subinterface                 encapsulation             disable (boolean): Specifies to disable the subinterface                 encapsulation          Returns:             True if the operation succeeds otherwise False is returned
Configures the interface description          EosVersion:             4.13.7M          Args:             name (string): The interface identifier.  It must be a full                 interface name (ie Ethernet, not Et)             value (string): The value to set the description to.             default (boolean): Specifies to default the interface description             disable (boolean): Specifies to negate the interface description          Returns:             True if the operation succeeds otherwise False
Configures the interface shutdown state          Default configuration for set_shutdown is disable=True, meaning         'no shutdown'. Setting both default and disable to False will         effectively enable shutdown on the interface.          Args:             name (string): The interface identifier.  It must be a full                 interface name (ie Ethernet, not Et)              default (boolean): Specifies to default the interface shutdown              disable (boolean): Specifies to disable interface shutdown, i.e.                 disable=True => no shutdown          Returns:             True if the operation succeeds otherwise False is returned
Returns an interface as a set of key/value pairs          Args:             name (string): the interface identifier to retrieve the from                 the configuration          Returns:             A Python dictionary object of key/value pairs that represent             the current configuration for the specified node.  If the             specified interface name does not exist, then None is returned::                  {                     "name": <string>,                     "type": "ethernet",                     "sflow": [true, false],                     "flowcontrol_send": [on, off],                     "flowcontrol_receive": [on, off]                 }
Scans the config block and returns the flowcontrol send value          Args:             config (str): The interface config block to scan          Returns:             dict: Returns a dict object with the flowcontrol send value                 retrieved from the config block.  The returned dict object                 is intended to be merged into the interface resource dict
Scans the config block and returns the flowcontrol receive value          Args:             config (str): The interface config block to scan          Returns:             dict: Returns a dict object with the flowcontrol receive value                 retrieved from the config block.  The returned dict object                 is intended to be merged into the interface resource dict
Configures the interface flowcontrol send value          Args:             name (string): The interface identifier.  It must be a full                 interface name (ie Ethernet, not Et)              value (boolean): True if the interface should enable sending flow                 control packets, otherwise False              default (boolean): Specifies to default the interface flow                 control send value              disable (boolean): Specifies to disable the interface flow                 control send value          Returns:             True if the operation succeeds otherwise False is returned
Configures the interface flowcontrol receive value          Args:             name (string): The interface identifier.  It must be a full                 interface name (ie Ethernet, not Et)              value (boolean): True if the interface should enable receiving                 flow control packets, otherwise False              default (boolean): Specifies to default the interface flow                 control receive value              disable (boolean): Specifies to disable the interface flow                 control receive value          Returns:             True if the operation succeeds otherwise False is returned
Configures the interface flowcontrol value          Args:             name (string): The interface identifier.  It must be a full                 interface name (ie Ethernet, not Et)              direction (string): one of either 'send' or 'receive'              value (boolean): True if the interface should enable flow control                 packet handling, otherwise False              default (boolean): Specifies to default the interface flow control                 send or receive value              disable (boolean): Specifies to disable the interface flow control                 send or receive value          Returns:             True if the operation succeeds otherwise False is returned
Configures the sFlow state on the interface          Args:             name (string): The interface identifier.  It must be a full                 interface name (ie Ethernet, not Et)              value (boolean): True if sFlow should be enabled otherwise False              default (boolean): Specifies the default value for sFlow              disable (boolean): Specifies to disable sFlow          Returns:             True if the operation succeeds otherwise False is returned
Applies a VRF to the interface             Note: VRF being applied to interface must already exist in switch                config. Ethernet port must be in routed mode. This functionality                can also be handled in the VRF api.             Args:                name (str): The interface identifier.  It must be a full                    interface name (ie Ethernet, not Et)                vrf (str): The vrf name to be applied to the interface                default (bool): Specifies the default value for VRF                disable (bool): Specifies to disable VRF             Returns:                True if the operation succeeds otherwise False is returned
Returns a Port-Channel interface as a set of key/value pairs          Args:             name (str): The interface identifier to retrieve from the                 running-configuration          Returns:             A Python dictionary object of key/value pairs that represents             the interface configuration.  If the specified interface             does not exist, then None is returned::                  {                     "name": <string>,                     "type": "portchannel",                     "members": <arrary of interface names>,                     "minimum_links: <integer>,                     "lacp_mode": [on, active, passive]                 }
Returns the LACP mode for the specified Port-Channel interface          Args:             name(str): The Port-Channel interface name to return the LACP                 mode for from the configuration          Returns:             The configured LACP mode for the interface.  Valid mode values                 are 'on', 'passive', 'active'
Returns the member interfaces for the specified Port-Channel          Args:             name(str): The Port-channel interface name to return the member                 interfaces for          Returns:             A list of physical interface names that belong to the specified                 interface
Configures the array of member interfaces for the Port-Channel          Args:             name(str): The Port-Channel interface name to configure the member                 interfaces              members(list): The list of Ethernet interfaces that should be                 member interfaces              mode(str): The LACP mode to configure the member interfaces to.                 Valid values are 'on, 'passive', 'active'. When there are                 existing channel-group members and their lacp mode differs                 from this attribute, all of those members will be removed and                 then re-added using the specified lacp mode. If this attribute                 is omitted, the existing lacp mode will be used for new                 member additions.          Returns:             True if the operation succeeds otherwise False
Configures the LACP mode of the member interfaces          Args:             name(str): The Port-Channel interface name to configure the                 LACP mode              mode(str): The LACP mode to configure the member interfaces to.                 Valid values are 'on, 'passive', 'active'          Returns:             True if the operation succeeds otherwise False
Configures the Port-Channel lacp_fallback          Args:             name(str): The Port-Channel interface name              mode(str): The Port-Channel LACP fallback setting                 Valid values are 'disabled', 'static', 'individual':                  * static  - Fallback to static LAG mode                 * individual - Fallback to individual ports                 * disabled - Disable LACP fallback          Returns:             True if the operation succeeds otherwise False is returned
Configures the Port-Channel LACP fallback timeout            The fallback timeout configures the period an interface in            fallback mode remains in LACP mode without receiving a PDU.          Args:             name(str): The Port-Channel interface name              value(int): port-channel lacp fallback timeout in seconds          Returns:             True if the operation succeeds otherwise False is returned
Returns a Vxlan interface as a set of key/value pairs          The Vxlan interface resource returns the following:              * name (str): The name of the interface             * type (str): Always returns 'vxlan'             * source_interface (str): The vxlan source-interface value             * multicast_group (str): The vxlan multicast-group value             * udp_port (int): The vxlan udp-port value             * vlans (dict): The vlan to vni mappings             * flood_list (list): The list of global VTEP flood list             * multicast_decap (bool): If the mutlicast decap                                       feature is configured          Args:             name (str): The interface identifier to retrieve from the                 running-configuration          Returns:             A Python dictionary object of key/value pairs that represents                 the interface configuration.  If the specified interface                 does not exist, then None is returned
Parses the conf block and returns the vxlan source-interface value          Parses the provided configuration block and returns the value of         vxlan source-interface.  If the value is not configured, this method         will return DEFAULT_SRC_INTF instead.          Args:             config (str): The Vxlan config block to scan          Return:             dict: A dict object intended to be merged into the resource dict
Adds a new VTEP endpoint to the global or local flood list          EosVersion:             4.13.7M          Args:             name (str): The name of the interface to configure             vtep (str): The IP address of the remote VTEP endpoint to add             vlan (str): The VLAN ID associated with this VTEP.  If the VLAN             keyword is used, then the VTEP is configured as a local flood             endpoing          Returns:             True if the command completes successfully
Removes a VTEP endpoint from the global or local flood list          EosVersion:             4.13.7M          Args:             name (str): The name of the interface to configure             vtep (str): The IP address of the remote VTEP endpoint to add             vlan (str): The VLAN ID associated with this VTEP.  If the VLAN             keyword is used, then the VTEP is configured as a local flood             endpoing          Returns:             True if the command completes successfully
Adds a new vlan to vni mapping for the interface          EosVersion:             4.13.7M          Args:             vlan (str, int): The vlan id to map to the vni             vni (str, int): The vni value to use          Returns:             True if the command completes successfully
Returns all local users configuration as a resource dict          Returns:             dict: A dict of usernames with a nested resource dict object
Scans the config block and returns the username as a dict          Args:             config (str): The config block to parse          Returns:             dict: A resource dict that is intended to be merged into the                 user resource
Creates a new user on the local system.          Creating users requires either a secret (password) or the nopassword         keyword to be specified.          Args:             name (str): The name of the user to craete              nopassword (bool): Configures the user to be able to authenticate                 without a password challenage              secret (str): The secret (password) to assign to this user              encryption (str): Specifies how the secret is encoded.  Valid                 values are "cleartext", "md5", "sha512".  The default is                 "cleartext"          Returns:             True if the operation was successful otherwise False          Raises:             TypeError: if the required arguments are not satisfied
Creates a new user on the local node          Args:             name (str): The name of the user to craete              secret (str): The secret (password) to assign to this user              encryption (str): Specifies how the secret is encoded.  Valid                 values are "cleartext", "md5", "sha512".  The default is                 "cleartext"          Returns:             True if the operation was successful otherwise False
Configures the user privilege value in EOS          Args:             name (str): The name of the user to craete              value (int): The privilege value to assign to the user.  Valid                 values are in the range of 0 to 15          Returns:             True if the operation was successful otherwise False          Raises:             TypeError: if the value is not in the valid range
Configures the user role vale in EOS          Args:             name (str): The name of the user to create              value (str): The value to configure for the user role              default (bool): Configure the user role using the EOS CLI                 default command              disable (bool): Negate the user role using the EOS CLI no command          Returns:             True if the operation was successful otherwise False
Creates a connection instance based on the transport      This function creates the EapiConnection object based on the desired     transport.  It looks up the transport class in the TRANSPORTS global     dictionary.      Args:         transport (string): The transport to use to create the instance.         **kwargs: Arbitrary keyword arguments.      Returns:         An instance of a connection object based on the transport      Raises:         TypeError: A TypeError is raised if the transport keyword is not             found in the list (keys) of available transports.
Creates a connection using the supplied settings      This function will create a connection to an Arista EOS node using     the arguments.  All arguments are optional with default values.      Args:         transport (str): Specifies the type of connection transport to use.             Valid values for the connection are socket, http_local, http, and             https.  The default value is specified in DEFAULT_TRANSPORT         host (str): The IP addres or DNS host name of the connection device.             The default value is 'localhost'         username (str): The username to pass to the device to authenticate             the eAPI connection.   The default value is 'admin'         password (str): The password to pass to the device to authenticate             the eAPI connection.  The default value is ''         port (int): The TCP port of the endpoint for the eAPI connection.  If             this keyword is not specified, the default value is automatically             determined by the transport type. (http=80, https=443)         return_node (bool): Returns a Node object if True, otherwise             returns an EapiConnection object.       Returns:         An instance of an EapiConnection object for the specified transport.
Creates a node instance based on an entry from the config      This function will retrieve the settings for the specified connection     from the config and return a Node instance.  The configuration must     be loaded prior to calling this function.      Args:         name (str): The name of the connection to load from the config.  The             name argument should be the connection name (everything right of             the colon from the INI file)      Returns:         This function will return an instance of Node with the settings             from the config instance.      Raises:         AttributeError: raised if the specified configuration name is not             found in the loaded configuration
Returns all of the loaded connections names as a list
Loads the eapi.conf file          This method will use the module variable CONFIG_SEARCH_PATH to         attempt to locate a valid eapi.conf file if a filename is not already         configured.   This method will load the first eapi.conf file it         finds and then return.          The CONFIG_SEARCH_PATH can be overridden using an environment variable         by setting EAPI_CONF.
Reads the file specified by filename          This method will load the eapi.conf file specified by filename into         the instance object.  It will also add the default connection localhost         if it was not defined in the eapi.conf file          Args:             filename (str): The full path to the file to load
Generates the tags with collection with hosts
Reloades the configuration          This method will reload the configuration instance using the last         known filename.  Note this method will initially clear the         configuration and reload all entries.
Returns the properties for a connection name          This method will return the settings for the configuration specified         by name.  Note that the name argument should only be the name.          For instance, give the following eapi.conf file          .. code-block:: ini              [connection:veos01]             transport: http          The name to use to retrieve the configuration would be veos01              >>> pyeapi.client.config.get_connection('veos01')          Args:             name (str): The name of the connection to return          Returns:             A Python dictionary object of key/value pairs that represent                 the node configuration.  If the name provided in the argument                 is not found, then None is returned.
Adds a connection to the configuration          This method will add a connection to the configuration.  The connection         added is only available for the lifetime of the object and is not         persisted.          Note:             If a call is made to load() or reload(), any connections added             with this method must be re-added to the config instance          Args:             name (str): The name of the connection to add to the config.  The                 name provided will automatically be prepended with the string                 connection:             **kwargs (dict); The set of properties used to provide the node                 configuration
Parses version and model information out of 'show version' output         and uses the output to populate class properties.
Configures the node with the specified commands          This method is used to send configuration commands to the node.  It         will take either a string or a list and prepend the necessary commands         to put the session into config mode.          Args:             commands (str, list): The commands to send to the node in config                 mode.  If the commands argument is a string it will be cast to                 a list.                 The list of commands will also be prepended with the                 necessary commands to put the session in config mode.             **kwargs: Additional keyword arguments for expanded eAPI                 functionality. Only supported eAPI params are used in building                 the request          Returns:             The config method will return a list of dictionaries with the                 output from each command.  The function will strip the                 response from any commands it prepends.
Returns a section of the config          Args:             regex (str): A valid regular expression used to select sections                 of configuration to return             config (str): The configuration to return.  Valid values for config                 are "running_config" or "startup_config".  The default value                 is "running_config"          Returns:             The configuration section as a string object.
Sends the array of commands to the node in enable mode          This method will send the commands to the node and evaluate         the results.  If a command fails due to an encoding error,         then the command set will be re-issued individual with text         encoding.          Args:             commands (list): The list of commands to send to the node              encoding (str): The requested encoding of the command output.                 Valid values for encoding are JSON or text              strict (bool): If False, this method will attempt to run a                 command with text encoding if JSON encoding fails             send_enable (bool): If True the enable command will be                                prepended to the command list automatically.             **kwargs: Additional keyword arguments for expanded eAPI                 functionality. Only supported eAPI params are used in building                 the request          Returns:             A dict object that includes the response for each command along                 with the encoding          Raises:             TypeError:                 This method does not support sending configure                 commands and will raise a TypeError if configuration commands                 are found in the list of commands provided                  This method will also raise a TypeError if the specified                 encoding is not one of 'json' or 'text'              CommandError: This method will raise a CommandError if any one                 of the commands fails.
Sends the commands over the transport to the device          This method sends the commands to the device using the nodes         transport.  This is a lower layer function that shouldn't normally         need to be used, preferring instead to use config() or enable().          Args:             commands (list): The ordered list of commands to send to the                 device using the transport             encoding (str): The encoding method to use for the request and                 excpected response.             send_enable (bool): If True the enable command will be                                prepended to the command list automatically.             **kwargs: Additional keyword arguments for expanded eAPI                 functionality. Only supported eAPI params are used in building                 the request          Returns:             This method will return the raw response from the connection                 which is a Python dictionary object.
Loads the specified api module          This method is the API autoload mechanism that will load the API         module specified by the name argument.  The API module will be loaded         and look first for an initialize() function and secondly for an         instance() function.  In both cases, the node object is passed to         the module.          Args:             name (str): The name of the module to load.  The name should be                 the name of the python file to import             namespace (str): The namespace to use to load the module.  The                 default value is 'pyeapi.api'          Returns:             The API module loaded with the node instance.
Retreives the config from the node          This method will retrieve the config from the node as either a string         or a list object.  The config to retrieve can be specified as either         the startup-config or the running-config.          Args:             config (str): Specifies to return either the nodes startup-config                 or running-config.  The default value is the running-config             params (str): A string of keywords to append to the command for                 retrieving the config.             as_string (boo): Flag that determines the response.  If True, then                 the configuration is returned as a raw string.  If False, then                 the configuration is returned as a list.  The default value is                 False          Returns:             This method will return either a string or a list depending on the             states of the as_string keyword argument.          Raises:             TypeError: If the specified config is not one of either                 'running-config' or 'startup-config'
Provides a method to retrieve all routemap configuration         related to the name attribute.          Args:             name (string): The name of the routemap.          Returns:             None if the specified routemap does not exists. If the routermap             exists a dictionary will be provided as follows::                  {                     'deny': {                             30: {                                     'continue': 200,                                     'description': None,                                     'match': ['as 2000',                                               'source-protocol ospf',                                               'interface Ethernet2'],                                     'set': []                                 }                             },                     'permit': {                             10: {                                     'continue': 100,                                     'description': None,                                     'match': ['interface Ethernet1'],                                     'set': ['tag 50']},                             20: {                                     'continue': 200,                                     'description': None,                                     'match': ['as 2000',                                               'source-protocol ospf',                                               'interface Ethernet2'],                                     'set': []                                 }                             }                 }
Creates a new routemap on the node          Note:             This method will attempt to create the routemap regardless             if the routemap exists or not.  If the routemap already exists             then this method will still return True.          Args:             name (string): The full name of the routemap.             action (string): The action to take for this routemap clause.             seqno (integer): The sequence number for the routemap clause.          Returns:             True if the routemap could be created otherwise False (see Note)
Deletes the routemap from the node          Note:             This method will attempt to delete the routemap from the nodes             operational config.  If the routemap does not exist then this             method will not perform any changes but still return True          Args:             name (string): The full name of the routemap.             action (string): The action to take for this routemap clause.             seqno (integer): The sequence number for the routemap clause.          Returns:             True if the routemap could be deleted otherwise False (see Node)
Defaults the routemap on the node          Note:             This method will attempt to default the routemap from the nodes             operational config. Since routemaps do not exist by default,             the default action is essentially a negation and the result will             be the removal of the routemap clause.             If the routemap does not exist then this             method will not perform any changes but still return True          Args:             name (string): The full name of the routemap.             action (string): The action to take for this routemap clause.             seqno (integer): The sequence number for the routemap clause.          Returns:             True if the routemap could be deleted otherwise False (see Node)
Configures the match statements within the routemap clause.         The final configuration of match statements will reflect the list         of statements passed into the statements attribute. This implies         match statements found in the routemap that are not specified in the         statements attribute will be removed.          Args:             name (string): The full name of the routemap.             action (string): The action to take for this routemap clause.             seqno (integer): The sequence number for the routemap clause.             statements (list): A list of the match-related statements. Note                                that the statements should omit the leading                                match.          Returns:             True if the operation succeeds otherwise False
Configures the routemap continue value          Args:             name (string): The full name of the routemap.             action (string): The action to take for this routemap clause.             seqno (integer): The sequence number for the routemap clause.             value (integer): The value to configure for the routemap continue             default (bool): Specifies to default the routemap continue value             disable (bool): Specifies to negate the routemap continue value          Returns:             True if the operation succeeds otherwise False is returned
Configures the routemap description          Args:             name (string): The full name of the routemap.             action (string): The action to take for this routemap clause.             seqno (integer): The sequence number for the routemap clause.             value (string): The value to configure for the routemap description             default (bool): Specifies to default the routemap description value             disable (bool): Specifies to negate the routemap description          Returns:             True if the operation succeeds otherwise False is returned
calc the scaled  ensemble differences from the mean
propagate the ensemble forward using sweep.
write a PEST-compatible binary file.  The data format is     [int,int,float] for i,j,value.  It is autodetected during     the read with Matrix.from_binary().      Parameters     ----------     x : numpy.sparse         coo sparse matrix     row_names : list         list of row_names     col_names : list         list of col_names     filename : str         filename to save binary file     droptol : float         absolute value tolerance to make values smaller than zero.  Default is None     chunk : int         number of elements to write in a single pass.  Default is None
Concatenate Matrix objects.  Tries either axis.      Parameters     ----------     mats: list         list of Matrix objects      Returns     -------     Matrix : Matrix
find the common elements in two lists.  used to support auto align         might be faster with sets      Parameters     ----------     list1 : list         a list of objects     list2 : list         a list of objects      Returns     -------     list : list         list of common objects shared by list1 and list2
reset self.__x private attribute          Parameters         ----------         x : numpy.ndarray         copy : bool             flag to make a copy of 'x'. Defaule is True                  Note         ----         makes a copy of 'x' argument
Overload of numpy.ndarray.__mult__(): element-wise multiplication.         Tries to speedup by checking for scalars of diagonal matrices on         either side of operator          Parameters         ----------         other : scalar,numpy.ndarray,Matrix object             the thing for element-wise multiplication          Returns         -------         Matrix : Matrix
private method to set SVD components.          Note: this should not be called directly
check if matrices are aligned for element-wise operations          Parameters         ----------         other : Matrix          Returns         -------         bool : bool             True if aligned, False if not aligned
get a 2D representation of x.  If not self.isdiagonal, simply         return reference to self.x, otherwise, constructs and returns         a 2D, diagonal ndarray          Returns         -------         numpy.ndarray : numpy.ndarray
get the implied, 2D shape of self          Returns         -------         tuple : tuple             length 2 tuple of ints
transpose operation of self          Returns         -------         Matrix : Matrix             transpose of self
inversion operation of self          Returns         -------         Matrix : Matrix             inverse of self
Get the number of singular components with a singular         value ratio greater than or equal to eigthresh          Parameters         ----------         eigthresh : float             the ratio of the largest to smallest singular value          Returns         -------         int : int             number of singular components
Get the (optionally) truncated SVD components          Parameters         ----------         maxsing : int             the number of singular components to use.  If None,             maxsing is calculated using Matrix.get_maxsing() and eigthresh         eigthresh : float             the ratio of largest to smallest singular components to use             for truncation.  Ignored if maxsing is not None         truncate : bool             flag to truncate components. If False, U, s, and V will be zeroed out instead of truncated.             Default is True          Returns         -------         u : Matrix             (optionally) truncated left singular vectors         s : Matrix             (optionally) truncated singular value matrix         v : Matrix             (optionally) truncated right singular vectors
The pseudo inverse of self.  Formed using truncated singular         value decomposition and Matrix.pseudo_inv_components          Parameters         ----------         maxsing : int             the number of singular components to use.  If None,             maxsing is calculated using Matrix.get_maxsing() and eigthresh         eigthresh : float             the ratio of largest to smallest singular components to use             for truncation.  Ignored if maxsing is not None          Returns         -------         Matrix : Matrix
square root operation          Returns         -------         Matrix : Matrix             square root of self
Get the full singular value matrix of self          Returns         -------         Matrix : Matrix
get an 2D instance of self with all zeros          Returns         -------         Matrix : Matrix
get the row and col indices of names. If axis is None, two ndarrays                 are returned, corresponding the indices of names for each axis          Parameters         ----------         names : iterable             column and/or row names         axis : (int) (optional)             the axis to search.          Returns         -------         numpy.ndarray : numpy.ndarray             indices of names.
get the row and col indices of names. If axis is None, two ndarrays                 are returned, corresponding the indices of names for each axis          Parameters         ----------         names : iterable             column and/or row names         axis : (int) (optional)             the axis to search.          Returns         -------         numpy.ndarray : numpy.ndarray             indices of names.
Create a pyemu.Matrix from the Ensemble.          Parameters         ----------             typ : pyemu.Matrix or derived type                 the type of matrix to return          Returns         -------         pyemu.Matrix : pyemu.Matrix
overload of pandas.DataFrame.drop()          Parameters         ----------         arg : iterable             argument to pass to pandas.DataFrame.drop()          Returns         -------         Ensemble : Ensemble
overload of pandas.DataFrame.dropna()          Parameters         ----------         *args : list             positional args to pass to pandas.DataFrame.dropna()         **kwargs : dict             keyword args to pass to pandas.DataFrame.dropna()          Returns         -------         Ensemble : Ensemble
draw random realizations from a multivariate             Gaussian distribution          Parameters         ----------             cov: pyemu.Cov             covariance structure to draw from         num_reals: int             number of realizations to generate         names : list             list of columns names to draw for.  If None, values all names             are drawn
plot ensemble histograms to multipage pdf          Parameters         ----------         bins : int             number of bins         facecolor : str             color         plot_cols : list of str             subset of ensemble columns to plot.  If None, all are plotted.             Default is None         filename : str             pdf filename. Default is "ensemble.pdf"         func_dict : dict             a dict of functions to apply to specific columns (e.g., np.log10)          **kwargs : dict             keyword args to pass to plot_utils.ensemble_helper()          Returns         -------         None
class method constructor to create an Ensemble from         a pandas.DataFrame          Parameters         ----------         **kwargs : dict             optional args to pass to the             Ensemble Constructor.  Expects 'df' in kwargs.keys()             that must be a pandas.DataFrame instance          Returns         -------             Ensemble : Ensemble
make a deep copy of self          Returns         -------         Ensemble : Ensemble
calculate the approximate covariance matrix implied by the ensemble using         mean-differencing operation at the core of EnKF          Parameters         ----------             localizer : pyemu.Matrix                 covariance localizer to apply          Returns         -------             cov : pyemu.Cov                 covariance matrix
get the deviations of the ensemble value from the mean vector          Returns         -------             en : pyemu.Ensemble                 Ensemble of deviations from the mean
overload of Ensemble.copy()          Returns         -------         ObservationEnsemble : ObservationEnsemble
property decorated method to get mean values of observation noise.         This is a zero-valued pandas.Series          Returns         -------         mean_values : pandas Series
draw realizations of observation noise and add to mean_values         Note: only draws noise realizations for non-zero weighted observations         zero-weighted observations are set to mean value for all realizations          Parameters         ----------         cov : pyemu.Cov             covariance matrix that describes the support volume around the             mean values.         num_reals : int             number of realizations to draw
property decorated method to get a new ObservationEnsemble         of only non-zero weighted observations          Returns         -------         ObservationEnsemble : ObservationEnsemble
this is an experiemental method to help speed up independent draws         for a really large (>1E6) ensemble sizes.          Parameters         ----------         pst : pyemu.Pst             a control file instance         num_reals : int             number of realizations to draw          Returns         -------             ObservationEnsemble : ObservationEnsemble
instantiate an observation obsemble from a jco-type file          Parameters         ----------         pst : pyemu.Pst             a Pst instance         filename : str             the binary file name          Returns         -------         oe : ObservationEnsemble
property decorated method to get a vector of L2 norm (phi)         for the realizations.  The ObservationEnsemble.pst.weights can be         updated prior to calling this method to evaluate new weighting strategies          Return         ------         pandas.DataFrame : pandas.DataFrame
add "base" control file values as a realization
overload of pandas.DataFrame.dropna()          Parameters         ----------         *args : list             positional args to pass to pandas.DataFrame.dropna()         **kwargs : dict             keyword args to pass to pandas.DataFrame.dropna()          Returns         -------         Ensemble : Ensemble
overload of Ensemble.copy()          Returns         -------         ParameterEnsemble : ParameterEnsemble
the mean value vector while respecting log transform          Returns         -------         mean_values : pandas.Series
Get the names of adjustable parameters in the ParameterEnsemble          Returns         -------         list : list             adjustable parameter names
the upper bound vector while respecting log transform          Returns         -------         ubnd : pandas.Series
the lower bound vector while respecting log transform          Returns         -------         lbnd : pandas.Series
indexer for fixed status          Returns         -------         fixed_indexer : pandas.Series
draw realizations of parameter values          Parameters         ----------         cov : pyemu.Cov             covariance matrix that describes the support around             the mean parameter values         num_reals : int             number of realizations to generate         how : str             distribution to use to generate realizations.  Options are             'normal' or 'uniform'.  Default is 'normal'.  If 'uniform',             cov argument is ignored         enforce_bounds : str             how to enforce parameter bound violations.  Options are             'reset' (reset individual violating values), 'drop' (drop realizations             that have one or more violating values.  Default is None (no bounds enforcement)
Draw parameter realizations from a (log10) uniform distribution         described by the parameter bounds.  Respect Log10 transformation          Parameters         ----------         num_reals : int             number of realizations to generate
instantiate a parameter ensemble from uniform draws          Parameters         ----------         pst : pyemu.Pst             a control file instance         num_reals : int             number of realizations to generate          Returns         -------         ParameterEnsemble : ParameterEnsemble
instantiate a parameter ensemble from a sparse covariance matrix.         This is an advanced user method that assumes you know what you are doing         - few guard rails...          Parameters         ----------         pst : pyemu.Pst             a control file instance         cov : (pyemu.SparseMatrix)             sparse covariance matrix to use for drawing         num_reals : int             number of realizations to generate          Returns         -------         ParameterEnsemble : ParameterEnsemble
instantiate a parameter ensemble from a covariance matrix          Parameters         ----------         pst : pyemu.Pst             a control file instance         cov : (pyemu.Cov)             covariance matrix to use for drawing         num_reals : int             number of realizations to generate         use_homegrown : bool             flag to use home-grown full cov draws...much faster             than numpy...         group_chunks : bool             flag to break up draws by par groups.  Only applies             to homegrown, full cov case. Default is False         fill_fixed : bool             flag to fill in fixed parameters from the pst into the             ensemble using the parval1 from the pst.  Default is True         enforce_bounds : bool             flag to enforce parameter bounds from the pst.  realized             parameter values that violate bounds are simply changed to the             value of the violated bound.  Default is False          Returns         -------         ParameterEnsemble : ParameterEnsemble
instaniate a parameter ensemble from stochastic draws using a mixture of         distributions.  Available distributions include (log) "uniform", (log) "triangular",         and (log) "gaussian". log transformation is respected.          Parameters         ----------         pst : pyemu.Pst             a Pst instance         how_dict : dict             a dictionary of parnme keys and 'how' values, where "how" can be "uniform",             "triangular", or "gaussian".         default : str             the default distribution to use for parameter not listed in how_dict         num_reals : int             number of realizations to draw         cov : pyemu.Cov             an optional Cov instance to use for drawing from gaussian distribution.  If None,             and "gaussian" is listed in how_dict (or default), then a diagonal covariance matrix             is constructed from the parameter bounds in the pst.  Default is None         sigma_range : float              the number of standard deviations implied by the bounds in the pst.  Only used if              "gaussian" is in how_dict (or default) and cov is None.  Default is 6.         enforce_bounds : boolean             flag to enforce parameter bounds in resulting ParameterEnsemble.             Only matters if "gaussian" is in values of how_dict.  Default is True.         partial : bool             flag to allow a partial ensemble (not all pars included). Default is False
instantiate an parameter obsemble from a jco-type file          Parameters         ----------         pst : pyemu.Pst             a Pst instance         filename : str             the binary file name          Returns         -------         pe : ParameterEnsemble
Private method to remove log10 transformation from ensemble          Parameters         ----------         inplace: bool             back transform self in place          Returns         ------         ParameterEnsemble : ParameterEnsemble             if inplace if False          Note         ----         Don't call this method unless you know what you are doing
Private method to perform log10 transformation for ensemble          Parameters         ----------         inplace: bool             transform self in place          Returns         -------         ParameterEnsemble : ParameterEnsemble             if inplace is False          Note         ----         Don't call this method unless you know what you are doing
project the ensemble using the null-space Monte Carlo method          Parameters         ----------         projection_matrix : pyemu.Matrix             projection operator - must already respect log transform          inplace : bool             project self or return a new ParameterEnsemble instance          log: pyemu.Logger             for logging progress          enforce_bounds : str             parameter bound enforcement flag. 'drop' removes             offending realizations, 'reset' resets offending values          Returns         -------         ParameterEnsemble : ParameterEnsemble             if inplace is False
entry point for bounds enforcement.  This gets called for the         draw method(s), so users shouldn't need to call this          Parameters         ----------         enforce_bounds : str             can be 'reset' to reset offending values or 'drop' to drop             offending realizations
enforce parameter bounds on the ensemble by dropping         violating realizations
enforce parameter bounds on the ensemble by resetting         violating vals to bound
create a parameter ensemble from parfiles.  Accepts parfiles with less than the         parameters in the control (get NaNs in the ensemble) or extra parameters in the         parfiles (get dropped)          Parameters:             pst : pyemu.Pst              parfile_names : list of str                 par file names              real_names : str                 optional list of realization names. If None, a single integer counter is used          Returns:             pyemu.ParameterEnsemble
overload of pandas.DataFrame.to_csv() to account         for parameter transformation so that the saved         ParameterEnsemble csv is not in Log10 space          Parameters         ----------         *args : list             positional arguments to pass to pandas.DataFrame.to_csv()         **kwrags : dict             keyword arguments to pass to pandas.DataFrame.to_csv()          Note         ----         this function back-transforms inplace with respect to         log10 before writing
write the parameter ensemble to a jco-style binary file          Parameters         ----------         filename : str             the filename to write          Returns         -------         None           Note         ----         this function back-transforms inplace with respect to         log10 before writing
write the parameter ensemble to PEST-style parameter files          Parameters         ----------         prefix: str             file prefix for par files          Note         ----         this function back-transforms inplace with respect to         log10 before writing
add "base" control file values as a realization
get the deviations of the ensemble value from the mean vector          Returns         -------             en : pyemu.Ensemble                 Ensemble of deviations from the mean
Initialize.  Depending on arguments, draws or loads         initial parameter observations ensembles and runs the initial parameter         ensemble          Parameters         ----------             num_reals : int                 the number of realizations to draw.  Ignored if parensemble/obsensemble                 are not None             enforce_bounds : str                 how to enfore parameter bound transgression.  options are                 reset, drop, or None             parensemble : pyemu.ParameterEnsemble or str                 a parameter ensemble or filename to use as the initial                 parameter ensemble.  If not None, then obsenemble must not be                 None             obsensemble : pyemu.ObservationEnsemble or str                 an observation ensemble or filename to use as the initial                 observation ensemble.  If not None, then parensemble must                 not be None             restart_obsensemble : pyemu.ObservationEnsemble or str                 an observation ensemble or filename to use as an                 evaluated observation ensemble.  If not None, this will skip the initial                 parameter ensemble evaluation - user beware!
for the enkf formulation, this simply moves the ensemble forward by running the model         once for each realization
Ayman here!!!.  some peices that may be useful:         self.parcov = parameter covariance matrix         self.obscov = obseravtion noise covariance matrix         self.parensmeble = current parameter ensemble         self.obsensemble = current observation (model output) ensemble          the ensemble instances have two useful methods:         Ensemble.covariance_matrix() - get an empirical covariance matrix         Ensemble.get_deviations() - get an ensemble of deviations around the mean vector          If you use pyemu.Matrix (and pyemu.Cov) for the linear algebra parts, you don't         have to worry about the alignment of the matrices (pyemu will dynamically reorder/align         based in row and/or col names).  The Matrix instance also has a .inv for the inverse         as well as .s, .u and .v for the SVD components (gets dynamically evaluated if you try to         access these attributes)            Following Evensen 2003..
update performs the analysis, then runs the forecast using the updated self.parensemble.         This can be called repeatedly to iterate...
Loop over time windows and apply da         :return:
- The function prepares the model for this time cycle          - Any time-dependant forcing should be handled here. This includes temporal stresses, boundary conditions, and          initial conditions.          - Two options are available (1) template files to update input parameters                                      (2) use results from previous cycle to update input files using instruction                                      files.                                      (3) the user must prepare a pair of files : .tpl(or .ins) and corresponding file to change         :param time_index:         :return:
Function found from:         https://eli.thegreenplace.net/2011/10/19/perls-guess-if-file-is-text-or-binary-implemented-in-python         Returns True if file is most likely a text file         Returns False if file is most likely a binary file         Uses heuristics to guess whether the given file is text or binary,         by reading a single block of bytes from the file.         If more than 30% of the chars in the block are non-text, or there         are NUL ('\x00') bytes in the block, assume this is a binary file.
an OS agnostic function to execute a command line      Parameters     ----------     cmd_str : str         the str to execute with os.system()      cwd : str         the directory to execute the command in      verbose : bool         flag to echo to stdout complete cmd str      Note     ----     uses platform to detect OS and adds .exe suffix or ./ prefix as appropriate      for Windows, if os.system returns non-zero, raises exception      Example     -------     ``>>>import pyemu``      ``>>>pyemu.helpers.run("pestpp pest.pst")``
start a group of pest(++) slaves on the local machine      Parameters     ----------     slave_dir :  str         the path to a complete set of input files     exe_rel_path : str         the relative path to the pest(++) executable from within the slave_dir     pst_rel_path : str         the relative path to the pst file from within the slave_dir     num_slaves : int         number of slaves to start. defaults to number of cores     slave_root : str         the root to make the new slave directories in     rel_path: str         the relative path to where pest(++) should be run from within the         slave_dir, defaults to the uppermost level of the slave dir     local: bool         flag for using "localhost" instead of hostname on slave command line     cleanup: bool         flag to remove slave directories once processes exit     master_dir: str         name of directory for master instance.  If master_dir         exists, then it will be removed.  If master_dir is None,         no master instance will be started     verbose : bool         flag to echo useful information to stdout     silent_master : bool         flag to pipe master output to devnull.  This is only for         pestpp Travis testing. Default is False      Note     ----     if all slaves (and optionally master) exit gracefully, then the slave     dirs will be removed unless cleanup is false      Example     -------     ``>>>import pyemu``      start 10 slaves using the directory "template" as the base case and     also start a master instance in a directory "master".      ``>>>pyemu.helpers.start_slaves("template","pestpp","pest.pst",10,master_dir="master")``
write the regularization section to an open         file handle          Parameters         ----------         f : file handle
write an SVD section to a file handle          Parameters         ----------         f : file handle
parse values from lines of the SVD section          Parameters         ----------         lines : list
get a generic (default) control section dataframe                  Returns         -------         pandas.DataFrame : pandas.DataFrame
cast the string lines for a pest control file into actual inputs          Parameters         ----------         lines : list             strings from pest control file
list the entries and current values in the control data section          Returns         -------         formatted_values : pandas.Series
write control data section to a file                  Parameters         ----------         f: file handle or string filename
read an existing PEST-type structure file into a GeoStruct instance      Parameters     ----------     struct_file : (str)         existing pest-type structure file     return_type :  (object)         the instance type to return.  Default is GeoStruct      Returns     -------     GeoStruct : list or GeoStruct      Note     ----     if only on structure is listed in struct_file, then return type     is GeoStruct.  Otherwise, return type is a list of GeoStruct      Example     -------     ``>>>import pyemu``      ``>>>gs = pyemu.utils.geostats.reads_struct_file("struct.dat")``
Function to instantiate a Vario2d from a PEST-style structure file      Parameters     ----------     f : (file handle)         file handle opened for reading      Returns     -------     Vario2d : Vario2d         Vario2d derived type
function to read information from a PEST-style structure file      Parameters     ----------     f : (file handle)         file handle open for reading      Returns     -------     nugget : float         the GeoStruct nugget     transform : str         the GeoStruct transformation     variogram_info : dict         dictionary of structure-level variogram information
function to read an SGEMS-type variogram XML file into     a GeoStruct      Parameters     ----------     xml_file : (str)         SGEMS variogram XML file     return_type :  (object)         the instance type to return.  Default is GeoStruct      Returns     -------     GeoStruct : GeoStruct       Example     -------     ``>>>import pyemu``      ``>>>gs = pyemu.utils.geostats.read_sgems_variogram_xml("sgems.xml")``
function to read a GSLIB point data file into a pandas.DataFrame      Parameters     ----------     filename : (str)         GSLIB file     attr_name : (str)         the column name in the dataframe for the attribute.  If None, GSLIB file         can have only 3 columns.  attr_name must be in the GSLIB file header     x_idx : (int)         the index of the x-coordinate information in the GSLIB file. Default is         0 (first column)     y_idx : (int)         the index of the y-coordinate information in the GSLIB file.         Default is 1 (second column)      Returns     -------     df : pandas.DataFrame      Raises     ------     exception if attr_name is None and GSLIB file has more than 3 columns      Note     ----     assigns generic point names ("pt0, pt1, etc)      Example     -------     ``>>>import pyemu``      ``>>>df = pyemu.utiils.geostats.gslib_2_dataframe("prop.gslib",attr_name="hk")``
read an SGEM experimental variogram into a sequence of     pandas.DataFrames      Parameters     ----------     filename : (str)         an SGEMS experimental variogram XML file      Returns     -------     dfs : list         a list of pandas.DataFrames of x, y, pairs for each         division in the experimental variogram
A python replication of the PEST fac2real utility for creating a     structure grid array from previously calculated kriging factors (weights)      Parameters     ----------     pp_file : (str)         PEST-type pilot points file     factors_file : (str)         PEST-style factors file     out_file : (str)         filename of array to write.  If None, array is returned, else         value of out_file is returned.  Default is "test.ref".     upper_lim : (float)         maximum interpolated value in the array.  Values greater than         upper_lim are set to fill_value     lower_lim : (float)         minimum interpolated value in the array.  Values less than lower_lim         are set to fill_value     fill_value : (float)         the value to assign array nodes that are not interpolated       Returns     -------     arr : numpy.ndarray         if out_file is None     out_file : str         if out_file it not None      Example     -------     ``>>>import pyemu``      ``>>>pyemu.utils.geostats.fac2real("hkpp.dat",out_file="hk_layer_1.ref")``
function to parse a factor file line.  Used by fac2real()      Parameters     ----------     line : (str)         a factor line from a factor file      Returns     -------     inode : int         the inode of the grid node     itrans : int         flag for transformation of the grid node     fac_data : dict         a dictionary of point number, factor
write a PEST-style structure file          Parameters         ----------         f : (str or file handle)             file to write the GeoStruct information to
build a pyemu.Cov instance from GeoStruct                  Parameters                 ----------                 x : (iterable of floats)                     x-coordinate locations                 y : (iterable of floats)                     y-coordinate locations                 names : (iterable of str)                    (parameter) names of locations.                  Returns                 -------                 sparse : pyemu.SparseMatrix                     the sparse covariance matrix implied by this GeoStruct for the x,y pairs.                  Example                 -------                 ``>>>pp_df = pyemu.pp_utils.pp_file_to_dataframe("hkpp.dat")``                  ``>>>cov = gs.covariance_matrix(pp_df.x,pp_df.y,pp_df.name)``
build a pyemu.Cov instance from GeoStruct          Parameters         ----------         x : (iterable of floats)             x-coordinate locations         y : (iterable of floats)             y-coordinate locations         names : (iterable of str)             names of location. If None, cov must not be None.  Default is None.         cov : (pyemu.Cov) instance             an existing Cov instance.  The contribution of this GeoStruct is added             to cov.  If cov is None, names must not be None. Default is None          Returns         -------         cov : pyemu.Cov             the covariance matrix implied by this GeoStruct for the x,y pairs.             cov has row and column names supplied by the names argument unless             the "cov" argument was passed.          Note         ----         either "names" or "cov" must be passed.  If "cov" is passed, cov.shape         must equal len(x) and len(y).          Example         -------         ``>>>pp_df = pyemu.pp_utils.pp_file_to_dataframe("hkpp.dat")``          ``>>>cov = gs.covariance_matrix(pp_df.x,pp_df.y,pp_df.name)``
get the covariance between two points implied by the GeoStruct.         This is used during the ordinary kriging process to get the RHS          Parameters         ----------         pt0 : (iterable length 2 of floats)         pt1 : (iterable length 2 of floats)          Returns         -------         covariance : float             the covariance between pt0 and pt1 implied by the GeoStruct
Get the covariance between point x0,y0 and the points         contained in xother, yother.          Parameters         ----------         x0 : (float)             x-coordinate         y0 : (float)             y-coordinate         xother : (iterable of floats)             x-coordinate of other points         yother : (iterable of floats)             y-coordinate of other points          Returns         -------         cov : numpy.ndarray             a 1-D array of covariance between point x0,y0 and the             points contained in xother, yother.  len(cov) = len(xother) =             len(yother)
get the sill of the GeoStruct          Return         ------         sill : float             the sill of the (nested) GeoStruct, including nugget and contribution             from each variogram
make a cheap plot of the GeoStruct          Parameters         ----------         **kwargs : (dict)             keyword arguments to use for plotting.          Returns         -------         ax : matplotlib.pyplot.axis             the axis with the GeoStruct plot          Note         ----         optional arguments include "ax" (an existing axis),         "individuals" (plot each variogram on a separate axis),         "legend" (add a legend to the plot(s)).  All other kwargs         are passed to matplotlib.pyplot.plot()
check for point_data entries that are closer than         EPSILON distance - this will cause a singular kriging matrix.          Parameters         ----------         rectify : (boolean)             flag to fix the problems with point_data             by dropping additional points that are             closer than EPSILON distance.  Default is False          Note         ----         this method will issue warnings for points that are closer         than EPSILON distance
calculate kriging factors (weights) for a structured grid.          Parameters         ----------         spatial_reference : (flopy.utils.reference.SpatialReference)             a spatial reference that describes the orientation and             spatail projection of the the structured grid         zone_array : (numpy.ndarray)             an integer array of zones to use for kriging.  If not None,             then point_data must also contain a "zone" column.  point_data             entries with a zone value not found in zone_array will be skipped.             If None, then all point_data will (potentially) be used for             interpolating each grid node. Default is None         minpts_interp : (int)             minimum number of point_data entires to use for interpolation at             a given grid node.  grid nodes with less than minpts_interp             point_data found will be skipped (assigned np.NaN).  Defaut is 1         maxpts_interp : (int)             maximum number of point_data entries to use for interpolation at             a given grid node.  A larger maxpts_interp will yield "smoother"             interplation, but using a large maxpts_interp will slow the             (already) slow kriging solution process and may lead to             memory errors. Default is 20.         search_radius : (float)             the size of the region around a given grid node to search for             point_data entries. Default is 1.0e+10         verbose : (boolean)             a flag to  echo process to stdout during the interpolatino process.             Default is False         var_filename : (str)             a filename to save the kriging variance for each interpolated grid node.             Default is None.         forgive : (boolean)             flag to continue if inversion of the kriging matrix failes at one or more             grid nodes.  Inversion usually fails if the kriging matrix is singular,             resulting from point_data entries closer than EPSILON distance.  If True,             warnings are issued for each failed inversion.  If False, an exception             is raised for failed matrix inversion.          Returns         -------         df : pandas.DataFrame             a dataframe with information summarizing the ordinary kriging             process for each grid node          Note         ----         this method calls OrdinaryKrige.calc_factors()           Example         -------         ``>>>import flopy``          ``>>>import pyemu``          ``>>>v = pyemu.utils.geostats.ExpVario(a=1000,contribution=1.0)``          ``>>>gs = pyemu.utils.geostats.GeoStruct(variograms=v,nugget=0.5)``          ``>>>pp_df = pyemu.pp_utils.pp_file_to_dataframe("hkpp.dat")``          ``>>>ok = pyemu.utils.geostats.OrdinaryKrige(gs,pp_df)``          ``>>>m = flopy.modflow.Modflow.load("mymodel.nam")``          ``>>>df = ok.calc_factors_grid(m.sr,zone_array=m.bas6.ibound[0].array,``          ``>>>                          var_filename="ok_var.dat")``          ``>>>ok.to_grid_factor_file("factors.dat")``
calculate ordinary kriging factors (weights) for the points         represented by arguments x and y          Parameters         ----------         x : (iterable of floats)             x-coordinates to calculate kriging factors for         y : (iterable of floats)             y-coordinates to calculate kriging factors for         minpts_interp : (int)             minimum number of point_data entires to use for interpolation at             a given x,y interplation point.  interpolation points with less             than minpts_interp point_data found will be skipped             (assigned np.NaN).  Defaut is 1         maxpts_interp : (int)             maximum number of point_data entries to use for interpolation at             a given x,y interpolation point.  A larger maxpts_interp will             yield "smoother" interplation, but using a large maxpts_interp             will slow the (already) slow kriging solution process and may             lead to memory errors. Default is 20.         search_radius : (float)             the size of the region around a given x,y interpolation point to search for             point_data entries. Default is 1.0e+10         verbose : (boolean)             a flag to  echo process to stdout during the interpolatino process.             Default is False         forgive : (boolean)             flag to continue if inversion of the kriging matrix failes at one or more             interpolation points.  Inversion usually fails if the kriging matrix is singular,             resulting from point_data entries closer than EPSILON distance.  If True,             warnings are issued for each failed inversion.  If False, an exception             is raised for failed matrix inversion.          Returns         -------         df : pandas.DataFrame             a dataframe with information summarizing the ordinary kriging             process for each interpolation points
write a grid-based PEST-style factors file.  This file can be used with         the fac2real() method to write an interpolated structured array          Parameters         ----------         filename : (str)             factor filename         points_file : (str)             points filename to add to the header of the factors file.             Not used by fac2real() method.  Default is "points.junk"         zone_file : (str)             zone filename to add to the header of the factors file.             Not used by fac2real() method.  Default is "zone.junk"          Note         ----         this method should be called after OrdinaryKirge.calc_factors_grid()
write the Vario2d to a PEST-style structure file          Parameters         ----------         f : (str or file handle)             item to write to
get the rotation coefficents in radians          Returns         -------         rotation_coefs : list             the rotation coefficients implied by Vario2d.bearing
get a cheap plot of the Vario2d          Parameters         ----------         **kwargs : (dict)             keyword arguments to use for plotting          Returns         -------         ax : matplotlib.pyplot.axis          Note         ----         optional arguments in kwargs include         "ax" (existing matplotlib.pyplot.axis).  Other         kwargs are passed to matplotlib.pyplot.plot()
build a pyemu.SparseMatrix instance implied by Vario2d          Parameters         ----------         x : (iterable of floats)             x-coordinate locations         y : (iterable of floats)             y-coordinate locations         names : (iterable of str)             names of locations. If None, cov must not be None         iidx : 1-D ndarray             i row indices         jidx : 1-D ndarray             j col indices         data : 1-D ndarray             nonzero entries           Returns         -------         None
build a pyemu.Cov instance implied by Vario2d          Parameters         ----------         x : (iterable of floats)             x-coordinate locations         y : (iterable of floats)             y-coordinate locations         names : (iterable of str)             names of locations. If None, cov must not be None         cov : (pyemu.Cov)             an existing Cov instance.  Vario2d contribution is added to cov          Returns         -------         cov : pyemu.Cov          Note         ----         either names or cov must not be None.
private method to rotate points         according to Vario2d.bearing and Vario2d.anisotropy          Parameters         ----------         dx : (float or numpy.ndarray)             x-coordinates to rotate         dy : (float or numpy.ndarray)             y-coordinates to rotate          Returns         -------             dxx : (float or numpy.ndarray)                 rotated x-coordinates             dyy : (float or numpy.ndarray)                 rotated y-coordinates
get the covariance between base point x0,y0 and         other points xother,yother implied by Vario2d          Parameters         ----------         x0 : (float)             x-coordinate of base point         y0 : (float)             y-coordinate of base point         xother : (float or numpy.ndarray)             x-coordinates of other points         yother : (float or numpy.ndarray)             y-coordinates of other points          Returns         -------         cov : numpy.ndarray             covariance between base point and other points implied by             Vario2d.          Note         ----         len(cov) = len(xother) = len(yother)
get the covarince between two points implied by Vario2d          Parameters         ----------         pt0 : (iterable of len 2)             first point x and y         pt1 : (iterable of len 2)             second point x and y          Returns         -------         cov : float             covariance between pt0 and pt1
private method exponential variogram "h" function          Parameters         ----------         h : (float or numpy.ndarray)             distance(s)          Returns         -------         h_function : float or numpy.ndarray             the value of the "h" function implied by the ExpVario
private method for the gaussian variogram "h" function          Parameters         ----------         h : (float or numpy.ndarray)             distance(s)          Returns         -------         h_function : float or numpy.ndarray             the value of the "h" function implied by the GauVario
private method for the spherical variogram "h" function          Parameters         ----------         h : (float or numpy.ndarray)             distance(s)          Returns         -------         h_function : float or numpy.ndarray             the value of the "h" function implied by the SphVario
log a one time statement          Parameters         ----------         phrase : str             statement to log
log something that happened.  The first time phrase is passed the         start time is saved.  The second time the phrase is logged, the         elapsed time is written          Parameters         ----------             phrase : str                 the thing that happened
write a warning to the log file.          Parameters         ----------         message : str             the warning text
log an exception, close the log file, then raise the exception          Parameters         ----------         message : str             the exception message          Raises         ------             exception with message
write a template file for a modflow parameter value file.     Uses names in the first column in the pval file as par names.      Parameters     ----------     pval_file : str         parameter value file     tpl_file : str, optional         template file to write.  If None, use <pval_file>.tpl.         Default is None      Returns     -------     df : pandas.DataFrame         pandas DataFrame with control file parameter information
write an instruction file for a modflow head observation file      Parameters     ----------     hob_file : str         modflow hob file      Returns     -------     df : pandas.DataFrame         pandas DataFrame with control file observation information
write an instruction file for a modflow hydmod file      Parameters     ----------     hydmod_file : str         modflow hydmod file       Returns     -------     df : pandas.DataFrame         pandas DataFrame with control file observation information      Note     ----     calls modflow_read_hydmod_file()
read in a binary hydmod file and return a dataframe of the results      Parameters     ----------     hydmod_file : str         modflow hydmod binary file     hydmod_outfile : str         output file to write.  If None, use <hydmod_file>.dat.         Default is None      Returns     -------     df : pandas.DataFrame         pandas DataFrame with hymod_file values      Note     ----     requires flopy
setup regularly-spaced (gridded) pilot point parameterization      Parameters     ----------     ml : flopy.mbase         a flopy mbase dervied type.  If None, sr must not be None.     sr : flopy.utils.reference.SpatialReference         a spatial reference use to locate the model grid in space.  If None,         ml must not be None.  Default is None     ibound : numpy.ndarray         the modflow ibound integer array.  Used to set pilot points only in active areas.         If None and ml is None, then pilot points are set in all rows and columns according to         every_n_cell.  Default is None.     prefix_dict : dict         a dictionary of pilot point parameter prefix, layer pairs.  example : {"hk":[0,1,2,3]} would         setup pilot points with the prefix "hk" for model layers 1 - 4 (zero based). If None, a generic set         of pilot points with the "pp" prefix are setup for a generic nrowXncol grid. Default is None     use_ibound_zones : bool         a flag to use the greater-than-zero values in the ibound as pilot point zones.  If False,ibound         values greater than zero are treated as a single zone.  Default is False.     pp_dir : str         directory to write pilot point files to.  Default is '.'     tpl_dir : str         directory to write pilot point template file to.  Default is '.'     shapename : str         name of shapefile to write that containts pilot point information. Default is "pp.shp"      Returns     -------         pp_df : pandas.DataFrame             a dataframe summarizing pilot point information (same information             written to shapename
setup observations of gw (and optionally sw) mass budgets from mt3dusgs list file.  writes         an instruction file and also a _setup_.csv to use when constructing a pest         control file          Parameters         ----------         list_filename : str                 modflow list file         gw_filename : str             output filename that will contain the gw budget observations. Default is             "mtlist_gw.dat"         sw_filename : str             output filename that will contain the sw budget observations. Default is             "mtlist_sw.dat"         start_datetime : str             an str that can be parsed into a pandas.TimeStamp.  used to give budget             observations meaningful names         gw_prefix : str             a prefix to add to the GW budget observations.  Useful if processing             more than one list file as part of the forward run process. Default is 'gw'.         sw_prefix : str             a prefix to add to the SW budget observations.  Useful if processing             more than one list file as part of the forward run process. Default is 'sw'.         save_setup_file : (boolean)             a flag to save _setup_<list_filename>.csv file that contains useful             control file information          Returns         -------         frun_line, ins_filenames, df :str, list(str), pandas.DataFrame             the command to add to the forward run script, the names of the instruction             files and a dataframe with information for constructing a control file.  If INSCHEK fails             to run, df = None          Note         ----         This function uses INSCHEK to get observation values; the observation values are         the values of the list file list_filename.  If INSCHEK fails to run, the obseravtion         values are set to 1.0E+10          the instruction files are named <out_filename>.ins          It is recommended to use the default value for gw_filename or sw_filename.
write an instruction file for a MODFLOW list file      Parameters     ----------     ins_filename : str         name of the instruction file to write     df : pandas.DataFrame         the dataframe of list file entries     prefix : str         the prefix to add to the column names to form         obseravtions names
process an MT3D list file to extract mass budget entries.      Parameters     ----------     list_filename : str         the mt3d list file     gw_filename : str         the name of the output file with gw mass budget information.         Default is "mtlist_gw.dat"     sw_filename : str         the name of the output file with sw mass budget information.         Default is "mtlist_sw.dat"     start_datatime : str         an str that can be cast to a pandas.TimeStamp.  Used to give         observations a meaningful name      Returns     -------     gw : pandas.DataFrame         the gw mass dataframe     sw : pandas.DataFrame (optional)         the sw mass dataframe      Note     ----     requires flopy      if SFT is not active, no SW mass budget will be returned
setup observations of budget volume and flux from modflow list file.  writes     an instruction file and also a _setup_.csv to use when constructing a pest     control file      Parameters     ----------     list_filename : str             modflow list file     flx_filename : str         output filename that will contain the budget flux observations. Default is         "flux.dat"     vol_filename : str)         output filename that will contain the budget volume observations.  Default         is "vol.dat"     start_datetime : str         an str that can be parsed into a pandas.TimeStamp.  used to give budget         observations meaningful names     prefix : str         a prefix to add to the water budget observations.  Useful if processing         more than one list file as part of the forward run process. Default is ''.     save_setup_file : (boolean)         a flag to save _setup_<list_filename>.csv file that contains useful         control file information      Returns     -------     df : pandas.DataFrame         a dataframe with information for constructing a control file.  If INSCHEK fails         to run, reutrns None      Note     ----     This function uses INSCHEK to get observation values; the observation values are     the values of the list file list_filename.  If INSCHEK fails to run, the obseravtion     values are set to 1.0E+10      the instruction files are named <flux_file>.ins and <vol_file>.ins, respectively      It is recommended to use the default values for flux_file and vol_file.
process a MODFLOW list file to extract flux and volume water budget entries.      Parameters     ----------     list_filename : str         the modflow list file     flx_filename : str         the name of the output file with water budget flux information.         Default is "flux.dat"     vol_filename : str         the name of the output file with water budget volume information.         Default is "vol.dat"     start_datatime : str         an str that can be cast to a pandas.TimeStamp.  Used to give         observations a meaningful name      Returns     -------     flx : pandas.DataFrame         the flux dataframe     vol : pandas.DataFrame         the volume dataframe      Note     ----     requires flopy
write an instruction file for a MODFLOW list file      Parameters     ----------     ins_filename : str         name of the instruction file to write     df : pandas.DataFrame         the dataframe of list file entries     prefix : str         the prefix to add to the column names to form         obseravtions names
a function to setup extracting time-series from a binary modflow     head save (or equivalent format - ucn, sub, etc).  Writes     an instruction file and a _set_ csv      Parameters     ----------     hds_file : str         binary filename     kij_dict : dict         dictionary of site_name: [k,i,j] pairs     prefix : str         string to prepend to site_name when forming obsnme's.  Default is None     include_path : bool         flag to prepend hds_file path. Useful for setting up         process in separate directory for where python is running.     model : flopy.mbase         a flopy model.  If passed, the observation names will have the datetime of the         observation appended to them.  If None, the observation names will have the         stress period appended to them. Default is None.     postprocess_inact : float         Inactive flag in heads/ucn file e.g. mt.btn.cinit      Returns     -------        Note     ----     This function writes hds_timeseries.config that must be in the same     dir where apply_hds_timeseries() is called during the forward run      assumes model time units are days!!!
Dirty function to post process concentrations in inactive/dry cells
a function to setup using all values from a     layer-stress period pair for observations.  Writes     an instruction file and a _setup_ csv used     construct a control file.      Parameters     ----------     hds_file : str         a MODFLOW head-save file.  If the hds_file endswith 'ucn',         then the file is treated as a UcnFile type.     kperk_pairs : iterable         an iterable of pairs of kper (zero-based stress         period index) and k (zero-based layer index) to         setup observations for.  If None, then a shit-ton         of observations may be produced!     skip : variable         a value or function used to determine which values         to skip when setting up observations.  If np.scalar(skip)         is True, then values equal to skip will not be used.         If skip can also be a np.ndarry with dimensions equal to the model.         Obscervations are set up only for cells with Non-zero values in the array.         If not np.ndarray or np.scalar(skip), then skip will be treated as a lambda function that         returns np.NaN if the value should be skipped.     prefix : str         the prefix to use for the observation names. default is "hds".      Returns     -------     (forward_run_line, df) : str, pd.DataFrame         a python code str to add to the forward run script and the setup info for the observations      Note     ----     requires flopy      writes <hds_file>.dat.ins instruction file      writes _setup_<hds_file>.csv which contains much     useful information for construction a control file
function to find the last time step (kstp) for a     give stress period (kper) in a modflow head save file.       Parameters     ----------     hds : flopy.utils.HeadFile      kper : int         the zero-index stress period number      Returns     -------     kstp : int         the zero-based last time step during stress period         kper in the head save file
process a modflow head save file.  A companion function to     setup_hds_obs that is called during the forward run process      Parameters     ----------     hds_file : str         a modflow head save filename. if hds_file ends with 'ucn',         then the file is treated as a UcnFile type.      Note     ----     requires flopy      writes <hds_file>.dat      expects <hds_file>.dat.ins to exist      uses pyemu.pst_utils.parse_ins_file to get observation names
writes an instruction file for a mt3d-usgs sft output file      Parameters     ----------         sft_file : str             the sft output file (ASCII)         ins_file : str             the name of the instruction file to create.  If None, the name             is <sft_file>.ins.  Default is None         start_datetime : str             a pandas.to_datetime() compatible str.  If not None,             then the resulting observation names have the datetime             suffix.  If None, the suffix is the output totim.  Default             is None         times : iterable             a container of times to make observations for.  If None, all times are used.             Default is None.         ncomp : int             number of components in transport model. Default is 1.       Returns     -------         df : pandas.DataFrame             a dataframe with obsnme and obsval for the sft simulated concentrations and flows.             If inschek was not successfully run, then returns None       Note     ----         setups up observations for SW conc, GW conc and flowgw for all times and reaches.
Setup multiplier parameters for SFR segment data.  Just handles the     standard input case, not all the cryptic SFR options.  Loads the dis, bas, and sfr files     with flopy using model_ws.  However, expects that apply_sfr_seg_parameters() will be called     from within model_ws at runtime.      Parameters     ----------         nam_file : str             MODFLOw name file.  DIS, BAS, and SFR must be available as pathed in the nam_file         model_ws : str             model workspace for flopy to load the MODFLOW model from         OR         nam_file : flopy.modflow.mf.Modflow             flopy modflow model object         par_cols : list(str)             segment data entires to parameterize         tie_hcond : bool             flag to use same mult par for hcond1 and hcond2 for a given segment.  Default is True         include_temporal_pars : bool             flag to include spatially-global multipliers for each par_col for each stress period.  Default is False      Returns     -------         df : pandas.DataFrame             a dataframe with useful parameter setup information      Note     ----         the number (and numbering) of segment data entries must consistent across         all stress periods.         writes <nam_file>+"_backup_.sfr" as the backup of the original sfr file         skips values = 0.0 since multipliers don't work for these
Setup multiplier paramters for reach data, when reachinput option is specififed in sfr.     Similare to setup_sfr_seg_parameters() method will apply params to sfr reachdata     Can load the dis, bas, and sfr files with flopy using model_ws. Or can pass a model object (SFR loading can be slow)     However, expects that apply_sfr_reach_parameters() will be called     from within model_ws at runtime.      Parameters     ----------         nam_file : str             MODFLOw name file.  DIS, BAS, and SFR must be available as pathed in the nam_file         model_ws : str             model workspace for flopy to load the MODFLOW model from         OR         nam_file : flopy.modflow.mf.Modflow             flopy modflow model object          par_cols : list(str)             segment data entires to parameterize      Returns     -------         df : pandas.DataFrame             a dataframe with useful parameter setup information      Note     ----         skips values = 0.0 since multipliers don't work for these
apply the SFR segement multiplier parameters.  Expected to be run in the same dir     as the model exists      Parameters     ----------         reach_pars : bool             if reach paramters need to be applied      Returns     -------         sfr : flopy.modflow.ModflowSfr instance      Note     ----         expects "sfr_seg_pars.config" to exist         expects <nam_file>+"_backup_.sfr" to exist
setup observations using the sfr ASCII output file.  Setups     the ability to aggregate flows for groups of segments.  Applies     only flow to aquier and flow out.      Parameters     ----------     sft_out_file : str         the existing SFR output file     seg_group_dict : dict         a dictionary of SFR segements to aggregate together for a single obs.         the key value in the dict is the base observation name. If None, all segments         are used as individual observations. Default is None     model : flopy.mbase         a flopy model.  If passed, the observation names will have the datetime of the         observation appended to them.  If None, the observation names will have the         stress period appended to them. Default is None.     include_path : bool         flag to prepend sfr_out_file path to sfr_obs.config.  Useful for setting up         process in separate directory for where python is running.       Returns     -------     df : pd.DataFrame         dataframe of obsnme, obsval and obgnme if inschek run was successful.  Else None      Note     ----     This function writes "sfr_obs.config" which must be kept in the dir where     "apply_sfr_obs()" is being called during the forward run
apply the sfr observation process - pairs with setup_sfr_obs().     requires sfr_obs.config.  Writes <sfr_out_file>.processed, where     <sfr_out_file> is defined in "sfr_obs.config"       Parameters     ----------     None      Returns     -------     df : pd.DataFrame         a dataframe of aggregrated sfr segment aquifer and outflow
load an ASCII SFR output file into a dictionary of kper: dataframes.  aggregates     flow to aquifer for segments and returns and flow out at downstream end of segment.      Parameters     ----------     sfr_out_file : str         SFR ASCII output file      Returns     -------         sfr_dict : dict             dictionary of {kper:dataframe}
setup observations using the sfr ASCII output file.  Setups     sfr point observations using segment and reach numbers.      Parameters     ----------     sft_out_file : str         the existing SFR output file     seg_reach : dict, list or pandas.DataFrame         a dict, or list of SFR [segment,reach] pairs identifying observation locations.         If dict the key value in the dict is the base observation name.         If None, all reaches are used as individual observations. Default is None - THIS MAY SET UP A LOT OF OBS!     model : flopy.mbase         a flopy model.  If passed, the observation names will have the datetime of the         observation appended to them.  If None, the observation names will have the         stress period appended to them. Default is None.     include_path : bool         flag to prepend sfr_out_file path to sfr_obs.config.  Useful for setting up         process in separate directory for where python is running.       Returns     -------     df : pd.DataFrame         dataframe of obsnme, obsval and obgnme if inschek run was successful.  Else None      Note     ----     This function writes "sfr_reach_obs.config" which must be kept in the dir where     "apply_sfr_reach_obs()" is being called during the forward run
apply the sfr reach observation process - pairs with setup_sfr_reach_obs().     requires sfr_reach_obs.config.  Writes <sfr_out_file>.processed, where     <sfr_out_file> is defined in "sfr_reach_obs.config"       Parameters     ----------     None      Returns     -------     df : pd.DataFrame         a dataframe of sfr aquifer and outflow ad segment,reach locations
writes an instruction file for an SFR gage output file to read Flow only at all times          Parameters         ----------             gage_output_file : str                 the gage output filename (ASCII).              ins_file : str                 the name of the instruction file to create.  If None, the name                 is <gage_output_file>.ins.  Default is None              parse_filename : bool                 if True, get the gage_num parameter by parsing the gage output file filename                 if False, get the gage number from the file itself          Returns         -------             df : pandas.DataFrame                 a dataframe with obsnme and obsval for the sfr simulated flows.                 If inschek was not successfully run, then returns None             ins_file : str                 file name of instructions file relating to gage output.             obs_file : str                 file name of processed gage output for all times           Note         ----             sets up observations for gage outputs only for the Flow column.              if parse_namefile is true, only text up to first '.' is used as the gage_num          TODO : allow other observation types and align explicitly with times - now returns all values
writes an instruction file for a mt3d-usgs sft output file      Parameters     ----------         gage_file : str             the gage output file (ASCII)         ins_file : str             the name of the instruction file to create.  If None, the name             is <gage_file>.processed.ins.  Default is None         start_datetime : str             a pandas.to_datetime() compatible str.  If not None,             then the resulting observation names have the datetime             suffix.  If None, the suffix is the output totim.  Default             is None         times : iterable             a container of times to make observations for.  If None, all times are used.             Default is None.       Returns     -------         df : pandas.DataFrame             a dataframe with obsnme and obsval for the sft simulated concentrations and flows.             If inschek was not successfully run, then returns None         ins_file : str             file name of instructions file relating to gage output.         obs_file : str             file name of processed gage output (processed according to times passed above.)       Note     ----         setups up observations for gage outputs (all columns).
write a template file for an hfb using multipliers per zone (double yuck!)      Parameters     ----------         m : flopy.modflow.Modflow instance with an HFB file      Returns     -------         (hfb_mults, tpl_filename) : (dict, str)             a dictionary with original unique HFB conductivity values and their corresponding parameter names             and the name of the template file
write a template file for an hfb (yuck!)      Parameters     ----------         m : flopy.modflow.Modflow instance with an HFB file      Returns     -------         (tpl_filename, df) : (str, pandas.DataFrame)             the name of the template file and a dataframe with useful info.
get a pandas dataframe of prior and posterior for all predictions          Returns:             pandas.DataFrame : pandas.DataFrame                 a dataframe with prior and posterior uncertainty estimates                 for all forecasts (predictions)
get the posterior parameter covariance matrix.  If Schur.__posterior_parameter         is None, the posterior parameter covariance matrix is calculated via         Schur compliment before returning          Returns         -------         posterior_parameter : pyemu.Cov             the posterior parameter covariance matrix          Note         ----         returns a reference          Example         -------         ``>>>import pyemu``          ``>>>sc = pyemu.Schur(jco="pest.jcb")``          ``>>>post_cov = sc.posterior_parameter``          ``>>>post_cov.to_ascii("post.cov")``
get the posterior expectation for parameters using Bayes linear         estimation          Returns         -------         post_expt : pandas.DataFrame             a dataframe with prior and posterior parameter expectations
get the prior and posterior forecast (prediction) expectations.          Returns         -------         pandas.DataFrame : pandas.DataFrame             dataframe with prior and posterior forecast expected values
get posterior forecast (prediction) variances          Returns         -------         dict : dict             a dictionary of forecast names, posterior variance pairs          Note         ----         This method is not as easy to use as Schur.get_forecast_summary(), please         use it instead
get a summary of the parameter uncertainty          Parameters         ----------         include_map : bool             if True, add the prior and posterior expectations             and report standard deviation instead of variance          Returns         -------         pandas.DataFrame : pandas.DataFrame             dataframe of prior,posterior variances and percent             uncertainty reduction of each parameter          Note         ----         this is the primary method for accessing parameter uncertainty         estimates - use this!          Example         -------         ``>>>import matplotlib.pyplot as plt``          ``>>>import pyemu``          ``>>>sc = pyemu.Schur(jco="pest.jcb")``          ``>>>sc = pyemu.Schur(jco="pest.jcb",forecasts=["fore1","fore2"])``          ``>>>par_sum = sc.get_parameter_summary()``          ``>>>par_sum.plot(kind="bar")``          ``>>>plt.show()``
get a summary of the forecast uncertainty          Parameters         ----------         include_map : bool             if True, add the prior and posterior expectations             and report standard deviation instead of variance          Returns         -------         pandas.DataFrame : pandas.DataFrame                 dataframe of prior,posterior variances and percent                 uncertainty reduction of each parameter          Note         ----         this is the primary method for accessing forecast uncertainty         estimates - use this!          Example         -------         ``>>>import matplotlib.pyplot as plt``          ``>>>import pyemu``          This usage assumes you have set the ``++forecasts()`` argument in the         control file:          ``>>>sc = pyemu.Schur(jco="pest.jcb")``          or, you can pass the forecasts directly, assuming the forecasts are         names of zero-weight observations:          ``>>>sc = pyemu.Schur(jco="pest.jcb",forecasts=["fore1","fore2"])``          ``>>>fore_sum = sc.get_forecast_summary()``          ``>>>fore_sum.plot(kind="bar")``          ``>>>plt.show()``
private method get the prior and posterior uncertainty reduction as a result of         some parameter becoming perfectly known          Parameters         ----------         parameter_names : list             parameter that are perfectly known          Returns         -------         dict : dict             dictionary of forecast name,  [prior uncertainty w/o parameter_names,                 % posterior uncertainty w/o parameter names]          Note         ----         this method is used by get_parameter_contribution() method - don't         call this method directly
get a new Schur instance that includes conditional update from         some parameters becoming known perfectly          Parameters         ----------         parameter_names : list             parameters that are to be treated as notionally perfectly             known          Returns         -------         la_cond : Schur             a new Schur instance conditional on perfect knowledge             of some parameters          Note         ----         this method is used by the get_parameter_contribution() method -         don't call this method directly
get a dataframe the prior and posterior uncertainty         reduction as a result of some parameter becoming perfectly known          Parameters         ----------         parlist_dict : dict             a nested dictionary-list of groups of parameters             that are to be treated as perfectly known.  key values become             row labels in returned dataframe.  If None, each adjustable parameter             is sequentially treated as known and the returned dataframe             has row labels for each adjustable parameter         include_prior_results : bool             flag to return a multi-indexed dataframe with both conditional             prior and posterior forecast uncertainty estimates.  Default is False          Returns         -------         pandas.DataFrame : pandas.DataFrame             a dataframe that summarizes the parameter contribution analysis.             The dataframe has index (row labels) of the keys in parlist_dict             and a column labels of forecast names.  The values in the dataframe             are the posterior variance of the forecast conditional on perfect             knowledge of the parameters in the values of parlist_dict.  Varies             depending on `include_prior_results`.          Example         -------         ``>>>import pyemu``          ``>>>sc = pyemu.Schur(jco="pest.jcb")``          ``>>>df = sc.get_par_contribution()``
get the forecast uncertainty contribution from each parameter         group.  Just some sugar for get_contribution_dataframe() - this method         automatically constructs the parlist_dict argument where the keys are the         group names and the values are the adjustable parameters in the groups          Parameters         ----------         include_prior_results : bool             flag to return a multi-indexed dataframe with both conditional             prior and posterior forecast uncertainty estimates.  Default is False           Returns         -------         pandas.DataFrame : pandas.DataFrame             a dataframe that summarizes the parameter contribution analysis.             The dataframe has index (row labels) that are the parameter groups             and a column labels of forecast names.  The values in the dataframe             are the posterior variance of the forecast conditional on perfect             knowledge of the adjustable parameters in each parameter groups             Varies depending on `include_prior_results`.
get a dataframe fo the posterior uncertainty         as a results of added some observations          Parameters         ----------         obslist_dict : dict             a nested dictionary-list of groups of observations             that are to be treated as gained.  key values become             row labels in returned dataframe. If None, then every zero-weighted             observation is tested sequentially. Default is None         base_obslist : list             observation names to treat as the "existing" observations.             The values of obslist_dict will be added to this list during             each test.  If None, then the values in obslist_dict will             be treated as the entire calibration dataset.  That is, there             are no existing data. Default is None.  Standard practice would             be to pass this argument as Schur.pst.nnz_obs_names.         reset_zero_weight : (boolean or float)             a flag to reset observations with zero weight in either             obslist_dict or base_obslist. The value of reset_zero_weights             can be cast to a float,then that value will be assigned to             zero weight obs.  Otherwise, zero weight obs will be given a             weight of 1.0.  Default is False.          Returns         -------         pandas.DataFrame : pandas.DataFrame             dataframe with row labels (index) of obslist_dict.keys() and             columns of forecast_name.  The values in the dataframe are the             posterior variance of the forecasts resulting from notional inversion             using the observations in obslist_dict[key value] plus the observations             in base_obslist (if any)          Note         ----         all observations listed in obslist_dict and base_obslist with zero         weights will be dropped unless reset_zero_weight is set          Example         -------         ``>>>import pyemu``          ``>>>sc = pyemu.Schur(jco="pest.jcb")``          ``>>>df = sc.get_added_obs_importance(base_obslist=sc.pst.nnz_obs_names,reset_zero=True)``
get a dataframe the posterior uncertainty         as a result of losing some observations          Parameters         ----------         obslist_dict : dict             dictionary of groups of observations             that are to be treated as lost.  key values become             row labels in returned dataframe. If None, then test every             (nonzero weight - see reset_zero_weight) observation         reset_zero_weight : bool or float             a flag to reset observations with zero weight in obslist_dict.             If the value of reset_zero_weights can be cast to a float,             then that value will be assigned to zero weight obs.  Otherwise,             zero weight obs will be given a weight of 1.0          Returns         -------         pandas.DataFrame : pandas.DataFrame             a dataframe with index of obslist_dict.keys() and columns             of forecast names.  The values in the dataframe are the posterior             variances of the forecasts resulting from losing the information             contained in obslist_dict[key value]          Note         ----         all observations listed in obslist_dict with zero         weights will be dropped unless reset_zero_weight is set          Example         -------         ``>>>import pyemu``          ``>>>sc = pyemu.Schur(jco="pest.jcb")``          ``df = sc.get_removed_obs_importance()``
find the most important observation(s) by sequentially evaluating         the importance of the observations in obslist_dict. The most important observations         from each iteration is added to base_obslist and removed obslist_dict for the         next iteration.  In this way, the added observation importance values include         the conditional information from the last iteration.          Parameters         ----------         forecast : str             name of the forecast to use in the ranking process.  If             more than one forecast has been listed, this argument is required         niter : int             number of sequential iterations         obslist_dict dict:             nested dictionary-list of  groups of observations             that are to be treated as gained.  key values become             row labels in result dataframe. If None, then test every observation             individually         base_obslist : list             observation names to treat as the "existing" observations.             The values of obslist_dict will be added to this list during testing.             If None, then each list in the values of obslist_dict will be             treated as an individual calibration dataset.         reset_zero_weight : (boolean or float)             a flag to reset observations with zero weight in either             obslist_dict or base_obslist. If the value of reset_zero_weights             can be cast to a float,then that value will be assigned to             zero weight obs.  Otherwise, zero weight obs will be given a weight of 1.0          Returns         -------         pandas.DataFrame : pandas.DataFrame             DataFrame with columns of best obslist_dict key for each iteration.             Columns of forecast variance percent reduction for this iteration,             (percent reduction compared to initial base case)           Example         -------         ``>>>import pyemu``          ``>>>sc = pyemu.Schur(jco="pest.jcb")``          ``>>>df = sc.next_most_added_importance_obs(forecast="fore1",``          ``>>>      base_obslist=sc.pst.nnz_obs_names,reset_zero=True``
find the largest parameter(s) contribution for prior and posterior         forecast  by sequentially evaluating the contribution of parameters in         parlist_dict.  The largest contributing parameters from each iteration are         treated as known perfectly for the remaining iterations.  In this way, the         next iteration seeks the next most influential group of parameters.          Parameters         ----------         forecast : str             name of the forecast to use in the ranking process.  If             more than one forecast has been listed, this argument is required         parlist_dict : dict             a nested dictionary-list of groups of parameters             that are to be treated as perfectly known.  key values become             row labels in dataframe          Returns         -------         pandas.DataFrame : pandas.DataFrame             a dataframe with index of iteration number and columns             of parlist_dict keys.  The values are the results of the knowing             each parlist_dict entry expressed as posterior variance reduction
private: set the omitted_predictions attribute
private: set the omitted_parcov attribute
private: set the omitted jco attribute
get the omitted prediction sensitivity vectors (stored as         a pyemu.Matrix)          Returns         -------         omitted_predictions : pyemu.Matrix             a matrix of prediction sensitivity vectors (column wise) to             omitted parameters          Note         ----         returns a reference                  if ErrorVariance.__omitted_predictions is not set, then dynamically load the         attribute before returning
get the omitted jco          Returns         -------         omitted_jco : pyemu.Jco          Note         ----         returns a reference                  if ErrorVariance.__omitted_jco is None,         then dynamically load the attribute before returning
get the omitted prior parameter covariance matrix          Returns         -------         omitted_parcov : pyemu.Cov          Note         ----         returns a reference                  If ErrorVariance.__omitted_parcov is None,         attribute is dynamically loaded
get a pandas dataframe of error variance results indexed             on singular value and (prediction name,<errvar term>)          Parameters         ----------         singular_values : list             singular values to test.  defaults to             range(0,min(nnz_obs,nadj_par) + 1)          Returns         -------         pandas.DataFrame : pandas.DataFrame             multi-indexed pandas dataframe
get the parameter identifiability as a pandas dataframe          Parameters         ----------         singular_value : int             the singular spectrum truncation point. Defaults to minimum of             non-zero-weighted observations and adjustable parameters         precondition : bool             flag to use the preconditioned hessian (xtqt + sigma_theta^-1).             Default is False          Returns         -------         pandas.DataFrame : pandas.DataFrame             A pandas dataframe of the V_1**2 Matrix with the             identifiability in the column labeled "ident"
get the error variance of all three terms at a singluar value          Parameters         ----------         singular_value : int             singular value to test          Returns         -------         dict : dict             dictionary of (err var term,prediction_name), standard_deviation pairs
get resolution Matrix (V_1 * V_1^T) at a singular value          Parameters         ----------         singular_value : int             singular value to calc R at          Returns         -------         R : pyemu.Matrix             resolution matrix at singular_value
get I - R at singular value          Parameters         ----------         singular_value : int             singular value to calc R at          Returns         -------         I - R : pyemu.Matrix             identity matrix minus resolution matrix at singular_value
get the parameter solution Matrix at a singular value             V_1 * S_1^(_1) * U_1^T          Parameters         ----------         singular_value : int             singular value to calc R at          Returns         -------         G : pyemu.Matrix             parameter solution matrix at singular value
get the null space term (first term) contribution to prediction error variance             at a singular value.  used to construct error variance dataframe          Parameters         ----------         singular_value : int             singular value to calc first term at          Returns         -------         dict : dict             dictionary of ("first",prediction_names),error variance pairs at singular_value
get the null space term (first term) contribution to parameter error variance             at a singular value          Parameters         ----------         singular_value : int             singular value to calc first term at          Returns         -------         first_term : pyemu.Cov             first term contribution to parameter error variance
get the solution space contribution to predictive error variance             at a singular value (y^t * G * obscov * G^T * y).  Used to construct             error variance dataframe          Parameters         ----------         singular_value : int             singular value to calc second term at          Returns         -------         dict : dict             dictionary of ("second",prediction_names), error variance
get the solution space contribution to parameter error variance              at a singular value (G * obscov * G^T)          Parameters         ----------         singular_value : int             singular value to calc second term at          Returns         -------         second_parameter : pyemu.Cov             second term contribution to parameter error variance
get the omitted parameter contribution to prediction error variance          at a singular value. used to construct error variance dataframe          Parameters         ----------         singular_value : int             singular value to calc third term at          Returns         -------         dict : dict             dictionary of ("third",prediction_names),error variance
get the omitted parameter contribution to parameter error variance              at a singular value (G * omitted_jco * Sigma_(omitted_pars) * omitted_jco^T * G^T)          Parameters         ----------         singular_value : int             singular value to calc third term at          Returns         -------         third_parameter : pyemu.Cov             0.0 if need_omitted is False
Initialize the iES process.  Depending on arguments, draws or loads         initial parameter observations ensembles and runs the initial parameter         ensemble          Parameters         ----------             num_reals : int                 the number of realizations to draw.  Ignored if parensemble/obsensemble                 are not None             init_lambda : float                 the initial lambda to use.  During subsequent updates, the lambda is                 updated according to upgrade success             enforce_bounds : str                 how to enfore parameter bound transgression.  options are                 reset, drop, or None             parensemble : pyemu.ParameterEnsemble or str                 a parameter ensemble or filename to use as the initial                 parameter ensemble.  If not None, then obsenemble must not be                 None             obsensemble : pyemu.ObservationEnsemble or str                 an observation ensemble or filename to use as the initial                 observation ensemble.  If not None, then parensemble must                 not be None             restart_obsensemble : pyemu.ObservationEnsemble or str                 an observation ensemble or filename to use as an                 evaluated observation ensemble.  If not None, this will skip the initial                 parameter ensemble evaluation - user beware!             regul_factor : float                 the regularization penalty fraction of the composite objective.  The                 Prurist, MAP solution would be regul_factor = 1.0, yielding equal                 parts measurement and regularization to the composite objective function.                 Default is 0.0, which means only seek to minimize the measurement objective                 function             use_approx_prior : bool                 a flag to use the inverse, square root of the prior ccovariance matrix                 for scaling the upgrade calculation.  If True, this matrix is not used.                 Default is True             build_empirical_prior : bool                 flag to build the prior parameter covariance matrix from an existing parensemble.                 If True and parensemble is None, an exception is raised           Example         -------         ``>>>import pyemu``          ``>>>es = pyemu.EnsembleSmoother(pst="pest.pst")``          ``>>>es.initialize(num_reals=100)``
get an empty/generic localizer matrix that can be filled          Returns         -------             localizer : pyemu.Matrix                 matrix with nnz obs names for rows and adj par names for columns
update the iES one GLM cycle          Parameters         ----------             lambda_mults : list                 a list of lambda multipliers to test.  Each lambda mult value will require                 evaluating (a subset of) the parameter ensemble.             localizer : pyemu.Matrix                 a jacobian localizing matrix             run_subset : int                 the number of realizations to test for each lambda_mult value.  For example,                 if run_subset = 30 and num_reals=100, the first 30 realizations will be run (in                 parallel) for each lambda_mult value.  Then the best lambda_mult is selected and the                 remaining 70 realizations for that lambda_mult value are run (in parallel).             use_approx : bool                  a flag to use the MLE or MAP upgrade solution.  True indicates use MLE solution             calc_only : bool                 a flag to calculate the upgrade matrix only (not run the ensemble). This is mostly for                 debugging and testing on travis. Default is False          Example         -------          ``>>>import pyemu``          ``>>>es = pyemu.EnsembleSmoother(pst="pest.pst")``          ``>>>es.initialize(num_reals=100)``          ``>>>es.update(lambda_mults=[0.1,1.0,10.0],run_subset=30)``
setup grid-based pilot points.  Uses the ibound to determine     where to set pilot points. pilot points are given generic ``pp_``     names unless ``prefix_dict`` is passed.  Write template files and a     shapefile as well...hopefully this is useful to someone...      Parameters     ----------     ml : flopy.mbase         a flopy mbase dervied type.  If None, sr must not be None.     sr : flopy.utils.reference.SpatialReference         a spatial reference use to locate the model grid in space.  If None,         ml must not be None.  Default is None     ibound : numpy.ndarray         the modflow ibound integer array.  Used to set pilot points only in active areas (!=0).         If None and ml is None, then pilot points are set in all rows and columns according to         every_n_cell.  Default is None.     prefix_dict : dict         a dictionary of layer index, pilot point parameter prefixes.         Example : ``{0:["hk_"],1:["vka_"]}`` would setup pilot points with         the prefix ``"hk"`` for model layers 1 and ``"vka"`` for model layer 2 (zero based).         If None, a generic set of pilot points with the "pp" prefix are setup         for a generic nrowXncol grid. Default is None.     use_ibound_zones : bool         a flag to use the greater-than-zero values in the ibound as pilot point zones.  If False,ibound         values greater than zero are treated as a single zone.  Default is False.     pp_dir : str         directory to write pilot point files to.  Default is '.'     tpl_dir : str         directory to write pilot point template file to.  Default is '.'     shapename : str         name of shapefile to write that contains pilot point information. Default is "pp.shp"      Returns     -------     pp_df : pandas.DataFrame         a dataframe summarizing pilot point information (same information         written to shapename     Example     -------     ``>>>import flopy``      ``>>>from pyemu.utils import setup_pilotpoints_grid``      ``>>>m = flopy.modflow.Modfow.load("mymodel.nam")``      ``>>>setup_pilotpoints_grid(m,prefix_dict={0:['hk_'],1:['vka_']},``      ``>>>                       every_n_cell=3,shapename='layer1_pp.shp')``
read a pilot point file to a pandas Dataframe      Parameters     ----------     pp_filename : str         pilot point file      Returns     -------     df : pandas.DataFrame         a dataframe with pp_utils.PP_NAMES for columns
read a pilot points template file to a pandas dataframe      Parameters     ----------     tpl_filename : str         pilot points template file      Returns     -------     df : pandas.DataFrame         a dataframe with "parnme" included
write pilot points dataframe to a shapefile      Parameters     ----------     pp_df : pandas.DataFrame or str         pilot point dataframe or a pilot point filename.  Dataframe         must include "x" and "y"     shapename : str         shapefile name.  If None, pp_df must be str and shapefile         is saved as <pp_df>.shp      Note     ----         requires pyshp
write a pilot points dataframe to a pilot points file      Parameters     ----------     filename : str         pilot points file to write     pp_df : pandas.DataFrame         a dataframe that has columns "x","y","zone", and "value"
write a template file for a pilot points file      Parameters     ----------     pp_file : str         pilot points file     tpl_file : str         template file name to write.  If None, append ".tpl" to         the pp_file arg. Default is None     name_prefix : str         name to prepend to parameter names for each pilot point.  For example,         if ``name_prefix = "hk_"``, then each pilot point parameter will be named         "hk_0001","hk_0002", etc.  If None, parameter names from pp_df.name         are used.  Default is None.      Returns     -------         pp_df : pandas.DataFrame             a dataframe with pilot point information (name,x,y,zone,parval1)              with the parameter information (parnme,tpl_str)
an OS agnostic function to execute command      Parameters     ----------     cmd_str : str         the str to execute with os.system()      cwd : str         the directory to execute the command in      verbose : bool         flag to echo to stdout complete cmd str      Note     ----     uses platform to detect OS and adds .exe or ./ as appropriate      for Windows, if os.system returns non-zero, raises exception      Example     -------     ``>>>import pyemu``      ``>>>pyemu.helpers.run("pestpp pest.pst")``
run fieldgen and return a dataframe with the realizations      Parameters     ----------     m : flopy.mbase         a floy model instance     num_reals : int         number of realizations to generate     struct_dict : dict         key-value pairs of pyemu.GeoStruct instances and lists of prefix strings.  Example: {gs1:['hk','ss']}     cwd : str         working director where to execute fieldgen.  If None, m.model_ws is used.  Default is None      Returns     -------     df : pandas.DataFrame         a dataframe of realizations.  Columns are named to include structure, prefix and realization number.  Index         includes i-j position      Note     ----     only Ordinary kriging is supported     only a single zone is supported
a helper function to construct a parameter ensenble from a full prior covariance matrix     implied by the geostatistical structure(s) in struct_dict.  This function is much more efficient     for problems with lots of pars (>200K).      Parameters     ----------     pst : pyemu.Pst         a control file (or the name of control file)     struct_dict : dict         a python dict of GeoStruct (or structure file), and list of pp tpl files pairs         If the values in the dict are pd.DataFrames, then they must have an         'x','y', and 'parnme' column.  If the filename ends in '.csv',         then a pd.DataFrame is loaded, otherwise a pilot points file is loaded.     num_reals : int         number of realizations to draw.  Default is 100     sigma_range : float         a float representing the number of standard deviations implied by parameter bounds.         Default is 4.0, which implies 95% confidence parameter bounds.     verbose : bool         flag for stdout.      Returns     -------      par_ens : pyemu.ParameterEnsemble       Example     -------     ``>>>import pyemu``      ``>>>pst = pyemu.Pst("pest.pst")``      ``>>>sd = {"struct.dat":["hkpp.dat.tpl","vka.dat.tpl"]}``      ``>>>pe = pyemu.helpers.geostatistical_draws(pst,struct_dict=sd,num_reals=100)``      ``>>>pe.to_csv("par_ensemble.csv")``
a helper function to construct a full prior covariance matrix using     a mixture of geostastical structures and parameter bounds information.     The covariance of parameters associated with geostatistical structures is defined     as a mixture of GeoStruct and bounds.  That is, the GeoStruct is used to construct a     pyemu.Cov, then the entire pyemu.Cov is scaled by the uncertainty implied by the bounds and     sigma_range. Sounds complicated...      Parameters     ----------     pst : pyemu.Pst         a control file (or the name of control file)     struct_dict : dict         a python dict of GeoStruct (or structure file), and list of pp tpl files pairs         If the values in the dict are pd.DataFrames, then they must have an         'x','y', and 'parnme' column.  If the filename ends in '.csv',         then a pd.DataFrame is loaded, otherwise a pilot points file is loaded.     sigma_range : float         a float representing the number of standard deviations implied by parameter bounds.         Default is 4.0, which implies 95% confidence parameter bounds.     verbose : bool         flag for stdout.      Returns     -------     Cov : pyemu.SparseMatrix         a sparse covariance matrix that includes all adjustable parameters in the control         file.      Example     -------     ``>>>import pyemu``      ``>>>pst = pyemu.Pst("pest.pst")``      ``>>>sd = {"struct.dat":["hkpp.dat.tpl","vka.dat.tpl"]}``      ``>>>cov = pyemu.helpers.sparse_geostatistical_prior_builder(pst,struct_dict=sd)``      ``>>>cov.to_coo("prior.jcb")``
a helper function to construct a full prior covariance matrix using     a mixture of geostastical structures and parameter bounds information.     The covariance of parameters associated with geostatistical structures is defined     as a mixture of GeoStruct and bounds.  That is, the GeoStruct is used to construct a     pyemu.Cov, then the entire pyemu.Cov is scaled by the uncertainty implied by the bounds and     sigma_range. Sounds complicated...      Parameters     ----------     pst : pyemu.Pst         a control file (or the name of control file)     struct_dict : dict         a python dict of GeoStruct (or structure file), and list of pp tpl files pairs         If the values in the dict are pd.DataFrames, then they must have an         'x','y', and 'parnme' column.  If the filename ends in '.csv',         then a pd.DataFrame is loaded, otherwise a pilot points file is loaded.     sigma_range : float         a float representing the number of standard deviations implied by parameter bounds.         Default is 4.0, which implies 95% confidence parameter bounds.     par_knowledge_dict : dict         used to condition on existing knowledge about parameters.  This functionality is         currently in dev - don't use it.     verbose : bool         stdout flag     Returns     -------     Cov : pyemu.Cov         a covariance matrix that includes all adjustable parameters in the control         file.      Example     -------     ``>>>import pyemu``      ``>>>pst = pyemu.Pst("pest.pst")``      ``>>>sd = {"struct.dat":["hkpp.dat.tpl","vka.dat.tpl"]}``      ``>>>cov = pyemu.helpers.geostatistical_prior_builder(pst,struct_dict=sd)``      ``>>>cov.to_ascii("prior.cov")``
experimental function to include conditional prior information     for one or more parameters in a full covariance matrix
setup a karhuenen-Loeve based parameterization for a given     geostatistical structure.      Parameters     ----------     num_eig : int         number of basis vectors to retain in the reduced basis     sr : flopy.reference.SpatialReference      struct : str or pyemu.geostats.Geostruct         geostatistical structure (or file containing one)      array_dict : dict         a dict of arrays to setup as KL-based parameters.  The key becomes the         parameter name prefix. The total number of parameters is         len(array_dict) * num_eig      basis_file : str         the name of the PEST-format binary file where the reduced basis will be saved      tpl_file : str         the name of the template file to make.  The template         file is a csv file with the parameter names, the         original factor values,and the template entries.         The original values can be used to set the parval1         entries in the control file      Returns     -------     back_array_dict : dict         a dictionary of back transformed arrays.  This is useful to see         how much "smoothing" is taking place compared to the original         arrays.      Note     ----     requires flopy      Example     -------     ``>>>import flopy``      ``>>>import pyemu``      ``>>>m = flopy.modflow.Modflow.load("mymodel.nam")``      ``>>>a_dict = {"hk":m.lpf.hk[0].array}``      ``>>>ba_dict = pyemu.helpers.kl_setup(10,m.sr,"struct.dat",a_dict)``
Applies a KL parameterization transform from basis factors to model     input arrays.  Companion function to kl_setup()      Parameters     ----------     par_file : str         the csv file to get factor values from.  Must contain         the following columns: name, new_val, org_val     basis_file : str         the binary file that contains the reduced basis      par_to_file_dict : dict         a mapping from KL parameter prefixes to array file names.      Note     ----     This is the companion function to kl_setup.      This function should be called during the forward run      Example     -------     ``>>>import pyemu``      ``>>>pyemu.helpers.kl_apply("kl.dat","basis.dat",{"hk":"hk_layer_1.dat",(100,100))``
setup preferred-value regularization      Parameters     ----------     pst : pyemu.Pst         the control file instance     parbounds : bool         flag to weight the prior information equations according         to parameter bound width - approx the KL transform. Default         is True     par_groups : list         parameter groups to build PI equations for.  If None, all         adjustable parameters are used. Default is None      reset : bool         flag to reset the prior_information attribute of the pst         instance.  Default is True      Example     -------     ``>>>import pyemu``      ``>>>pst = pyemu.Pst("pest.pst")``      ``>>>pyemu.helpers.zero_order_tikhonov(pst)``
sets regularization weights from parameter bounds     which approximates the KL expansion.  Called by     zero_order_tikhonov().      Parameters     ----------     pst : pyemu.Pst         a control file instance
setup preferred-difference regularization from a covariance matrix.     The weights on the prior information equations are the Pearson     correlation coefficients implied by covariance matrix.      Parameters     ----------     pst : pyemu.Pst         pst instance     cov : pyemu.Cov         covariance matrix instance     reset : bool         drop all other pi equations.  If False, append to         existing pi equations     abs_drop_tol : float         tolerance to control how many pi equations are written.         If the Pearson C is less than abs_drop_tol, the prior information         equation will not be included in the control file      Example     -------     ``>>>import pyemu``      ``>>>pst = pyemu.Pst("pest.pst")``      ``>>>cov = pyemu.Cov.from_ascii("prior.cov")``      ``>>>pyemu.helpers.first_order_pearson_tikhonov(pst,cov,abs_drop_tol=0.25)``
Make a template file just assuming a list of parameter names the values of which should be     listed in order in a model input file     Args:         parnames: list of names from which to make a template file         tplfilename: filename for TPL file (default: model.input.tpl)      Returns:         writes a file <tplfilename> with each parameter name on a line
writes an instruction file that assumes wanting to read the values names in obsnames in order     one per line from a model output file     Args:         obsnames: list of obsnames to read in         insfilename: filename for INS file (default: model.output.ins)      Returns:         writes a file <insfilename> with each observation read off a line
Creates a Pst object from a list of parameter names and a list of observation names.     Default values are provided for the TPL and INS     Args:         parnames: list of names from which to make a template file         obsnames: list of obsnames to read in         tplfilename: filename for TPL file (default: model.input.tpl)         insfilename: filename for INS file (default: model.output.ins)      Returns:         Pst object
start a group of pest(++) slaves on the local machine      Parameters     ----------     slave_dir :  str         the path to a complete set of input files     exe_rel_path : str         the relative path to the pest(++) executable from within the slave_dir     pst_rel_path : str         the relative path to the pst file from within the slave_dir     num_slaves : int         number of slaves to start. defaults to number of cores     slave_root : str         the root to make the new slave directories in     rel_path: str         the relative path to where pest(++) should be run from within the         slave_dir, defaults to the uppermost level of the slave dir     local: bool         flag for using "localhost" instead of hostname on slave command line     cleanup: bool         flag to remove slave directories once processes exit     master_dir: str         name of directory for master instance.  If master_dir         exists, then it will be removed.  If master_dir is None,         no master instance will be started     verbose : bool         flag to echo useful information to stdout      Note     ----     if all slaves (and optionally master) exit gracefully, then the slave     dirs will be removed unless cleanup is false      Example     -------     ``>>>import pyemu``      start 10 slaves using the directory "template" as the base case and     also start a master instance in a directory "master".      ``>>>pyemu.helpers.start_slaves("template","pestpp","pest.pst",10,master_dir="master")``
read pars and obs from a specific run in a pest++ serialized run storage file into     pandas.DataFrame(s)      Parameters     ----------     filename : str         the name of the run storage file     irun : int         the run id to process. If 'all', then all runs are read. Default is 0     with_metadata : bool         flag to return run stats and info txt as well      Returns     -------     par_df : pandas.DataFrame         parameter information     obs_df : pandas.DataFrame         observation information     metadata : pandas.DataFrame         run status and info txt.
read pars and obs from a pest++ serialized run storage file (e.g., .rnj) and return      pyemu.Jco.  This can then be passed to Jco.to_binary or Jco.to_coo, etc., to write jco file     in a subsequent step to avoid memory resource issues associated with very large problems.      Parameters     ----------     rnj_filename : str         the name of the run storage file     pst_filename : str         the name of the pst file      Returns     -------     jco_cols : pyemu.Jco       TODO     ----     Check rnj file contains transformed par vals (i.e., in model input space)      Currently only returns pyemu.Jco; doesn't write jco file due to memory     issues associated with very large problems      Compare rnj and jco from Freyberg problem in autotests
a helper function to find template/input file pairs and     instruction file/output file pairs.  the return values from this     function can be passed straight to pyemu.Pst.from_io_files() classmethod     constructor. Assumes the template file names are <input_file>.tpl and     instruction file names are <output_file>.ins.      Parameters     ----------     d : str         directory to search for interface files      Returns     -------     tpl_files : list         list of template files in d     in_files : list         list of input files in d     ins_files : list         list of instruction files in d     out_files : list         list of output files in d
generate a Pst instance from the model io files.  If 'inschek'     is available (either in the current directory or registered     with the system variables) and the model output files are available     , then the observation values in the control file will be set to the     values of the model-simulated equivalents to observations.  This can be     useful for testing      Parameters     ----------     tpl_files : list         list of pest template files     in_files : list         list of corresponding model input files     ins_files : list         list of pest instruction files     out_files: list         list of corresponding model output files     pst_filename : str         name of file to write the control file to.  If None,         control file is not written.  Default is None      Returns     -------     pst : pyemu.Pst       Example     -------     ``>>>import pyemu``      this will construct a new Pst instance from template and instruction files     found in the current directory, assuming that the naming convention follows     that listed in parse_dir_for_io_files()      ``>>>pst = pyemu.helpers.pst_from_io_files(*pyemu.helpers.parse_dir_for_io_files('.'))``
a function to apply array-based multipler parameters.  Used to implement     the parameterization constructed by PstFromFlopyModel during a forward run      Parameters     ----------     arr_par_file : str     path to csv file detailing parameter array multipliers      Note     ----     "arr_pars.csv" - is written by PstFromFlopy      the function should be added to the forward_run.py script but can be called on any correctly formatted csv
a function to apply boundary condition multiplier parameters.  Used to implement     the parameterization constructed by PstFromFlopyModel during a forward run      Note     ----     requires either "temporal_list_pars.csv" or "spatial_list_pars.csv"      should be added to the forward_run.py script
a function to apply HFB multiplier parameters.  Used to implement     the parameterization constructed by write_hfb_zone_multipliers_template()      This is to account for the horrible HFB6 format that differs from other BCs making this a special case      Note     ----     requires "hfb_pars.csv"      should be added to the forward_run.py script
write a constant (uniform) template file      Parameters     ----------     name : str         the base parameter name     tpl_file : str         the template file to write - include path     zn_array : numpy.ndarray         an array used to skip inactive cells      Returns     -------     df : pandas.DataFrame         a dataframe with parameter information
write a grid-based template file     Parameters     ----------     name : str         the base parameter name     tpl_file : str         the template file to write - include path     zn_array : numpy.ndarray         an array used to skip inactive cells      Returns     -------     df : pandas.DataFrame         a dataframe with parameter information
helper function to plot gaussian distrbutions from prior and posterior     means and standard deviations      Parameters     ----------     df : pandas.DataFrame         a dataframe and csv file.  Must have columns named:         'prior_mean','prior_stdev','post_mean','post_stdev'.  If loaded         from a csv file, column 0 is assumed to tbe the index     ax: matplotlib.pyplot.axis         If None, and not subplots, then one is created         and all distributions are plotted on a single plot     label_post: bool         flag to add text labels to the peak of the posterior     label_prior: bool         flag to add text labels to the peak of the prior     subplots: (boolean)         flag to use subplots.  If True, then 6 axes per page         are used and a single prior and posterior is plotted on each     figsize: tuple         matplotlib figure size      Returns     -------     figs : list         list of figures     axes : list         list of axes      Note     ----     This is useful for demystifying FOSM results      if subplots is False, a single axis is returned      Example     -------     ``>>>import matplotlib.pyplot as plt``      ``>>>import pyemu``      ``>>>pyemu.helpers.plot_summary_distributions("pest.par.usum.csv")``      ``>>>plt.show()``
get an x and y numpy.ndarray that spans the +/- 4     standard deviation range of a gaussian distribution with     a given mean and standard deviation. useful for plotting      Parameters     ----------     mean : float         the mean of the distribution     stdev : float         the standard deviation of the distribution     num_pts : int         the number of points in the returned ndarrays.         Default is 50      Returns     -------     x : numpy.ndarray         the x-values of the distribution     y : numpy.ndarray         the y-values of the distribution
function write a pandas dataframe to a template file.     Parameters     ----------     filename : str         template filename     df : pandas.DataFrame         dataframe to write     sep : char         separate to pass to df.to_csv(). default is ','     tpl_marker : char         template file marker.  default is '~'     kwargs : dict         additional keyword args to pass to df.to_csv()      Returns     -------     None      Note     ----     If you don't use this function, make sure that you flush the     file handle before df.to_csv() and you pass mode='a' to to_csv()
setup a fake forward run for a pst.  The fake     forward run simply copies existing backup versions of     model output files to the outfiles pest(pp) is looking     for.  This is really a development option for debugging      Parameters     ----------     pst : pyemu.Pst      new_pst_name : str      org_cwd : str         existing working dir     new_cwd : str         new working dir
setup sfr ASCII observations
setup multiplier parameters for sfr segment data         Adding support for reachinput (and isfropt = 1)
setup non-mult parameters for hfb (yuck!)
setup the directories to use for multiplier parameterization.  Directories         are make within the PstFromFlopyModel.m.model_ws directory
setup the flopy.mbase instance for use with multipler parameters.         Changes model_ws, sets external_path and writes new MODFLOW input         files          Parameters         ----------         model : flopy.mbase             flopy model instance         org_model_ws : str             the orginal model working space         new_model_ws : str             the new model working space
get the latest counter for a certain parameter type.          Parameters         ----------         name : str             the parameter type          Returns         -------         count : int             the latest count for a parameter type          Note         ----         calling this function increments the counter for the passed         parameter type
prepare multipler arrays.  Copies existing model input arrays and         writes generic (ones) multiplier arrays
write a flopy.utils.Util2D instance to an ASCII text file using the         Util2D filename          Parameters         ----------         u2d : flopy.utils.Util2D          Returns         -------         filename : str             the name of the file written (without path)
write a template file a for a constant (uniform) multiplier parameter          Parameters         ----------         name : str             the base parameter name         tpl_file : str             the template file to write         zn_array : numpy.ndarray             an array used to skip inactive cells          Returns         -------         df : pandas.DataFrame             a dataframe with parameter information
write a template file a for grid-based multiplier parameters          Parameters         ----------         name : str             the base parameter name         tpl_file : str             the template file to write         zn_array : numpy.ndarray             an array used to skip inactive cells          Returns         -------         df : pandas.DataFrame             a dataframe with parameter information
prepare grid-based parameterizations
prepare pilot point based parameterizations          Parameters         ----------         mlt_df : pandas.DataFrame             a dataframe with multiplier array information          Note         ----         calls pyemu.pp_utils.setup_pilot_points_grid()
prepare KL based parameterizations          Parameters         ----------         mlt_df : pandas.DataFrame             a dataframe with multiplier array information          Note         ----         calls pyemu.helpers.setup_kl()
main entry point for setting up array multipler parameters
main entry point for setting up observations
draw like a boss!          Parameters         ----------             num_reals : int                 number of realizations to generate. Default is 100             sigma_range : float                 number of standard deviations represented by the parameter bounds.  Default                 is 6.          Returns         -------             cov : pyemu.Cov             a full covariance matrix
build a prior parameter covariance matrix.          Parameters         ----------             fmt : str                 the format to save the cov matrix.  Options are "ascii","binary","uncfile", "coo".                 default is "ascii"             filename : str                 the filename to save the prior cov matrix to.  If None, the name is formed using                 model nam_file name.  Default is None.             droptol : float                 tolerance for dropping near-zero values when writing compressed binary.                 Default is None             chunk : int                 chunk size to write in a single pass - for binary only             sparse : bool                 flag to build a pyemu.SparseMatrix format cov matrix.  Default is False             sigma_range : float                 number of standard deviations represented by the parameter bounds.  Default                 is 6.          Returns         -------             cov : pyemu.Cov             a full covariance matrix
build the pest control file using the parameterizations and         observations.          Parameters         ----------             filename : str                 the filename to save the pst to.  If None, the name if formed from                 the model namfile name.  Default is None.          Note         ----         calls pyemu.Pst.from_io_files          calls PESTCHEK
add external (existing) template files and instrution files to the         Pst instance
write the forward run script forward_run.py
parse the iterable from a property or boundary condition argument          Parameters         ----------         k : int or iterable int             the iterable         vals : iterable of ints             the acceptable values that k may contain          Returns         -------         k_vals : iterable of int             parsed k values
parse package-iterable pairs from a property or boundary condition         argument          Parameters         ----------         pakattr : iterable len 2           Returns         -------         pak : flopy.PakBase             the flopy package from the model instance         attr : (varies)             the flopy attribute from pak.  Could be Util2D, Util3D,             Transient2D, or MfList         attrname : (str)             the name of the attribute for MfList type.  Only returned if             attr is MfList.  For example, if attr is MfList and pak is             flopy.modflow.ModflowWel, then attrname can only be "flux"
main entry point for setting up list multiplier                 parameters
helper to setup list multiplier parameters for a given         k, pak, attr set.          Parameters         ----------         k : int or iterable of int             the zero-based stress period indices         pak : flopy.PakBase=             the MODFLOW package         attr : MfList             the MfList instance         col : str             the column name in the MfList recarray to parameterize
setup modflow head save file observations for given kper (zero-based         stress period index) and k (zero-based layer index) pairs using the         kperk argument.          Note         ----             this can setup a shit-ton of observations              this is useful for dataworth analyses or for monitoring             water levels as forecasts
setup observations from PEST-style SMP file pairs
setup observations from the MODFLOW HOB package
setup observations from the MODFLOW HYDMOD package
setup observations from the MODFLOW list file for         volume and flux water buget information
write a template file for a 2D array.          Parameters         ----------         name : str             the base parameter name         tpl_file : str             the template file to write - include path         zone_array : numpy.ndarray             an array used to skip inactive cells.  Values less than 1 are             not parameterized and are assigned a value of fill_value.             Default is None.         fill_value : float             value to fill in values that are skipped.  Default is 1.0.          Returns         -------         df : pandas.DataFrame             a dataframe with parameter information
load a residual file into a pandas.DataFrame          Parameters         ----------         resfile : str             residual file name          Returns         -------         pandas.DataFrame : pandas.DataFrame
load ensemble file for residual into a pandas.DataFrame          Parameters         ----------         enfile : str             ensemble file name          Returns         -------         pandas.DataFrame : pandas.DataFrame
load a pest-compatible .par file into a pandas.DataFrame      Parameters     ----------     parfile : str         pest parameter file name      Returns     -------     pandas.DataFrame : pandas.DataFrame
write a pest parameter file from a dataframe      Parameters     ----------     df : (pandas.DataFrame)         dataframe with column names that correspond to the entries         in the parameter data section of a pest control file     parfile : str         name of the parameter file to write
parse a pest template file to get the parameter names      Parameters     ----------     tpl_file : str         template file name      Returns     -------     par_names : list         list of parameter names
write parameter values to a model input files using a template files with     current parameter values (stored in Pst.parameter_data.parval1).     This is a simple implementation of what PEST does.  It does not     handle all the special cases, just a basic function...user beware      Parameters     ----------     pst : (pyemu.Pst)         a Pst instance
write parameter values to model input files using template files      Parameters     ----------     parvals : dict or pandas.Series         a way to look up parameter values using parameter names     tpl_file : str         template file     in_file : str         input file
method to find the start and end parameter markers     on a template file line.  Used by write_to_template()      Parameters     ----------     marker : str         template file marker char     line : str         template file line      Returns     -------     indices : list         list of start and end indices (zero based)
parse a pest instruction file to get observation names      Parameters     ----------     ins_file : str         instruction file name      Returns     -------     list of observation names
split up an instruction file line to get the observation names      Parameters     ----------     string : str         instruction file line      Returns     -------     obs_names : list         list of observation names
helper function to populate a generic Pst dataframe attribute.  This     function is called as part of constructing a generic Pst instance      Parameters     ----------     index : (varies)         something to use as the dataframe index     columns: (varies)         something to use as the dataframe columns     default_dict : (dict)         dictionary of default values for columns     dtype : numpy.dtype         dtype used to cast dataframe columns      Returns     -------     new_df : pandas.DataFrame
generate a generic pst instance.  This can used to later fill in     the Pst parts programatically.      Parameters     ----------     par_names : (list)         parameter names to setup     obs_names : (list)         observation names to setup      Returns     -------     new_pst : pyemu.Pst
generate a new pyemu.Pst instance from model interface files.  This     function is emulated in the Pst.from_io_files() class method.      Parameters     ----------     tpl_files : (list)         template file names     in_files : (list)         model input file names     ins_files : (list)         instruction file names     out_files : (list)         model output file names     pst_filename : str         filename to save new pyemu.Pst.  If None, Pst is not written.         default is None      Returns     -------     new_pst : pyemu.Pst
attempt to run INSCHEK for each instruction file, model output     file pair in a pyemu.Pst.  If the run is successful, the INSCHEK written     .obf file is used to populate the pst.observation_data.obsval attribute      Parameters     ----------     pst : (pyemu.Pst)
read the phi components from a record file by iteration      Parameters     ----------     recfile : str         pest record file name      Returns     -------     iters : dict         nested dictionary of iteration number, {group,contribution}
create a generic residual dataframe filled with np.NaN for     missing information      Parameters     ----------     observation_data : pandas.DataFrame         pyemu.Pst.observation_data      Returns     -------     res_df : pandas.DataFrame
fixes the issue where some terrible fortran program may have     written a floating point format without the 'e' - like 1.0-3, really?!      Parameters     ----------     pst_filename : str         the pest control file     clean_filename : str         the new pest control file to write. Default is "clean.pst"      Returns     -------     None
get the weighted total objective function          Returns         -------         phi : float             sum of squared residuals
get the individual components of the total objective function          Returns         -------         dict : dict             dictionary of observation group, contribution to total phi          Raises         ------         Assertion error if Pst.observation_data groups don't match         Pst.res groups
get the individual components of the total objective function             normalized to the total PHI being 1.0          Returns         -------         dict : dict             dictionary of observation group, normalized contribution to total phi          Raises         ------         Assertion error if self.observation_data groups don't match         self.res groups
reset the private Pst.res attribute          Parameters         ----------         res : (varies)             something to use as Pst.res attribute
get the residuals dataframe attribute          Returns         -------         res : pandas.DataFrame          Note         ----         if the Pst.__res attribute has not been loaded,         this call loads the res dataframe from a file
number of prior information equations          Returns         -------         nprior : int             the number of prior info equations
get the number of non-zero weighted observations          Returns         -------         nnz_obs : int             the number of non-zeros weighted observations
get the number of observations          Returns         -------         nobs : int             the number of observations
get the number of adjustable parameters (not fixed or tied)          Returns         -------         npar_adj : int             the number of adjustable parameters
get number of parameters          Returns         -------         npar : int             the number of parameters
return a dictionary of  parameter names in each parameter group.          Returns:             dictionary
get the forecast names from the pestpp options (if any).         Returns None if no forecasts are named          Returns         -------         forecast_names : list             a list of forecast names.
get the observation groups          Returns         -------         obs_groups : list             a list of unique observation groups
get the observation groups that contain at least one non-zero weighted          observation          Returns         -------         nnz_obs_groups : list             a list of observation groups that contain at             least one non-zero weighted observation
get the parameter groups with atleast one adjustable parameter          Returns         -------         adj_par_groups : list             a list of parameter groups with  at least one adjustable parameter
get the prior info groups          Returns         -------         prior_groups : list             a list of prior information groups
get the prior information names          Returns         -------         prior_names : list             a list of prior information names
get the adjustable (not fixed or tied) parameter names          Returns         -------         adj_par_names : list             list of adjustable (not fixed or tied) parameter names
get the non-zero weight observation names          Returns         -------         nnz_obs_names : list             a list of non-zero weighted observation names
get the zero-weighted observation names          Returns         -------          zero_weight_obs_names : list              a list of zero-weighted observation names
a private method to read part of an open file into a pandas.DataFrame.          Parameters         ----------         f : file object         nrows : int             number of rows to read         names : list             names to set the columns of the dataframe with         converters : dict             dictionary of lambda functions to convert strings             to numerical format         defaults : dict             dictionary of default values to assign columns.             Default is None          Returns         -------         pandas.DataFrame : pandas.DataFrame
load a version 2 control file
entry point load the pest control file.  sniffs the first non-comment line to detect the version (if present)          Parameters         ----------         filename : str             pst filename          Raises         ------             lots of exceptions for incorrect format
load a version 1 pest control file information          Parameters         ----------         filename : str             pst filename          Raises         ------             lots of exceptions for incorrect format
private method to synchronize the control section counters with the         various parts of the control file.  This is usually called during the         Pst.write() method.
private method to synchronize parameter groups section with         the parameter data section
private method to get the parameter names from prior information         equations.  Sets a 'names' column in Pst.prior_information that is a list         of parameter names
a helper to construct a new prior information equation.          Parameters         ----------         par_names : list             parameter names in the equation         pilbl : str             name to assign the prior information equation.  If None,             a generic equation name is formed. Default is None         rhs : (float)             the right-hand side of the equation         weight : (float)             the weight of the equation         obs_group : str             the observation group for the equation. Default is 'pi_obgnme'         coef_dict : dict             a dictionary of parameter name, coefficient pairs to assign             leading coefficients for one or more parameters in the equation.             If a parameter is not listed, 1.0 is used for its coefficients.             Default is {}
rectify the prior information equation with the current state of the         parameter_data dataframe.  Equations that list fixed, tied or missing parameters         are removed. This method is called during Pst.write()
main entry point to write a pest control file.           Parameters         ----------         new_filename : str             name of the new pest control file         update_regul : (boolean)             flag to update zero-order Tikhonov prior information             equations to prefer the current parameter values         version : int             flag for which version of control file to write (must be 1 or 2).             if None, uses Pst._version, which set in the constructor and modified             during the load
write a version 1 pest control file          Parameters         ----------         new_filename : str             name of the new pest control file         update_regul : (boolean)             flag to update zero-order Tikhonov prior information             equations to prefer the current parameter values
get a new pst object with subset of parameters and/or observations          Parameters         ----------         par_names : list             a list of parameter names to have in the new Pst instance.             If None, all parameters are in the new Pst instance. Default             is None         obs_names : list             a list of observation names to have in the new Pst instance.             If None, all observations are in teh new Pst instance. Default             is None          Returns         -------         Pst : Pst             a new Pst instance
replicates the pest parrep util. replaces the parval1 field in the             parameter data section dataframe          Parameters         ----------         parfile : str             parameter file to use.  If None, try to use             a parameter file that corresponds to the case name.             Default is None         enforce_hounds : bool             flag to enforce parameter bounds after parameter values are updated.             This is useful because PEST and PEST++ round the parameter values in the             par file, which may cause slight bound violations
adjusts the weights by group of the observations based on the phi components         in a pest record file so that total phi is equal to the number of         non-zero weighted observations          Parameters         ----------         recfile : str             record file name.  If None, try to use a record file             with the Pst case name.  Default is None         original_ceiling : bool             flag to keep weights from increasing - this is generally a good idea.             Default is True
adjusts the weights by group of the observations based on the phi components         in a pest residual file so that total phi is equal to the number of         non-zero weighted observations          Parameters         ----------         resfile : str             residual file name.  If None, try to use a residual file             with the Pst case name.  Default is None         original_ceiling : bool             flag to keep weights from increasing - this is generally a good idea.             Default is True
adjusts the weights of each non-zero weight observation based         on the residual in the pest residual file so each observations contribution         to phi is 1.0          Parameters         ----------         resfile : str             residual file name.  If None, try to use a residual file             with the Pst case name.  Default is None         original_ceiling : bool             flag to keep weights from increasing - this is generally a good idea.             Default is True
resets the weights of observations by group to account for         residual phi components.          Parameters         ----------         components : dict             a dictionary of obs group:phi contribution pairs         original_ceiling : bool             flag to keep weights from increasing
private method to reset weights based on target phi values         for each group.  This method should not be called directly          Parameters         ----------         target_phis : dict             target phi contribution for groups to reweight         res_idxs : dict             the index positions of each group of interest             in the res dataframe         obs_idxs : dict             the index positions of each group of interest             in the observation data dataframe
a private method to reset the weight for a list of observation names.  Supports the         data worth analyses in pyemu.Schur class.  This method only adjusts         observation weights in the current weight is nonzero.  User beware!         Parameters         ----------         obslist : list             list of observation names         weight : (float)             new weight to assign
reset the weights of observation groups to contribute a specified         amount to the composite objective function          Parameters         ----------         obs_dict : dict             dictionary of obs name,new contribution pairs         obsgrp_dict : dict             dictionary of obs group name,contribution pairs          Note         ----         if all observations in a named obs group have zero weight, they will be         assigned a non-zero weight so that the request phi contribution         can be met.  Similarly, any observations listed in obs_dict with zero         weight will also be reset
setup  weights inversely proportional to the observation value          Parameters         ----------         fraction_stdev : float             the fraction portion of the observation             val to treat as the standard deviation.  set to 1.0 for             inversely proportional         wmax : float             maximum weight to allow         leave_zero : bool             flag to leave existing zero weights
experimental method to calculate finite difference parameter         pertubations.  The pertubation values are added to the         Pst.parameter_data attribute          Note         ----         user beware!
experimental method to calculate parameter increments for use         in the finite difference pertubation calculations          Note         ----         user beware!
add transformed values to the Pst.parameter_data attribute
enforce bounds violation resulting from the         parameter pertubation calculations
create a Pst instance from model interface files. Assigns generic values for         parameter info.  Tries to use INSCHEK to set somewhat meaningful observation         values          Parameters         ----------         tpl_files : list             list of template file names         in_files : list             list of model input file names (pairs with template files)         ins_files : list             list of instruction file names         out_files : list             list of model output file names (pairs with instruction files)         pst_filename : str             name of control file to write.  If None, no file is written.             Default is None          Returns         -------         Pst : Pst          Note         ----         calls pyemu.helpers.pst_from_io_files()
add new parameters to a control file          Parameters         ----------             template_file : str                 template file             in_file : str(optional)                 model input file. If None, template_file.replace('.tpl','') is used             pst_path : str(optional)                 the path to append to the template_file and in_file in the control file.  If                 not None, then any existing path in front of the template or in file is split off                 and pst_path is prepended.  Default is None          Returns         -------         new_par_data : pandas.DataFrame             the data for the new parameters that were added. If no new parameters are in the             new template file, returns None          Note         ----         populates the new parameter information with default values
add new parameters to a control file          Parameters         ----------             ins_file : str                 instruction file             out_file : str                 model output file.  If None, then ins_file.replace(".ins","") is used.  Default is None             pst_path : str(optional)                 the path to append to the instruction file and out file in the control file.  If                 not None, then any existing path in front of the template or in file is split off                 and pst_path is prepended.  Default is None             inschek : bool                 flag to run inschek.  If successful, inscheck outputs are used as obsvals          Returns         -------         new_obs_data : pandas.DataFrame             the data for the new observations that were added          Note         ----         populates the new observation information with default values
get some common residual stats from the current obsvals,         weights and grouping in self.observation_data and the modelled values in         self.res.  The key here is 'current' because if obsval, weights and/or         groupings have changed in self.observation_data since the res file was generated         then the current values for obsval, weight and group are used          Parameters         ----------             nonzero : bool                 calculate stats using only nonzero-weighted observations.  This may seem                 obsvious to most users, but you never know....          Returns         -------             df : pd.DataFrame                 a dataframe with columns for groups names and indices of statistic name.          Note         ----             the normalized RMSE is normalized against the obsval range (max - min)
write a stand alone parameter summary latex table           Parameters         ----------         filename : str             latex filename. If None, use <case>.par.tex. Default is None         group_names: dict             par group names : table names for example {"w0":"well stress period 1"}.             Default is None         sigma_range : float             number of standard deviations represented by parameter bounds.  Default             is 4.0, implying 95% confidence bounds          Returns         -------         None
write a stand alone observation summary latex table                   Parameters                 ----------                 filename : str                     latex filename. If None, use <case>.par.tex. Default is None                 group_names: dict                     par group names : table names for example {"w0":"well stress period 1"}.                     Default is None                  Returns                 -------                 None
run a command related to the pst instance. If         write() has been called, then the filename passed to write         is in the command, otherwise the original constructor         filename is used          exe_name : str             the name of the executable to call.  Default is "pestpp"         cwd : str             the directory to execute the command in.  If None,             os.path.split(self.filename) is used to find             cwd.  Default is None
get the names of the observations that         are listed as less than inequality constraints.  Zero-         weighted obs are skipped          Returns         -------         pandas.Series : obsnme of obseravtions that are non-zero weighted                         less than constraints
get the names of the prior information eqs that         are listed as less than inequality constraints.  Zero-         weighted pi are skipped          Returns         -------         pandas.Series : pilbl of prior information that are non-zero weighted                         less than constraints
get the names of the observations that         are listed as greater than inequality constraints.  Zero-         weighted obs are skipped          Returns         -------         pandas.Series : obsnme of obseravtions that are non-zero weighted                         greater than constraints
get the names of the prior information eqs that         are listed as greater than inequality constraints.  Zero-         weighted pi are skipped          Returns         -------         pandas.Series : pilbl of prior information that are non-zero weighted                         greater than constraints
calculate the various parameter change limits used in pest.         Works in control file values space (not log transformed space).  Also         adds columns for effective upper and lower which account for par bounds and the         value of parchglim          Returns         -------             df : pandas.DataFrame                 a copy of self.parameter_data with columns for relative and factor change limits         Note         ----             does not yet support absolute parameter change limits!
a private method to deduce and load a filename into a matrix object.         Uses extension: 'jco' or 'jcb': binary, 'mat','vec' or 'cov': ASCII,         'unc': pest uncertainty file.          Parameters         ----------         filename : str             the name of the file          Returns         -------         m : pyemu.Matrix          Raises         ------         exception for unrecognized extension
private method set the pst attribute
private method to set the jco attribute from a file or a matrix object
private method to set the parcov attribute from:                 a pest control file (parameter bounds)                 a pst object                 a matrix object                 an uncert file                 an ascii matrix file
private method to set the obscov attribute from:                 a pest control file (observation weights)                 a pst object                 a matrix object                 an uncert file                 an ascii matrix file
private method set the predictions attribute from:                 mixed list of row names, matrix files and ndarrays                 a single row name                 an ascii file             can be none if only interested in parameters.
wrapper around pyemu.Pst.nnz_obs_names for listing non-zero         observation names          Returns         -------         nnz_obs_names : list             pyemu.Pst.nnz_obs_names
wrapper around pyemu.Pst.adj_par_names for list adjustable parameter         names          Returns         -------         adj_par_names : list             pyemu.Pst.adj_par_names
property decorated prediction iterator          Returns         -------         iterator : iterator             iterator on prediction sensitivity vectors (matrix)
get the pyemu.Pst attribute          Returns         -------         pst : pyemu.Pst          Note         ----         returns a references                  If LinearAnalysis.__pst is None, then the pst attribute is         dynamically loaded before returning
get the KL parcov scaling matrix attribute.  Create the attribute if         it has not yet been created          Returns         -------         fehalf : pyemu.Matrix
get the square root of the cofactor matrix attribute. Create the attribute if         it has not yet been created          Returns         -------         qhalf : pyemu.Matrix
get the half normal matrix attribute.  Create the attribute if         it has not yet been created          Returns         -------         qhalfx : pyemu.Matrix
get the normal matrix attribute. Create the attribute if         it has not yet been created          Returns         -------         xtqx : pyemu.Matrix
get the maximum likelihood parameter estimate.          Returns         -------          post_expt : pandas.Series             the maximum likelihood parameter estimates
get a dict of prior prediction variances          Returns         -------         prior_prediction : dict             dictionary of prediction name, prior variance pairs
apply karhuene-loeve scaling to the jacobian matrix.          Note         ----         This scaling is not necessary for analyses using Schur's         complement, but can be very important for error variance         analyses.  This operation effectively transfers prior knowledge         specified in the parcov to the jacobian and reset parcov to the         identity matrix.
drop regularization and prior information observation from the jco
reset the LinearAnalysis.pst attribute          Parameters         ----------         arg : (str or matrix)             the value to assign to the pst attribute
reset the parcov attribute to None          Parameters         ----------         arg : str or pyemu.Matrix             the value to assign to the parcov attribute.  If None,             the private __parcov attribute is cleared but not reset
reset the obscov attribute to None          Parameters         ----------         arg : str or pyemu.Matrix             the value to assign to the obscov attribute.  If None,             the private __obscov attribute is cleared but not reset
drop the prior information from the jco and pst attributes
method to get a new LinearAnalysis class using a         subset of parameters and/or observations          Parameters         ----------         par_names : list             par names for new object         obs_names : list             obs names for new object         astype : pyemu.Schur or pyemu.ErrVar             type to cast the new object.  If None, return type is             same as self          Returns         -------         new : LinearAnalysis
reset the elements of obscov by scaling the implied weights         based on the phi components in res_file so that the total phi         is equal to the number of non-zero weights.          Parameters         ----------         resfile : str             residual file to use.  If None, residual file with case name is             sought. default is None          Note         ----         calls pyemu.Pst.adjust_weights_resfile()
get a dataframe of composite scaled sensitivities.  Includes both         PEST-style and Hill-style.          Returns         -------         css : pandas.DataFrame
get a dataframe of composite observation sensitivity, as returned by PEST in the         seo file.          Note that this formulation deviates slightly from the PEST documentation in that the         values are divided by (npar-1) rather than by (npar).          The equation is cso_j = ((Q^1/2*J*J^T*Q^1/2)^1/2)_jj/(NPAR-1)         Returns:         cso : pandas.DataFrame
helper function to plot gaussian distrbutions from prior and posterior     means and standard deviations      Parameters     ----------     df : pandas.DataFrame         a dataframe and csv file.  Must have columns named:         'prior_mean','prior_stdev','post_mean','post_stdev'.  If loaded         from a csv file, column 0 is assumed to tbe the index     ax: matplotlib.pyplot.axis         If None, and not subplots, then one is created         and all distributions are plotted on a single plot     label_post: bool         flag to add text labels to the peak of the posterior     label_prior: bool         flag to add text labels to the peak of the prior     subplots: (boolean)         flag to use subplots.  If True, then 6 axes per page         are used and a single prior and posterior is plotted on each     figsize: tuple         matplotlib figure size      Returns     -------     figs : list         list of figures     axes : list         list of axes      Note     ----     This is useful for demystifying FOSM results      if subplots is False, a single axis is returned      Example     -------     ``>>>import matplotlib.pyplot as plt``      ``>>>import pyemu``      ``>>>pyemu.helpers.plot_summary_distributions("pest.par.usum.csv")``      ``>>>plt.show()``
get an x and y numpy.ndarray that spans the +/- 4     standard deviation range of a gaussian distribution with     a given mean and standard deviation. useful for plotting      Parameters     ----------     mean : float         the mean of the distribution     stdev : float         the standard deviation of the distribution     num_pts : int         the number of points in the returned ndarrays.         Default is 50      Returns     -------     x : numpy.ndarray         the x-values of the distribution     y : numpy.ndarray         the y-values of the distribution
make plot of phi vs number of model runs - requires     available pestpp .iobj file         Parameters         ----------         pst : pyemu.Pst         logger : Logger             if None, a generic one is created.  Default is None         filename : str             PDF filename to save figures to.  If None, figures are returned.  Default is None         kwargs : dict             optional keyword args to pass to plotting functions
make 1-to-1 plots and also observed vs residual by observation group     Parameters     ----------     pst : pyemu.Pst     logger : Logger         if None, a generic one is created.  Default is None     filename : str         PDF filename to save figures to.  If None, figures are returned.  Default is None     kwargs : dict         optional keyword args to pass to plotting functions      TODO: color symbols by weight
timeseries plot helper...in progress
Plot a stacked bar chart of identifiability based on a identifiability dataframe      Parameters     ----------         id_df : pandas dataframe of identifiability         nsv : number of singular values to consider         logger : pyemu.Logger         kwargs : dict of keyword arguments      Returns     -------     ax : matplotlib.Axis         Example     -------     ``>>> import pyemu``     ``>>> pest_obj = pyemu.Pst(pest_control_file)``     ``>>> ev = pyemu.ErrVar(jco='freyberg_jac.jcb'))``     ``>>> id_df = ev.get_identifiability_dataframe(singular_value=48)``     ``>>> pyemu.plot_id_bar(id_df, nsv=12, figsize=(12,4)``
plot current phi components as a pie chart.      Parameters     ----------     pst : pyemu.Pst     logger : pyemu.Logger     kwargs : dict         accepts 'include_zero' as a flag to include phi groups with         only zero-weight obs (not sure why anyone would do this, but         whatevs).     Returns     -------     ax : matplotlib.Axis
helper to plot prior parameter histograms implied by     parameter bounds. Saves a multipage pdf named <case>.prior.pdf      Parameters     ----------     pst : pyemu.Pst     logger : pyemu.Logger     filename : str         PDF filename to save plots to. If None, return figs without saving.  Default is None.     kwargs : dict         accepts 'grouper' as dict to group parameters on to a single axis (use         parameter groups if not passed),         'unqiue_only' to only show unique mean-stdev combinations within a         given group      Returns     -------     None      TODO     ----     external parcov, unique mean-std pairs
helper function to plot ensemble histograms      Parameters     ----------     ensemble : varies         the ensemble argument can be a pandas.DataFrame or derived type or a str, which         is treated as a filename.  Optionally, ensemble can be a list of these types or         a dict, in which case, the keys are treated as facecolor str (e.g., 'b', 'y', etc).     facecolor : str         the histogram facecolor.  Only applies if ensemble is a single thing     plot_cols : enumerable         a collection of columns (in form of a list of parameters, or a dict with keys for         parsing plot axes and values of parameters) from the ensemble(s) to plot.  If None,         (the union of) all cols are plotted. Default is None     filename : str         the name of the pdf to create.  If None, return figs without saving.  Default is None.     func_dict : dict         a dictionary of unary functions (e.g., np.log10_ to apply to columns.  Key is         column name.  Default is None     sync_bins : bool         flag to use the same bin edges for all ensembles. Only applies if more than         one ensemble is being plotted.  Default is True     deter_vals : dict         dict of deterministic values to plot as a vertical line. key is ensemble columnn name     std_window : float         the number of standard deviations around the mean to mark as vertical lines.  If None,         nothing happens.  Default is None     deter_range : bool         flag to set xlims to deterministic value +/- std window.  If True, std_window must not be None.         Default is False
helper function to plot first and second moment change histograms      Parameters     ----------     ensemble1 : varies         str or pd.DataFrames     ensemble2 : varies         str or pd.DataFrame     pst : pyemu.Pst         pst instance     facecolor : str         the histogram facecolor.     filename : str         the name of the pdf to create. If None, return figs without saving.  Default is None.
helper function to plot ensemble 1-to-1 plots sbowing the simulated range      Parameters     ----------     ensemble : varies         the ensemble argument can be a pandas.DataFrame or derived type or a str, which         is treated as a fileanme.  Optionally, ensemble can be a list of these types or         a dict, in which case, the keys are treated as facecolor str (e.g., 'b', 'y', etc).     pst : pyemu.Pst         pst instance     facecolor : str         the histogram facecolor.  Only applies if ensemble is a single thing     filename : str         the name of the pdf to create. If None, return figs without saving.  Default is None.     base_ensemble : varies         an optional ensemble argument for the observations + noise ensemble.         This will be plotted as a transparent red bar on the 1to1 plot.
create an instruction file for an smp file      Parameters     ----------     smp_filename : str         existing smp file     ins_filename: str         instruction file to create.  If None, create         an instruction file using the smp filename         with the ".ins" suffix     use_generic_names : bool         flag to force observations names to use a generic         int counter instead of trying to use a datetime str     gwutils_compliant : bool         flag to use instruction set that is compliant with the         pest gw utils (fixed format instructions).  If false,         use free format (with whitespace) instruction set     datetime_format : str         str to pass to datetime.strptime in the smp_to_dataframe() function     prefix : str          a prefix to add to the front of the obsnmes.  Default is ''       Returns     -------     df : pandas.DataFrame         dataframe instance of the smp file with the observation names and         instruction lines as additional columns
write a dataframe as an smp file      Parameters     ----------     dataframe : pandas.DataFrame     smp_filename : str         smp file to write     name_col: str         the column in the dataframe the marks the site namne     datetime_col: str         the column in the dataframe that is a datetime instance     value_col: str         the column in the dataframe that is the values     datetime_format: str         either 'dd/mm/yyyy' or 'mm/dd/yyy'     value_format: str         a python float-compatible format
datetime parser to help load smp files      Parameters     ----------     items : iterable         something or somethings to try to parse into datetimes      Returns     -------     dt : iterable         the cast datetime things
load an smp file into a pandas dataframe (stacked in wide format)      Parameters     ----------     smp_filename : str         smp filename to load     datetime_format : str         should be either "%m/%d/%Y %H:%M:%S" or "%d/%m/%Y %H:%M:%S"         If None, then we will try to deduce the format for you, which         always dangerous      Returns     -------     df : pandas.DataFrame
get the number of solution space dimensions given         a ratio between the largest and smallest singular values          Parameters         ----------         epsilon: float             singular value ratio          Returns         -------         nsing : float             number of singular components above the epsilon ratio threshold                  Note         -----             If nsing == nadj_par, then None is returned
get a null-space projection matrix of XTQX          Parameters         ----------         nsing: int             optional number of singular components to use             If Nonte, then nsing is determined from             call to MonteCarlo.get_nsing()                  Returns         -------         v2_proj : pyemu.Matrix             the null-space projection matrix (V2V2^T)
draw stochastic realizations of parameters and            optionally observations, filling MonteCarlo.parensemble and            optionally MonteCarlo.obsensemble.          Parameters         ----------         num_reals : int             number of realization to generate         par_file : str             parameter file to use as mean values. If None,             use MonteCarlo.pst.parameter_data.parval1.             Default is None         obs : bool             add a realization of measurement noise to observation values,             forming MonteCarlo.obsensemble.Default is False         enforce_bounds : str             enforce parameter bounds based on control file information.             options are 'reset', 'drop' or None.  Default is None         how : str             type of distribution to draw from. Must be in ["gaussian","uniform"]             default is "gaussian".          Example         -------         ``>>>import pyemu``          ``>>>mc = pyemu.MonteCarlo(pst="pest.pst")``          ``>>>mc.draw(1000)``
perform the null-space projection operations for null-space monte carlo          Parameters         ----------         par_file: str             an optional file of parameter values to use         nsing: int             number of singular values to in forming null subspace matrix         inplace: bool             overwrite the existing parameter ensemble with the             projected values         enforce_bounds: str             how to enforce parameter bounds.  can be None, 'reset', or 'drop'.             Default is None          Returns         -------         par_en : pyemu.ParameterEnsemble             if inplace is False, otherwise None          Note         ----         to use this method, the MonteCarlo instance must have been constructed         with the ``jco`` argument.          Example         -------         ``>>>import pyemu``          ``>>>mc = pyemu.MonteCarlo(jco="pest.jcb")``          ``>>>mc.draw(1000)``          ``>>>mc.project_parensemble(par_file="final.par",nsing=100)``
write parameter and optionally observation realizations             to a series of pest control files          Parameters         ----------         prefix: str             pest control file prefix          existing_jco: str             filename of an existing jacobian matrix to add to the             pest++ options in the control file.  This is useful for             NSMC since this jco can be used to get the first set of             parameter upgrades for free!  Needs to be the path the jco             file as seen from the location where pest++ will be run          noptmax: int             value of NOPTMAX to set in new pest control files          Example         -------         ``>>>import pyemu``          ``>>>mc = pyemu.MonteCarlo(jco="pest.jcb")``          ``>>>mc.draw(1000, obs=True)``          ``>>>mc.write_psts("mc_", existing_jco="pest.jcb", noptmax=1)``
identify which candidate solutions in obs_df (rows)         are feasible with respect obs constraints (obs_df)          Parameters         ----------         obs_df : pandas.DataFrame             a dataframe with columns of obs names and rows of realizations         risk : float             risk value. If != 0.5, then risk shifting is used.  Otherwise, the             obsval in Pst is used.  Default is 0.5.           Returns         -------         is_feasible : pandas.Series             series with obs_df.index and bool values
identify which candidate solutions are pareto non-dominated -         super patheically slow...          Parameters         ----------         obs_df : pandas.DataFrame             dataframe with columns of observation names and rows of realizations          Returns         -------         is_dominated : pandas.Series             series with index of obs_df and bool series
identify which candidate solutions are pareto non-dominated continuously updated,         but still slow          Parameters         ----------         obs_df : pandas.DataFrame             dataframe with columns of observation names and rows of realizations          Returns         -------         is_dominated : pandas.Series             series with index of obs_df and bool series
identify which candidate solutions are pareto non-dominated using Kungs algorithm          Parameters         ----------         obs_df : pandas.DataFrame             dataframe with columns of observation names and rows of realizations          Returns         -------         is_dominated : pandas.Series             series with index of obs_df and bool series
determine the crowding distance for each candidate solution          Parameters         ----------         obs_df : pandas.DataFrame             dataframe with columns of observation names and rows of realizations          Returns         -------         crowd_distance : pandas.Series             series with index of obs_df and values of crowd distance
Assign the primary unicode to the glyph.         This will be an integer or None.          Subclasses may override this method.
This must return an int or float.         If the glyph has no outlines, this must return `None`.          Subclasses may override this method.
value will be an int or float.          Subclasses may override this method.
This must return an int or float.         If the glyph has no outlines, this must return `None`.          Subclasses may override this method.
value will be an int or float.          Subclasses may override this method.
This must return an int or float.         If the glyph has no outlines, this must return `None`.          Subclasses may override this method.
value will be an int or float.          Subclasses may override this method.
This must return an int or float.         If the glyph has no outlines, this must return `None`.          Subclasses may override this method.
value will be an int or float.          Subclasses may override this method.
Draw the glyph's outline data (contours and components) to         the given :ref:`type-pen`.              >>> glyph.draw(pen)          If ``contours`` is set to ``False``, the glyph's         contours will not be drawn.              >>> glyph.draw(pen, contours=False)          If ``components`` is set to ``False``, the glyph's         components will not be drawn.              >>> glyph.draw(pen, components=False)
Draw the glyph's outline data (contours and components) to         the given :ref:`type-pointpen`.              >>> glyph.drawPoints(pointPen)          If ``contours`` is set to ``False``, the glyph's         contours will not be drawn.              >>> glyph.drawPoints(pointPen, contours=False)          If ``components`` is set to ``False``, the glyph's         components will not be drawn.              >>> glyph.drawPoints(pointPen, components=False)
Clear the glyph.              >>> glyph.clear()          This clears:          - contours         - components         - anchors         - guidelines         - image          It's possible to turn off the clearing of portions of         the glyph with the listed arguments.              >>> glyph.clear(guidelines=False)
Subclasses may override this method.
Append the data from ``other`` to new objects in this glyph.         This will append:          - contours         - components         - anchors         - guidelines              >>> glyph.appendGlyph(otherGlyph)          ``offset`` indicates the x and y shift values that should         be applied to the appended data. It must be a :ref:`type-coordinate`         value or ``None``. If ``None`` is given, the offset will be ``(0, 0)``.              >>> glyph.appendGlyph(otherGlyph, (100, 0))
Subclasses may override this method.
This must return an iterator that returns wrapped contours.          Subclasses may override this method.
Append a contour containing the same data as ``contour``         to this glyph.              >>> contour = glyph.appendContour(contour)          This will return a :class:`BaseContour` object representing         the new contour in the glyph. ``offset`` indicates the x and         y shift values that should be applied to the appended data.         It must be a :ref:`type-coordinate` value or ``None``. If         ``None`` is given, the offset will be ``(0, 0)``.              >>> contour = glyph.appendContour(contour, (100, 0))
contour will be an object with a drawPoints method.          offset will be a valid offset (x, y).          This must return the new contour.          Subclasses may override this method.
Remove ``contour`` from the glyph.              >>> glyph.removeContour(contour)          ``contour`` may be a :ref:`BaseContour` or an :ref:`type-int`         representing a contour index.
Subclasses may override this method.
Append a component to this glyph.              >>> component = glyph.appendComponent("A")          This will return a :class:`BaseComponent` object representing         the new component in the glyph. ``offset`` indicates the x and         y shift values that should be applied to the appended component.         It must be a :ref:`type-coordinate` value or ``None``. If         ``None`` is given, the offset will be ``(0, 0)``.              >>> component = glyph.appendComponent("A", offset=(10, 20))          ``scale`` indicates the x and y scale values that should be         applied to the appended component. It must be a         :ref:`type-scale` value or ``None``. If ``None`` is given,         the scale will be ``(1.0, 1.0)``.              >>> component = glyph.appendComponent("A", scale=(1.0, 2.0))          ``component`` may be a :class:`BaseComponent` object from which         attribute values will be copied. If ``baseGlyph``, ``offset``         or ``scale`` are specified as arguments, those values will be used         instead of the values in the given component object.
baseGlyph will be a valid glyph name.         The baseGlyph may or may not be in the layer.          offset will be a valid offset (x, y).         scale will be a valid scale (x, y).         identifier will be a valid, nonconflicting identifier.          This must return the new component.          Subclasses may override this method.
Remove ``component`` from the glyph.              >>> glyph.removeComponent(component)          ``component`` may be a :ref:`BaseComponent` or an         :ref:`type-int` representing a component index.
Subclasses may override this method.
Append an anchor to this glyph.              >>> anchor = glyph.appendAnchor("top", (10, 20))          This will return a :class:`BaseAnchor` object representing         the new anchor in the glyph. ``name`` indicated the name to         be assigned to the anchor. It must be a :ref:`type-string`         or ``None``. ``position`` indicates the x and y location         to be applied to the anchor. It must be a         :ref:`type-coordinate` value. ``color`` indicates the color         to be applied to the anchor. It must be a :ref:`type-color`         or ``None``.              >>> anchor = glyph.appendAnchor("top", (10, 20), color=(1, 0, 0, 1))          ``anchor`` may be a :class:`BaseAnchor` object from which         attribute values will be copied. If ``name``, ``position``         or ``color`` are specified as arguments, those values will         be used instead of the values in the given anchor object.
Remove ``anchor`` from the glyph.              >>> glyph.removeAnchor(anchor)          ``anchor`` may be an :ref:`BaseAnchor` or an         :ref:`type-int` representing an anchor index.
Subclasses may override this method.
position will be a valid position (x, y).         angle will be a valid angle.         name will be a valid guideline name or None.         color will be a valid color or None .         identifier will be a valid, nonconflicting identifier.          This must return the new guideline.          Subclasses may override this method.
Remove ``guideline`` from the glyph.              >>> glyph.removeGuideline(guideline)          ``guideline`` may be a :ref:`BaseGuideline` or an         :ref:`type-int` representing an guideline index.
Subclasses may override this method.
Subclasses may override this method.
%s         **width** indicates if the glyph's width should be scaled.         **height** indicates if the glyph's height should be scaled.          The origin must not be specified when scaling the width or height.
Subclasses may override this method.
Interpolate the contents of this glyph at location ``factor``         in a linear interpolation between ``minGlyph`` and ``maxGlyph``.              >>> glyph.interpolate(0.5, otherGlyph1, otherGlyph2)          ``factor`` may be a :ref:`type-int-float` or a tuple containing         two :ref:`type-int-float` values representing x and y factors.              >>> glyph.interpolate((0.5, 1.0), otherGlyph1, otherGlyph2)          ``minGlyph`` must be a :class:`BaseGlyph` and will be located at 0.0         in the interpolation range. ``maxGlyph`` must be a :class:`BaseGlyph`         and will be located at 1.0 in the interpolation range. If ``round``         is ``True``, the contents of the glyph will be rounded to integers         after the interpolation is performed.              >>> glyph.interpolate(0.5, otherGlyph1, otherGlyph2, round=True)          This method assumes that ``minGlyph`` and ``maxGlyph`` are completely         compatible with each other for interpolation. If not, any errors         encountered will raise a :class:`FontPartsError`. If ``suppressError``         is ``True``, no exception will be raised and errors will be silently         ignored.
Subclasses may override this method.
This is the environment implementation of         :meth:`BaseGlyph.isCompatible`.          Subclasses may override this method.
Determine if ``point`` is in the black or white of the glyph.              >>> glyph.pointInside((40, 65))             True          ``point`` must be a :ref:`type-coordinate`.
Subclasses may override this method.
Subclasses may override this method.
Get the :ref:`type-glyph-layer` with ``name`` in this glyph.              >>> glyphLayer = glyph.getLayer("foreground")
name will be a string, but there may not be a         layer with a name matching the string. If not,         a ``ValueError`` must be raised.          Subclasses may override this method.
Make a new layer with ``name`` in this glyph.              >>> glyphLayer = glyph.newLayer("background")          This will return the new :ref:`type-glyph-layer`.         If the layer already exists in this glyph, it         will be cleared.
Remove ``layer`` from this glyph.              >>> glyph.removeLayer("background")          Layer can be a :ref:`type-glyph-layer` or a :ref:`type-string`         representing a layer name.
Set the image in the glyph. This will return the         assigned :class:`BaseImage`. The image data can be         defined via ``path`` to an image file:              >>> image = glyph.addImage(path="/path/to/my/image.png")          The image data can be defined with raw image data         via ``data``.              >>> image = glyph.addImage(data=someImageData)          If ``path`` and ``data`` are both provided, a         :class:`FontPartsError` will be raised. The supported         image formats will vary across environments. Refer         to :class:`BaseImage` for complete details.          ``scale`` indicates the x and y scale values that should be         applied to the image. It must be a :ref:`type-scale` value         or ``None``.              >>> image = glyph.addImage(path="/p/t/image.png", scale=(0.5, 1.0))          ``position`` indicates the x and y location of the lower left         point of the image.              >>> image = glyph.addImage(path="/p/t/image.png", position=(10, 20))          ``color`` indicates the color to be applied to the image. It must         be a :ref:`type-color` or ``None``.              >>> image = glyph.addImage(path="/p/t/image.png", color=(1, 0, 0, 0.5))
This will return the glyph's contents as a string in         `GLIF format <http://unifiedfontobject.org/versions/ufo3/glyphs/glif/>`_.              >>> xml = glyph.writeGlyphToString()          ``glyphFormatVersion`` must be a :ref:`type-int` that defines         the preferred GLIF format version.
Get the anchor's index.         This must return an ``int``.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseAnchor.transformBy`.          **matrix** will be a :ref:`type-transformation`.         that has been normalized with         :func:`normalizers.normalizeTransformationMatrix`.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseAnchor.isCompatible`.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseAnchor.round`.          Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Return the Lib as a ``dict``.          This is a backwards compatibility method.
Returns the contents of the named key.         **key** is a :ref:`type-string`, and the returned values will         either be ``list`` of key contents or ``None`` if no key was         found. ::              >>> font.lib["public.glyphOrder"]             ["A", "B", "C"]          It is important to understand that any changes to the returned key         contents will not be reflected in the Lib object. If one wants to         make a change to the key contents, one should do the following::              >>> lib = font.lib["public.glyphOrder"]             >>> lib.remove("A")             >>> font.lib["public.glyphOrder"] = lib
Removes the **key** from the Lib and returns the ``list`` of         key members. If no key is found, **default** is returned.         **key** is a :ref:`type-string`. This must return either         **default** or a ``list`` of items as :ref:`type-string`.              >>> font.lib.pop("public.glyphOrder")             ["A", "B", "C"]
Subclass get items         to get support for all methods in an given object
Make a new glyph with **name** in the layer. ::              >>> glyph = layer.newGlyph("A")          The newly created :class:`BaseGlyph` will be returned.          If the glyph exists in the layer and clear is set to ``False``,         the existing glyph will be returned, otherwise the default         behavior is to clear the exisiting glyph.
Insert **glyph** into the layer. ::              >>> glyph = layer.insertGlyph(otherGlyph, name="A")          This method is deprecated. :meth:`BaseFont.__setitem__` instead.
This is the environment implementation of         :meth:`BaseLayer.__setitem__` and :meth:`BaseFont.__setitem__`.         This must return an instance of a :class:`BaseGlyph` subclass.         **glyph** will be a glyph object with the attributes necessary         for copying as defined in :meth:`BaseGlyph.copy` An environment         must not insert **glyph** directly. Instead the data from         **glyph** should be copied to a new glyph instead. **name**         will be a :ref:`type-string` representing a glyph name. It         will have been normalized with :func:`normalizers.normalizeGlyphName`.         **name** will have been tested to make sure that no glyph with         the same name exists in the layer.          Subclasses may override this method.
Subclasses may override this method.
Copy data from **source** into this layer.         Refer to :meth:`BaseLayer.copy` for a list         of values that will be copied.
Interpolate all possible data in the layer. ::              >>> layer.interpolate(0.5, otherLayer1, otherLayer2)             >>> layer.interpolate((0.5, 2.0), otherLayer1, otherLayer2, round=False)          The interpolation occurs on a 0 to 1.0 range where **minLayer**         is located at 0 and **maxLayer** is located at 1.0. **factor**         is the interpolation value. It may be less than 0 and greater         than 1.0. It may be a :ref:`type-int-float` or a tuple of         two :ref:`type-int-float`. If it is a tuple, the first         number indicates the x factor and the second number indicates         the y factor. **round** indicates if the result should be         rounded to integers. **suppressError** indicates if incompatible         data should be ignored or if an error should be raised when         such incompatibilities are found.
This is the environment implementation of         :meth:`BaseLayer.interpolate`.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseLayer.isCompatible`.          Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
This is the environment implementation of         :meth:`BaseComponent.isCompatible`.          Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Round coordinates.
Create a new font. **familyName** will be assigned     to ``font.info.familyName`` and **styleName**     will be assigned to ``font.info.styleName``. These     are optional and default to ``None``. If **showInterface**     is ``False``, the font should be created without     graphical interface. The default for **showInterface**     is ``True``.      ::          from fontParts.world import *          font = NewFont()         font = NewFont(familyName="My Family", styleName="My Style")         font = NewFont(showInterface=False)
Get a list of all open fonts. Optionally, provide a     value for ``sortOptions`` to sort the fonts. See     :meth:`world.FontList.sortBy` for options.       ::          from fontParts.world import *          fonts = AllFonts()         for font in fonts:             # do something          fonts = AllFonts("magic")         for font in fonts:             # do something          fonts = AllFonts(["familyName", "styleName"])         for font in fonts:             # do something
Returns 0 if the font is italic.     Returns 1 if the font is not italic.
Returns 0 if the font is monospace.     Returns 1 if the font is not monospace.
Sort ``fonts`` with the ordering preferences defined         by ``sortBy``. ``sortBy`` must be one of the following:          * sort description string         * :class:`BaseInfo` attribute name         * sort value function         * list/tuple containing sort description strings, :class:`BaseInfo`           attribute names and/or sort value functions         * ``"magic"``          Sort Description Strings         ------------------------          The sort description strings, and how they modify the sort, are:          +--------------------+--------------------------------------+         | ``"familyName"``   | Family names by alphabetical order.  |         +--------------------+--------------------------------------+         | ``"styleName"``    | Style names by alphabetical order.   |         +--------------------+--------------------------------------+         | ``"isItalic"``     | Italics before romans.               |         +--------------------+--------------------------------------+         | ``"isRoman"``      | Romans before italics.               |         +--------------------+--------------------------------------+         | ``"widthValue"``   | Width values by numerical order.     |         +--------------------+--------------------------------------+         | ``"weightValue"``  | Weight values by numerical order.    |         +--------------------+--------------------------------------+         | ``"monospace"``    | Monospaced before proportional.      |         +--------------------+--------------------------------------+         | ``"proportional"`` | Proportional before monospaced.      |         +--------------------+--------------------------------------+          ::              >>> fonts.sortBy("familyName", "styleName")           Font Info Attribute Names         -------------------------          Any :class:`BaseFont` attribute name may be included as         a sort option. For example, to sort by x-height value,         you'd use the ``"xHeight"`` attribute name.          ::              >>> fonts.sortBy("xHeight")           Sort Value Function         -------------------          A sort value function must be a function that accepts         one argument, ``font``. This function must return         a sortable value for the given font. For example:          ::              def glyphCountSortValue(font):                 return len(font)              >>> fonts.sortBy(glyphCountSortValue)          A list of sort description strings and/or sort functions         may also be provided. This should be in order of most         to least important. For example, to sort by family name         and then style name, do this:           "magic"         -------          If "magic" is given for ``sortBy``, the fonts will be         sorted based on this sort description sequence:          * ``"familyName"``         * ``"isProportional"``         * ``"widthValue"``         * ``"weightValue"``         * ``"styleName"``         * ``"isRoman"``          ::              >>> fonts.sortBy("magic")
Get a list of fonts that match the (attribute, value)         combinations in ``attributeValuePairs``.          ::              >>> subFonts = fonts.getFontsByFontInfoAttribute(("xHeight", 20))             >>> subFonts = fonts.getFontsByFontInfoAttribute(("xHeight", 20), ("descender", -150))          This will return an instance of :class:`BaseFontList`.
Get the guideline's index.         This must return an ``int``.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseGuideline.transformBy`.          **matrix** will be a :ref:`type-transformation`.         that has been normalized with :func:`normalizers.normalizeTransformationMatrix`.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseGuideline.isCompatible`.          Subclasses may override this method.
Normalizes a font's file format version.      * **value** must be a :ref:`type-int`.     * Returned value will be a ``int``.
Normalizes layer order.      ** **value** must be a ``tuple`` or ``list``.     * **value** items must normalize as layer names with       :func:`normalizeLayerName`.     * **value** must contain layers that exist in **font**.     * **value** must not contain duplicate layers.     * Returned ``tuple`` will be unencoded ``unicode`` strings       for each layer name.
Normalizes default layer name.      * **value** must normalize as layer name with       :func:`normalizeLayerName`.     * **value** must be a layer in **font**.     * Returned value will be an unencoded ``unicode`` string.
Normalizes glyph order.      ** **value** must be a ``tuple`` or ``list``.     * **value** items must normalize as glyph names with       :func:`normalizeGlyphName`.     * **value** must not repeat glyph names.     * Returned value will be a ``tuple`` of unencoded ``unicode`` strings.
Normalizes kerning key.      * **value** must be a ``tuple`` or ``list``.     * **value** must contain only two members.     * **value** items must be :ref:`type-string`.     * **value** items must be at least one character long.     * Returned value will be a two member ``tuple`` of unencoded       ``unicode`` strings.
Normalizes kerning value.      * **value** must be an :ref:`type-int-float`.     * Returned value is the same type as input value.
Normalizes group value.      * **value** must be a ``list``.     * **value** items must normalize as glyph names with       :func:`normalizeGlyphName`.     * Returned value will be a ``tuple`` of unencoded ``unicode`` strings.
Normalizes feature text.      * **value** must be a :ref:`type-string`.     * Returned value will be an unencoded ``unicode`` string.
Normalizes lib value.      * **value** must not be ``None``.     * Returned value is the same type as the input value.
Normalizes glyph unicodes.      * **value** must be a ``list``.     * **value** items must normalize as glyph unicodes with       :func:`normalizeGlyphUnicode`.     * **value** must not repeat unicode values.     * Returned value will be a ``tuple`` of ints.
Normalizes glyph unicode.      * **value** must be an int or hex (represented as a string).     * **value** must be in a unicode range.     * Returned value will be an ``int``.
Normalizes glyph width.      * **value** must be a :ref:`type-int-float`.     * Returned value is the same type as the input value.
Normalizes glyph left margin.      * **value** must be a :ref:`type-int-float` or `None`.     * Returned value is the same type as the input value.
Normalizes glyph right margin.      * **value** must be a :ref:`type-int-float` or `None`.     * Returned value is the same type as the input value.
Normalizes glyph height.      * **value** must be a :ref:`type-int-float`.     * Returned value is the same type as the input value.
Normalizes glyph bottom margin.      * **value** must be a :ref:`type-int-float` or `None`.     * Returned value is the same type as the input value.
Normalizes glyph top margin.      * **value** must be a :ref:`type-int-float` or `None`.     * Returned value is the same type as the input value.
Normalizes glyph format version for saving to XML string.      * **value** must be a :ref:`type-int-float` of either 1 or 2.     * Returned value will be an int.
Normalizes point type.      * **value** must be an string.     * **value** must be one of the following:        +----------+       | move     |       +----------+       | line     |       +----------+       | offcurve |       +----------+       | curve    |       +----------+       | qcurve   |       +----------+      * Returned value will be an unencoded ``unicode`` string.
Normalizes point name.      * **value** must be a :ref:`type-string`.     * **value** must be at least one character long.     * Returned value will be an unencoded ``unicode`` string.
Normalizes bPoint type.      * **value** must be an string.     * **value** must be one of the following:        +--------+       | corner |       +--------+       | curve  |       +--------+      * Returned value will be an unencoded ``unicode`` string.
Normalizes an internal object type.      * **value** must be a instance of **cls**.     * Returned value is the same type as the input value.
Normalizes a boolean.      * **value** must be an ``int`` with value of 0 or 1, or a ``bool``.     * Returned value will be a boolean.
Normalizes index.      * **value** must be an ``int`` or ``None``.     * Returned value is the same type as the input value.
Normalizes identifier.      * **value** must be an :ref:`type-string` or `None`.     * **value** must not be longer than 100 characters.     * **value** must not contain a character out the range of 0x20 - 0x7E.     * Returned value is an unencoded ``unicode`` string.
Normalizes x coordinate.      * **value** must be an :ref:`type-int-float`.     * Returned value is the same type as the input value.
Normalizes y coordinate.      * **value** must be an :ref:`type-int-float`.     * Returned value is the same type as the input value.
Normalizes coordinate tuple.      * **value** must be a ``tuple`` or ``list``.     * **value** must have exactly two items.     * **value** items must be an :ref:`type-int-float`.     * Returned value is a ``tuple`` of two values of the same type as       the input values.
Normalizes area.      * **value** must be a positive :ref:`type-int-float`.
Normalizes an angle.      * Value must be a :ref:`type-int-float`.     * Value must be between -360 and 360.     * If the value is negative, it is normalized by adding it to 360     * Returned value is a ``float`` between 0 and 360.
Normalizes :ref:`type-color`.      * **value** must be an ``tuple`` or ``list``.     * **value** must have exactly four items.     * **value** color components must be between 0 and 1.     * Returned value is a ``tuple`` containing four ``float`` values.
Normalizes Glyph Note.      * **value** must be a :ref:`type-string`.     * Returned value is an unencoded ``unicode`` string
Normalizes file path.      * **value** must be a :ref:`type-string`.     * Returned value is an unencoded ``unicode`` string
Normalizes interpolation factor.      * **value** must be an :ref:`type-int-float`, ``tuple`` or ``list``.     * If **value** is a ``tuple`` or ``list``, it must have exactly two items.       These items must be instances of :ref:`type-int-float`.     * Returned value is a ``tuple`` of two ``float``.
Normalizes rounding.      Python 2 and Python 3 handing the rounding of halves (0.5, 1.5, etc)     differently. This normalizes rounding to be the same (Python 3 style)     in both environments.      * **value** must be an :ref:`type-int-float`     * Returned value is a ``int``
An ask a string dialog, a `message` is required.     Optionally a `value` and `title` can be provided.      ::          from fontParts.ui import AskString         print(AskString("who are you?"))
An ask yes, no or cancel dialog, a `message` is required.     Optionally a `title`, `default` and `informativeText` can be provided.     The `default` option is to indicate which button is the default button.      ::          from fontParts.ui import AskYesNoCancel         print(AskYesNoCancel("who are you?"))
A dialog to search a glyph for a provided  font.     Optionally a `message`, `title` and `allFonts` can be provided.           from fontParts.ui import FindGlyph         from fontParts.world import CurrentFont         glyph = FindGlyph(CurrentFont())         print(glyph)
An get file dialog.     Optionally a `message`, `title`, `directory`, `fileName` and     `allowsMultipleSelection` can be provided.      ::          from fontParts.ui import GetFile         print(GetFile())
An message dialog.     Optionally a `message`, `title` and `informativeText` can be provided.      ::          from fontParts.ui import Message         print(Message("This is a message"))
A dialgo to search a given list.     Optionally a `message`, `title` and `allFonts` can be provided.      ::          from fontParts.ui import SearchList         result = SearchList(["a", "b", "c"])         print(result)
Select a font from all open fonts.     Optionally a `message`, `title` and `allFonts` can be provided.     If `allFonts` is `None` it will list all open fonts.      ::          from fontParts.ui import SelectFont         font = SelectFont()         print(font)
Select a glyph for a given font.     Optionally a `message` and `title` can be provided.      ::          from fontParts.ui import SelectGlyph         font = CurrentFont()         glyph = SelectGlyph(font)         print(glyph)
A progess bar dialog.     Optionally a `title`, `ticks` and `label` can be provided.      ::          from fontParts.ui import ProgressBar          bar = ProgressBar()         # do something         bar.close()
Returns a ``list`` of the group or groups associated with         **glyphName**.         **glyphName** will be an :ref:`type-string`. If no group is found         to contain **glyphName** an empty ``list`` will be returned. ::              >>> font.groups.findGlyph("A")             ["A_accented"]
This is the environment implementation of         :meth:`BaseGroups.findGlyph`. **glyphName** will be         an :ref:`type-string`.          Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Returns the contents of the named group.         **groupName** is a :ref:`type-string`, and the returned values will         either be :ref:`type-immutable-list` of group contents or ``None``         if no group was found. ::              >>> font.groups["myGroup"]             ("A", "B", "C")          It is important to understand that any changes to the returned group         contents will not be reflected in the Groups object. If one wants to         make a change to the group contents, one should do the following::              >>> group = font.groups["myGroup"]             >>> group.remove("A")             >>> font.groups["myGroup"] = group
Removes the **groupName** from the Groups and returns the list of         group members. If no group is found, **default** is returned.         **groupName** is a :ref:`type-string`. This must return either         **default** or a :ref:`type-immutable-list` of glyph names as         :ref:`type-string`.              >>> font.groups.pop("myGroup")             ("A", "B", "C")
Get the point's index.         This must return an ``int``.          Subclasses may override this method.
Copy this object into a new object of the same type.         The returned object will not have a parent object.
Subclasses may override this method.         If so, they should call the super.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Transform the object.              >>> obj.transformBy((0.5, 0, 0, 2.0, 10, 0))             >>> obj.transformBy((0.5, 0, 0, 2.0, 10, 0), origin=(500, 500))          **matrix** must be a :ref:`type-transformation`.         **origin** defines the point at with the transformation         should originate. It must be a :ref:`type-coordinate`         or ``None``. The default is ``(0, 0)``.
Move the object.              >>> obj.transformBy((10, 0))          **value** must be an iterable containing two         :ref:`type-int-float` values defining the x and y         values to move the object by.
This is the environment implementation of         :meth:`BaseObject.moveBy`.          **value** will be an iterable containing two         :ref:`type-int-float` values defining the x and y         values to move the object by. It will have been         normalized with :func:`normalizers.normalizeTransformationOffset`.          Subclasses may override this method.
Scale the object.              >>> obj.transformBy(2.0)             >>> obj.transformBy((0.5, 2.0), origin=(500, 500))          **value** must be an iterable containing two         :ref:`type-int-float` values defining the x and y         values to scale the object by. **origin** defines the         point at with the scale should originate. It must be         a :ref:`type-coordinate` or ``None``. The default is         ``(0, 0)``.
This is the environment implementation of         :meth:`BaseObject.scaleBy`.          **value** will be an iterable containing two         :ref:`type-int-float` values defining the x and y         values to scale the object by. It will have been         normalized with :func:`normalizers.normalizeTransformationScale`.         **origin** will be a :ref:`type-coordinate` defining         the point at which the scale should orginate.          Subclasses may override this method.
Rotate the object.              >>> obj.transformBy(45)             >>> obj.transformBy(45, origin=(500, 500))          **value** must be a :ref:`type-int-float` values         defining the angle to rotate the object by. **origin**         defines the point at with the rotation should originate.         It must be a :ref:`type-coordinate` or ``None``.         The default is ``(0, 0)``.
This is the environment implementation of         :meth:`BaseObject.rotateBy`.          **value** will be a :ref:`type-int-float` value         defining the value to rotate the object by.         It will have been normalized with         :func:`normalizers.normalizeRotationAngle`.         **origin** will be a :ref:`type-coordinate` defining         the point at which the rotation should orginate.          Subclasses may override this method.
Skew the object.              >>> obj.skewBy(11)             >>> obj.skewBy((25, 10), origin=(500, 500))          **value** must be rone of the following:          * single :ref:`type-int-float` indicating the           value to skew the x direction by.         * iterable cointaining type :ref:`type-int-float`           defining the values to skew the x and y directions by.          **origin** defines the point at with the skew should         originate. It must be a :ref:`type-coordinate` or         ``None``. The default is ``(0, 0)``.
This is the environment implementation of         :meth:`BaseObject.skewBy`.          **value** will be an iterable containing two         :ref:`type-int-float` values defining the x and y         values to skew the object by. It will have been         normalized with :func:`normalizers.normalizeTransformationSkewAngle`.         **origin** will be a :ref:`type-coordinate` defining         the point at which the skew should orginate.          Subclasses may override this method.
Evaluate interpolation compatibility with other.
Subclasses may override this method.
Fetches the version number from the package's __init__.py file
Run bump2version.main() with the specified arguments.
Use the default text $EDITOR to write release notes.         If $EDITOR is not set, use 'nano'.
Subclasses may override this method.          If a subclass does not override this method,         it must implement '_get_attributeName' methods         for all Info methods.
Subclasses may override this method.          If a subclass does not override this method,         it must implement '_set_attributeName' methods         for all Info methods.
Subclasses may override this method.
Replaces the contents of this info object with the contents of ``mathInfo``.              >>> font.fromMathInfo(mg)          ``mathInfo`` must be an object following the         `MathInfo protocol <https://github.com/typesupply/fontMath>`_.
Subclasses may override this method.
Subclasses may override this method.
Interpolate all pairs between minInfo and maxInfo.         The interpolation occurs on a 0 to 1.0 range where minInfo         is located at 0 and maxInfo is located at 1.0.          factor is the interpolation value. It may be less than 0         and greater than 1.0. It may be a number (integer, float)         or a tuple of two numbers. If it is a tuple, the first         number indicates the x factor and the second number         indicates the y factor.          round indicates if the result should be rounded to integers.          suppressError indicates if incompatible data should be ignored         or if an error should be raised when such incompatibilities are found.
Subclasses may override this method.
Compile a sass file (and dependencies) into a single css file.
Take CSS file and automatically add browser prefixes with postCSS autoprefixer
Copy data from **source** into this font.         Refer to :meth:`BaseFont.copy` for a list         of values that will be copied.
Save the font to **path**.              >>> font.save()             >>> font.save("/path/to/my/font-2.ufo")          If **path** is None, use the font's original location.         The file type must be inferred from the file extension         of the given path. If no file extension is given, the         environment may fall back to the format of its choice.         **showProgress** indicates if a progress indicator should         be displayed during the operation. Environments may or may         not implement this behavior. **formatVersion** indicates         the format version that should be used for writing the given         file type. For example, if 2 is given for formatVersion         and the file type being written if UFO, the file is to         be written in UFO 2 format. This value is not limited         to UFO format versions. If no format version is given,         the original format version of the file should be preserved.         If there is no original format version it is implied that         the format version is the latest version for the file         type as supported by the environment.          .. note::             Environments may define their own rules governing when            a file should be saved into its original location and            when it should not. For example, a font opened from a            compiled OpenType font may not be written back into            the original OpenType font.
+--------------+--------------------------------------------------------------------+         | mactype1     | Mac Type 1 font (generates suitcase  and LWFN file)                |         +--------------+--------------------------------------------------------------------+         | macttf       | Mac TrueType font (generates suitcase)                             |         +--------------+--------------------------------------------------------------------+         | macttdfont   | Mac TrueType font (generates suitcase with resources in data fork) |         +--------------+--------------------------------------------------------------------+         | otfcff       | PS OpenType (CFF-based) font (OTF)                                 |         +--------------+--------------------------------------------------------------------+         | otfttf       | PC TrueType/TT OpenType font (TTF)                                 |         +--------------+--------------------------------------------------------------------+         | pctype1      | PC Type 1 font (binary/PFB)                                        |         +--------------+--------------------------------------------------------------------+         | pcmm         | PC MultipleMaster font (PFB)                                       |         +--------------+--------------------------------------------------------------------+         | pctype1ascii | PC Type 1 font (ASCII/PFA)                                         |         +--------------+--------------------------------------------------------------------+         | pcmmascii    | PC MultipleMaster font (ASCII/PFA)                                 |         +--------------+--------------------------------------------------------------------+         | ufo1         | UFO format version 1                                               |         +--------------+--------------------------------------------------------------------+         | ufo2         | UFO format version 2                                               |         +--------------+--------------------------------------------------------------------+         | ufo3         | UFO format version 3                                               |         +--------------+--------------------------------------------------------------------+         | unixascii    | UNIX ASCII font (ASCII/PFA)                                        |         +--------------+--------------------------------------------------------------------+
Generate the font to another format.              >>> font.generate("otfcff")             >>> font.generate("otfcff", "/path/to/my/font.otf")          **format** defines the file format to output.         Standard format identifiers can be found in :attr:`BaseFont.generateFormatToExtension`:           Environments are not required to support all of these         and environments may define their own format types.         **path** defines the location where the new file should         be created. If a file already exists at that location,         it will be overwritten by the new file. If **path** defines         a directory, the file will be output as the current         file name, with the appropriate suffix for the format,         into the given directory. If no **path** is given, the         file will be output into the same directory as the source         font with the file named with the current file name,         with the appropriate suffix for the format.          Environments may allow unique keyword arguments in this         method. For example, if a tool allows decomposing components         during a generate routine it may allow this:              >>> font.generate("otfcff", "/p/f.otf", decompose=True)
This is the environment implementation of         :meth:`BaseFont.getFlatKerning`.          Subclasses may override this method.
This is the environment implementation of         :attr:`BaseFont.defaultLayer`. Return the         default layer as a :class:`BaseLayer` object.         The layer will be normalized with         :func:`normalizers.normalizeLayer`.          Subclasses must override this method.
Get the :class:`BaseLayer` with **name**.              >>> layer = font.getLayer("My Layer 2")
This is the environment implementation of         :meth:`BaseFont.getLayer`. **name** will         be a :ref:`type-string`. It will have been         normalized with :func:`normalizers.normalizeLayerName`         and it will have been verified as an existing layer.         This must return an instance of :class:`BaseLayer`.          Subclasses may override this method.
Make a new layer with **name** and **color**.         **name** must be a :ref:`type-string` and         **color** must be a :ref:`type-color` or ``None``.              >>> layer = font.newLayer("My Layer 3")          The will return the newly created         :class:`BaseLayer`.
Remove the layer with **name** from the font.              >>> font.removeLayer("My Layer 3")
Insert **layer** into the font. ::              >>> layer = font.insertLayer(otherLayer, name="layer 2")          This will not insert the layer directly.         Rather, a new layer will be created and the data from         **layer** will be copied to to the new layer. **name**         indicates the name that should be assigned to the layer         after insertion. If **name** is not given, the layer's         original name must be used. If the layer does not have         a name, an error must be raised. The data that will be         inserted from **layer** is the same data as documented         in :meth:`BaseLayer.copy`.
This is the environment implementation of :meth:`BaseFont.insertLayer`.         This must return an instance of a :class:`BaseLayer` subclass.         **layer** will be a layer object with the attributes necessary         for copying as defined in :meth:`BaseLayer.copy` An environment         must not insert **layer** directly. Instead the data from **layer**         should be copied to a new layer. **name** will be a :ref:`type-string`         representing a glyph layer. It will have been normalized with         :func:`normalizers.normalizeLayerName`. **name** will have been         tested to make sure that no layer with the same name exists in the font.          Subclasses may override this method.
Duplicate the layer with **layerName**, assign         **newLayerName** to the new layer and insert the         new layer into the font. ::              >>> layer = font.duplicateLayer("layer 1", "layer 2")
This is the environment implementation of :meth:`BaseFont.duplicateLayer`.         **layerName** will be a :ref:`type-string` representing a valid layer name.         The value will have been normalized with :func:`normalizers.normalizeLayerName`         and **layerName** will be a layer that exists in the font. **newLayerName**         will be a :ref:`type-string` representing a valid layer name. The value will         have been normalized with :func:`normalizers.normalizeLayerName` and         **newLayerName** will have been tested to make sure that no layer with         the same name exists in the font. This must return an instance of a         :class:`BaseLayer` subclass.          Subclasses may override this method.
Assign **layerName** to the layer currently named         **otherLayerName** and assign the name **otherLayerName**         to the layer currently named **layerName**.              >>> font.swapLayerNames("before drawing revisions", "after drawing revisions")
This is the environment implementation of :meth:`BaseFont.swapLayerNames`.         **layerName** will be a :ref:`type-string` representing a valid layer name.         The value will have been normalized with :func:`normalizers.normalizeLayerName`         and **layerName** will be a layer that exists in the font. **otherLayerName**         will be a :ref:`type-string` representing a valid layer name. The value will         have been normalized with :func:`normalizers.normalizeLayerName` and         **otherLayerName** will be a layer that exists in the font.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseFont.newGlyph`. **name** will be         a :ref:`type-string` representing a valid         glyph name. The value will have been tested         to make sure that an existing glyph in the         default layer does not have an identical name.         The value will have been normalized with         :func:`normalizers.normalizeGlyphName`. This         must return an instance of :class:`BaseGlyph`         representing the new glyph.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseFont.removeGlyph`. **name** will         be a :ref:`type-string` representing an         existing glyph in the default layer. The         value will have been normalized with         :func:`normalizers.normalizeGlyphName`.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseFont.round`.          Subclasses may override this method.
Append a new guideline to the font.              >>> guideline = font.appendGuideline((50, 0), 90)             >>> guideline = font.appendGuideline((0, 540), 0, name="overshoot",             >>> color=(0, 0, 0, 0.2))          **position** must be a :ref:`type-coordinate`         indicating the position of the guideline.         **angle** indicates the :ref:`type-angle` of         the guideline. **name** indicates the name         for the guideline. This must be a :ref:`type-string`         or ``None``. **color** indicates the color for         the guideline. This must be a :ref:`type-color`         or ``None``. This will return the newly created         :class:`BaseGuidline` object.          ``guideline`` may be a :class:`BaseGuideline` object from which         attribute values will be copied. If ``position``, ``angle``, ``name``         or ``color`` are specified as arguments, those values will be used         instead of the values in the given guideline object.
Interpolate all possible data in the font.              >>> font.interpolate(0.5, otherFont1, otherFont2)             >>> font.interpolate((0.5, 2.0), otherFont1, otherFont2, round=False)          The interpolation occurs on a 0 to 1.0 range where **minFont**         is located at 0 and **maxFont** is located at 1.0. **factor**         is the interpolation value. It may be less than 0 and greater         than 1.0. It may be a :ref:`type-int-float` or a tuple of         two :ref:`type-int-float`. If it is a tuple, the first         number indicates the x factor and the second number indicates         the y factor. **round** indicates if the result should be         rounded to integers. **suppressError** indicates if incompatible         data should be ignored or if an error should be raised when         such incompatibilities are found.
This is the environment implementation of         :meth:`BaseFont.interpolate`.          Subclasses may override this method.
This is the environment implementation of         :meth:`BaseFont.isCompatible`.          Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
Subclasses may override this method.
This is the environment implementation of         :meth:`BaseSegment.isCompatible`.          Subclasses may override this method.
Create a unique identifier for and assign it to ``point``.         If the point already has an identifier, the existing         identifier will be returned.              >>> contour.getIdentifierForPoint(point)             'ILHGJlygfds'          ``point`` must be a :class:`BasePoint`. The returned value         will be a :ref:`type-identifier`.
Subclasses may override this method.
This is the environment implementation of         :meth:`BaseContour.isCompatible`.          Subclasses may override this method.
Determine if ``otherContour`` is in the black or white of this contour.              >>> contour.contourInside(otherContour)             True          ``contour`` must be a :class:`BaseContour`.
Subclasses may override this method.
Append a segment to the contour.
Subclasses may override this method.
Insert a segment into the contour.
Subclasses may override this method.
Remove segment from the contour.         If ``preserveCurve`` is set to ``True`` an attempt         will be made to preserve the shape of the curve         if the environment supports that functionality.
segment will be a valid segment index.         preserveCurve will be a boolean.          Subclasses may override this method.
Set the first segment on the contour.         segment can be a segment object or an index.
Subclasses may override this method.
Append a bPoint to the contour.
Subclasses may override this method.
Insert a bPoint at index in the contour.
Subclasses may override this method.
Remove the bpoint from the contour.         bpoint can be a point object or an index.
index will be a valid index.          Subclasses may override this method.
Subclasses may override this method.
Append a point to the contour.
Insert a point into the contour.
position will be a valid position (x, y).         type will be a valid type.         smooth will be a valid boolean.         name will be a valid name or None.         identifier will be a valid identifier or None.         The identifier will not have been tested for uniqueness.          Subclasses must override this method.
Remove the point from the contour.         point can be a point object or an index.         If ``preserveCurve`` is set to ``True`` an attempt         will be made to preserve the shape of the curve         if the environment supports that functionality.
Scales all kerning values by **factor**. **factor** will be an         :ref:`type-int-float`, ``tuple`` or ``list``. The first value of the         **factor** will be used to scale the kerning values.              >>> myKerning.scaleBy(2)             >>> myKerning.scaleBy((2,3))
This is the environment implementation of :meth:`BaseKerning.scaleBy`.         **factor** will be a ``tuple``.          Subclasses may override this method.
Rounds the kerning values to increments of **multiple**,         which will be an ``int``.          The default behavior is to round to increments of 1.
This is the environment implementation of         :meth:`BaseKerning.round`. **multiple** will be an ``int``.          Subclasses may override this method.
Interpolates all pairs between two :class:`BaseKerning` objects:          **minKerning** and **maxKerning**. The interpolation occurs on a         0 to 1.0 range where **minKerning** is located at 0 and         **maxKerning** is located at 1.0. The kerning data is replaced by         the interpolated kerning.          * **factor** is the interpolation value. It may be less than 0           and greater than 1.0. It may be an :ref:`type-int-float`,           ``tuple`` or ``list``. If it is a ``tuple`` or ``list``,           the first number indicates the x factor and the second number           indicates the y factor.         * **round** is a ``bool`` indicating if the result should be rounded to           ``int``\s. The default behavior is to round interpolated kerning.         * **suppressError** is a ``bool`` indicating if incompatible data should           be ignored or if an error should be raised when such incompatibilities           are found. The default behavior is to ignore incompatible data.              >>> myKerning.interpolate(kerningOne, kerningTwo)
This is the environment implementation of :meth:`BaseKerning.interpolate`.          * **factor** will be an :ref:`type-int-float`, ``tuple`` or ``list``.         * **minKerning** will be a :class:`BaseKerning` object.         * **maxKerning** will be a :class:`BaseKerning` object.         * **round** will be a ``bool`` indicating if the interpolated kerning           should be rounded.         * **suppressError** will be a ``bool`` indicating if incompatible data           should be ignored.          Subclasses may override this method.
Return the Kerning as a ``dict``.          This is a backwards compatibility method.
Returns the value for the kerning pair.         **pair** is a ``tuple`` of two :ref:`type-string`\s, and the returned         values will either be :ref:`type-int-float` or ``None``         if no pair was found. ::              >>> font.kerning[("A", "V")]             -25          It is important to understand that any changes to the returned value         will not be reflected in the Kerning object. If one wants to make a change to         the value, one should do the following::              >>> value = font.kerning[("A", "V")]             >>> value += 10             >>> font.kerning[("A", "V")] = value
Returns the value for the kerning pair.         **pair** is a ``tuple`` of two :ref:`type-string`\s, and the returned         values will either be :ref:`type-int-float` or ``None``         if no pair was found. ::              >>> font.kerning[("A", "V")]             -25
This is the environment implementation of         :attr:`BaseKerning.find`. This must return an         :ref:`type-int-float` or `default`.
Removes the **pair** from the Kerning and returns the value as an ``int``.         If no pair is found, **default** is returned. **pair** is a         ``tuple`` of two :ref:`type-string`\s. This must return either          **default** or a :ref:`type-int-float`.              >>> font.kerning.pop(("A", "V"))             -20             >>> font.kerning.pop(("A", "W"))             -10.5
Note that int values are accepted.
Convert signed angle float like -42.42 to int 60000 per degree,         normalized to positive value.
Convert signed angle float like -427.42 to int 60000 per degree.          Value is normalized to a positive value less than 360 degrees.
The effective value of *attr_name* on this placeholder shape; its         directly-applied value if it has one, otherwise the value on the         layout placeholder it inherits from.
Return the attribute value, e.g. 'width' of the base placeholder this         placeholder inherits from.
Return the layout placeholder this slide placeholder inherits from.         Not to be confused with an instance of |BasePlaceholder|         (necessarily).
Substitute *element* for this placeholder element in the shapetree.         This placeholder's `._element` attribute is set to |None| and its         original element is free for garbage collection. Any attribute access         (including a method call) on this placeholder after this call raises         |AttributeError|.
Return the master placeholder this layout placeholder inherits from.
Return the notes master placeholder this notes slide placeholder         inherits from, or |None| if no placeholder of the matching type is         present.
Return a |PlaceholderGraphicFrame| object containing a new chart of         *chart_type* depicting *chart_data* and having the same position and         size as this placeholder. *chart_type* is one of the         :ref:`XlChartType` enumeration values. *chart_data* is a |ChartData|         object populated with the categories and series values for the chart.         Note that the new |Chart| object is not returned directly. The chart         object may be accessed using the         :attr:`~.PlaceholderGraphicFrame.chart` property of the returned         |PlaceholderGraphicFrame| object.
Return a newly created `p:graphicFrame` element having the specified         position and size and containing the chart identified by *rId*.
Return a |PlaceholderPicture| object depicting the image in         *image_file*, which may be either a path (string) or a file-like         object. The image is cropped to fill the entire space of the         placeholder. A |PlaceholderPicture| object has all the properties and         methods of a |Picture| shape except that the value of its         :attr:`~._BaseSlidePlaceholder.shape_type` property is         `MSO_SHAPE_TYPE.PLACEHOLDER` instead of `MSO_SHAPE_TYPE.PICTURE`.
Return a new `p:pic` element depicting the image in *image_file*,         suitable for use as a placeholder. In particular this means not         having an `a:xfrm` element, allowing its extents to be inherited from         its layout placeholder.
Return an (rId, description, image_size) 3-tuple identifying the         related image part containing *image_file* and describing the image.
Return a |PlaceholderGraphicFrame| object containing a table of         *rows* rows and *cols* columns. The position and width of the table         are those of the placeholder and its height is proportional to the         number of rows. A |PlaceholderGraphicFrame| object has all the         properties and methods of a |GraphicFrame| shape except that the         value of its :attr:`~._BaseSlidePlaceholder.shape_type` property is         unconditionally `MSO_SHAPE_TYPE.PLACEHOLDER`. Note that the return         value is not the new table but rather *contains* the new table. The         table can be accessed using the         :attr:`~.PlaceholderGraphicFrame.table` property of the returned         |PlaceholderGraphicFrame| object.
Return a newly added `p:graphicFrame` element containing an empty         table with *rows* rows and *cols* columns, positioned at the location         of this placeholder and having its same width. The table's height is         determined by the number of rows.
|GroupShapes| object for this group.          The |GroupShapes| object provides access to the group's member shapes         and provides methods for adding new ones.
create (or recreate) the auto shape type tables
load adjustment values and their default values from XML
print symbolic constant definitions for msoAutoShapeType
print symbolic constant definitions for msoAutoShapeType
print spec dictionary for msoAutoShapeType
calculate desc string, wrapped if too long
split string *s* into list of strings no longer than *length*
convert upper snake case string to mixed case, e.g. MIXED_CASE becomes     MixedCase
Return arguments object formed by parsing the command line used to launch     the program.
load adjustment values for auto shape types in self
Return *text* as a unicode string. All text in Python 3 is unicode, so     this just returns *text* unchanged.
Return direct child of *element* having *child_tagname* or :class:`None`     if no such child element is present.
Return list containing the direct children of *element* having     *child_tagname*.
Add attribute with default value to element if it doesn't already exist.
Add child element with *tag* to *parent* if it doesn't already exist.
Return a ``<p:graphicFrame>`` element tree populated with a chart         element.
Return a new ``<p:graphicFrame>`` element tree suitable for         containing a table or chart. Note that a graphicFrame element is not         a valid shape until it contains a graphical object such as a table.
Return a ``<p:graphicFrame>`` element tree populated with a table         element.
Return a newly added <a:lumMod> child element.
Return a newly added <a:lumOff> child element.
Return short-form prefixed tag from fully qualified (Clark notation)     tagname.
Return fully qualified (Clark notation) tagname corresponding to     short-form prefixed tagname *tag*.
Return complex type element with name *typename*
Return definition element with name *defname*
Add an element to this ComplexType and also append it to element dict         of parent type graph.
blipFill = ElementDef('p:blipFill', 'CT_BlipFillProperties')         blipFill.add_child('a:blip', cardinality='?')         blipFill.add_attributes('dpi', 'rotWithShape')
Selectively add placeholder shape elements from *notes_master* to the         shapes collection of this notes slide. Z-order of placeholders is         preserved. Certain placeholders (header, date, footer) are not         cloned.
Return the notes placeholder on this notes slide, the shape that         contains the actual notes text. Return |None| if no notes placeholder         is present; while this is probably uncommon, it can happen if the         notes master does not have a body placeholder, or if the notes         placeholder has been deleted from the notes slide.
Return a newly added slide that inherits layout from *slide_layout*.
Return the slide identified by integer *slide_id* in this         presentation, or *default* if not found.
Map *slide* to an integer representing its zero-based position in         this slide collection. Raises |ValueError| on *slide* not present.
Generate a reference to each layout placeholder on this slide layout         that should be cloned to a slide when the layout is applied to that         slide.
Tuple of slide objects based on this slide layout.
Return SlideLayout object having *name* or *default* if not found.
Return zero-based index of *slide_layout* in this collection.          Raises ValueError if *slide_layout* is not present in this collection.
Remove *slide_layout* from the collection.          Raises ValueError when *slide_layout* is in use; a slide layout which is the         basis for one or more slides cannot be removed.
|Slides| object containing the slides in this presentation.
Return the `c:tx[c:rich]` subtree, newly created if not present.
Remove any `c:tx[c:rich]` child, or do nothing if not present.
Return the `c:dLbl` element representing the label of the point at         index *idx*.
Return a newly created `c:dLbl` element having `c:idx` child of *idx*         and inserted in numeric sequence among the `c:dLbl` children of this         element.
Set the value of the ``<c:max>`` child element to the float *value*,         or remove the max element if *value* is |None|.
Set the value of the ``<c:min>`` child element to the float *value*,         or remove the min element if *value* is |None|.
Return a new |FreeformBuilder| object.          The initial pen location is specified (in local coordinates) by         (*start_x*, *start_y*).
Add a straight line segment to each point in *vertices*.          *vertices* must be an iterable of (x, y) pairs (2-tuples). Each x and         y value is rounded to the nearest integer before use. The optional         *close* parameter determines whether the resulting contour is         *closed* or left *open*.          Returns this |FreeformBuilder| object so it can be used in chained         calls.
Return new freeform shape positioned relative to specified offset.          *origin_x* and *origin_y* locate the origin of the local coordinate         system in slide coordinates (EMU), perhaps most conveniently by use         of a |Length| object.          Note that this method may be called more than once to add multiple         shapes of the same geometry in different locations on the slide.
Move pen to (x, y) (local coordinates) without drawing line.          Returns this |FreeformBuilder| object so it can be used in chained         calls.
Return x distance of shape origin from local coordinate origin.          The returned integer represents the leftmost extent of the freeform         shape, in local coordinates. Note that the bounding box of the shape         need not start at the local origin.
Return y distance of shape origin from local coordinate origin.          The returned integer represents the topmost extent of the freeform         shape, in local coordinates. Note that the bounding box of the shape         need not start at the local origin.
Add a freeform `p:sp` element having no drawing elements.          *origin_x* and *origin_y* are specified in slide coordinates, and         represent the location of the local coordinates origin on the slide.
Add a |_LineSegment| operation to the drawing sequence.
Return integer width of this shape's path in local units.
Return integer height of this shape's path in local units.
Translate local coordinates point to shape coordinates.          Shape coordinates have the same unit as local coordinates, but are         offset such that the origin of the shape coordinate system (0, 0) is         located at the top-left corner of the shape bounding box.
Return a newly created `a:path` element added to *sp*.          The returned `a:path` element has an `a:moveTo` element representing         the shape starting point as its only child.
Return a new _LineSegment object ending at point *(x, y)*.          Both *x* and *y* are rounded to the nearest integer before use.
Add `a:lnTo` element to *path* for this line segment.          Returns the `a:lnTo` element newly added to the path.
Return a |FillFormat| instance initialized to the settings contained         in *eg_fillProperties_parent*, which must be an element having         EG_FillProperties in its child element sequence in the XML schema.
Sets the fill type to noFill, i.e. transparent.
Sets the fill type to gradient.          If the fill is not already a gradient, a default gradient is added.         The default gradient corresponds to the default in the built-in         PowerPoint "White" template. This gradient is linear at angle         90-degrees (upward), with two stops. The first stop is Accent-1 with         tint 100%, shade 100%, and satMod 130%. The second stop is Accent-1         with tint 50%, shade 100%, and satMod 350%.
Angle in float degrees of line of a linear gradient.          Read/Write. May be |None|, indicating the angle should be inherited         from the style hierarchy. An angle of 0.0 corresponds to         a left-to-right gradient. Increasing angles represent         counter-clockwise rotation of the line, for example 90.0 represents         a bottom-to-top gradient. Raises |TypeError| when the fill type is         not MSO_FILL_TYPE.GRADIENT. Raises |ValueError| for a non-linear         gradient (e.g. a radial gradient).
|GradientStops| object providing access to stops of this gradient.          Raises |TypeError| when fill is not gradient (call `fill.gradient()`         first). Each stop represents a color between which the gradient         smoothly transitions.
Selects the pattern fill type.          Note that calling this method does not by itself set a foreground or         background color of the pattern. Rather it enables subsequent         assignments to properties like fore_color to set the pattern and         colors.
Sets the fill type to solid, i.e. a solid color. Note that calling         this method does not set a color or by itself cause the shape to         appear with a solid color fill; rather it enables subsequent         assignments to properties like fore_color to set the color.
Angle in float degrees of line of a linear gradient.          Read/Write. May be |None|, indicating the angle is inherited from the         style hierarchy. An angle of 0.0 corresponds to a left-to-right         gradient. Increasing angles represent clockwise rotation of the line,         for example 90.0 represents a top-to-bottom gradient. Raises         |TypeError| when the fill type is not MSO_FILL_TYPE.GRADIENT. Raises         |ValueError| for a non-linear gradient (e.g. a radial gradient).
The |ChartFormat| object providing access to the shape formatting         properties of this data point, such as line and fill.
The |Marker| instance for this point, providing access to the visual         properties of the data point marker, such as fill and line. Setting         these properties overrides any value set at the series level.
Append a new ``<p:sp>`` shape to the group/shapetree having the         properties specified in call.
Append a new ``<p:cxnSp>`` shape to the group/shapetree having the         properties specified in call.
Append a new freeform `p:sp` with specified position and size.
Return `p:grpSp` element newly appended to this shape tree.          The element contains no sub-shapes, is positioned at (0, 0), and has         width and height of zero.
Append a ``<p:pic>`` shape to the group/shapetree having properties         as specified in call.
Append a newly-created placeholder ``<p:sp>`` shape having the         specified placeholder properties.
Append a ``<p:graphicFrame>`` shape containing a table as specified         in call.
Append a newly-created textbox ``<p:sp>`` shape having the specified         position and size.
Generate each child of this ``<p:spTree>`` element that corresponds         to a shape, in the sequence they appear in the XML.
Maximum int value assigned as @id in this slide.          This is generally a shape-id, but ids can be assigned to other         objects so we just check all @id values anywhere in the document         (XML id-values have document scope).          In practice, its minimum value is 1 because the spTree element itself         is always assigned id="1".
Return new "loose" `p:grpSp` element having *id_* and *name*.
Adjust x, y, cx, and cy to incorporate all contained shapes.          This would typically be called when a contained shape is added,         removed, or its position or size updated.          This method is recursive "upwards" since a change in a group shape         can change the position and size of its containing group.
(x, y, cx, cy) tuple representing net position and size.          The values are formed as a composite of the contained child shapes.
Return unique shape id suitable for use with a new shape element.          The returned id is the next available positive integer drawing object         id in shape tree, starting from 1 and making use of any gaps in         numbering. In practice, the minimum id is 2 because the spTree         element itself is always assigned id="1".
Add, remove, or leave alone the ``<c:legend>`` child element depending         on current state and *bool_value*. If *bool_value* is |True| and no         ``<c:legend>`` element is present, a new default element is added.         When |False|, any existing legend element is removed.
Return a new ``<c:chart>`` element
Always add a ``<c:autoUpdate val="0"/>`` child so auto-updating         behavior is off by default.
Generate each xChart child element in document.
Return the last `<c:ser>` element in the last xChart element, based         on series order (not necessarily the same element as document order).
Return the next available `c:ser/c:idx` value within the scope of         this chart, the maximum idx value found on existing series,         incremented by one.
Return the next available `c:ser/c:order` value within the scope of         this chart, the maximum order value found on existing series,         incremented by one.
Return cell at *row_idx*, *col_idx*.          Return value is an instance of |_Cell|. *row_idx* and *col_idx* are         zero-based, e.g. cell(0, 0) is the top, left cell in the table.
Called by a row when its height changes, triggering the graphic frame         to recalculate its total height (as the sum of the row heights).
Called by a column when its width changes, triggering the graphic         frame to recalculate its total width (as the sum of the column         widths).
Create merged cell from this cell to *other_cell*.          This cell and *other_cell* specify opposite corners of the merged         cell range. Either diagonal of the cell region may be specified in         either order, e.g. self=bottom-right, other_cell=top-left, etc.          Raises |ValueError| if the specified range already contains merged         cells anywhere within its extents or if *other_cell* is not in the         same table as *self*.
Remove merge from this (merge-origin) cell.          The merged cell represented by this object will be "unmerged",         yielding a separate unmerged cell for each grid cell previously         spanned by this merge.          Raises |ValueError| when this cell is not a merge-origin cell. Test         with `.is_merge_origin` before calling.
Return a new |EmbeddedXlsxPart| instance added to *package* and         containing *xlsx_blob*.
Return a 'loose' lxml element having the tag specified by *nsptag_str*.     *nsptag_str* must contain the standard namespace prefix, e.g. 'a:tbl'.     The resulting element is an instance of the custom element class for this     tag name if one is defined.
Remove all child elements having tagname in *tagnames*.
Override of ``lxml`` _Element.xpath() method to provide standard Open         XML namespace mapping in centralized location.
Set `a:srcRect` child to crop according to *cropping* values.
The |ColorFormat| instance that provides access to the color settings         for this line. Essentially a shortcut for ``line.fill.fore_color``.         As a side-effect, accessing this property causes the line fill type         to be set to ``MSO_FILL.SOLID``. If this sounds risky for your use         case, use ``line.fill.type`` to non-destructively discover the         existing fill type.
The |ChartPart| object containing the chart in this graphic frame.
Unique integer identifying the type of this shape, e.g.         ``MSO_SHAPE_TYPE.TABLE``.
The |Table| object contained in this graphic frame. Raises         |ValueError| if this graphic frame does not contain a table.
Write ``[Content_Types].xml`` part to the physical package with an         appropriate content type lookup target for each part in *parts*.
Return content types XML mapping each part in *parts* to the         appropriate content type and suitable for storage as         ``[Content_Types].xml`` in an OPC package.
Return a (width, height) pair representing the size of *text* in English     Metric Units (EMU) when rendered at *point_size* in the font defined in     *font_file*.
Return the largest whole-number point size less than or equal to         *max_size* that allows *text* to fit completely within *extents* when         rendered using font defined in *font_file*.
Return the largest whole-number point size less than or equal to         *max_size* that this fitter can fit.
Return a (line, remainder) pair where *line* is the longest line in         *line_source* that will fit in this fitter's width and *remainder* is         a |_LineSource| object containing the text following the break point.
Return a function taking a text string value and returns |True| if         that text fits in this fitter when rendered at *point_size*. Used as         predicate for _break_line()
Return a function taking an integer point size argument that returns         |True| if the text in this fitter can be wrapped to fit entirely         within its extents when rendered at that point size.
Return a sequence of str values representing the text in         *line_source* wrapped within this fitter when rendered at         *point_size*.
Return the largest item in or under this node that satisfies         *predicate*.
Return the root of a balanced binary search tree populated with the         values in iterable *iseq*.
Insert a new node containing *value* into this tree such that its         structure as a binary search tree is preserved.
A string representation of the tree rooted in this node, useful for         debugging purposes.
Return a (medial_value, greater_values, lesser_values) 3-tuple         obtained by bisecting sequence *seq*.
Insert the new values contained in *seq* into this tree such that         a balanced tree is produced.
Return an integer representing the number of hierarchical levels in         this category collection. Returns 1 for non-hierarchical categories         and 0 if no categories are present (generally meaning no series are         present).
Return a sequence of tuples, each containing the flattened hierarchy         of category labels for a leaf category. Each tuple is in parent ->         child order, e.g. ``('US', 'CA', 'San Francisco')``, with the leaf         category appearing last. If this categories collection is         non-hierarchical, each tuple will contain only a leaf category label.         If the plot has no series (and therefore no categories), an empty         tuple is returned.
Return a sequence of |CategoryLevel| objects representing the         hierarchy of this category collection. The sequence is empty when the         category collection is not hierarchical, that is, contains only         leaf-level categories. The levels are ordered from the leaf level to         the root level; so the first level will contain the same categories         as this category collection.
Generate a ``tuple`` object for each leaf category in this         collection, containing the leaf category followed by its "parent"         categories, e.g. ``('San Francisco', 'CA', 'USA'). Each tuple will be         the same length as the number of levels (excepting certain edge         cases which I believe always indicate a chart construction error).
Return a tuple formed by recursively concatenating *categories* with         its next ancestor from *levels*. The idx value of the first category         in *categories* determines parentage in all levels. The returned         sequence is in child -> parent order. A parent category is the         Category object in a next level having the maximum idx value not         exceeding that of the leaf category.
Return a new |ChartPart| instance added to *package* containing         a chart of *chart_type* and depicting *chart_data*.
Replace the Excel spreadsheet in the related |EmbeddedXlsxPart| with         the Excel binary in *xlsx_blob*, adding a new |EmbeddedXlsxPart| if         there isn't one.
Return the related |EmbeddedXlsxPart| object having its rId at         `c:chartSpace/c:externalData/@rId` or |None| if there is no         `<c:externalData>` element.
Set the related |EmbeddedXlsxPart| to *xlsx_part*. Assume one does         not already exist.
Generate a 3-tuple `(partname, content_type, blob)` for each of the         serialized parts in the package.
Return a list of |_SerializedPart| instances corresponding to the         parts in *phys_reader* accessible by walking the relationship graph         starting with *pkg_srels*.
Return |_SerializedRelationshipCollection| instance populated with         relationships for source identified by *source_uri*.
Generate a 3-tuple `(partname, blob, srels)` for each of the parts in         *phys_reader* by walking the relationship graph rooted at srels.
Return a new |_ContentTypeMap| instance populated with the contents         of *content_types_xml*.
Return |_SerializedRelationshipCollection| instance loaded with the         relationships contained in *rels_item_xml*. Returns an empty         collection if *rels_item_xml* is |None|.
Return an instance of |Table| appropriate to *tag*, loaded from     *font_file* with content of *length* starting at *offset*.
Return the absolute path to the installed OpenType font having         *family_name* and the styles *is_bold* and *is_italic*.
Return a dict mapping a font descriptor to its font file path,         containing all the font files resident on the current machine. The         font descriptor is a (family_name, is_bold, is_italic) 3-tuple.
Return a sequence of directory paths likely to contain fonts on the         current platform.
Generate the OpenType font files found in and under *directory*. Each         item is a key/value pair. The key is a (family_name, is_bold,         is_italic) 3-tuple, like ('Arial', True, False), and the value is the         absolute path to the font file.
Return a sequence of directory paths on a Mac in which fonts are         likely to be located.
Generate a (tag, offset, length) 3-tuple for each of the tables in         this font file.
A mapping of OpenType table tag, e.g. 'name', to a table object         providing access to the contents of that table.
Return *length* bytes from this stream starting at *offset*.
Return a tuple containing the C-struct fields in this stream         specified by *template* and starting at *offset*.
The name of the typeface family for this font, e.g. 'Arial'.
Return the unicode name decoded from *raw_name* using the encoding         implied by the combination of *platform_id* and *encoding_id*.
Generate a key/value pair for each name in this table. The key is a         (platform_id, name_id) 2-tuple and the value is the unicode text         corresponding to that key.
Return the *length* bytes comprising the encoded string in *bufr* at         *str_offset* in the strings area beginning at *strings_offset*.
Return a (platform_id, name_id, name) 3-tuple like (0, 1, 'Arial')         for the name at *idx* position in *bufr*. *strings_offset* is the         index into *bufr* where actual name strings begin. The returned name         is a unicode string.
Return the unicode name string at *name_str_offset* or |None| if         decoding its format is not supported.
Return a new ``<p:tbl>`` element tree.
Generalized getter for the boolean properties on the ``<a:tblPr>``         child element. Defaults to False if *propname* attribute is missing         or ``<a:tblPr>`` element itself is not present.
Generalized setter for boolean properties on the ``<a:tblPr>`` child         element, setting *propname* attribute appropriately based on *value*.         If *value* is True, the attribute is set to "1"; a tblPr child         element is added if necessary. If *value* is False, the *propname*         attribute is removed if present, allowing its default value of False         to be its effective value.
Set value of anchor attribute on ``<a:tcPr>`` child element
Append `a:p` elements taken from *spanned_tc*.          Any non-empty paragraph elements in *spanned_tc* are removed and         appended to the text-frame of this cell. If *spanned_tc* is left with         no content after this process, a single empty `a:p` element is added         to ensure the cell is compliant with the spec.
True if cell is top-left in merged cell range.
str text contained in cell
Generalized method to get margin values.
Set value of marX attribute on ``<a:tcPr>`` child element. If *marX*         is |None|, the marX attribute is removed. *marX* is a string, one of         ``('marL', 'marR', 'marT', 'marB')``.
Return instance created from merge-origin tc element.
True if one or more cells in range are part of a merged cell.
(row_count, col_count) pair describing size of range.
True if both cells provided to constructor are in same table.
Generate each `a:tc` element in non-first rows of range.
Generate each `a:tc` element in leftmost column of range.
Generate each `a:tc` element in topmost row of range.
Move all paragraphs in range to origin cell.
Index of row following last row of range
A (left, top, width, height) tuple describing range extents.          Note this is normalized to accommodate the various orderings of the         corner cells provided on construction, which may be in any of four         configurations such as (top-left, bottom-right),         (bottom-left, top-right), etc.
Index of column following the last column in range
A member of the :ref:`PpActionType` enumeration, such as         `PP_ACTION.HYPERLINK`, indicating the type of action that will result         when the specified shape or text is clicked or the mouse pointer is         positioned over the shape during a slide show.
A reference to the slide in this presentation that is the target of         the slide jump action in this shape. Slide jump actions include         `PP_ACTION.FIRST_SLIDE`, `LAST_SLIDE`, `NEXT_SLIDE`,         `PREVIOUS_SLIDE`, and `NAMED_SLIDE`. Returns |None| for all other         actions. In particular, the `LAST_SLIDE_VIEWED` action and the `PLAY`         (start other presentation) actions are not supported.          A slide object may be assigned to this property, which makes the         shape an "internal hyperlink" to the assigened slide::              slide, target_slide = prs.slides[0], prs.slides[1]             shape = slide.shapes[0]             shape.target_slide = target_slide          Assigning |None| removes any slide jump action. Note that this is         accomplished by removing any action present (such as a hyperlink),         without first checking that it is a slide jump action.
Reference to the `a:hlinkClick` or `h:hlinkHover` element for this         click action. Returns |None| if the element is not present.
Read/write. The URL of the hyperlink. URL can be on http, https,         mailto, or file scheme; others may work. Returns |None| if no         hyperlink is defined, including when another action such as         `RUN_MACRO` is defined on the object. Assigning |None| removes any         action defined on the object, whether it is a hyperlink action or         not.
Get the `a:hlinkClick` or `a:hlinkHover` element for the Hyperlink         object, depending on the value of `self._hover`. Create one if not         present.
Remove the a:hlinkClick or a:hlinkHover element, including dropping         any relationship it might have.
The |Font| object that provides access to the text properties for         these data labels, such as bold, italic, etc.
Read/write boolean specifying whether number formatting should be         taken from the source spreadsheet rather than the value of         :meth:`number_format`.
The |Font| object providing text formatting for this data label.          This font object is used to customize the appearance of automatically         inserted text, such as the data point value. The font applies to the         entire data label. More granular control of the appearance of custom         data label text is controlled by a font object on runs in the text         frame.
Return |True| if this data label has a text frame (implying it has         custom data label text), and |False| otherwise. Assigning |True|         causes a text frame to be added if not already present. Assigning         |False| causes any existing text frame to be removed along with any         text contained in the text frame.
Read/write :ref:`XlDataLabelPosition` member specifying the position         of this data label with respect to its data point, or |None| if no         position is specified. Assigning |None| causes PowerPoint to choose         the default position, which varies by chart type.
Return the `c:rich` element representing the text frame for this data         label, newly created with its ancestors if not present.
Return the `c:tx` element for this data label, with its `c:rich`         child and descendants, newly created if not yet present.
|True| if legend should be located inside plot area.          Read/write boolean specifying whether legend should be placed inside         the plot area. In many cases this will cause it to be superimposed on         the chart itself. Assigning |None| to this property causes any         `c:overlay` element to be removed, which is interpreted the same as         |True|. This use case should rarely be required and assigning         a boolean value is recommended.
Read/write :ref:`XlLegendPosition` enumeration value specifying the         general region of the chart in which to place the legend.
Return a new ``<Relationship>`` element.
Add a child ``<Default>`` element with attributes set to parameter         values.
Add a child ``<Override>`` element with attributes set to parameter         values.
Return an (rId, slide) pair of a newly created blank slide that         inherits appearance from *slide_layout*.
Return the |Slide| object identified by *slide_id* (in this         presentation), or |None| if not found.
Return the |NotesMasterPart| object for this presentation. If the         presentation does not have a notes master, one is created from         a default template. The same single instance is returned on each         call.
Assign incrementing partnames like ``/ppt/slides/slide9.xml`` to the         slide parts identified by *rIds*, in the order their id appears in         that sequence. The name portion is always ``slide``. The number part         forms a continuous sequence starting at 1 (e.g. 1, 2, ... 10, ...).         The extension is always ``.xml``.
Return the slide identifier associated with *slide_part* in this         presentation.
Return |PackURI| instance containing the partname for a slide to be         appended to this slide collection, e.g. ``/ppt/slides/slide9.xml``         for a slide collection containing 8 slides.
Return |Image| object containing poster frame for this movie.          Returns |None| if this movie has no poster frame (uncommon).
Member of MSO_SHAPE indicating masking shape.          A picture can be masked by any of the so-called "auto-shapes"         available in PowerPoint, such as an ellipse or triangle. When         a picture is masked by a shape, the shape assumes the same dimensions         as the picture and the portion of the picture outside the shape         boundaries does not appear. Note the default value for         a newly-inserted picture is `MSO_AUTO_SHAPE_TYPE.RECTANGLE`, which         performs no cropping because the extents of the rectangle exactly         correspond to the extents of the picture.          The available shapes correspond to the members of         :ref:`MsoAutoShapeType`.          The return value can also be |None|, indicating the picture either         has no geometry (not expected) or has custom geometry, like         a freeform shape. A picture with no geometry will have no visible         representation on the slide, although it can be selected. This is         because without geometry, there is no "inside-the-shape" for it to         appear in.
An |Image| object providing access to the properties and bytes of the         image in this picture shape.
Return a |PackURI| instance representing the next available partname         matching *tmpl*, which is a printf (%)-style template string         containing a single replacement item, a '%d' to be used to insert the         integer portion of the partname. Example: '/ppt/slides/slide%d.xml'
Return an element loaded from the XML in the template file identified by     *template_name*.
Register *cls* to be constructed when the oxml parser encounters an     element having name *nsptag_name*. *nsptag_name* is a string of the form     ``nspfx:tagroot``, e.g. ``'w:document'``.
Return an instance of the appropriate subclass of _BaseSeries based on the     xChart element *ser* appears in.
The string label given to this series, appears as the title of the         column for this series in the Excel worksheet. It also appears as the         label for this series in the legend.
Read-only. A sequence containing the float values for this series, in         the order they appear on the chart.
|True| if a point having a value less than zero should appear with a         fill different than those with a positive value. |False| if the fill         should be the same regardless of the bar's value. When |True|, a bar         with a solid fill appears with white fill; in a bar with gradient         fill, the direction of the gradient is reversed, e.g. dark -> light         instead of light -> dark. The term "invert" here should be understood         to mean "invert the *direction* of the *fill gradient*".
Read/write boolean specifying whether to use curve smoothing to         form the line connecting the data points in this series into         a continuous curve. If |False|, a series of straight line segments         are used to connect the points.
Generate each float Y value in this series, in the order they appear         on the chart. A value of `None` represents a missing Y value         (corresponding to a blank Excel cell).
Return a sequence representing the `c:pt` elements under the `c:cat`         element of the first series in this xChart element. A category having         no value will have no corresponding `c:pt` element; |None| will         appear in that position in such cases. Items appear in `idx` order.         Only those in the first ``<c:lvl>`` element are included in the case         of multi-level categories.
Return the value of the ``./c:grouping{val=?}`` attribute, taking         defaults into account when items are not present.
Generate each ``<c:ser>`` child element in this xChart in         c:order/@val sequence (not document or c:idx order).
Return the value of the ``./c:grouping{val=?}`` attribute, taking         defaults into account when items are not present.
Return a dict containing the subset namespace prefix mappings specified by     *prefixes*. Any number of namespace prefixes can be supplied, e.g.     namespaces('a', 'r', 'p').
Remove all paragraphs except one empty one.
Fit text-frame text entirely within bounds of its shape.          Make the text in this text frame fit entirely within the bounds of         its shape by setting word wrap on and applying the "best-fit" font         size to all the text it contains. :attr:`TextFrame.auto_size` is set         to :attr:`MSO_AUTO_SIZE.NONE`. The font size will not be set larger         than *max_size* points. If the path to a matching TrueType font is         provided as *font_file*, that font file will be used for the font         metrics. If *font_file* is |None|, best efforts are made to locate         a font file with matchhing *font_family*, *bold*, and *italic*         installed on the current system (usually succeeds if the font is         installed).
Immutable sequence of |_Paragraph| instances corresponding to the         paragraphs in this text frame. A text frame always contains at least         one paragraph.
Read-write setting determining whether lines of text in this shape         are wrapped to fit within the shape's width. Valid values are True,         False, or None. True and False turn word wrap on and off,         respectively. Assigning None to word wrap causes any word wrap         setting to be removed from the text frame, causing it to inherit this         setting from its style hierarchy.
Arrange all the text in this text frame to fit inside its extents by         setting auto size off, wrap on, and setting the font of all its text         to *font_family*, *font_size*, *is_bold*, and *is_italic*.
Return the largest integer point size not greater than *max_size*         that allows all the text in this text frame to fit inside its extents         when rendered using the font described by *family*, *bold*, and         *italic*. If *font_file* is specified, it is used to calculate the         fit, whether or not it matches *family*, *bold*, and *italic*.
A (cx, cy) 2-tuple representing the effective rendering area for text         within this text frame when margins are taken into account.
Set the font properties of all the text in this text frame to         *family*, *size*, *bold*, and *italic*.
Get or set the language id of this |Font| instance. The language id         is a member of the :ref:`MsoLanguageId` enumeration. Assigning |None|         removes any language setting, the same behavior as assigning         `MSO_LANGUAGE_ID.NONE`.
Read/write. |True|, |False|, |None|, or a member of the         :ref:`MsoTextUnderlineType` enumeration indicating the underline         setting for this font. |None| is the default and indicates the         underline setting should be inherited from the style hierarchy, such         as from a placeholder. |True| indicates single underline. |False|         indicates no underline. Other settings such as double and wavy         underlining are indicated with members of the         :ref:`MsoTextUnderlineType` enumeration.
Read/write. The URL of the hyperlink. URL can be on http, https,         mailto, or file scheme; others may work.
Remove all content from this paragraph. Paragraph properties are         preserved. Content includes runs, line breaks, and fields.
Immutable sequence of |_Run| objects corresponding to the runs in         this paragraph.
Read/write :ref:`XlTickMark` value specifying the type of major tick         mark to display on this axis.
Read/write :ref:`XlTickMark` value specifying the type of minor tick         mark for this axis.
Read/write :ref:`XlTickLabelPosition` value specifying where the tick         labels for this axis should appear.
Read/write. |True| if axis is visible, |False| otherwise.
Member of :ref:`XlAxisCrosses` enumeration specifying the point on         this axis where the other axis crosses, such as auto/zero, minimum,         or maximum. Returns `XL_AXIS_CROSSES.CUSTOM` when a specific numeric         crossing point (e.g. 1.5) is defined.
The axis element in the same group (primary/secondary) that crosses         this axis.
Return a new |Video| object containing video in *movie_file*.          *movie_file* can be either a path (string) or a file-like         (e.g. StringIO) object.
Return the file extension for this video, e.g. 'mp4'.          The extension is that from the actual filename if known. Otherwise         it is the lowercase canonical extension for the video's MIME type.         'vid' is used if the MIME type is 'video/unknown'.
Return the Y value for data point *idx* in this cache, or None if no         value is present for that data point.
Return the `c:dLbl` element representing the label for the data point         at offset *idx* in this series, or |None| if not present.
Return the `c:dPt` child representing the visual properties of the         data point at index *idx*.
Return a newly created `a:lnTo` subtree with end point *(x, y)*.          The new `a:lnTo` element is appended to this `a:path` element.
Return a newly created `a:moveTo` subtree with point *(x, y)*.          The new `a:moveTo` element is appended to this `a:path` element.
Return a newly created `a:path` child element.
Remove any ``<a:gd>`` element children of ``<a:avLst>`` and replace         them with ones having (name, val) in *guides*.
Reference to `a:custGeom` descendant or |None| if not present.
True if this shape is an auto shape. A shape is an auto shape if it         has a ``<a:prstGeom>`` element and does not have a txBox="1" attribute         on cNvSpPr.
Return a new ``<p:sp>`` element tree configured as a base auto shape.
Return new `p:sp` element tree configured as freeform shape.          The returned shape has a `a:custGeom` subtree but no paths in its         path list.
Return a new ``<p:sp>`` element tree configured as a placeholder         shape.
Return a new ``<p:sp>`` element tree configured as a base textbox         shape.
Factory function returning appropriate XML writer object for     *chart_type*, loaded with *chart_type* and *chart_data*.
Return a |_BaseSeriesXmlRewriter| subclass appropriate to *chart_type*.
Return the ``<c:numRef>`` element specified by the parameters as         unicode text.
Return the ``<c:ptCount>`` and sequence of ``<c:pt>`` elements         corresponding to *values* as a single unicode text string.         `c:ptCount` refers to the number of `c:pt` elements in this sequence.         The `idx` attribute value for `c:pt` elements locates the data point         in the overall data point sequence of the chart and is started at         *offset*.
Return a ``<c:tx>`` oxml element for this series, containing the         series name.
Return the ``<c:tx>`` (tx is short for 'text') element for this         series as unicode text. This element contains the series name.
Rewrite the series data under *chartSpace* using the chart data         contents. All series-level formatting is left undisturbed. If         the chart data contains fewer series than *chartSpace*, the extra         series in *chartSpace* are deleted. If *chart_data* contains more         series than the *chartSpace* element, new series are added to the         last plot in the chart and series formatting is "cloned" from the         last series in that plot.
Add `c:ser` elements to the last xChart element in *plotArea*, cloned         from the last `c:ser` child of that last xChart.
Adjust the number of c:ser elements in *plotArea* to *new_ser_count*.         Excess c:ser elements are deleted from the end, along with any xChart         elements that are left empty as a result. Series elements are         considered in xChart + series order. Any new c:ser elements required         are added to the last xChart element and cloned from the last c:ser         element in that xChart.
Remove the last *count* ser elements from *plotArea*. Any xChart         elements having no ser child elements after trimming are also         removed.
Return the ``<c:cat>`` element XML for this series, as an oxml         element.
The unicode XML snippet for the ``<c:cat>`` element for this series,         containing the category labels and spreadsheet reference.
The ``<c:val>`` XML for this series, as an oxml element.
Return the unicode XML snippet for the ``<c:val>`` element describing         this series, containing the series values and their spreadsheet range         reference.
The unicode XML snippet for the ``<c:pt>`` elements when category         labels are numeric (including date type).
The unicode XML snippet for the ``<c:pt>`` elements containing the         category names for this series.
The unicode XML snippet for the ``<c:lvl>`` elements containing         multi-level category names.
The unicode XML snippet containing the ``<c:pt>`` elements containing         the values for this series.
Return the ``<c:xVal>`` element for this series as an oxml element.         This element contains the X values for this series.
Return the ``<c:xVal>`` element for this series as unicode text. This         element contains the X values for this series.
Return the ``<c:yVal>`` element for this series as an oxml element.         This element contains the Y values for this series.
Return the ``<c:yVal>`` element for this series as unicode text. This         element contains the Y values for this series.
Return the ``<c:bubbleSize>`` element for this series as an oxml         element. This element contains the bubble size values for this         series.
Return the ``<c:bubbleSize>`` element for this series as unicode         text. This element contains the bubble size values for all the         data points in the chart.
Rewrite the ``<c:tx>``, ``<c:cat>`` and ``<c:val>`` child elements         of *ser* based on the values in *series_data*.
Rewrite the ``<c:tx>``, ``<c:cat>`` and ``<c:val>`` child elements         of *ser* based on the values in *series_data*.
Rewrite the ``<c:tx>``, ``<c:xVal>`` and ``<c:yVal>`` child elements         of *ser* based on the values in *series_data*.
Return *text* as a unicode string. *text* can be a 7-bit ASCII string,     a UTF-8 encoded 8-bit string, or unicode. String values are converted to     unicode assuming UTF-8 encoding. Unicode values are returned unchanged.
|ActionSetting| instance providing access to click behaviors.          Click behaviors are hyperlink-like behaviors including jumping to         a hyperlink (web page) or to another slide in the presentation. The         click action is that defined on the overall shape, not a run of text         within the shape. An |ActionSetting| object is always returned, even         when no click behavior is defined on the shape.
Return an instance of the appropriate shape proxy class for *shape_elm*.
Return an instance of the appropriate shape proxy class for *shape_elm*     on a slide layout.
Return an instance of the appropriate shape proxy class for *shape_elm*     on a slide master.
Return an instance of the appropriate shape proxy class for *shape_elm*     on a notes slide.
Return a placeholder shape of the appropriate type for *shape_elm*.
Return an instance of the appropriate shape proxy class for *shape_elm*     on a slide.
Add a new placeholder shape based on *placeholder*.
Return the base name for a placeholder of *ph_type* in this shape         collection. There is some variance between slide types, for example         a notes slide uses a different name for the body placeholder, so this         method can be overriden by subclasses.
Generate each child of the ``<p:spTree>`` element that corresponds to         a shape, in the sequence they appear in the XML.
Next unique placeholder name for placeholder shape of type *ph_type*,         with id number *id* and orientation *orient*. Usually will be standard         placeholder root name suffixed with id-1, e.g.         _next_ph_name(ST_PlaceholderType.TBL, 4, 'horz') ==>         'Table Placeholder 3'. The number is incremented as necessary to make         the name unique within the collection. If *orient* is ``'vert'``, the         placeholder name is prefixed with ``'Vertical '``.
Return a unique shape id suitable for use with a new shape.          The returned id is 1 greater than the maximum shape id used so far.         In practice, the minimum id is 2 because the spTree element is always         assigned id="1".
Add a new chart of *chart_type* to the slide.          The chart is positioned at (*x*, *y*), has size (*cx*, *cy*), and         depicts *chart_data*. *chart_type* is one of the :ref:`XlChartType`         enumeration values. *chart_data* is a |ChartData| object populated         with the categories and series values for the chart.          Note that a |GraphicFrame| shape object is returned, not the |Chart|         object contained in that graphic frame shape. The chart object may be         accessed using the :attr:`chart` property of the returned         |GraphicFrame| object.
Add a newly created connector shape to the end of this shape tree.          *connector_type* is a member of the :ref:`MsoConnectorType`         enumeration and the end-point values are specified as EMU values. The         returned connector is of type *connector_type* and has begin and end         points as specified.
Return a |GroupShape| object newly appended to this shape tree.          The group shape is empty and must be populated with shapes using         methods on its shape tree, available on its `.shapes` property. The         position and extents of the group shape are determined by the shapes         it contains; its position and extents are recalculated each time         a shape is added to it.
Add picture shape displaying image in *image_file*.          *image_file* can be either a path to a file (a string) or a file-like         object. The picture is positioned with its top-left corner at (*top*,         *left*). If *width* and *height* are both |None|, the native size of         the image is used. If only one of *width* or *height* is used, the         unspecified dimension is calculated to preserve the aspect ratio of         the image. If both are specified, the picture is stretched to fit,         without regard to its native aspect ratio.
Return new |Shape| object appended to this shape tree.          *autoshape_type_id* is a member of :ref:`MsoAutoShapeType` e.g.         ``MSO_SHAPE.RECTANGLE`` specifying the type of shape to be added. The         remaining arguments specify the new shape's position and size.
Return newly added text box shape appended to this shape tree.          The text box is of the specified size, located at the specified         position on the slide.
Return |FreeformBuilder| object to specify a freeform shape.          The optional *start_x* and *start_y* arguments specify the starting         pen position in local coordinates. They will be rounded to the         nearest integer before use and each default to zero.          The optional *scale* argument specifies the size of local coordinates         proportional to slide coordinates (EMU). If the vertical scale is         different than the horizontal scale (local coordinate units are         "rectangular"), a pair of numeric values can be provided as the         *scale* argument, e.g. `scale=(1.0, 2.0)`. In this case the first         number is interpreted as the horizontal (X) scale and the second as         the vertical (Y) scale.          A convenient method for calculating scale is to divide a |Length|         object by an equivalent count of local coordinate units, e.g.         `scale = Inches(1)/1000` for 1000 local units per inch.
Return the index of *shape* in this sequence.          Raises |ValueError| if *shape* is not in the collection.
Return new `p:graphicFrame` element appended to this shape tree.          The `p:graphicFrame` element has the specified position and size and         refers to the chart part identified by *rId*.
Return a newly-added `p:cxnSp` element as specified.          The `p:cxnSp` element is for a connector of *connector_type*         beginning at (*begin_x*, *begin_y*) and extending to         (*end_x*, *end_y*).
Return a newly appended `p:pic` element as specified.          The `p:pic` element displays the image in *image_part* with size and         position specified by *x*, *y*, *cx*, and *cy*. The element is         appended to the shape tree, causing it to be displayed first in         z-order on the slide.
Return newly-added `p:sp` element as specified.          `p:sp` element is of *autoshape_type* at position (*x*, *y*) and of         size (*cx*, *cy*).
Return newly-appended textbox `p:sp` element.          Element has position (*x*, *y*) and size (*cx*, *cy*).
Return newly added movie shape displaying video in *movie_file*.          **EXPERIMENTAL.** This method has important limitations:          * The size must be specified; no auto-scaling such as that provided           by :meth:`add_picture` is performed.         * The MIME type of the video file should be specified, e.g.           'video/mp4'. The provided video file is not interrogated for its           type. The MIME type `video/unknown` is used by default (and works           fine in tests as of this writing).         * A poster frame image must be provided, it cannot be automatically           extracted from the video file. If no poster frame is provided, the           default "media loudspeaker" image will be used.          Return a newly added movie shape to the slide, positioned at (*left*,         *top*), having size (*width*, *height*), and containing *movie_file*.         Before the video is started, *poster_frame_image* is displayed as         a placeholder for the video.
Add a |GraphicFrame| object containing a table with the specified         number of *rows* and *cols* and the specified position and size.         *width* is evenly distributed between the columns of the new table.         Likewise, *height* is evenly distributed between the rows. Note that         the ``.table`` property on the returned |GraphicFrame| shape must be         used to access the enclosed |Table| object.
The title placeholder shape on the slide or |None| if the slide has         no title placeholder.
Return a newly added ``<p:graphicFrame>`` element containing a table         as specified by the parameters.
Add a `p:video` element under `p:sld/p:timing`.          The element will refer to the specified *pic* element by its shape         id, and cause the video play controls to appear for that video.
Return the base name for a placeholder of *ph_type* in this shape         collection. A notes slide uses a different name for the body         placeholder and has some unique placeholder types, so this         method overrides the default in the base class.
Return the first placeholder shape with matching *idx* value, or         *default* if not found.
Return the first placeholder shape with type *ph_type* (e.g. 'body'),         or *default* if no such placeholder shape is present in the         collection.
Return a new `p:pic` element containing video in *movie_file*.          If *mime_type* is None, 'video/unknown' is used. If         *poster_frame_file* is None, the default "media loudspeaker" image is         used.
Return the new `p:pic` element referencing the video.
Return the rId of relationship to poster frame image.          The poster frame is the image used to represent the video before it's         played.
Return the rIds for relationships to media part for video.          This is where the media part and its relationships to the slide are         actually created.
Read/write |float| representing normalized adjustment value for this         adjustment. Actual values are a large-ish integer expressed in shape         coordinates, nominally between 0 and 100,000. The effective value is         normalized to a corresponding value nominally between 0.0 and 1.0.         Intuitively this represents the proportion of the width or height of         the shape at which the adjustment value is located from its starting         point. For simple shapes such as a rounded rectangle, this intuitive         correspondence holds. For more complicated shapes and at more extreme         shape proportions (e.g. width is much greater than height), the value         can become negative or greater than 1.0.
Return an initialized list of adjustment values based on the contents         of *prstGeom*
Write ``<a:gd>`` elements to the XML, one for each adjustment value.         Any existing guide elements are overwritten.
Update |Adjustment| instances in *adjustments* with actual values         held in *guides*, a list of ``<a:gd>`` elements. Guides with a name         that does not match an adjustment object are skipped.
Unique integer identifying the type of this shape, like         ``MSO_SHAPE_TYPE.TEXT_BOX``.
Return a new ``<p:cxnSp>`` element tree configured as a base         connector.
Instance of |CoreProperties| holding the read/write Dublin Core         document properties for this presentation. Creates a default core         properties part if one is not present (not common).
Return a |PackURI| instance representing the next available image         partname, by sequence number. *ext* is used as the extention on the         returned partname.
Return |PackURI| instance for next available media partname.          Partname is first available, starting at sequence number 1. Empty         sequence numbers are reused. *ext* is used as the extension on the         returned partname.
Return an |ImagePart| object containing the image in *image_file*,         which is either a path to an image file or a file-like object         containing an image. If an image part containing this same image         already exists, that instance is returned, otherwise a new image part         is created.
Return an |ImagePart| object belonging to this package or |None| if         no matching image part is found. The image part is identified by the         SHA1 hash digest of the image binary it contains.
Return a |MediaPart| object containing the media in *media*.          If this package already contains a media part for the same         bytestream, that instance is returned, otherwise a new media part is         created.
Return an ``(image_part, rId)`` 2-tuple corresponding to an         |ImagePart| object containing the image in *image_file*, and related         to this slide with the key *rId*. If either the image part or         relationship already exists, they are reused, otherwise they are         newly created.
Create and return a default notes master part, including creating the         new theme it requires.
Create and return a standalone, default notes master part based on         the built-in template (without any related parts, such as theme).
Create and return a default theme part suitable for use with a notes         master.
Create and return a new notes slide part based on the notes master         and related to both the notes master part and *slide_part*. If no         notes master is present, create one based on the default template.
Create and return a new notes slide part that is fully related, but         has no shape content (i.e. placeholders not cloned).
Return a newly-created blank slide part having *partname* and related         to *slide_layout_part*.
Return the rId of a new |ChartPart| object containing a chart of         *chart_type*, displaying *chart_data*, and related to the slide         contained in this part.
Return rIds for media and video relationships to media part.          A new |MediaPart| object is created if it does not already exist         (such as would occur if the same video appeared more than once in          a presentation). Two relationships to the media part are created,         one each with MEDIA and VIDEO relationship types. The need for two         appears to be for legacy support for an earlier (pre-Office 2010)         PowerPoint media embedding strategy.
The |NotesSlide| instance associated with this slide. If the slide         does not have a notes slide, a new one is created. The same single         instance is returned on each call.
Return a newly created |NotesSlidePart| object related to this slide         part. Caller is responsible for ensuring this slide doesn't already         have a notes slide part.
Return a new `p:bgPr` element with noFill properties.
Return `p:bg/p:bgPr` grandchild.          If no such grandchild is present, any existing `p:bg` child is first         removed and a new default `p:bg` with noFill settings is added.
Establish a `p:bg` child with no-fill settings.          Any existing `p:bg` child is first removed.
Return parent element for a new `p:video` child element.          The `p:video` element causes play controls to appear under a video         shape (pic shape containing video). There can be more than one video         shape on a slide, which causes the precondition to vary. It needs to         handle the case when there is no `p:sld/p:timing` element and when         that element already exists. If the case isn't simple, it just nukes         what's there and adds a fresh one. This could theoretically remove         desired existing timing information, but there isn't any evidence         available to me one way or the other, so I've taken the simple         approach.
Add `./p:timing/p:tnLst/p:par/p:cTn/p:childTnLst` descendant.          Any existing `p:timing` child element is ruthlessly removed and         replaced.
Add a new `p:video` child element for movie having *shape_id*.
Return the next available unique ID (int) for p:cTn element.
Return the byte stream of an Excel file formatted as chart data for         the category chart specified in the chart data object.
Enable XlsxWriter Worksheet object to be opened, operated on, and         then automatically closed within a `with` statement. A filename or         stream object (such as a ``BytesIO`` instance) is expected as         *xlsx_file*.
The Excel worksheet reference to the categories for this chart (not         including the column heading).
Return str Excel column reference like 'BQ' for *column_number*.          *column_number* is an int in the range 1-16384 inclusive, where         1 maps to column 'A'.
Write the chart data contents to *worksheet* in category chart         layout. Write categories starting in the first column starting in         the second row, and proceeding one column per category level (for         charts having multi-level categories). Write series as columns         starting in the next following column, placing the series title in         the first cell.
The letter of the Excel worksheet column in which the data for a         series appears.
Write the categories column(s) to *worksheet*. Categories start in         the first column starting in the second row, and proceeding one         column per category level (for charts having multi-level categories).         A date category is formatted as a date. All others are formatted         `General`.
Write a category column defined by *level* to *worksheet* at offset         *col* and formatted with *num_format*.
Write the series column(s) to *worksheet*. Series start in the column         following the last categories column, placing the series title in the         first cell.
Return the number of rows preceding the data table for *series* in         the Excel worksheet.
The Excel worksheet reference to the X values for this chart (not         including the column label).
The Excel worksheet reference to the Y values for this chart (not         including the column label).
The Excel worksheet reference to the range containing the bubble         sizes for *series* (not including the column heading cell).
Write chart data contents to *worksheet* in the bubble chart layout.         Write the data for each series to a separate three-column table with         X values in column A, Y values in column B, and bubble sizes in         column C. Place the series label in the first (heading) cell of the         values column.
Return an instance of the appropriate subclass of _BasePlot based on the     tagname of *xChart*.
|DataLabels| instance providing properties and methods on the         collection of data labels associated with this plot.
Add, remove, or leave alone the ``<c:dLbls>`` child element depending         on current state and assigned *value*. If *value* is |True| and no         ``<c:dLbls>`` element is present, a new default element is added with         default child elements and settings. When |False|, any existing dLbls         element is removed.
Read/write boolean value specifying whether to use a different color         for each of the points in this plot. Only effective when there is         a single series; PowerPoint automatically varies color by series when         more than one series is present.
Set the value of the ``<c:overlap>`` child element to *int_value*,         or remove the overlap element if *int_value* is 0.
Return the member of :ref:`XlChartType` that corresponds to the chart         type of *plot*.
Value of `p:blipFill/a:blip/@r:embed`.          Returns |None| if not present.
Set cropping values in `p:blipFill/a:srcRect` such that an image of         *image_size* will stretch to exactly fit *view_size* when its aspect         ratio is preserved.
Return a new `p:pic` placeholder element populated with the supplied         parameters.
Return a new ``<p:pic>`` element tree configured with the supplied         parameters.
Return a new `p:pic` populated with the specified video.
Return a (left, top, right, bottom) 4-tuple containing the cropping         values required to display an image of *image_size* in *view_size*         when stretched proportionately. Each value is a percentage expressed         as a fraction of 1.0, e.g. 0.425 represents 42.5%. *image_size* and         *view_size* are each (width, height) pairs.
Value of `p:blipFill/a:srcRect/@{attr_name}` or 0.0 if not present.
The category axis of this chart. In the case of an XY or Bubble         chart, this is the X axis. Raises |ValueError| if no category         axis is defined (as is the case for a pie chart, for example).
Font object controlling text format defaults for this chart.
Read/write boolean, specifying whether this chart has a title.          Assigning |True| causes a title to be added if not already present.         Assigning |False| removes any existing title along with its text and         settings.
A |Legend| object providing access to the properties of the legend         for this chart.
The sequence of plots in this chart. A plot, called a *chart group*         in the Microsoft API, is a distinct sequence of one or more series         depicted in a particular charting type. For example, a chart having         a series plotted as a line overlaid on three series plotted as         columns would have two plots; the first corresponding to the three         column series and the second to the line series. Plots are sequenced         in the order drawn, i.e. back-most to front-most. Supports *len()*,         membership (e.g. ``p in plots``), iteration, slicing, and indexed         access (e.g. ``plot = plots[i]``).
Use the categories and series values in the |ChartData| object         *chart_data* to replace those in the XML and Excel worksheet for this         chart.
The |ValueAxis| object providing access to properties of the value         axis of this chart. Raises |ValueError| if the chart has no value         axis.
Return a new |ImagePart| instance containing *image*, which is an         |Image| object.
Return scaled image dimensions in EMU based on the combination of         parameters supplied. If *scaled_cx* and *scaled_cy* are both |None|,         the native image size is returned. If neither *scaled_cx* nor         *scaled_cy* is |None|, their values are returned unchanged. If         a value is provided for either *scaled_cx* or *scaled_cy* and the         other is |None|, the missing value is calculated such that the         image's aspect ratio is preserved.
A (width, height) 2-tuple representing the native dimensions of the         image in EMU, calculated based on the image DPI value, if present,         assuming 72 dpi as a default.
Return a new |Image| object loaded from *image_file*, which can be         either a path (string) or a file-like object.
A (horz_dpi, vert_dpi) 2-tuple specifying the dots-per-inch         resolution of this image. A default value of (72, 72) is used if the         dpi is not specified in the image file.
Canonical file extension for this image e.g. ``'png'``. The returned         extension is all lowercase and is the canonical extension for the         content type of this image, regardless of what extension may have         been used in its filename, if any.
A tuple containing useful image properties extracted from this image         using Pillow (Python Imaging Library, or 'PIL').
``<a:defRPr>`` element of required first ``p`` child, added with its         ancestors if not present. Used when element is a ``<c:txPr>`` in         a chart and the ``p`` element is used only to specify formatting, not         content.
True if only a single empty `a:p` element is present.
The autofit setting for the text frame, a member of the         ``MSO_AUTO_SIZE`` enumeration.
Add an <a:hlinkClick> child element with r:id attribute set to *rId*.
The text of the ``<a:t>`` child element.
Return a newly appended <a:r> element.
Append `a:r` and `a:br` elements to *p* based on *text*.          Any `\n` characters in *text* delimit `a:r` (run) elements and         themselves are translated to `a:br` (line-break) elements.
A sequence containing the text-container child elements of this         ``<a:p>`` element, i.e. (a:r|a:br|a:fld).
The spacing between baselines of successive lines in this paragraph.         A float value indicates a number of lines. A |Length| value indicates         a fixed spacing. Value is contained in `./a:lnSpc/a:spcPts/@val` or         `./a:lnSpc/a:spcPct/@val`. Value is |None| if no element is present.
The EMU equivalent of the centipoints value in         `./a:spcAft/a:spcPts/@val`.
The EMU equivalent of the centipoints value in         `./a:spcBef/a:spcPts/@val`.
Set spacing to *value* lines, e.g. 1.75 lines. A ./a:spcPts child is         removed if present.
Set spacing to *value* points. A ./a:spcPct child is removed if         present.
**EXPERIMENTAL** - *The current implementation only works properly         with rectangular shapes, such as pictures and rectangles. Use with         other shape types may cause unexpected visual alignment of the         connected end-point and could lead to a load error if cxn_pt_idx         exceeds the connection point count available on the connected shape.         That said, a quick test should reveal what to expect when using this         method with other shape types.*          Connect the beginning of this connector to *shape* at the connection         point specified by *cxn_pt_idx*. Each shape has zero or more         connection points and they are identified by index, starting with 0.         Generally, the first connection point of a shape is at the top center         of its bounding box and numbering proceeds counter-clockwise from         there. However this is only a convention and may vary, especially         with non built-in shapes.
Return the X-position of the begin point of this connector, in         English Metric Units (as a |Length| object).
Return the Y-position of the begin point of this connector, in         English Metric Units (as a |Length| object).
**EXPERIMENTAL** - *The current implementation only works properly         with rectangular shapes, such as pictures and rectangles. Use with         other shape types may cause unexpected visual alignment of the         connected end-point and could lead to a load error if cxn_pt_idx         exceeds the connection point count available on the connected shape.         That said, a quick test should reveal what to expect when using this         method with other shape types.*          Connect the ending of this connector to *shape* at the connection         point specified by *cxn_pt_idx*.
Return the X-position of the end point of this connector, in English         Metric Units (as a |Length| object).
Return the Y-position of the end point of this connector, in English         Metric Units (as a |Length| object).
Add or update a stCxn element for this connector that connects its         begin point to the connection point of *shape* specified by         *cxn_pt_idx*.
Add or update an endCxn element for this connector that connects its         end point to the connection point of *shape* specified by         *cxn_pt_idx*.
Move the begin point of this connector to coordinates of the         connection point of *shape* specified by *cxn_pt_idx*.
Move the end point of this connector to the coordinates of the         connection point of *shape* specified by *cxn_pt_idx*.
A dictionary containing any key-value pairs present in the query         portion of the `ppaction://` URL in the action attribute. For example         `{'id':'0', 'return':'true'}` in         'ppaction://customshow?id=0&return=true'. Returns an empty dictionary         if the URL contains no query string or if no action attribute is         present.
The host portion of the `ppaction://` URL contained in the action         attribute. For example 'customshow' in         'ppaction://customshow?id=0&return=true'. Returns |None| if no action         attribute is present.
The total integer number of data points appearing in the series of         this chart that are prior to *series* in this sequence.
Return the integer index of *series* in this sequence.
The formatting template string that determines how a number in this         series is formatted, both in the chart and in the Excel spreadsheet;         for example '#,##0.0'. If not specified for this series, it is         inherited from the parent chart data object.
The formatting template string that determines how the value of this         data point is formatted, both in the chart and in the Excel         spreadsheet; for example '#,##0.0'. If not specified for this data         point, it is inherited from the parent series data object.
Add a series to this data set entitled *name* and having the data         points specified by *values*, an iterable of numeric values.         *number_format* specifies how the series values will be displayed,         and may be a string, e.g. '#,##0' corresponding to an Excel number         format.
Return a newly created |data.Category| object having *label* and         appended to the end of this category sequence. *label* can be         a string, a number, a datetime.date, or datetime.datetime object. All         category labels in a chart must be the same type. All category labels         in a chart having multi-level categories must be strings.          Creating a chart from chart data having date categories will cause         the chart to have a |DateAxis| for its category axis.
Return |True| if the first category in this collection has a date         label (as opposed to str or numeric). A date label is one of type         datetime.date or datetime.datetime. Returns |False| otherwise,         including when this category collection is empty. It also returns         False when this category collection is hierarchical, because         hierarchical categories can only be written as string labels.
Return |True| if the first category in this collection has a numeric         label (as opposed to a string label), including if that value is         a datetime.date or datetime.datetime object (as those are converted         to integers for storage in Excel). Returns |False| otherwise,         including when this category collection is empty. It also returns         False when this category collection is hierarchical, because         hierarchical categories can only be written as string labels.
The number of hierarchy levels in this category graph. Returns 0 if         it contains no categories.
The offset of *category* in the overall sequence of leaf categories.         A non-leaf category gets the index of its first sub-category.
A generator of (idx, label) sequences representing the category         hierarchy from the bottom up. The first level contains all leaf         categories, and each subsequent is the next level up.
Read/write. Return a string representing the number format used in         Excel to format these category values, e.g. '0.0' or 'mm/dd/yyyy'.         This string is only relevant when the categories are numeric or date         type, although it returns 'General' without error when the categories         are string labels. Assigning |None| causes the default number format         to be used, based on the type of the category labels.
Return a newly created |data.Category| object having *label* and         appended to the end of the sub-category sequence for this category.
The number of hierarchy levels rooted at this category node. Returns         1 if this category has no sub-categories.
The offset of *sub_category* in the overall sequence of leaf         categories.
The number of leaf category nodes under this category. Returns         1 if this category has no sub-categories.
The string representation of the numeric (or date) label of this         category, suitable for use in the XML `c:pt` element for this         category. The optional *date_1904* parameter specifies the epoch used         for calculating Excel date numbers.
Return an integer representing the date label of this category as the         number of days since January 1, 1900 (or 1904 if date_1904 is         |True|).
Return a CategoryDataPoint object newly created with value *value*,         an optional *number_format*, and appended to this sequence.
Return an |XySeriesData| object newly created and added at the end of         this sequence, identified by *name* and values formatted with         *number_format*.
Return a |BubbleSeriesData| object newly created and added at the end         of this sequence, and having series named *name* and values formatted         with *number_format*.
Return an XyDataPoint object newly created with values *x* and *y*,         and appended to this sequence.
Append a new BubbleDataPoint object having the values *x*, *y*, and         *size*. The optional *number_format* is used to format the Y value.         If not provided, the number format is inherited from the series data.
Return the ASCII characters in the file specified by *path* and *paths*.     The file path is determined by concatenating *path* and any members of     *paths* with a directory separator in between.
Return a reference to a newly created <p:sldId> child element having         its r:id attribute set to *rId*.
Return the next available slide ID as an int. Valid slide IDs start         at 256. The next integer value greater than the max value in use is         chosen, which minimizes that chance of reusing the id of a deleted         slide.
Return a |Presentation| object loaded from *pptx*, where *pptx* can be     either a path to a ``.pptx`` file (a string) or a file-like object. If     *pptx* is missing or ``None``, the built-in default presentation     "template" is loaded.
Return the path to the built-in default .pptx package.
Return |True| if *prs_part* is a valid main document part, |False|     otherwise.
Return new |MediaPart| instance containing *media*.          *media* must be a |Media| object.
Set the value of ./c:manualLayout/c:x@val to *offset* and         ./c:manualLayout/c:xMode@val to "factor". Remove ./c:manualLayout if         *offset* == 0.
The float value in ./c:x@val when ./c:xMode@val == "factor". 0.0 when         ./c:x is not present or ./c:xMode@val != "factor".
Set the value of ./c:x@val to *offset* and ./c:xMode@val to "factor".
Get URL for requests
Generate oAuth1.0a URL
Do requests
POST requests
PUT requests
Returns the URL with OAuth params
Generate OAuth Signature
Normalize parameters
Generate nonce number
Compute several Topic Models in parallel using the "lda" package. Use a single or multiple document term matrices     `data` and optionally a list of varying parameters `varying_parameters`. Pass parameters in `constant_parameters`     dict to each model calculation. Use at maximum `n_max_processes` processors or use all available processors if None     is passed.     `data` can be either a Document-Term-Matrix (NumPy array/matrix, SciPy sparse matrix) or a dict with document ID ->     Document-Term-Matrix mapping when calculating models for multiple corpora (named multiple documents).      If `data` is a dict of named documents, this function will return a dict with document ID -> result list. Otherwise     it will only return a result list. A result list always is a list containing tuples `(parameter_set, model)` where     `parameter_set` is a dict of the used parameters.
Compute several Topic Models in parallel using the "lda" package. Calculate the models using a list of varying     parameters `varying_parameters` on a single Document-Term-Matrix `data`. Pass parameters in `constant_parameters`     dict to each model calculation. Use at maximum `n_max_processes` processors or use all available processors if None     is passed.     `data` must be a Document-Term-Matrix (NumPy array/matrix, SciPy sparse matrix).     Will return a list of size `len(varying_parameters)` containing tuples `(parameter_set, eval_results)` where     `parameter_set` is a dict of the used parameters and `eval_results` is a dict of metric names -> metric results.
Print `n_top` top values from a LDA model's distribution `distrib`. Can be used for topic-word distributions and     document-topic distributions.
Print `n_top` values from a LDA model's topic-word distributions.
Print `n_top` values from a LDA model's document-topic distributions.
Save a LDA model as pickle file.
Plot a heatmap for a document-topic distribution `doc_topic_distrib` to a matplotlib Figure `fig` and Axes `ax`     using `doc_labels` as document labels on the y-axis and topics from 1 to `n_topics=doc_topic_distrib.shape[1]` on     the x-axis.     Custom topic labels can be passed as `topic_labels`.     A subset of documents can be specified either with a sequence `which_documents` containing a subset of document     labels from `doc_labels` or `which_document_indices` containing a sequence of document indices.     A subset of topics can be specified either with a sequence `which_topics` containing sequence of numbers between     [1, n_topics] or `which_topic_indices` which is a number between [0, n_topics-1]     Additional arguments can be passed via `kwargs` to `plot_heatmap`.      Please note that it is almost always necessary to select a subset of your document-topic distribution with the     `which_documents` or `which_topics` parameters, as otherwise the amount of data to be plotted will be too high     to give a reasonable picture.
Plot a heatmap for a topic-word distribution `topic_word_distrib` to a matplotlib Figure `fig` and Axes `ax`     using `vocab` as vocabulary on the x-axis and topics from 1 to `n_topics=doc_topic_distrib.shape[1]` on     the y-axis.     A subset of words from `vocab` can be specified either directly with a sequence `which_words` or     `which_document_indices` containing a sequence of word indices in `vocab`.     A subset of topics can be specified either with a sequence `which_topics` containing sequence of numbers between     [1, n_topics] or `which_topic_indices` which is a number between [0, n_topics-1]     Additional arguments can be passed via `kwargs` to `plot_heatmap`.      Please note that it is almost always necessary to select a subset of your topic-word distribution with the     `which_words` or `which_topics` parameters, as otherwise the amount of data to be plotted will be too high     to give a reasonable picture.
helper function to plot a heatmap for a 2D matrix `data` using matplotlib's "matshow" function
Plot the evaluation results from `eval_results`. `eval_results` must be a sequence containing `(param, values)`     tuples, where `param` is the parameter value to appear on the x axis and `values` can be a dict structure     containing the metric values. `eval_results` can be created using the `results_by_parameter` function from the     `topicmod.common` module.     Set `metric` to plot only a specific metric.     Set `xaxislabel` for a label on the x-axis.     Set `yaxislabel` for a label on the y-axis.     Set `title` for a plot title.     Options in a dict `subplots_opts` will be passed to `plt.subplots(...)`.     Options in a dict `subplots_adjust_opts` will be passed to `fig.subplots_adjust(...)`.     `figsize` can be set to a tuple `(width, height)` or to `"auto"` (default) which will set the size to     `(8, 2 * <num. of metrics>)`.
For each word in the vocab of `dtm` (i.e. its columns), return how often it occurs at least `min_val` times.     If `proportions` is True, return proportions scaled to the number of documents instead of absolute numbers.
For each unique pair of words `w1, w2` in the vocab of `dtm` (i.e. its columns), return how often both occur     together at least `min_val` times. If `proportions` is True, return proportions scaled to the number of documents     instead of absolute numbers.
Return the term proportions given the document-term matrix `dtm`
Apply Part-of-Speech (POS) tagging on each token.         Uses the default NLTK tagger if no language-specific tagger could be loaded (English is assumed then as         language). The default NLTK tagger uses Penn Treebank tagset         (https://ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).         The default German tagger based on TIGER corpus uses the STTS tagset         (http://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/TagSets/stts-table.html).
Filter tokens for a specific POS tag (if `required_pos` is a string) or several POS tags (if `required_pos`         is a list/tuple/set of strings). The POS tag depends on the tagset used during tagging. If `simplify_pos` is         True, then the tags are matched to the following simplified forms:         - 'N' for nouns         - 'V' for verbs         - 'ADJ' for adjectives         - 'ADV' for adverbs         - None for all other
Apply a custom filter function `filter_func` to all tokens or ngrams (if `to_ngrams` is True).         `filter_func` must accept a single parameter: a dictionary of structure `{<doc_label>: <tokens list>}`. It         must return a dictionary with the same structure.          This function can only be run on a single process, hence it could be slow for large corpora.
Create worker processes and queues. Distribute the work evenly across worker processes. Optionally         send initial states defined in list `initial_states` to each worker process.
Return marginal topic distribution p(T) (topic proportions) given the document-topic distribution (theta)     `doc_topic_distrib` and the document lengths `doc_lengths`. The latter can be calculated with `get_doc_lengths()`.
Order a vector of `words` by a `score`, either `least_to_most` or reverse. Optionally return only the top `n`     results.
Calculate word saliency according to Chuang et al. 2012.     saliency(w) = p(w) * distinctiveness(w)      J. Chuang, C. Manning, J. Heer 2012: "Termite: Visualization Techniques for Assessing Textual Topic Models"
Return words in `vocab` ordered by saliency score.
Order the words from `vocab` by "saliency score" (Chuang et al. 2012) from most to least salient. Optionally only     return the `n` most salient words.      J. Chuang, C. Manning, J. Heer 2012: "Termite: Visualization Techniques for Assessing Textual Topic Models"
Order the words from `vocab` by "saliency score" (Chuang et al. 2012) from least to most salient. Optionally only     return the `n` least salient words.      J. Chuang, C. Manning, J. Heer 2012: "Termite: Visualization Techniques for Assessing Textual Topic Models"
Calculate word distinctiveness according to Chuang et al. 2012.     distinctiveness(w) = KL(P(T|w), P(T)) = sum_T(P(T|w) log(P(T|w)/P(T)))     with P(T) .. marginal topic distribution          P(T|w) .. prob. of a topic given a word      J. Chuang, C. Manning, J. Heer 2012: "Termite: Visualization Techniques for Assessing Textual Topic Models"
Return words in `vocab` ordered by distinctiveness score.
Order the words from `vocab` by "distinctiveness score" (Chuang et al. 2012) from most to least distinctive.     Optionally only return the `n` most distinctive words.      J. Chuang, C. Manning, J. Heer 2012: "Termite: Visualization Techniques for Assessing Textual Topic Models"
Order the words from `vocab` by "distinctiveness score" (Chuang et al. 2012) from least to most distinctive.     Optionally only return the `n` least distinctive words.      J. Chuang, C. Manning, J. Heer 2012: "Termite: Visualization Techniques for Assessing Textual Topic Models"
Calculate the topic-word relevance score with a lambda parameter `lambda_` according to Sievert and Shirley 2014.     relevance(w,T|lambda) = lambda * log phi_{w,t} + (1-lambda) * log (phi_{w,t} / p(w))     with phi  .. topic-word distribution          p(w) .. marginal word probability
Get words from `vocab` for `topic` ordered by most to least relevance (Sievert and Shirley 2014) using the relevance     matrix `rel_mat` obtained from `get_topic_word_relevance()`.     Optionally only return the `n` most relevant words.
Get words from `vocab` for `topic` ordered by least to most relevance (Sievert and Shirley 2014) using the relevance     matrix `rel_mat` obtained from `get_topic_word_relevance()`.     Optionally only return the `n` least relevant words.
Generate topic labels derived from the top words of each topic. The top words are determined from the     relevance score (Sievert and Shirley 2014) depending on `lambda_`. Specify the number of top words in the label     with `n_words`. If `n_words` is None, a minimum number of words will be used to create unique labels for each     topic. Topic labels are formed by joining the top words with `labels_glue` and formatting them with     `labels_format`. Placeholders in `labels_format` are `{i0}` (zero-based topic index),     `{i1}` (one-based topic index) and `{topwords}` (top words glued with `labels_glue`).
Get `top_n` values from LDA model's distribution `distrib` as DataFrame. Can be used for topic-word distributions     and document-topic distributions. Set `row_labels` to a format string or a list. Set `col_labels` to a format     string for the column names. Set `val_labels` to return value labels instead of pure values (probabilities).
Filter topics defined as topic-word distribution `topic_word_distrib` across vocabulary `vocab` for a word (pass a     string) or multiple words/patterns `w` (pass a list of strings). Either run pattern(s) `w` against the list of     top words per topic (use `top_n` for number of words in top words list) or specify a minimum topic-word probability     `thresh`, resulting in a list of words above this threshold for each topic, which will be used for pattern matching.     You can also specify `top_n` *and* `thresh`.     Set the `match` parameter according to the options provided by `filter_tokens.token_match()` (exact matching, RE or     glob matching). Use `cond` to specify whether at only *one* match suffices per topic when a list of patterns `w` is     passed (`cond='any'`) or *all* patterns must match (`cond='all'`).     By default, this function returns a NumPy array containing the *indices* of topics that passed the filter criteria.     If `return_words_and_matches` is True, this function additonally returns a NumPy array with the top words for each     topic and a NumPy array with the pattern matches for each topic.
Exclude topics with the indices `excl_topic_indices` from the document-topic distribution `doc_topic_distrib` (i.e.     delete the respective columns in this matrix) and optionally re-normalize the distribution so that the rows sum up     to 1 if `renormalize` is set to `True`.     Optionally also strip the topics from the topic-word distribution `topic_word_distrib` (i.e. remove the respective     rows).      If `topic_word_distrib` is given, return a tuple with the updated doc.-topic and topic-word distributions, else     return only the updated doc.-topic distribution.      *WARNING:* The topics to be excluded are specified by *zero-based indices*.
Take string of `lines`, split into list of lines using `splitchar` (or don't if `splitchar` evaluates to False) and     then split them into individual paragraphs. A paragraph must be divided by at     least `break_on_num_newlines` line breaks (empty lines) from another paragraph.     Return a list of paragraphs, each paragraph containing a string of sentences.
From a dict `docs` with document ID -> terms/tokens list mapping, generate an array of vocabulary (i.e.     unique terms of the whole corpus `docs`), an array of document labels (i.e. document IDs), a dict     with document ID -> document terms array mapping and a sum of the number of unique terms per document.      The returned variable `sum_uniques_per_doc` tells us how many elements will be non-zero in a DTM which     will be created later. Hence this is the allocation size for the sparse DTM.      This function provides the input for create_sparse_dtm().      Return a tuple with:     - np.array of vocabulary     - np.array of document names     - dict with mapping: document name -> np.array of document terms     - overall sum of unique terms per document (allocation size for the sparse DTM)
Create a sparse document-term-matrix (DTM) as scipy "coo_matrix" from vocabulary array `vocab`, document     IDs/labels array `doc_labels`, dict of doc_label -> document terms `docs_terms` and the sum of unique terms     per document `sum_uniques_per_doc`.     The DTM's rows are document names, its columns are indices in `vocab`, hence a value `DTM[j, k]` is the     term frequency of term `vocab[k]` in `docnames[j]`.      Memory requirement: about 3 * <sum_uniques_per_doc>.
Implementing a slightly modified feature detector.         @param tokens: The tokens from the sentence to tag.         @param index: The current token index to tag.         @param history: The previous tagged tokens.
Helper function to pickle `data` in `picklefile`.
Helper function to unpickle data from `picklefile`.
Return a simplified POS tag for a full POS tag `pos` belonging to a tagset `tagset`. By default the WordNet     tagset is assumed.     Does the following conversion by default:     - all N... (noun) tags to 'N'     - all V... (verb) tags to 'V'     - all ADJ... (adjective) tags to 'ADJ'     - all ADV... (adverb) tags to 'ADV'     - all other to None     Does the following conversion by with `tagset=='penn'`:     - all N... (noun) tags to 'N'     - all V... (verb) tags to 'V'     - all JJ... (adjective) tags to 'ADJ'     - all RB... (adverb) tags to 'ADV'     - all other to None
Select an area/"window" inside of a 2D array/matrix `mat` specified by either a sequence of     row indices `row_indices` and/or a sequence of column indices `col_indices`.     Returns the specified area as a *view* of the data if `copy` is False, else it will return a copy.
Bring a 1D NumPy array with at least two values in `values` to a linearly normalized range of [0, 1].
Implementation of greed partitioning algorithm as explained in https://stackoverflow.com/a/6670011     for a dict `elems_dict` containing elements with label -> weight mapping. The elements are placed in     `k` bins such that the difference of sums of weights in each bin is minimized. The algorithm     does not always find the optimal solution.     If `return_only_labels` is False, returns a list of `k` dicts with label -> weight mapping,     else returns a list of `k` lists containing only the labels.
Return a NumPy array signaling matches between `pattern` and `tokens`. `pattern` is a string that will be     compared with each element in sequence `tokens` either as exact string equality (`match_type` is 'exact') or     regular expression (`match_type` is 'regex') or glob pattern (`match_type` is 'glob').
Estimation of the probability of held-out documents according to Wallach et al. 2009 [1] using a document-topic     estimation `theta_test` that was estimated via held-out documents `dtm_test` on a trained model with a     topic-word distribution `phi_train` and a document-topic prior `alpha`. Draw `n_samples` according to `theta_test`     for each document in `dtm_test` (memory consumption and run time can be very high for larger `n_samples` and     a large amount of big documents in `dtm_test`).      A document-topic estimation `theta_test` can be obtained from a trained model from the "lda" package or scikit-learn     package with the `transform()` method.      Adopted MATLAB code originally from Ian Murray, 2009     See https://people.cs.umass.edu/~wallach/code/etm/     MATLAB code downloaded from https://people.cs.umass.edu/~wallach/code/etm/lda_eval_matlab_code_20120930.tar.gz      Note: requires gmpy2 package for multiple-precision arithmetic to avoid numerical underflow.           see https://github.com/aleaxit/gmpy      [1] Wallach, H.M., Murray, I., Salakhutdinov, R. and Mimno, D., 2009. Evaluation methods for topic models.
Rajkumar Arun, V. Suresh, C. E. Veni Madhavan, and M. N. Narasimha Murthy. 2010. On finding the natural number of     topics with latent dirichlet allocation: Some observations. In Advances in knowledge discovery and data mining,     Mohammed J. Zaki, Jeffrey Xu Yu, Balaraman Ravindran and Vikram Pudi (eds.). Springer Berlin Heidelberg, 391–402.     http://doi.org/10.1007/978-3-642-13657-3_43
Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy     of Sciences 101, suppl 1: 5228–5235. http://doi.org/10.1073/pnas.0307752101      Calculates the harmonic mean of the loglikelihood values `logliks` as in Griffiths, Steyvers 2004. Burnin values     should already be removed from `logliks`.      Note: requires gmpy2 package for multiple-precision arithmetic to avoid numerical underflow.           see https://github.com/aleaxit/gmpy
Calculate coherence metric according to Mimno et al. 2011 (a.k.a. "U_Mass" coherence metric). There are two     modifications to the originally suggested measure:     - uses a different epsilon by default (set `eps=1` for original)     - uses a normalizing constant by default (set `normalize=False` for original)      Provide a topic word distribution $\phi$ as `topic_word_distrib` and a document-term-matrix `dtm` (can be sparse).     `top_n` controls how many most probable words per topic are selected.      By default, it will return a NumPy array of coherence values per topic (same ordering as in `topic_word_distrib`).     Set `return_mean` to True to return the mean of all topics instead.      D. Mimno, H. Wallach, E. Talley, M. Leenders, A. McCullum 2011: Optimizing semantic coherence in topic models
Calculate model coherence using Gensim's `CoherenceModel` [1,2]. Note: this function also supports models from     `lda` and `sklearn` (by passing `topic_word_distrib`, `dtm` and `vocab`)!      Define which measure to use with parameter `measure`:     - u_mass     - c_v     - c_uci     - c_npmi      Provide a topic word distribution $\phi$ as `topic_word_distrib` OR a Gensim model `gensim_model`     and the corpus' vocabulary as `vocab` OR pass a gensim corpus as `gensim_corpus`.     `top_n` controls how many most probable words per topic are selected.      If measure is `u_mass`, a document-term-matrix `dtm` or `gensim_corpus` must be provided and `texts` can be None.     If any other measure than `u_mass` is used, tokenized input as `texts` must be provided as 2D list:     ```     [['some', 'text', ...],          # doc. 1      ['some', 'more', ...],          # doc. 2      ['another', 'document', ...]]   # doc. 3     ```      If `return_coh_model` is True, the whole `CoherenceModel` instance will be returned, otherwise:     - if `return_mean` is True, the mean coherence value will be returned     - if `return_mean` is False, a list of coherence values (for each topic) will be returned      Provided `kwargs` will be passed to `CoherenceModel()` or `CoherenceModel.get_coherence_per_topic()`.      [1]: https://radimrehurek.com/gensim/models/coherencemodel.html     [2]: https://rare-technologies.com/what-is-topic-coherence/
Takes a list of evaluation results `res` returned by a LDA evaluation function (a list in the form     `[(parameter_set_1, {'<metric_name>': result_1, ...}), ..., (parameter_set_n, {'<metric_name>': result_n, ...})]`)     and returns a list with tuple pairs using  only the parameter `param` from the parameter sets in the evaluation     results such that the returned list is     `[(param_1, {'<metric_name>': result_1, ...}), ..., (param_n, {'<metric_name>': result_n, ...})]`.     Optionally order either by parameter value (`sort_by=None` - the default) or by result metric     (`sort_by='<metric name>'`).
docstring_insert - a decorator to insert API-docparts dynamically.
endpoint - decorator to manipulate the REST-service endpoint.      The endpoint decorator sets the endpoint and the method for the class     to access the REST-service.
abstractclass - class decorator.      make sure the class is abstract and cannot be used on it's own.      @abstractclass     class A(object):         def __init__(self, *args, **kwargs):             # logic             pass      class B(A):         pass      a = A()   # results in an AssertionError     b = B()   # works fine
convert a named granularity into seconds.      get value in seconds for named granularities: M1, M5 ... H1 etc.      >>> print(granularity_to_time("M5"))     300
return a list of all classes in a module.
InstrumentsCandlesFactory - generate InstrumentCandles requests.      InstrumentsCandlesFactory is used to retrieve historical data by     automatically generating consecutive requests when the OANDA limit     of *count* records is exceeded.      This is known by calculating the number of candles between *from* and     *to*. If *to* is not specified *to* will be equal to *now*.      The *count* parameter is only used to control the number of records to     retrieve in a single request.      The *includeFirst* parameter is forced to make sure that results do     no have a 1-record gap between consecutive requests.      Parameters     ----------      instrument : string (required)         the instrument to create the order for      params: params (optional)         the parameters to specify the historical range,         see the REST-V20 docs regarding 'instrument' at developer.oanda.com         If no params are specified, just a single InstrumentsCandles request         will be generated acting the same as if you had just created it         directly.      Example     -------      The *oandapyV20.API* client processes requests as objects. So,     downloading large historical batches simply comes down to:      >>> import json     >>> from oandapyV20 import API     >>> from oandapyV20.contrib.factories import InstrumentsCandlesFactory     >>>     >>> client = API(access_token=...)     >>> instrument, granularity = "EUR_USD", "M15"     >>> _from = "2017-01-01T00:00:00Z"     >>> params = {     ...    "from": _from,     ...    "granularity": granularity,     ...    "count": 2500,     ... }     >>> with open("/tmp/{}.{}".format(instrument, granularity), "w") as OUT:     >>>     # The factory returns a generator generating consecutive     >>>     # requests to retrieve full history from date 'from' till 'to'     >>>     for r in InstrumentsCandlesFactory(instrument=instrument,     ...                                        params=params)     >>>         client.request(r)     >>>         OUT.write(json.dumps(r.response.get('candles'), indent=2))      .. note:: Normally you can't combine *from*, *to* and *count*.               When *count* specified, it is used to calculate the gap between               *to* and *from*. The *params* passed to the generated request               itself does contain the *count* parameter.
__request.          make the actual request. This method is called by the         request method in case of 'regular' API-calls. Or indirectly by         the__stream_request method if it concerns a 'streaming' call.
__stream_request.          make a 'stream' request. This method is called by         the 'request' method after it has determined which         call applies: regular or streaming.
Perform a request for the APIRequest instance 'endpoint'.          Parameters         ----------         endpoint : APIRequest             The endpoint parameter contains an instance of an APIRequest             containing the endpoint, method and optionally other parameters             or body data.          Raises         ------             V20Error in case of HTTP response code >= 400
Dynamically create the definition classes from module 'mod'.
Ensure raising of IntegrityError on unique constraint violations.      In earlier versions of hdbcli it doesn't raise the hdbcli.dbapi.IntegrityError     exception for unique constraint violations. To support also older versions     of hdbcli this decorator inspects the raised exception and will rewrite the     exception based on HANA's error code.
Pretty print format some XML so it's readable.
Find the return value in a CIM response.      The xmlns is needed because everything in CIM is a million levels     of namespace indirection.
Find the return value in a CIM response.      The xmlns is needed because everything in CIM is a million levels     of namespace indirection.
Power on the box.
Sets the machine to boot to boot_device on its next reboot          Will default back to normal boot list on the reboot that follows.
Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a     JSON5 document) to a Python object.
Serialize ``obj`` to a JSON5-formatted ``str``.
Serialize ``obj`` to a JSON5-formatted stream to ``fp`` (a ``.write()``-     supporting file-like object).
Callback called when a connection CLOSE frame is received.         This callback will process the received CLOSE error to determine if         the connection is recoverable or whether it should be shutdown.         :param error: The error information from the close          frame.         :type error: ~uamqp.errors.ErrorResponse
Callback called whenever the underlying Connection undergoes         a change of state. This function wraps the states as Enums for logging         purposes.         :param previous_state: The previous Connection state.         :type previous_state: int         :param new_state: The new Connection state.         :type new_state: int
Close the connection, and close any associated         CBS authentication session.
Redirect the connection to an alternative endpoint.         :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Perform a single Connection iteration.
Lock the connection for a given number of seconds.          :param seconds: Length of time to lock the connection.         :type seconds: int
Callback called when a link DETACH frame is received.         This callback will process the received DETACH error to determine if         the link is recoverable or whether it should be shutdown.          :param error: The error information from the detach          frame.         :type error: ~uamqp.errors.ErrorResponse
Get the state of the MessageSender and its underlying Link.          :rtype: ~uamqp.constants.MessageSenderState
Add a single message to the internal pending queue to be processed         by the Connection without waiting for it to be sent.          :param message: The message to send.         :type message: ~uamqp.message.Message         :param callback: The callback to be run once a disposition is received          in receipt of the message. The callback must take three arguments, the message,          the send result and the optional delivery condition (exception).         :type callback:          callable[~uamqp.message.Message, ~uamqp.constants.MessageSendResult, ~uamqp.errors.MessageException]         :param timeout: An expiry time for the message added to the queue. If the          message is not sent within this timeout it will be discarded with an error          state. If set to 0, the message will not expire. The default is 0.
Redirect the client endpoint using a Link DETACH redirect         response.          :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Asynchronously open the client. The client can create a new Connection         or an existing Connection can be passed in. This existing Connection         may have an existing CBS authentication Session, which will be         used for this client as well. Otherwise a new Session will be         created.          :param connection: An existing Connection that may be shared between          multiple clients.         :type connetion: ~uamqp.async_ops.connection_async.ConnectionAsync
Close the client asynchronously. This includes closing the Session         and CBS authentication layer as well as the Connection.         If the client was opened using an external Connection,         this will be left intact.
Run an asynchronous request/response operation. These are frequently used         for management tasks against a $management node, however any node name can be         specified and the available options will depend on the target service.          :param message: The message to send in the management request.         :type message: ~uamqp.message.Message         :param operation: The type of operation to be performed. This value will          be service-specific, but common values include READ, CREATE and UPDATE.          This value will be added as an application property on the message.         :type operation: bytes or str         :param op_type: The type on which to carry out the operation. This will          be specific to the entities of the service. This value will be added as          an application property on the message.         :type op_type: bytes         :param node: The target node. Default is `b"$management"`.         :type node: bytes or str         :param timeout: Provide an optional timeout in milliseconds within which a response          to the management request must be received.         :type timeout: int         :param callback: The function to process the returned parameters of the management          request including status code and a description if available. This can be used          to reformat the response or raise an error based on content. The function must          take 3 arguments - status code, response message and description.         :type callback: ~callable[int, bytes, ~uamqp.message.Message]         :param status_code_field: Provide an alternate name for the status code in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusCode"`.         :type status_code_field: bytes or str         :param description_fields: Provide an alternate name for the description in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusDescription"`.         :type description_fields: bytes or str         :rtype: ~uamqp.message.Message
Whether the authentication handshake is complete during         connection initialization.          :rtype: bool
Whether the handler has completed all start up processes such as         establishing the connection, session, link and authentication, and         is not ready to process messages.          :rtype: bool
Run a single connection iteration asynchronously.         This will return `True` if the connection is still open         and ready to be used for further work, or `False` if it needs         to be shut down.          :rtype: bool         :raises: TimeoutError or ~uamqp.errors.ClientTimeout if CBS authentication timeout reached.
Determine whether the client is ready to start sending messages.         To be ready, the connection must be open and authentication complete,         The Session, Link and MessageSender must be open and in non-errored         states.          :rtype: bool         :raises: ~uamqp.errors.MessageHandlerError if the MessageSender          goes into an error state.
MessageSender Link is now open - perform message send         on all pending messages.         Will return True if operation successful and client can remain open for         further work.          :rtype: bool
Redirect the client endpoint using a Link DETACH redirect         response.          :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Send a single message or batched message asynchronously.          :param messages: A message to send. This can either be a single instance          of ~uamqp.message.Message, or multiple messages wrapped in an instance          of ~uamqp.message.BatchMessage.         :type message: ~uamqp.message.Message         :param close_on_done: Close the client once the message is sent. Default is `False`.         :type close_on_done: bool         :raises: ~uamqp.errors.MessageException if message fails to send after retry policy          is exhausted.
Send all pending messages in the queue asynchronously.         This will return a list of the send result of all the pending         messages so it can be determined if any messages failed to send.         This function will open the client if it is not already open.          :param close_on_done: Close the client once the messages are sent.          Default is `True`.         :type close_on_done: bool         :rtype: list[~uamqp.constants.MessageState]
Determine whether the client is ready to start receiving messages.         To be ready, the connection must be open and authentication complete,         The Session, Link and MessageReceiver must be open and in non-errored         states.          :rtype: bool         :raises: ~uamqp.errors.MessageHandlerError if the MessageReceiver          goes into an error state.
MessageReceiver Link is now open - start receiving messages.         Will return True if operation successful and client can remain open for         further work.          :rtype: bool
Receive messages asynchronously. This function will run indefinitely,         until the client closes either via timeout, error or forced         interruption (e.g. keyboard interrupt).          If the receive client is configured with `auto_complete=True` then the messages that         have not been settled on completion of the provided callback will automatically be         accepted provided it has not expired. If an error occurs or the message has expired         it will be released. Alternatively if `auto_complete=False`, each message will need         to be explicitly settled during the callback, otherwise it will be released.          :param on_message_received: A callback to process messages as they arrive from the          service. It takes a single argument, a ~uamqp.message.Message object.         :type on_message_received: callable[~uamqp.message.Message]
Receive a batch of messages asynchronously. This method will return as soon as some         messages are available rather than waiting to achieve a specific batch size, and         therefore the number of messages returned per call will vary up to the maximum allowed.          If the receive client is configured with `auto_complete=True` then the messages received         in the batch returned by this function will already be settled. Alternatively, if         `auto_complete=False`, then each message will need to be explicitly settled before         it expires and is released.          :param max_batch_size: The maximum number of messages that can be returned in          one call. This value cannot be larger than the prefetch value, and if not specified,          the prefetch value will be used.         :type max_batch_size: int         :param on_message_received: A callback to process messages as they arrive from the          service. It takes a single argument, a ~uamqp.message.Message object.         :type on_message_received: callable[~uamqp.message.Message]         :param timeout: I timeout in milliseconds for which to wait to receive any messages.          If no messages are received in this time, an empty list will be returned. If set to          0, the client will continue to wait until at least one message is received. The          default is 0.         :type timeout: int
Receive messages by asynchronous generator. Messages returned in the         generator have already been accepted - if you wish to add logic to accept         or reject messages based on custom criteria, pass in a callback.          If the receive client is configured with `auto_complete=True` then the messages received         from the iterator returned by this function will be automatically settled when the iterator         is incremented. Alternatively, if `auto_complete=False`, then each message will need to         be explicitly settled before it expires and is released.          :param on_message_received: A callback to process messages as they arrive from the          service. It takes a single argument, a ~uamqp.message.Message object.         :type on_message_received: callable[~uamqp.message.Message]         :rtype: Generator[~uamqp.message.Message]
Redirect the client endpoint using a Link DETACH redirect         response.          :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Run a request/response operation. These are frequently used for management         tasks against a $management node, however any node name can be specified         and the available options will depend on the target service.          :param message: The message to send in the management request.         :type message: ~uamqp.message.Message         :param operation: The type of operation to be performed. This value will          be service-specific, but common values include READ, CREATE and UPDATE.          This value will be added as an application property on the message.         :type operation: bytes         :param op_type: The type on which to carry out the operation. This will          be specific to the entities of the service. This value will be added as          an application property on the message.         :type op_type: bytes         :param node: The target node. Default is `b"$management"`.         :type node: bytes         :param timeout: Provide an optional timeout in milliseconds within which a response          to the management request must be received.         :type timeout: int         :param status_code_field: Provide an alternate name for the status code in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusCode"`.         :type status_code_field: bytes         :param description_fields: Provide an alternate name for the description in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusDescription"`.         :type description_fields: bytes         :param encoding: The encoding to use for parameters supplied as strings.          Default is 'UTF-8'         :type encoding: str         :rtype: ~uamqp.message.Message
Parse a connection string such as those provided by the Azure portal.     Connection string should be formatted like: `Key=Value;Key=Value;Key=Value`.     The connection string will be parsed into a dictionary.      :param connect_str: The connection string.     :type connect_str: str     :rtype: dict[str, str]
Create a SAS token.      :param key_name: The username/key name/policy name for the token.     :type key_name: bytes     :param shared_access_key: The shared access key to generate the token from.     :type shared_access_key: bytes     :param scope: The token permissions scope.     :type scope: bytes     :param expiry: The lifetime of the generated token. Default is 1 hour.     :type expiry: ~datetime.timedelta     :rtype: bytes
Convert a Python integer value into equivalent C object.     Will attempt to use the smallest possible conversion, starting with int, then long     then double.
Wrap a Python type in the equivalent C AMQP type.     If the Python type has already been wrapped in a ~uamqp.types.AMQPType     object - then this will be used to select the appropriate C type.     - bool => c_uamqp.BoolValue     - int => c_uamqp.IntValue, LongValue, DoubleValue     - str => c_uamqp.StringValue     - bytes => c_uamqp.BinaryValue     - list/set/tuple => c_uamqp.ListValue     - dict => c_uamqp.DictValue (AMQP map)     - float => c_uamqp.DoubleValue     - uuid.UUID => c_uamqp.UUIDValue      :param value: The value to wrap.     :type value: ~uamqp.types.AMQPType     :rtype: uamqp.c_uamqp.AMQPValue
Perform a single Connection iteration asynchronously.
Lock the connection for a given number of seconds.          :param seconds: Length of time to lock the connection.         :type seconds: int
Redirect the connection to an alternative endpoint.         :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Close the connection asynchronously, and close any associated         CBS authentication session.
Setup the default underlying TLS IO layer. On Windows this is         Schannel, on Linux and MacOS this is OpenSSL.          :param hostname: The endpoint hostname.         :type hostname: bytes         :param port: The TLS port.         :type port: int
Close the authentication layer and cleanup         all the authentication wrapper objects.
Decode an AMQP message from a bytearray.         The returned message will not have a delivery context and         therefore will be considered to be in an "already settled" state.          :param data: The AMQP wire-encoded bytes to decode.         :type data: bytes or bytearray
Parse a message received from an AMQP service.          :param message: The received C message.         :type message: uamqp.c_uamqp.cMessage
Pre-emptively get the size of the message once it has been encoded         to go over the wire so we can raise an error if the message will be         rejected for being to large.          This method is not available for messages that have been received.          :rtype: int
Encode message to AMQP wire-encoded bytearray.          :rtype: bytearray
Return all the messages represented by this object.         This will always be a list of a single message.          :rtype: list[~uamqp.message.Message]
Get the underlying C message from this object.          :rtype: uamqp.c_uamqp.cMessage
Send a response disposition to the service to indicate that         a received message has been accepted. If the client is running in PeekLock         mode, the service will wait on this disposition. Otherwise it will         be ignored. Returns `True` is message was accepted, or `False` if the message         was already settled.          :rtype: bool         :raises: TypeError if the message is being sent rather than received.
Send a response disposition to the service to indicate that         a received message has been rejected. If the client is running in PeekLock         mode, the service will wait on this disposition. Otherwise it will         be ignored. A rejected message will increment the messages delivery count.         Returns `True` is message was rejected, or `False` if the message         was already settled.          :param condition: The AMQP rejection code. By default this is `amqp:internal-error`.         :type condition: bytes or str         :param description: A description/reason to accompany the rejection.         :type description: bytes or str         :rtype: bool         :raises: TypeError if the message is being sent rather than received.
Send a response disposition to the service to indicate that         a received message has been released. If the client is running in PeekLock         mode, the service will wait on this disposition. Otherwise it will         be ignored. A released message will not incremenet the messages         delivery count. Returns `True` is message was released, or `False` if the message         was already settled.          :rtype: bool         :raises: TypeError if the message is being sent rather than received.
Send a response disposition to the service to indicate that         a received message has been modified. If the client is running in PeekLock         mode, the service will wait on this disposition. Otherwise it will         be ignored. Returns `True` is message was modified, or `False` if the message         was already settled.          :param failed: Whether this delivery of this message failed. This does not          indicate whether subsequence deliveries of this message would also fail.         :type failed: bool         :param deliverable: Whether this message will be deliverable to this client          on subsequent deliveries - i.e. whether delivery is retryable.         :type deliverable: bool         :param annotations: Annotations to attach to response.         :type annotations: dict         :rtype: bool         :raises: TypeError if the message is being sent rather than received.
Create a ~uamqp.message.Message for a value supplied by the data         generator. Applies all properties and annotations to the message.          :rtype: ~uamqp.message.Message
Generate multiple ~uamqp.message.Message objects from a single data         stream that in total may exceed the maximum individual message size.         Data will be continuously added to a single message until that message         reaches a max allowable size, at which point it will be yielded and         a new message will be started.          :rtype: generator[~uamqp.message.Message]
Return all the messages represented by this object. This will convert         the batch data into individual Message objects, which may be one         or more if multi_messages is set to `True`.          :rtype: list[~uamqp.message.Message]
Get the underlying C reference from this object.          :rtype: uamqp.c_uamqp.cProperties
Append a section to the body.          :param data: The data to append.         :type data: str or bytes
Set a value as the message body. This can be any         Python data type and it will be automatically encoded         into an AMQP type. If a specific AMQP type is required, a         `types.AMQPType` can be used.          :param data: The data to send in the body.         :type data: ~uamqp.types.AMQPType
Get the underlying C reference from this object.          :rtype: uamqp.c_uamqp.cHeader
Create the AMQP session and the CBS channel with which         to negotiate the token.          :param connection: The underlying AMQP connection on which          to create the session.         :type connection: ~uamqp.connection.Connection         :param debug: Whether to emit network trace logging events for the          CBS session. Default is `False`. Logging events are set at INFO level.         :type debug: bool         :rtype: uamqp.c_uamqp.CBSTokenAuth
Close the CBS auth channel and session.
If a username and password are present - attempt to use them to         request a fresh SAS token.
Attempt to create a CBS token session using a Shared Access Key such         as is used to connect to Azure services.          :param uri: The AMQP endpoint URI. This must be provided as          a decoded string.         :type uri: str         :param key_name: The SAS token username, also referred to as the key          name or policy name.         :type key_name: str         :param shared_access_key: The SAS token password, also referred to as the key.         :type shared_access_key: str         :param expiry: The lifetime in seconds for the generated token. Default is 1 hour.         :type expiry: int         :param port: The TLS port - default for AMQP is 5671.         :type port: int         :param timeout: The timeout in seconds in which to negotiate the token.          The default value is 10 seconds.         :type timeout: int         :param retry_policy: The retry policy for the PUT token request. The default          retry policy has 3 retries.         :type retry_policy: ~uamqp.authentication.cbs_auth.TokenRetryPolicy         :param verify: The path to a user-defined certificate.         :type verify: str         :param http_proxy: HTTP proxy configuration. This should be a dictionary with          the following keys present: 'proxy_hostname' and 'proxy_port'. Additional optional          keys are 'username' and 'password'.         :type http_proxy: dict         :param encoding: The encoding to use if hostname is provided as a str.          Default is 'UTF-8'.         :type encoding: str
Callback called whenever the underlying Receiver undergoes a change         of state. This function wraps the states as Enums to prepare for         calling the public callback.          :param previous_state: The previous Receiver state.         :type previous_state: int         :param new_state: The new Receiver state.         :type new_state: int
Send a settle dispostition for a received message.          :param message_number: The delivery number of the message          to settle.         :type message_number: int         :response: The type of disposition to respond with, e.g. whether          the message was accepted, rejected or abandoned.         :type response: ~uamqp.errors.MessageResponse
Callback run on receipt of every message. If there is         a user-defined callback, this will be called.         Additionally if the client is retrieving messages for a batch         or iterator, the message will be added to an internal queue.          :param message: c_uamqp.Message
Callback called whenever the underlying Receiver undergoes a change         of state. This function can be overridden.          :param previous_state: The previous Receiver state.         :type previous_state: ~uamqp.constants.MessageReceiverState         :param new_state: The new Receiver state.         :type new_state: ~uamqp.constants.MessageReceiverState
Add a single message to the internal pending queue to be processed         by the Connection without waiting for it to be sent.          :param message: The message to send.         :type message: ~uamqp.message.Message         :param callback: The callback to be run once a disposition is received          in receipt of the message. The callback must take three arguments, the message,          the send result and the optional delivery condition (exception).         :type callback:          callable[~uamqp.message.Message, ~uamqp.constants.MessageSendResult, ~uamqp.errors.MessageException]         :param timeout: An expiry time for the message added to the queue. If the          message is not sent within this timeout it will be discarded with an error          state. If set to 0, the message will not expire. The default is 0.
Redirect the client endpoint using a Link DETACH redirect         response.          :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Open the client. The client can create a new Connection         or an existing Connection can be passed in. This existing Connection         may have an existing CBS authentication Session, which will be         used for this client as well. Otherwise a new Session will be         created.          :param connection: An existing Connection that may be shared between          multiple clients.         :type connetion: ~uamqp.connection.Connection
Close the client. This includes closing the Session         and CBS authentication layer as well as the Connection.         If the client was opened using an external Connection,         this will be left intact.          No further messages can be sent or received and the client         cannot be re-opened.          All pending, unsent messages will remain uncleared to allow         them to be inspected and queued to a new client.
Run a request/response operation. These are frequently used for management         tasks against a $management node, however any node name can be specified         and the available options will depend on the target service.          :param message: The message to send in the management request.         :type message: ~uamqp.message.Message         :param operation: The type of operation to be performed. This value will          be service-specific, but common values include READ, CREATE and UPDATE.          This value will be added as an application property on the message.         :type operation: bytes         :param op_type: The type on which to carry out the operation. This will          be specific to the entities of the service. This value will be added as          an application property on the message.         :type op_type: bytes         :param node: The target node. Default is `b"$management"`.         :type node: bytes         :param timeout: Provide an optional timeout in milliseconds within which a response          to the management request must be received.         :type timeout: int         :param callback: The function to process the returned parameters of the management          request including status code and a description if available. This can be used          to reformat the response or raise an error based on content. The function must          take 3 arguments - status code, response message and description.         :type callback: ~callable[int, bytes, ~uamqp.message.Message]         :param status_code_field: Provide an alternate name for the status code in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusCode"`.         :type status_code_field: bytes         :param description_fields: Provide an alternate name for the description in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusDescription"`.         :type description_fields: bytes         :rtype: ~uamqp.message.Message
Whether the authentication handshake is complete during         connection initialization.          :rtype: bool
Whether the handler has completed all start up processes such as         establishing the connection, session, link and authentication, and         is not ready to process messages.          :rtype: bool
Run a single connection iteration.         This will return `True` if the connection is still open         and ready to be used for further work, or `False` if it needs         to be shut down.          :rtype: bool         :raises: TimeoutError or ~uamqp.errors.ClientTimeout if CBS authentication timeout reached.
Determine whether the client is ready to start sending messages.         To be ready, the connection must be open and authentication complete,         The Session, Link and MessageSender must be open and in non-errored         states.          :rtype: bool         :raises: ~uamqp.errors.MessageHandlerError if the MessageSender          goes into an error state.
Callback run on a message send operation. If message         has a user defined callback, it will be called here. If the result         of the operation is failure, the message state will be reverted         to 'pending' up to the maximum retry count.          :param message: The message that was sent.         :type message: ~uamqp.message.Message         :param result: The result of the send operation.         :type result: int         :param error: An Exception if an error ocurred during the send operation.         :type error: ~Exception
MessageSender Link is now open - perform message send         on all pending messages.         Will return True if operation successful and client can remain open for         further work.          :rtype: bool
Redirect the client endpoint using a Link DETACH redirect         response.          :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Add one or more messages to the send queue.         No further action will be taken until either `SendClient.wait()`         or `SendClient.send_all_messages()` has been called.         The client does not need to be open yet for messages to be added         to the queue. Multiple messages can be queued at once:             - `send_client.queue_message(my_message)`             - `send_client.queue_message(message_1, message_2, message_3)`             - `send_client.queue_message(*my_message_list)`          :param messages: A message to send. This can either be a single instance          of `Message`, or multiple messages wrapped in an instance of `BatchMessage`.         :type message: ~uamqp.message.Message
Send a single message or batched message.          :param messages: A message to send. This can either be a single instance          of `Message`, or multiple messages wrapped in an instance of `BatchMessage`.         :type message: ~uamqp.message.Message         :param close_on_done: Close the client once the message is sent. Default is `False`.         :type close_on_done: bool         :raises: ~uamqp.errors.MessageException if message fails to send after retry policy          is exhausted.
Run the client until all pending message in the queue         have been processed. Returns whether the client is still running after the         messages have been processed, or whether a shutdown has been initiated.          :rtype: bool
Send all pending messages in the queue. This will return a list         of the send result of all the pending messages so it can be         determined if any messages failed to send.         This function will open the client if it is not already open.          :param close_on_done: Close the client once the messages are sent.          Default is `True`.         :type close_on_done: bool         :rtype: list[~uamqp.constants.MessageState]
MessageReceiver Link is now open - start receiving messages.         Will return True if operation successful and client can remain open for         further work.          :rtype: bool
Iterate over processed messages in the receive queue.          :rtype: generator[~uamqp.message.Message]
Callback run on receipt of every message. If there is         a user-defined callback, this will be called.         Additionally if the client is retrieving messages for a batch         or iterator, the message will be added to an internal queue.          :param message: Received message.         :type message: ~uamqp.message.Message
Receive a batch of messages. Messages returned in the batch have already been         accepted - if you wish to add logic to accept or reject messages based on custom         criteria, pass in a callback. This method will return as soon as some messages are         available rather than waiting to achieve a specific batch size, and therefore the         number of messages returned per call will vary up to the maximum allowed.          If the receive client is configured with `auto_complete=True` then the messages received         in the batch returned by this function will already be settled. Alternatively, if         `auto_complete=False`, then each message will need to be explicitly settled before         it expires and is released.          :param max_batch_size: The maximum number of messages that can be returned in          one call. This value cannot be larger than the prefetch value, and if not specified,          the prefetch value will be used.         :type max_batch_size: int         :param on_message_received: A callback to process messages as they arrive from the          service. It takes a single argument, a ~uamqp.message.Message object.         :type on_message_received: callable[~uamqp.message.Message]         :param timeout: I timeout in milliseconds for which to wait to receive any messages.          If no messages are received in this time, an empty list will be returned. If set to          0, the client will continue to wait until at least one message is received. The          default is 0.         :type timeout: int
Receive messages. This function will run indefinitely, until the client         closes either via timeout, error or forced interruption (e.g. keyboard interrupt).          If the receive client is configured with `auto_complete=True` then the messages that         have not been settled on completion of the provided callback will automatically be         accepted provided it has not expired. If an error occurs or the message has expired         it will be released. Alternatively if `auto_complete=False`, each message will need         to be explicitly settled during the callback, otherwise it will be released.          :param on_message_received: A callback to process messages as they arrive from the          service. It takes a single argument, a ~uamqp.message.Message object.         :type on_message_received: callable[~uamqp.message.Message]
Receive messages by generator. Messages returned in the generator have already been         accepted - if you wish to add logic to accept or reject messages based on custom         criteria, pass in a callback.          :param on_message_received: A callback to process messages as they arrive from the          service. It takes a single argument, a ~uamqp.message.Message object.         :type on_message_received: callable[~uamqp.message.Message]
Redirect the client endpoint using a Link DETACH redirect         response.          :param redirect: The Link DETACH redirect details.         :type redirect: ~uamqp.errors.LinkRedirect         :param auth: Authentication credentials to the redirected endpoint.         :type auth: ~uamqp.authentication.common.AMQPAuth
Send a single message to AMQP endpoint.      :param target: The target AMQP endpoint.     :type target: str, bytes or ~uamqp.address.Target     :param data: The contents of the message to send.     :type data: str, bytes or ~uamqp.message.Message     :param auth: The authentication credentials for the endpoint.      This should be one of the subclasses of uamqp.authentication.AMQPAuth. Currently      this includes:         - uamqp.authentication.SASLAnonymous         - uamqp.authentication.SASLPlain         - uamqp.authentication.SASTokenAuth      If no authentication is supplied, SASLAnnoymous will be used by default.     :type auth: ~uamqp.authentication.common.AMQPAuth     :param debug: Whether to turn on network trace logs. If `True`, trace logs      will be logged at INFO level. Default is `False`.     :type debug: bool     :return: A list of states for each message sent.     :rtype: list[~uamqp.constants.MessageState]
Receive a single message from an AMQP endpoint.      :param source: The AMQP source endpoint to receive from.     :type source: str, bytes or ~uamqp.address.Source     :param auth: The authentication credentials for the endpoint.      This should be one of the subclasses of uamqp.authentication.AMQPAuth. Currently      this includes:         - uamqp.authentication.SASLAnonymous         - uamqp.authentication.SASLPlain         - uamqp.authentication.SASTokenAuth      If no authentication is supplied, SASLAnnoymous will be used by default.     :type auth: ~uamqp.authentication.common.AMQPAuth     :param timeout: The timeout in milliseconds after which to return None if no messages      are retrieved. If set to `0` (the default), the receiver will not timeout and      will continue to wait for messages until interrupted.     :param debug: Whether to turn on network trace logs. If `True`, trace logs      will be logged at INFO level. Default is `False`.     :type debug: bool     :rtype: ~uamqp.message.Message or None
Receive a batch of messages from an AMQP endpoint.      :param source: The AMQP source endpoint to receive from.     :type source: str, bytes or ~uamqp.address.Source     :param auth: The authentication credentials for the endpoint.      This should be one of the subclasses of ~uamqp.authentication.AMQPAuth. Currently      this includes:         - uamqp.authentication.SASLAnonymous         - uamqp.authentication.SASLPlain         - uamqp.authentication.SASTokenAuth      If no authentication is supplied, SASLAnnoymous will be used by default.     :type auth: ~uamqp.authentication.common.AMQPAuth     :param max_batch_size: The maximum number of messages to return in a batch. If the      receiver receives a smaller number than this, it will not wait to return them so      the actual number returned can be anything up to this value. If the receiver reaches      a timeout, an empty list will be returned.     :param timeout: The timeout in milliseconds after which to return if no messages      are retrieved. If set to `0` (the default), the receiver will not timeout and      will continue to wait for messages until interrupted.     :param debug: Whether to turn on network trace logs. If `True`, trace logs      will be logged at INFO level. Default is `False`.     :type debug: bool     :rtype: list[~uamqp.message.Message]
Initialize the TLS/SSL platform to prepare it for         making AMQP requests. This only needs to happen once.
Confirm that supplied address is a valid URL and         has an `amqp` or `amqps` scheme.          :param address: The endpiont URL.         :type address: str         :rtype: ~urllib.parse.ParseResult
Get the filter on the source.          :param name: The name of the filter. This will be encoded as          an AMQP Symbol. By default this is set to b'apache.org:selector-filter:string'.         :type name: bytes
Set a filter on the endpoint. Only one filter         can be applied to an endpoint.          :param value: The filter to apply to the endpoint. Set to None for a NULL filter.         :type value: bytes or str or None         :param name: The name of the filter. This will be encoded as          an AMQP Symbol. By default this is set to b'apache.org:selector-filter:string'.         :type name: bytes         :param descriptor: The descriptor used if the filter is to be encoded as a described value.          This will be encoded as an AMQP Symbol. By default this is set to b'apache.org:selector-filter:string'.          Set to None if the filter should not be encoded as a described value.         :type descriptor: bytes or None
Create the async AMQP session and the CBS channel with which         to negotiate the token.          :param connection: The underlying AMQP connection on which          to create the session.         :type connection: ~uamqp.async_ops.connection_async.ConnectionAsync         :param debug: Whether to emit network trace logging events for the          CBS session. Default is `False`. Logging events are set at INFO level.         :type debug: bool         :param loop: A user specified event loop.         :type loop: ~asycnio.AbstractEventLoop         :rtype: uamqp.c_uamqp.CBSTokenAuth
Close the CBS auth channel and session asynchronously.
This coroutine is called periodically to check the status of the current         token if there is one, and request a new one if needed.         If the token request fails, it will be retried according to the retry policy.         A token refresh will be attempted if the token will expire soon.          This function will return a tuple of two booleans. The first represents whether         the token authentication has not completed within it's given timeout window. The         second indicates whether the token negotiation is still in progress.          :raises: ~uamqp.errors.AuthenticationException if the token authentication fails.         :raises: ~uamqp.errors.TokenExpired if the token has expired and cannot be          refreshed.         :rtype: tuple[bool, bool]
Asynchronously run a request/response operation. These are frequently used         for management tasks against a $management node, however any node name can be         specified and the available options will depend on the target service.          :param message: The message to send in the management request.         :type message: ~uamqp.message.Message         :param operation: The type of operation to be performed. This value will          be service-specific, but common values include READ, CREATE and UPDATE.          This value will be added as an application property on the message.         :type operation: bytes or str         :param op_type: The type on which to carry out the operation. This will          be specific to the entities of the service. This value will be added as          an application property on the message.         :type op_type: bytes or str         :param node: The target node. Default is `b"$management"`.         :type node: bytes or str         :param timeout: Provide an optional timeout in milliseconds within which a response          to the management request must be received.         :type timeout: int         :param status_code_field: Provide an alternate name for the status code in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusCode"`.         :type status_code_field: bytes or str         :param description_fields: Provide an alternate name for the description in the          response body which can vary between services due to the spec still being in draft.          The default is `b"statusDescription"`.         :type description_fields: bytes or str         :param encoding: The encoding to use for parameters supplied as strings.          Default is 'UTF-8'         :type encoding: str         :rtype: ~uamqp.message.Message
Asynchronously close any open management Links and close the Session.         Cleans up and C objects for both mgmt Links and Session.
Execute a request and wait on a response asynchronously.          :param operation: The type of operation to be performed. This value will          be service-specific, but common values include READ, CREATE and UPDATE.          This value will be added as an application property on the message.         :type operation: bytes         :param op_type: The type on which to carry out the operation. This will          be specific to the entities of the service. This value will be added as          an application property on the message.         :type op_type: bytes         :param message: The message to send in the management request.         :type message: ~uamqp.message.Message         :param timeout: Provide an optional timeout in milliseconds within which a response          to the management request must be received.         :type timeout: int         :rtype: ~uamqp.message.Message
Provide the entry point to the subreddit_stats command.
Return a markdown representation of simple statistics.
Fetch recent submissions in subreddit with boundaries.          Does not include posts within the last day as their scores may not be         representative.          :param max_duration: When set, specifies the number of days to include
Wrap the submissions_callback function.
Fetch top submissions by some top value.          :param top: One of week, month, year, all         :returns: True if any submissions were found.
Group comments by author.
Group submissions by author.
Submit the results to the subreddit. Has no return value (None).
Run stats and return the created Submission.
Return a markdown representation of the top commenters.
Return a markdown representation of the top submitters.
Return a markdown representation of the top submissions.
Return a markdown representation of the top comments.
Return a parser with common options used in the prawtools commands.
Provide the entry point in the the modutils command.
Add users to 'banned', 'contributor', or 'moderator'.
Remove flair that is not visible or has been set to empty.
Generate the flair, by user, for the subreddit.
Synchronize templates with flair that already exists on the site.          :param editable: Indicates that all the options should be editable.         :param limit: The minimum number of users that must share the flair             before it is added as a template.         :param static: A list of flair templates that will always be added.         :param sort: The order to sort the flair templates.         :param use_css: Include css in the templates.         :param use_text: Include text in the templates.
Send message to all users in `category`.
Display the current flair for all users in the subreddit.
Display statistics (number of users) for each unique flair item.
Display the list of users in `category`.
Return the URL for the comment without fetching its submission.
Provide the entry point into the reddit_alert program.
Returns the proper executor for the given object.      If the object has *_executors* and *_green_mode* members it returns     the submit callable for the executor corresponding to the green_mode.     Otherwise it returns the global executor for the given green_mode.      Note: *None* is a valid object.      :returns: submit callable
Make a function green. Can be used as a decorator.
Return a green verion of the given callback.
Submit an operation
Execute an operation and return the result.
method wrapper for printing all elements of a struct
method wrapper for printing all elements of a struct.
helper function to make internal sequences printable
helper method to register str and repr methods for structures
Alias a python package properly.      It ensures that modules are not duplicated by trying     to import and alias all the submodules recursively.
Return the given operation as an asyncio future.
Return a result from an asyncio future.
Submit an operation
Execute an operation and return the result.
Encode a 8 bit grayscale image as JPEG format             :param gray8: an object containning image information            :type gray8: :py:obj:`str` or :class:`numpy.ndarray` or seq< seq<element> >            :param width: image width. **MUST** be given if gray8 is a string or                          if it is a :class:`numpy.ndarray` with ndims != 2.                          Otherwise it is calculated internally.            :type width: :py:obj:`int`            :param height: image height. **MUST** be given if gray8 is a string                           or if it is a :class:`numpy.ndarray` with ndims != 2.                           Otherwise it is calculated internally.            :type height: :py:obj:`int`            :param quality: Quality of JPEG (0=poor quality 100=max quality) (default is 100.0)            :type quality: :py:obj:`float`         .. note::            When :class:`numpy.ndarray` is given:                 - gray8 **MUST** be CONTIGUOUS, ALIGNED                - if gray8.ndims != 2, width and height **MUST** be given and                  gray8.nbytes **MUST** match width*height                - if gray8.ndims == 2, gray8.itemsize **MUST** be 1 (typically,                  gray8.dtype is one of `numpy.dtype.byte`, `numpy.dtype.ubyte`,                  `numpy.dtype.int8` or `numpy.dtype.uint8`)         Example::             def read_myattr(self, attr):                enc = tango.EncodedAttribute()                data = numpy.arange(100, dtype=numpy.byte)                data = numpy.array((data,data,data))                enc.encode_jpeg_gray8(data)                attr.set_value(enc)
Encode a 8 bit grayscale image (no compression)             :param gray8: an object containning image information            :type gray8: :py:obj:`str` or :class:`numpy.ndarray` or seq< seq<element> >            :param width: image width. **MUST** be given if gray8 is a string or                          if it is a :class:`numpy.ndarray` with ndims != 2.                          Otherwise it is calculated internally.            :type width: :py:obj:`int`            :param height: image height. **MUST** be given if gray8 is a string                           or if it is a :class:`numpy.ndarray` with ndims != 2.                           Otherwise it is calculated internally.            :type height: :py:obj:`int`         .. note::            When :class:`numpy.ndarray` is given:                 - gray8 **MUST** be CONTIGUOUS, ALIGNED                - if gray8.ndims != 2, width and height **MUST** be given and                  gray8.nbytes **MUST** match width*height                - if gray8.ndims == 2, gray8.itemsize **MUST** be 1 (typically,                  gray8.dtype is one of `numpy.dtype.byte`, `numpy.dtype.ubyte`,                  `numpy.dtype.int8` or `numpy.dtype.uint8`)         Example::             def read_myattr(self, attr):                enc = tango.EncodedAttribute()                data = numpy.arange(100, dtype=numpy.byte)                data = numpy.array((data,data,data))                enc.encode_gray8(data)                attr.set_value(enc)
Internal usage only
Encode a 16 bit grayscale image (no compression)             :param gray16: an object containning image information            :type gray16: :py:obj:`str` or :py:obj:`buffer` or :class:`numpy.ndarray` or seq< seq<element> >            :param width: image width. **MUST** be given if gray16 is a string or                          if it is a :class:`numpy.ndarray` with ndims != 2.                          Otherwise it is calculated internally.            :type width: :py:obj:`int`            :param height: image height. **MUST** be given if gray16 is a string                           or if it is a :class:`numpy.ndarray` with ndims != 2.                           Otherwise it is calculated internally.            :type height: :py:obj:`int`         .. note::            When :class:`numpy.ndarray` is given:                 - gray16 **MUST** be CONTIGUOUS, ALIGNED                - if gray16.ndims != 2, width and height **MUST** be given and                  gray16.nbytes/2 **MUST** match width*height                - if gray16.ndims == 2, gray16.itemsize **MUST** be 2 (typically,                  gray16.dtype is one of `numpy.dtype.int16`, `numpy.dtype.uint16`,                  `numpy.dtype.short` or `numpy.dtype.ushort`)         Example::             def read_myattr(self, attr):                enc = tango.EncodedAttribute()                data = numpy.arange(100, dtype=numpy.int16)                data = numpy.array((data,data,data))                enc.encode_gray16(data)                attr.set_value(enc)
Encode a 24 bit rgb color image as JPEG format.             :param rgb24: an object containning image information            :type rgb24: :py:obj:`str` or :class:`numpy.ndarray` or seq< seq<element> >            :param width: image width. **MUST** be given if rgb24 is a string or                          if it is a :class:`numpy.ndarray` with ndims != 3.                          Otherwise it is calculated internally.            :type width: :py:obj:`int`            :param height: image height. **MUST** be given if rgb24 is a string                           or if it is a :class:`numpy.ndarray` with ndims != 3.                           Otherwise it is calculated internally.            :type height: :py:obj:`int`            :param quality: Quality of JPEG (0=poor quality 100=max quality) (default is 100.0)            :type quality: :py:obj:`float`         .. note::            When :class:`numpy.ndarray` is given:                 - rgb24 **MUST** be CONTIGUOUS, ALIGNED                - if rgb24.ndims != 3, width and height **MUST** be given and                  rgb24.nbytes/3 **MUST** match width*height                - if rgb24.ndims == 3, rgb24.itemsize **MUST** be 1 (typically,                  rgb24.dtype is one of `numpy.dtype.byte`, `numpy.dtype.ubyte`,                  `numpy.dtype.int8` or `numpy.dtype.uint8`) and shape **MUST** be                  (height, width, 3)         Example::             def read_myattr(self, attr):                enc = tango.EncodedAttribute()                # create an 'image' where each pixel is R=0x01, G=0x01, B=0x01                arr = numpy.ones((10,10,3), dtype=numpy.uint8)                enc.encode_jpeg_rgb24(data)                attr.set_value(enc)
Encode a 24 bit color image (no compression)             :param rgb24: an object containning image information            :type rgb24: :py:obj:`str` or :class:`numpy.ndarray` or seq< seq<element> >            :param width: image width. **MUST** be given if rgb24 is a string or                          if it is a :class:`numpy.ndarray` with ndims != 3.                          Otherwise it is calculated internally.            :type width: :py:obj:`int`            :param height: image height. **MUST** be given if rgb24 is a string                           or if it is a :class:`numpy.ndarray` with ndims != 3.                           Otherwise it is calculated internally.            :type height: :py:obj:`int`         .. note::            When :class:`numpy.ndarray` is given:                 - rgb24 **MUST** be CONTIGUOUS, ALIGNED                - if rgb24.ndims != 3, width and height **MUST** be given and                  rgb24.nbytes/3 **MUST** match width*height                - if rgb24.ndims == 3, rgb24.itemsize **MUST** be 1 (typically,                  rgb24.dtype is one of `numpy.dtype.byte`, `numpy.dtype.ubyte`,                  `numpy.dtype.int8` or `numpy.dtype.uint8`) and shape **MUST** be                  (height, width, 3)         Example::             def read_myattr(self, attr):                enc = tango.EncodedAttribute()                # create an 'image' where each pixel is R=0x01, G=0x01, B=0x01                arr = numpy.ones((10,10,3), dtype=numpy.uint8)                enc.encode_rgb24(data)                attr.set_value(enc)
Internal usage only
Encode a 32 bit rgb color image as JPEG format.             :param rgb32: an object containning image information            :type rgb32: :py:obj:`str` or :class:`numpy.ndarray` or seq< seq<element> >            :param width: image width. **MUST** be given if rgb32 is a string or                          if it is a :class:`numpy.ndarray` with ndims != 2.                          Otherwise it is calculated internally.            :type width: :py:obj:`int`            :param height: image height. **MUST** be given if rgb32 is a string                           or if it is a :class:`numpy.ndarray` with ndims != 2.                           Otherwise it is calculated internally.            :type height: :py:obj:`int`         .. note::            When :class:`numpy.ndarray` is given:                 - rgb32 **MUST** be CONTIGUOUS, ALIGNED                - if rgb32.ndims != 2, width and height **MUST** be given and                  rgb32.nbytes/4 **MUST** match width*height                - if rgb32.ndims == 2, rgb32.itemsize **MUST** be 4 (typically,                  rgb32.dtype is one of `numpy.dtype.int32`, `numpy.dtype.uint32`)         Example::             def read_myattr(self, attr):                enc = tango.EncodedAttribute()                data = numpy.arange(100, dtype=numpy.int32)                data = numpy.array((data,data,data))                enc.encode_jpeg_rgb32(data)                attr.set_value(enc)
Decode a 8 bits grayscale image (JPEG_GRAY8 or GRAY8) and returns a 8 bits gray scale image.          :param da: :class:`DeviceAttribute` that contains the image         :type da: :class:`DeviceAttribute`         :param extract_as: defaults to ExtractAs.Numpy         :type extract_as: ExtractAs         :return: the decoded data          - In case String string is choosen as extract method, a tuple is returned:             width<int>, height<int>, buffer<str>         - In case Numpy is choosen as extract method, a :class:`numpy.ndarray` is           returned with ndim=2, shape=(height, width) and dtype=numpy.uint8.         - In case Tuple or List are choosen, a tuple<tuple<int>> or list<list<int>>           is returned.         .. warning::            The PyTango calls that return a :class:`DeviceAttribute`            (like :meth:`DeviceProxy.read_attribute` or :meth:`DeviceProxy.command_inout`)            automatically extract the contents by default. This method requires            that the given :class:`DeviceAttribute` is obtained from a            call which **DOESN'T** extract the contents. Example::                 dev = tango.DeviceProxy("a/b/c")                da = dev.read_attribute("my_attr", extract_as=tango.ExtractAs.Nothing)                enc = tango.EncodedAttribute()                data = enc.decode_gray8(da)
Decode a 16 bits grayscale image (GRAY16) and returns a 16 bits gray scale image.          :param da: :class:`DeviceAttribute` that contains the image         :type da: :class:`DeviceAttribute`         :param extract_as: defaults to ExtractAs.Numpy         :type extract_as: ExtractAs         :return: the decoded data          - In case String string is choosen as extract method, a tuple is returned:             width<int>, height<int>, buffer<str>         - In case Numpy is choosen as extract method, a :class:`numpy.ndarray` is           returned with ndim=2, shape=(height, width) and dtype=numpy.uint16.         - In case Tuple or List are choosen, a tuple<tuple<int>> or list<list<int>>           is returned.         .. warning::            The PyTango calls that return a :class:`DeviceAttribute`            (like :meth:`DeviceProxy.read_attribute` or :meth:`DeviceProxy.command_inout`)            automatically extract the contents by default. This method requires            that the given :class:`DeviceAttribute` is obtained from a            call which **DOESN'T** extract the contents. Example::                 dev = tango.DeviceProxy("a/b/c")                da = dev.read_attribute("my_attr", extract_as=tango.ExtractAs.Nothing)                enc = tango.EncodedAttribute()                data = enc.decode_gray16(da)
Decode a color image (JPEG_RGB or RGB24) and returns a 32 bits RGB image.          :param da: :class:`DeviceAttribute` that contains the image         :type da: :class:`DeviceAttribute`         :param extract_as: defaults to ExtractAs.Numpy         :type extract_as: ExtractAs         :return: the decoded data          - In case String string is choosen as extract method, a tuple is returned:             width<int>, height<int>, buffer<str>         - In case Numpy is choosen as extract method, a :class:`numpy.ndarray` is           returned with ndim=2, shape=(height, width) and dtype=numpy.uint32.         - In case Tuple or List are choosen, a tuple<tuple<int>> or list<list<int>>           is returned.         .. warning::            The PyTango calls that return a :class:`DeviceAttribute`            (like :meth:`DeviceProxy.read_attribute` or :meth:`DeviceProxy.command_inout`)            automatically extract the contents by default. This method requires            that the given :class:`DeviceAttribute` is obtained from a            call which **DOESN'T** extract the contents. Example::                 dev = tango.DeviceProxy("a/b/c")                da = dev.read_attribute("my_attr", extract_as=tango.ExtractAs.Nothing)                enc = tango.EncodedAttribute()                data = enc.decode_rgb32(da)
Checks if method given by it's name for the given DeviceImpl     class has the correct signature. If a read/write method doesn't     have a parameter (the traditional Attribute), then the method is     wrapped into another method which has correct parameter definition     to make it work.      :param tango_device_klass: a DeviceImpl class     :type tango_device_klass: class     :param attribute: the attribute data information     :type attribute: AttrData
Checks if method given by it's name for the given DeviceImpl     class has the correct signature. If a read/write method doesn't     have a parameter (the traditional Attribute), then the method is     wrapped into another method which has correct parameter definition     to make it work.      :param tango_device_klass: a DeviceImpl class     :type tango_device_klass: class     :param attribute: the attribute data information     :type attribute: AttrData
Checks if the read and write methods have the correct signature.     If a read/write method doesn't have a parameter (the traditional     Attribute), then the method is wrapped into another method to make     this work.      :param tango_device_klass: a DeviceImpl class     :type tango_device_klass: class     :param attribute: the attribute data information     :type attribute: AttrData
Checks if method given by it's name for the given DeviceImpl     class has the correct signature. If a read/write method doesn't     have a parameter (the traditional Pipe), then the method is     wrapped into another method which has correct parameter definition     to make it work.      :param tango_device_klass: a DeviceImpl class     :type tango_device_klass: class     :param pipe: the pipe data information     :type pipe: PipeData
Checks if method given by it's name for the given DeviceImpl     class has the correct signature. If a read/write method doesn't     have a parameter (the traditional Pipe), then the method is     wrapped into another method which has correct parameter definition     to make it work.      :param tango_device_klass: a DeviceImpl class     :type tango_device_klass: class     :param pipe: the pipe data information     :type pipe: PipeData
Checks if the read and write methods have the correct signature.     If a read/write method doesn't have a parameter (the traditional     Pipe), then the method is wrapped into another method to make     this work.      :param tango_device_klass: a DeviceImpl class     :type tango_device_klass: class     :param pipe: the pipe data information     :type pipe: PipeData
Return tango data if the argument is a tango object,     False otherwise.
Patch tango objects before they are processed by the metaclass.
Declares a new tango command in a :class:`Device`.     To be used like a decorator in the methods you want to declare as     tango commands. The following example declares commands:          * `void TurnOn(void)`         * `void Ramp(DevDouble current)`         * `DevBool Pressurize(DevDouble pressure)`      ::          class PowerSupply(Device):              @command             def TurnOn(self):                 self.info_stream('Turning on the power supply')              @command(dtype_in=float)             def Ramp(self, current):                 self.info_stream('Ramping on %f...' % current)              @command(dtype_in=float, doc_in='the pressure to be set',                      dtype_out=bool, doc_out='True if it worked, False otherwise')             def Pressurize(self, pressure):                 self.info_stream('Pressurizing to %f...' % pressure)                 return True      .. note::         avoid using *dformat* parameter. If you need a SPECTRUM         attribute of say, boolean type, use instead ``dtype=(bool,)``.      :param dtype_in:         a :ref:`data type <pytango-hlapi-datatypes>` describing the         type of parameter. Default is None meaning no parameter.     :param dformat_in: parameter data format. Default is None.     :type dformat_in: AttrDataFormat     :param doc_in: parameter documentation     :type doc_in: str      :param dtype_out:         a :ref:`data type <pytango-hlapi-datatypes>` describing the         type of return value. Default is None meaning no return value.     :param dformat_out: return value data format. Default is None.     :type dformat_out: AttrDataFormat     :param doc_out: return value documentation     :type doc_out: str     :param display_level: display level for the command (optional)     :type display_level: DispLevel     :param polling_period: polling period in milliseconds (optional)     :type polling_period: int     :param green_mode:         set green mode on this specific command. Default value is None meaning         use the server green mode. Set it to GreenMode.Synchronous to force         a non green command in a green server.      .. versionadded:: 8.1.7         added green_mode option      .. versionadded:: 9.2.0         added display_level and polling_period optional argument
Provides a simple way to run a tango server. It handles exceptions     by writting a message to the msg_stream.      The `classes` parameter can be either a sequence of:      * :class:`~tango.server.Device` or     * a sequence of two elements       :class:`~tango.DeviceClass`, :class:`~tango.DeviceImpl` or     * a sequence of three elements       :class:`~tango.DeviceClass`, :class:`~tango.DeviceImpl`,       tango class name (str)      or a dictionary where:      * key is the tango class name     * value is either:         * a :class:`~tango.server.Device` class or         * a sequence of two elements           :class:`~tango.DeviceClass`, :class:`~tango.DeviceImpl`           or         * a sequence of three elements           :class:`~tango.DeviceClass`, :class:`~tango.DeviceImpl`,           tango class name (str)      The optional `post_init_callback` can be a callable (without     arguments) or a tuple where the first element is the callable,     the second is a list of arguments (optional) and the third is a     dictionary of keyword arguments (also optional).      .. note::        the order of registration of tango classes defines the order        tango uses to initialize the corresponding devices.        if using a dictionary as argument for classes be aware that the        order of registration becomes arbitrary. If you need a        predefined order use a sequence or an OrderedDict.      Example 1: registering and running a PowerSupply inheriting from     :class:`~tango.server.Device`::          from tango.server import Device, DeviceMeta, run          class PowerSupply(Device):             pass          run((PowerSupply,))      Example 2: registering and running a MyServer defined by tango     classes `MyServerClass` and `MyServer`::          from tango import Device_4Impl, DeviceClass         from tango.server import run          class MyServer(Device_4Impl):             pass          class MyServerClass(DeviceClass):             pass          run({'MyServer': (MyServerClass, MyServer)})      Example 3: registering and running a MyServer defined by tango     classes `MyServerClass` and `MyServer`::          from tango import Device_4Impl, DeviceClass         from tango.server import Device, DeviceMeta, run          class PowerSupply(Device):             pass          class MyServer(Device_4Impl):             pass          class MyServerClass(DeviceClass):             pass          run([PowerSupply, [MyServerClass, MyServer]])         # or: run({'MyServer': (MyServerClass, MyServer)})      :param classes:         a sequence of :class:`~tango.server.Device` classes or         a dictionary where keyword is the tango class name and value         is a sequence of Tango Device Class python class, and Tango         Device python class     :type classes: sequence or dict      :param args:         list of command line arguments [default: None, meaning use         sys.argv]     :type args: list      :param msg_stream:         stream where to put messages [default: sys.stdout]      :param util:         PyTango Util object [default: None meaning create a Util         instance]     :type util: :class:`~tango.Util`      :param event_loop: event_loop callable     :type event_loop: callable      :param post_init_callback:         an optional callback that is executed between the calls         Util.server_init and Util.server_run     :type post_init_callback:         callable or tuple (see description above)      :param raises:         Disable error handling and propagate exceptions from the server     :type raises: bool      :return: The Util singleton object     :rtype: :class:`~tango.Util`      .. versionadded:: 8.1.2      .. versionchanged:: 8.1.4         when classes argument is a sequence, the items can also be         a sequence <TangoClass, TangoClassClass>[, tango class name]      .. versionchanged:: 9.2.2         `raises` argument has been added
Since PyTango 8.1.2 it is just an alias to     :func:`~tango.server.run`. Use :func:`~tango.server.run`     instead.      .. versionadded:: 8.0.0      .. versionchanged:: 8.0.3         Added `util` keyword parameter.         Returns util object      .. versionchanged:: 8.1.1         Changed default msg_stream from *stderr* to *stdout*         Added `event_loop` keyword parameter.         Returns util object      .. versionchanged:: 8.1.2         Added `post_init_callback` keyword parameter      .. deprecated:: 8.1.2         Use :func:`~tango.server.run` instead.
Invoked to create dynamic attributes for the given devices.         Default implementation calls         :meth:`TT.initialize_dynamic_attributes` for each device          :param dev_list: list of devices         :type dev_list: :class:`tango.DeviceImpl`
Run the class as a device server.         It is based on the tango.server.run method.          The difference is that the device class         and server name are automatically given.          Args:             args (iterable): args as given in the tango.server.run method                              without the server name. If None, the sys.argv                              list is used             kwargs: the other keywords argument are as given                     in the tango.server.run method.
To be used as a decorator. Will define the decorated method         as a write attribute method to be called when client writes         the attribute
To be used as a decorator. Will define the decorated method         as a write pipe method to be called when client writes to the pipe
for internal usage only
Update database with existing devices
Helper that retuns a dict of devices for this server.          :return:             Returns a tuple of two elements:               - dict<tango class name : list of device names>               - dict<device names : tango class name>         :rtype: tuple<dict, dict>
:param member_filter:             callable(obj, tango_class_name, member_name, member) -> bool
Copy state from a future to a concurrent.futures.Future.
Internal helper to copy state from another Future.     The other Future may be a concurrent.futures.Future.
Chain two futures so that when one completes, so does the other.     The result (or exception) of source will be copied to destination.     If destination is cancelled, source gets cancelled too.     Compatible with both asyncio.Future and concurrent.futures.Future.
Submit a coroutine object to a given event loop.     Return a concurrent.futures.Future to access the result.
Helper function to initialize attribute config objects
set_enum_labels(self, enum_labels) -> None              Set default enumeration labels.          Parameters :             - enum_labels : (seq<str>) list of enumeration labels          New in PyTango 9.2.0
Return the given operation as a gevent future.
Execute an operation and return the result.
Experimental function. Not part of the official API
Get readme file contents without the badges.
A method to determine absolute path for a given relative path to the     directory where this setup.py script is located
for internal usage only
set_default_property_values(self, dev_class, class_prop, dev_prop) -> None                  Sets the default property values              Parameters :                 - dev_class : (DeviceClass) device class object                 - class_prop : (dict<str,>) class properties                 - dev_prop : (dict<str,>) device properties              Return     : None
get_class_properties(self, dev_class, class_prop) -> None                      Returns the class properties                  Parameters :                     - dev_class : (DeviceClass) the DeviceClass object                     - class_prop : [in, out] (dict<str, None>) the property names. Will be filled                                    with property values                  Return     : None
get_device_properties(self, dev, class_prop, dev_prop) -> None                      Returns the device properties                  Parameters :                     - dev : (DeviceImpl) the device object                     - class_prop : (dict<str, obj>) the class properties                     - dev_prop : [in,out] (dict<str, None>) the device property names                  Return     : None
get_property_type(self, prop_name, properties) -> CmdArgType                      Gets the property type for the given property name using the                     information given in properties                  Parameters :                     - prop_name : (str) property name                     - properties : (dict<str,data>) property data                  Return     : (CmdArgType) the tango type for the given property
get_property_values(self, prop_name, properties) -> obj                      Gets the property value                  Parameters :                     - prop_name : (str) property name                     - properties : (dict<str,obj>) properties                 Return     : (obj) the value for the given property name
Export a device to the database      :param argin: Str[0] = Device name     Str[1] = CORBA IOR     Str[2] = Device server process host name     Str[3] = Device server process PID or string ``null``     Str[4] = Device server process version     :type: tango.DevVarStringArray     :return:     :rtype: tango.DevVoid
Runs the Database DS as a standalone database. Run it with::          ./DataBaseds pydb-test -ORBendPoint giop:tcp::11000
Runs the Database device server embeded in another TANGO Database     (just like any other TANGO device server)
Get list of device domain name matching the specified          :param argin: The wildcard         :type: tango.DevString         :return: Device name domain list         :rtype: tango.DevVarStringArray
Mark all devices belonging to a specified device server         process as non exported          :param argin: Device server name (executable/instance)         :type: tango.DevString         :return:         :rtype: tango.DevVoid
Delete all attribute properties for the specified device attribute(s)          :param argin: str[0] = device name         Str[1]...str[n] = attribute name(s)         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Delete an attribute alias.          :param argin: Attriibute alias name.         :type: tango.DevString         :return:         :rtype: tango.DevVoid
Retrieve Tango class attribute property history          :param argin: Str[0] = Tango class         Str[1] = Attribute name         Str[2] = Property name         :type: tango.DevVarStringArray         :return: Str[0] = Attribute name         Str[1] = Property name         Str[2] = date         Str[3] = Property value number (array case)         Str[4] = Property value 1         Str[n] = Property value n         :rtype: tango.DevVarStringArray
Put device attribute property. This command adds the possibility to have attribute property         which are arrays. Not possible with the old DbPutDeviceAttributeProperty command.         This old command is not deleted for compatibility reasons.          :param argin: Str[0] = Device name         Str[1] = Attribute number         Str[2] = Attribute name         Str[3] = Property number         Str[4] = Property name         Str[5] = Property value number (array case)         Str[5] = Property value 1         Str[n] = Property value n (array case)         .....         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get attribute alias list for a specified filter          :param argin: attribute alias filter string (eg: att*)         :type: tango.DevString         :return: attribute aliases         :rtype: tango.DevVarStringArray
Query the database for device exported for the specified class.          :param argin: Class name         :type: tango.DevString         :return: Device exported list         :rtype: tango.DevVarStringArray
Define an alias for an attribute          :param argin: Str[0] = attribute name         Str[1] = attribute alias         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get list of device server process defined in database         with name matching the specified filter          :param argin: The filter         :type: tango.DevString         :return: Device server process name list         :rtype: tango.DevVarStringArray
delete a device attribute property from the database          :param argin: Str[0] = Device name         Str[1] = Attribute name         Str[2] = Property name         Str[n] = Property name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get a list of device name families for device name matching the         specified wildcard          :param argin: The wildcard         :type: tango.DevString         :return: Family list         :rtype: tango.DevVarStringArray
Get a list of devices whose names satisfy the filter.          :param argin: filter         :type: tango.DevString         :return: list of exported devices         :rtype: tango.DevVarStringArray
Create / Update free object property(ies)          :param argin: Str[0] = Object name         Str[1] = Property number         Str[2] = Property name         Str[3] = Property value number         Str[4] = Property value 1         Str[n] = Property value n         ....         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Delete free property from database          :param argin: Str[0]  = Object name         Str[1] = Property name         Str[n] = Property name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
This command supports array property compared to the old command called         DbGetClassAttributeProperty. The old command has not been deleted from the         server for compatibility reasons.          :param argin: Str[0] = Tango class name         Str[1] = Attribute name         Str[n] = Attribute name         :type: tango.DevVarStringArray         :return: Str[0] = Tango class name         Str[1] = Attribute property  number         Str[2] = Attribute property 1 name         Str[3] = Attribute property 1 value number (array case)         Str[4] = Attribute property 1 value         Str[n] = Attribute property 1 value (array case)         Str[n + 1] = Attribute property 2 name         Str[n + 2] = Attribute property 2 value number (array case)         Str[n + 3] = Attribute property 2 value         Str[n + m] = Attribute property 2 value (array case)         :rtype: tango.DevVarStringArray
Get a list of exported devices whose names satisfy the filter (wildcard is          :param argin: filter         :type: tango.DevString         :return: list of exported devices         :rtype: tango.DevVarStringArray
Return alias for device name if found.          :param argin: The device name         :type: tango.DevString         :return: The alias found         :rtype: tango.DevString
Create/Update class attribute property(ies) in database          :param argin: Str[0] = Tango class name         Str[1] = Attribute number         Str[2] = Attribute name         Str[3] = Property number         Str[4] = Property name         Str[5] = Property value         .....         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get property list for a given Tango class with a specified filter          :param argin: The filter         :type: tango.DevString         :return: Property name list         :rtype: tango.DevVarStringArray
Get device alias name with a specific filter          :param argin: The filter         :type: tango.DevString         :return: Device alias list         :rtype: tango.DevVarStringArray
delete a class attribute and all its properties from database          :param argin: Str[0] = Tango class name         Str[1] = Attribute name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Retrieve Tango class property history          :param argin: Str[0] = Tango class         Str[1] = Property name         :type: tango.DevVarStringArray         :return: Str[0] = Property name         Str[1] = date         Str[2] = Property value number (array case)         Str[3] = Property value 1         Str[n] = Property value n         :rtype: tango.DevVarStringArray
Delete  device attribute properties from database          :param argin: Str[0] = Device name         Str[1] = Attribute name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
This is a very low level command.         It executes the specified  SELECT command on TANGO database and returns its result without filter.          :param argin: MySql Select command         :type: tango.DevString         :return: MySql Select command result          - svalues : select results          - lvalue[n] : =0 if svalue[n] is null else =1          (last lvalue -1) is number of rows, (last lvalue) is number of fields         :rtype: tango.DevVarLongStringArray
Create/Update device attribute property(ies) in database          :param argin: Str[0] = Device name         Str[1] = Attribute number         Str[2] = Attribute name         Str[3] = Property number         Str[4] = Property name         Str[5] = Property value         .....         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get device attribute property(ies) value          :param argin: Str[0] = Device name         Str[1] = Attribute name         Str[n] = Attribute name         :type: tango.DevVarStringArray         :return: Str[0] = Device name         Str[1] = Attribute property  number         Str[2] = Attribute property 1 name         Str[3] = Attribute property 1 value         Str[n + 1] = Attribute property 2 name         Str[n + 2] = Attribute property 2 value         :rtype: tango.DevVarStringArray
Get list of property defined for a free object and matching the         specified filter          :param argin: Str[0] = Object name         Str[1] = filter         :type: tango.DevVarStringArray         :return: Property name list         :rtype: tango.DevVarStringArray
Get Tango classes/device list embedded in a specific device server          :param argin: Device server process name         :type: tango.DevString         :return: Str[0] = Device name         Str[1] = Tango class         Str[n] = Device name         Str[n + 1] = Tango class         :rtype: tango.DevVarStringArray
Mark a device as non exported in database          :param argin: Device name         :type: tango.DevString         :return:         :rtype: tango.DevVoid
Get device name from its alias.          :param argin: Alias name         :type: tango.DevString         :return: Device name         :rtype: tango.DevString
Delete a devcie from database          :param argin: device name         :type: tango.DevString         :return:         :rtype: tango.DevVoid
Return list of attributes matching the wildcard          for the specified device          :param argin: Str[0] = Device name         Str[1] = Wildcard         :type: tango.DevVarStringArray         :return: attribute name list         :rtype: tango.DevVarStringArray
Get info about all servers running on specified host, name, mode and level          :param argin: Host name         :type: tango.DevString         :return: Server info for all servers running on specified host         :rtype: tango.DevVarStringArray
Rename a device server process          :param argin: str[0] = old device server name (exec/instance)         str[1] =  new device server name (exec/instance)         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get host list with name matching the specified filter          :param argin: The filter         :type: tango.DevString         :return: Host name list         :rtype: tango.DevVarStringArray
Get class inheritance for the specified device.          :param argin: Device name         :type: tango.DevString         :return: Classes off the specified device.         [0] - is the class of the device.         [1] - is the class from the device class is inherited.         ........and so on         :rtype: tango.DevVarStringArray
Delete server from the database but dont delete device properties          :param argin: Device server name         :type: tango.DevString         :return:         :rtype: tango.DevVoid
Get the attribute name for the given alias.         If alias not found in database, returns an empty string.          :param argin: The attribute alias name         :type: tango.DevString         :return: The attribute name (device/attribute)         :rtype: tango.DevString
Get free object property          :param argin: Str[0] = Object name         Str[1] = Property name         Str[n] = Property name         :type: tango.DevVarStringArray         :return: Str[0] = Object name         Str[1] = Property number         Str[2] = Property name         Str[3] = Property value number (array case)         Str[4] = Property value 1         Str[n] = Property value n (array case)         Str[n + 1] = Property name         Str[n + 2] = Property value number (array case)         Str[n + 3] = Property value 1         Str[n + m] = Property value m         :rtype: tango.DevVarStringArray
Get list of Tango classes for a device server          :param argin: device server process name         :type: tango.DevString         :return: list of classes for this device server         :rtype: tango.DevVarStringArray
Create / Update device property(ies)          :param argin: Str[0] = Tango device name         Str[1] = Property number         Str[2] = Property name         Str[3] = Property value number         Str[4] = Property value 1         Str[n] = Property value n         ....         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Reset the timing attribute values.          :param :         :type: tango.DevVoid         :return:         :rtype: tango.DevVoid
Create / Update class property(ies)          :param argin: Str[0] = Tango class name         Str[1] = Property number         Str[2] = Property name         Str[3] = Property value number         Str[4] = Property value 1         Str[n] = Property value n         ....         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Import a device from the database          :param argin: Device name (or alias)         :type: tango.DevString         :return: Str[0] = device name         Str[1] = CORBA IOR         Str[2] = device version         Str[3] = device server process name         Str[4] = host name         Str[5] = Tango class name          Lg[0] = Exported flag         Lg[1] = Device server process PID         :rtype: tango.DevVarLongStringArray
Delete device property(ies)          :param argin: Str[0] = Device name         Str[1] = Property name         Str[n] = Property name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get Tango class for the specified device.          :param argin: Device name         :type: tango.DevString         :return: Device Tango class         :rtype: tango.DevString
Retrieve device attribute property history          :param argin: Str[0] = Device name         Str[1] = Attribute name         Str[2] = Property name         :type: tango.DevVarStringArray         :return: Str[0] = Attribute name         Str[1] = Property name         Str[2] = date         Str[3] = Property value number (array case)         Str[4] = Property value 1         Str[n] = Property value n         :rtype: tango.DevVarStringArray
Get info about host, mode and level for specified server          :param argin: server name         :type: tango.DevString         :return: server info         :rtype: tango.DevVarStringArray
Define alias for  a given device name          :param argin: Str[0] = device name         Str[1] = alias name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get property list belonging to the specified device and with         name matching the specified filter          :param argin: Str[0] = device name         Str[1] = Filter         :type: tango.DevVarStringArray         :return: Property name list         :rtype: tango.DevVarStringArray
Get list of device server process name running on host with name matching         the specified filter          :param argin: The filter         :type: tango.DevString         :return: Device server process name list         :rtype: tango.DevVarStringArray
Get list of free object defined in database with name         matching the specified filter          :param argin: The filter         :type: tango.DevString         :return: Object name list         :rtype: tango.DevVarStringArray
delete class attribute properties from database          :param argin: Str[0] = Tango class name         Str[1] = Attribute name         Str[2] = Property name         Str[n] = Property name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Returns the instance names found for specified server.          :param argin: Server name         :type: tango.DevString         :return: The instance names found for specified server.         :rtype: tango.DevVarStringArray
Get the attribute alias from the attribute name.         Returns one empty string if nothing found in database          :param argin: The attribute name (dev_name/att_name)         :type: tango.DevString         :return: The attribute alias name (or empty string)         :rtype: tango.DevString
Create a device server process entry in database          :param argin: Str[0] = Full device server name         Str[1] = Device(s) name         Str[2] = Tango class name         Str[n] = Device name         Str[n + 1] = Tango class name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get event channel info from database          :param argin: name of event channel or factory         :type: tango.DevString         :return: export information e.g. IOR         :rtype: tango.DevVarLongStringArray
Retrieve device  property history          :param argin: Str[0] = Device name         Str[1] = Property name         :type: tango.DevVarStringArray         :return: Str[0] = Property name         Str[1] = date         Str[2] = Property value number (array case)         Str[3] = Property value 1         Str[n] = Property value n         :rtype: tango.DevVarStringArray
Returns the list of server names found for the wildcard specified.         It returns only the server executable name without instance name as DbGetServerList.          :param argin: wildcard for server names.         :type: tango.DevString         :return: server names found.         :rtype: tango.DevVarStringArray
Retrieve device attribute properties. This command has the possibility to retrieve         device attribute properties which are arrays. It is not possible with the old         DbGetDeviceAttributeProperty command. Nevertheless, the old command has not been         deleted for compatibility reason          :param argin: Str[0] = Device name         Str[1] = Attribute name         Str[n] = Attribute name         :type: tango.DevVarStringArray         :return: Str[0] = Device name         Str[1] = Attribute property  number         Str[2] = Attribute property 1 name         Str[3] = Attribute property 1 value number (array case)         Str[4] = Attribute property 1 value         Str[n] = Attribute property 1 value (array case)         Str[n + 1] = Attribute property 2 name         Str[n + 2] = Attribute property 2 value number (array case)         Str[n + 3] = Attribute property 2 value         Str[n + m] = Attribute property 2 value (array case)         :rtype: tango.DevVarStringArray
Delete class properties from database          :param argin: Str[0] = Tango class name         Str[1] = Property name         Str[n] = Property name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Mark one event channel as non exported in database          :param argin: name of event channel or factory to unexport         :type: tango.DevString         :return: none         :rtype: tango.DevVoid
Get Tango class property(ies) value          :param argin: Str[0] = Tango class name         Str[1] = Attribute name         Str[n] = Attribute name         :type: tango.DevVarStringArray         :return: Str[0] = Tango class name         Str[1] = Attribute property  number         Str[2] = Attribute property 1 name         Str[3] = Attribute property 1 value         Str[n + 1] = Attribute property 2 name         Str[n + 2] = Attribute property 2 value         :rtype: tango.DevVarStringArray
This command adds support for array properties compared to the previous one         called DbPutClassAttributeProperty. The old comman is still there for compatibility reason          :param argin: Str[0] = Tango class name         Str[1] = Attribute number         Str[2] = Attribute name         Str[3] = Property number         Str[4] = Property name         Str[5] = Property value number (array case)         Str[5] = Property value 1         Str[n] = Property value n (array case)         .....         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Update server info including host, mode and level          :param argin: server info         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Delete a device alias.          :param argin: device alias name         :type: tango.DevString         :return:         :rtype: tango.DevVoid
Export Event channel to database          :param argin: Str[0] = event channel name (or factory name)         Str[1] = CORBA IOR         Str[2] = Notifd host name         Str[3] = Notifd pid         Str[4] = Notifd version         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Returns info from DbImportDevice and started/stopped dates.          :param argin: Device name         :type: tango.DevString         :return: Str[0] = Device name         Str[1] = CORBA IOR         Str[2] = Device version         Str[3] = Device Server name         Str[4] = Device Server process host name         Str[5] = Started date (or ? if not set)         Str[6] = Stopped date (or ? if not set)         Str[7] = Device class          Lg[0] = Device exported flag         Lg[1] = Device Server process PID (or -1 if not set)         :rtype: tango.DevVarLongStringArray
Retrieve object  property history          :param argin: Str[0] = Object name         Str[2] = Property name         :type: tango.DevVarStringArray         :return: Str[0] = Property name         Str[1] = date         Str[2] = Property value number (array case)         Str[3] = Property value 1         Str[n] = Property value n         :rtype: tango.DevVarStringArray
Get a list of device name members for device name matching the         specified filter          :param argin: The filter         :type: tango.DevString         :return: Device names member list         :rtype: tango.DevVarStringArray
Get Tango class list with a specified filter          :param argin: Filter         :type: tango.DevString         :return: Class list         :rtype: tango.DevVarStringArray
Get the attribute name from the given alias.         If the given alias is not found in database, returns an empty string          :param argin: The attribute alias         :type: tango.DevString         :return: The attribute name (dev_name/att_name)         :rtype: tango.DevString
delete info related to a Tango devvice server process          :param argin: Device server name         :type: tango.DevString         :return:         :rtype: tango.DevVoid
Get attrilute list for a given Tango class with a specified filter          :param argin: Str[0] = Tango class name         Str[1] = Attribute name filter (eg: att*)         :type: tango.DevVarStringArray         :return: Str[0] = Class attribute name         Str[n] = Class attribute name         :rtype: tango.DevVarStringArray
Add a Tango class device to a specific device server          :param argin: Str[0] = Full device server process name         Str[1] = Device name         Str[2] = Tango class name         :type: tango.DevVarStringArray         :return:         :rtype: tango.DevVoid
Get a list of devices for specified server and class.          :param argin: argin[0] : server name         argin[1] : class name         :type: tango.DevVarStringArray         :return: The list of devices for specified server and class.         :rtype: tango.DevVarStringArray
Return the given operation as a concurrent future.
Determines if the required PyTango version for the running     software is present. If not an exception is thrown.     Example usage::          from tango import requires_pytango          requires_pytango('7.1', conflicts=['8.1.1'], software='MyDS')      :param min_version:         minimum PyTango version [default: None, meaning no minimum         required]. If a string is given, it must be in the valid         version number format         (see: :class:`~distutils.version.LooseVersion`)     :type min_version:         None, str, :class:`~distutils.version.LooseVersion`     :param conflics:         a sequence of PyTango versions which conflict with the         software using it     :type conflics:         seq<str|LooseVersion>     :param software_name:         software name using tango. Used in the exception message     :type software_name: str      :raises Exception: if the required PyTango version is not met      New in PyTango 8.1.4
Determines if the required Tango version for the running     software is present. If not an exception is thrown.     Example usage::          from tango import requires_tango          requires_tango('7.1', conflicts=['8.1.1'], software='MyDS')      :param min_version:         minimum Tango version [default: None, meaning no minimum         required]. If a string is given, it must be in the valid         version number format         (see: :class:`~distutils.version.LooseVersion`)     :type min_version:         None, str, :class:`~distutils.version.LooseVersion`     :param conflics:         a sequence of Tango versions which conflict with the         software using it     :type conflics:         seq<str|LooseVersion>     :param software_name:         software name using Tango. Used in the exception message     :type software_name: str      :raises Exception: if the required Tango version is not met      New in PyTango 8.1.4
Return list of enumeration labels from Enum class.      The list is useful when creating an attribute, for the     `enum_labels` parameter.  The enumeration values are checked     to ensure they are unique, start at zero, and increment by one.      :param enum_cls: the Enum class to be inspected     :type enum_cls: :py:obj:`enum.Enum`      :return: List of label strings     :rtype: :py:obj:`list`      :raises EnumTypeError: in case the given class is invalid
Tells if the given tango type is numerical      :param tg_type: tango type     :type tg_type: :class:`tango.CmdArgType`     :param inc_array: (optional, default is False) determines if include array                       in the list of checked types     :type inc_array: :py:obj:`bool`      :return: True if the given tango type is a numerical or False otherwise     :rtype: :py:obj:`bool`
Tells if the given tango type is integer      :param tg_type: tango type     :type tg_type: :class:`tango.CmdArgType`     :param inc_array: (optional, default is False) determines if include array                       in the list of checked types     :type inc_array: :py:obj:`bool`      :return: True if the given tango type is integer or False otherwise     :rtype: :py:obj:`bool`
Tells if the given tango type is float      :param tg_type: tango type     :type tg_type: :class:`tango.CmdArgType`     :param inc_array: (optional, default is False) determines if include array                       in the list of checked types     :type inc_array: :py:obj:`bool`      :return: True if the given tango type is float or False otherwise     :rtype: :py:obj:`bool`
Tells if the given tango type is boolean      :param tg_type: tango type     :type tg_type: :class:`tango.CmdArgType`     :param inc_array: (optional, default is False) determines if include array                       in the list of checked types     :type inc_array: :py:obj:`bool`      :return: True if the given tango type is boolean or False otherwise     :rtype: :py:obj:`bool`
Tells if the given tango type is string      :param tg_type: tango type     :type tg_type: :class:`tango.CmdArgType`     :param inc_array: (optional, default is False) determines if include array                       in the list of checked types     :type inc_array: :py:obj:`bool`      :return: True if the given tango type is string or False otherwise     :rtype: :py:obj:`bool`
Converts a python sequence<str> object to a :class:`tango.StdStringVector`          :param seq: the sequence of strings         :type seq: sequence<:py:obj:`str`>         :param vec: (optional, default is None) an :class:`tango.StdStringVector`                     to be filled. If None is given, a new :class:`tango.StdStringVector`                     is created         :return: a :class:`tango.StdStringVector` filled with the same contents as seq         :rtype: :class:`tango.StdStringVector`
Converts a :class:`tango.StdStringVector` to a python sequence<str>          :param seq: the :class:`tango.StdStringVector`         :type seq: :class:`tango.StdStringVector`         :param vec: (optional, default is None) a python sequence to be filled.                      If None is given, a new list is created         :return: a python sequence filled with the same contents as seq         :rtype: sequence<str>
Converts a python sequence<float> object to a :class:`tango.StdDoubleVector`          :param seq: the sequence of floats         :type seq: sequence<:py:obj:`float`>         :param vec: (optional, default is None) an :class:`tango.StdDoubleVector`                     to be filled. If None is given, a new :class:`tango.StdDoubleVector`                     is created         :return: a :class:`tango.StdDoubleVector` filled with the same contents as seq         :rtype: :class:`tango.StdDoubleVector`
Converts a :class:`tango.StdDoubleVector` to a python sequence<float>          :param seq: the :class:`tango.StdDoubleVector`         :type seq: :class:`tango.StdDoubleVector`         :param vec: (optional, default is None) a python sequence to be filled.                      If None is given, a new list is created         :return: a python sequence filled with the same contents as seq         :rtype: sequence<float>
Converts a python sequence<DbDevInfo> object to a :class:`tango.DbDevInfos`          :param seq: the sequence of DbDevInfo         :type seq: sequence<DbDevInfo>         :param vec: (optional, default is None) an :class:`tango.DbDevInfos`                     to be filled. If None is given, a new :class:`tango.DbDevInfos`                     is created         :return: a :class:`tango.DbDevInfos` filled with the same contents as seq         :rtype: :class:`tango.DbDevInfos`
Converts a python sequence<DbDevExportInfo> object to a :class:`tango.DbDevExportInfos`          :param seq: the sequence of DbDevExportInfo         :type seq: sequence<DbDevExportInfo>         :param vec: (optional, default is None) an :class:`tango.DbDevExportInfos`                     to be filled. If None is given, a new :class:`tango.DbDevExportInfos`                     is created         :return: a :class:`tango.DbDevExportInfos` filled with the same contents as seq         :rtype: :class:`tango.DbDevExportInfos`
Converts a python sequence<DbDatum> object to a :class:`tango.DbData`          :param seq: the sequence of DbDatum         :type seq: sequence<DbDatum>         :param vec: (optional, default is None) an :class:`tango.DbData`                     to be filled. If None is given, a new :class:`tango.DbData`                     is created         :return: a :class:`tango.DbData` filled with the same contents as seq         :rtype: :class:`tango.DbData`
Translates a sequence<str> to a sequence of objects of give type and format          :param seq: the sequence         :type seq: sequence<str>         :param tg_type: tango type         :type tg_type: :class:`tango.CmdArgType`         :param tg_format: (optional, default is None, meaning SCALAR) tango format         :type tg_format: :class:`tango.AttrDataFormat`          :return: a new sequence
Converts a string into an object according to the given tango type             :param obj_str: the string to be converted            :type obj_str: :py:obj:`str`            :param tg_type: tango type            :type tg_type: :class:`tango.CmdArgType`            :return: an object calculated from the given string            :rtype: :py:obj:`object`
Converts a python object into a string according to the given tango type             :param obj: the object to be converted            :type obj: :py:obj:`object`            :param tg_type: tango type            :type tg_type: :class:`tango.CmdArgType`            :return: a string representation of the given object            :rtype: :py:obj:`str`
Copies documentation string of a method from the super class into the     rewritten method of the given class
Find user's home directory if possible. Otherwise raise error.      :return: user's home directory     :rtype: :py:obj:`str`      New in PyTango 7.1.4
Returns the value for the given environment name      Search order:          * a real environ var         * HOME/.tangorc         * /etc/tangorc      :param env_var_name: the environment variable name     :type env_var_name: str     :return: the value for the given environment name     :rtype: str      New in PyTango 7.1.4
Helper for dir2 implementation.
Default dir implementation.      Inspired by gist: katyukha/dirmixin.py     https://gist.github.com/katyukha/c6e5e2b829e247c9b009
A caseless way of checking if an item is in the list or not.         It returns None or the entry.
Remove the first occurence of an item, the caseless way.
Adds an item to the list and checks it's a string.
Extend the list with another list. Each member of the list must be         a string.
Counts references to 'item' in a caseless manner.         If item is not a string it will always return 0.
Provide an index of first occurence of item in the list. (or raise         a ValueError if item not present)         If item is not a string, will raise a TypeError.         minindex and maxindex are also optional arguments         s.index(x[, i[, j]]) return smallest k such that s[k] == x and i <= k < j
s.insert(i, x) same as s[i:i] = [x]         Raises TypeError if x isn't a string.
Internal usage only
Internal usage only
Internal usage only
Internal usage only
to_dev_failed(exc_type, exc_value, traceback) -> tango.DevFailed              Generate a TANGO DevFailed exception.             The exception is created with a single :class:`~tango.DevError`             object. A default value *tango.ErrSeverity.ERR* is defined for             the :class:`~tango.DevError` severity field.              The parameters are the same as the ones generates by a call to             :func:`sys.exc_info`.          Parameters :             - type : (class)  the exception type of the exception being handled             - value : (object) exception parameter (its associated value or the                       second argument to raise, which is always a class instance                       if the exception type is a class object)             - traceback : (traceback) traceback object          Return     : (tango.DevFailed) a tango exception object          New in PyTango 7.2.1
for internal usage only
Generate possible spelling corrections for word.
Substitute words in the string, according to the specified Normal,         e.g. "I'm" -> "I am"          :type str: str         :param str: The string to be mapped         :rtype: str
Substitute words in the string, according to the specified reflections,         e.g. "I'm" -> "you are"          :type str: str         :param str: The string to be mapped         :rtype: str
Substitute from Client statement into respose
spell correction
Generate a response to the user input.          :type text: str         :param text: The string to be mapped         :rtype: str
Main entry point of bonobo. It takes a graph and creates all the necessary plumbing around to execute it.      The only necessary argument is a :class:`Graph` instance, containing the logic you actually want to execute.      By default, this graph will be executed using the "threadpool" strategy: each graph node will be wrapped in a     thread, and executed in a loop until there is no more input to this node.      You can provide plugins factory objects in the plugins list, this function will add the necessary plugins for     interactive console execution and jupyter notebook execution if it detects correctly that it runs in this context.      You'll probably want to provide a services dictionary mapping service names to service instances.      :param Graph graph: The :class:`Graph` to execute.     :param str strategy: The :class:`bonobo.execution.strategies.base.Strategy` to use.     :param list plugins: The list of plugins to enhance execution.     :param dict services: The implementations of services this graph will use.     :return bonobo.execution.graph.GraphExecutionContext:
Wraps :obj:`fs.opener.registry.Registry.open_fs`, with default to local current working directory and expanding ~ in     path.      :param str fs_url: A filesystem URL     :param parse_result: A parsed filesystem URL.     :type parse_result: :class:`ParseResult`     :param bool writeable: True if the filesystem must be writeable.     :param bool create: True if the filesystem should be created if it does not exist.     :param str cwd: The current working directory (generally only relevant for OS filesystems).     :param str default_protocol: The protocol to use if one is not supplied in the FS URL (defaults to ``"osfs"``).     :returns: :class:`fs.base.FS` object
Read an env file into a collection of (name, value) tuples.
Creates an argument parser with arguments to override the system environment.      :api: bonobo.get_argument_parser      :param _parser:     :return:
Context manager to extract and apply environment related options from the provided argparser result.      A dictionnary with unknown options will be yielded, so the remaining options can be used by the caller.      :api: bonobo.patch_environ      :param mixed: ArgumentParser instance, Namespace, or dict.     :return:
Register a function as being part of an API, then returns the original function.
Open a HTTP connection to the URL and return a file-like object.
UNIX-like exit status, only coherent if the context has stopped.
Push a list of messages in the inputs of this graph's inputs, matching the output of special node "BEGIN" in         our graph.
Parse a 'key=val' option string into a python (key, val) pair      :param option: str     :return: tuple
Resolve a collection of strings into the matching python objects, defaulting to bonobo namespace if no package is provided.      Syntax for each string is path.to.package:attribute      :param transformations: tuple(str)     :return: tuple(object)
Transformation factory to reorder fields in a data stream.      :param fields:     :return: callable
Transformation factory that sets the field names on first iteration, without touching the values.      :param fields:     :return: callable
>>> UnpackItems(0)      :param items:     :param fields:     :param defaults:     :return: callable
Transformation factory that maps `function` on the values of a row.     It can be applied either to     1. all columns (`key=True`),     2. no column (`key=False`), or     3. a subset of columns by passing a callable, which takes column name and returns `bool`      (same as the parameter `function` in `filter`).      :param function: callable     :param key: bool or callable     :return: callable
Split an output into token tuple, real output tuple.      :param output:     :return: tuple, tuple
Starts this context, a.k.a the phase where you setup everything which will be necessary during the whole         lifetime of a transformation.          The "ContextCurrifier" is in charge of setting up a decorating stack, that includes both services and context         processors, and will call the actual node callable with additional parameters.
The actual infinite loop for this transformation.
A single step in the loop.          Basically gets an input bag, send it to the node, interpret the results.
Cleanup the context, after the loop ended.
Push a message list to this context's input queue.          :param mixed value: message
Transforms a pair of input/output into the real slim shoutput.          :param _input: Bag         :param _output: mixed         :return: Bag
Sends a message to all of this context's outputs.          :param mixed value: message         :param _control: if true, won't count in statistics.
Read from the input queue.          If Queue raises (like Timeout or Empty), stat won't be changed.
Create a strategy, or just returns it if it's already one.      :param name:      :return: Strategy
Check if the given argument is an instance of :class:`bonobo.config.ConfigurableMeta`, meaning it has all the     plumbery necessary to build :class:`bonobo.config.Configurable`-like instances.      :param mixed:     :param strict: should we consider partially configured objects?     :return: bool
If the given argument is somehow a :class:`bonobo.config.Configurable` object (either a subclass, an instance, or     a partially configured instance), then it will return a :class:`ConfigurableInspection` namedtuple, used to inspect     the configurable metadata (options). If you want to get the option values, you don't need this, it is only usefull     to perform introspection on a configurable.      If it's not looking like a configurable, it will raise a :class:`TypeError`.      :param mixed:     :return: ConfigurableInspection      :raise: TypeError
Gets valid user credentials from storage.      If nothing has been stored, or if the stored credentials are invalid,     the OAuth2 flow is completed to obtain the new credentials.      Returns:         Credentials, the obtained credential.
This allows to easily display a version snippet in Jupyter.
Extracts a list of cafes with on euro in Paris, renames the name, address and zipcode fields,     reorders the fields and formats to json and csv files.
Main callable for "bonobo" entrypoint.      Will load commands from "bonobo.commands" entrypoints, using stevedore.
Allow all readers to use eventually use output_fields XOR output_type options.
Write a row on the next line of given file.         Prefix is used for newlines.
Get a set of the outputs for a given node index.
Add a node without connections in this graph and returns its index.
Add a chain in this graph.
Iterate in topological order, based on networkx's topological_sort() function.
Find the index based on various strategies for a node, probably an input or output of chain. Supported         inputs are indexes, node values or names.
Shortcut for users whose theme is next to their conf.py.
Create or update a django model instance.      :param model:     :param defaults:     :param kwargs:     :return: object, created, updated
Context manager that monkey patches `bonobo.run` function with our current command logic.      :param runner: the callable that will handle the `run()` logic.
Only there for backward compatibility with third party extensions.     TODO: This should be deprecated (using the @deprecated decorator) in 0.7, and removed in 0.8 or 0.9.
Install requirements given a path to requirements.txt file.
If it's not a tuple, let's make a tuple of one item.     Otherwise, not changed.      :param tuple_or_mixed:     :return: tuple
Create a container with reasonable default service implementations for commonly use, standard-named, services.      Services:     - `fs` defaults to a fs2 instance based on current working directory     - `http`defaults to requests      :param services:     :return:
Create a reader instance, guessing its factory using filename (and eventually format).      :param name:     :param args:     :param format:     :param registry:     :param kwargs:      :return: mixed
Create a writer instance, guessing its factory using filename (and eventually format).      :param name:     :param args:     :param format:     :param registry:     :param kwargs:      :return: mixed
Returns a callable to build a reader for the provided filename, eventually forcing a format.          :param name: filename         :param format: format         :return: type
Returns a callable to build a writer for the provided filename, eventually forcing a format.          :param name: filename         :param format: format         :return: type
Run.
Create a Tile session.
Make a request against AirVisual.
Get all Tiles for a user's account.
Format:             {Command}{args length(little endian)}{str}         Length:             {4}{4}{str length}
get the permutation of specified range          example:         index    x   x_-2,-1_p         0        1         NaN         1       -1         NaN         2        3           2  (0.x > 0, and assigned to weight 2)         3        5           1  (2.x > 0, and assigned to weight 1)         4        1           3          :param df: data frame         :param column: the column to calculate p from         :param shifts: the range to consider         :return:
Get rate of change of column          :param df: DataFrame object         :param column: column name of the rate to calculate         :param shifts: days to shift, accept one shift only         :return: None
Get the column shifted by days          :param df: DataFrame object         :param column: name of the column to shift         :param shifts: days to shift, accept one shift only         :return: None
get the count of column in range (shifts)          example: kdjj_0_le_20_c         :param df: stock data         :param column: column name         :param shifts: range to count, only to previous         :return:
Calculate the RSV (Raw Stochastic Value) within N days          This value is essential for calculating KDJs         Current day is included in N         :param df: data         :param n_days: N days         :return: None
Calculate the RSI (Relative Strength Index) within N days          calculated based on the formula at:         https://en.wikipedia.org/wiki/Relative_strength_index          :param df: data         :param n_days: N days         :return: None
get smoothed moving average.          :param df: data         :param windows: range         :return: result series
Williams Overbought/Oversold Index          WMS=[(Hn—Ct)/(Hn—Ln)] ×100         Ct - the close price         Hn - N days high         Ln - N days low          :param df: data         :param n_days: N days         :return: None
Commodity Channel Index          CCI = (Typical Price  -  20-period SMA of TP) / (.015 x Mean Deviation)         Typical Price (TP) = (High + Low + Close)/3         TP is also implemented as 'middle'.          :param df: data         :param n_days: N days window         :return: None
True Range of the trading          tr = max[(high - low), abs(high - close_prev), abs(low - close_prev)]         :param df: data         :return: None
Average True Range          The average true range is an N-day smoothed moving average (SMMA) of         the true range values.  Default to 14 days.         https://en.wikipedia.org/wiki/Average_true_range          :param df: data         :return: None
get the default setting for DMI          including:         +DI: 14 days SMMA of +DM,         -DI: 14 days SMMA of -DM,         DX: based on +DI and -DI         ADX: 6 days SMMA of DX         :param df: data         :return:
Up move and down move          initialize up move and down move         :param df: data
+DM, positive directional moving          If window is not 1, calculate the SMMA of +DM         :param df: data         :param windows: range         :return:
-DM, negative directional moving accumulation          If window is not 1, return the SMA of -DM.         :param df: data         :param windows: range         :return:
+DI, positive directional moving index          :param df: data         :param windows: range         :return:
Get the K of KDJ          K ＝ 2/3 × (prev. K) +1/3 × (curr. RSV)         2/3 and 1/3 are the smooth parameters.         :param df: data         :param n_days: calculation range         :return: None
Get the D of KDJ          D = 2/3 × (prev. D) +1/3 × (curr. K)         2/3 and 1/3 are the smooth parameters.         :param df: data         :param n_days: calculation range         :return: None
Get the J of KDJ          J = 3K-2D         :param df: data         :param n_days: calculation range         :return: None
get simple moving average          :param df: data         :param column: column to calculate         :param windows: collection of window of simple moving average         :return: None
get exponential moving average          :param df: data         :param column: column to calculate         :param windows: collection of window of exponential moving average         :return: None
Get Bollinger bands.          boll_ub means the upper band of the Bollinger bands         boll_lb means the lower band of the Bollinger bands         boll_ub = MA + Kσ         boll_lb = MA − Kσ         M = BOLL_PERIOD         K = BOLL_STD_TIMES         :param df: data         :return: None
Moving Average Convergence Divergence          This function will initialize all following columns.          MACD Line (macd): (12-day EMA - 26-day EMA)         Signal Line (macds): 9-day EMA of MACD Line         MACD Histogram (macdh): MACD Line - Signal Line         :param df: data         :return: None
get moving standard deviation          :param df: data         :param column: column to calculate         :param windows: collection of window of moving standard deviation         :return: None
get moving variance          :param df: data         :param column: column to calculate         :param windows: collection of window of moving variance         :return: None
if the input is a `DataFrame`, convert it to this class.          :param index_column: the column that will be used as index,                              default to `date`         :param value: value to convert         :return: this extended class
Processes all API calls since last invocation, returning a list of data         in the order the API calls were created
change all underscored variants back to what the API is expecting
Supports all the search parameters in the API as well as python underscored variants
Set guid to -1 as default for not finding a user
Get a specific broadcast by guid
Get all broadcasts, with optional paging and limits.         Type filter can be 'scheduled', 'published' or 'failed'
Cancel a broadcast specified by guid
if "current" is false it will return all channels that a user             has published to in the past.              if publish_only is set to true, then return only the channels             that are publishable.              if settings is true, the API will make extra queries to return             the settings for each channel.
Return the prospects for the current API key.          Optionally start the result list at the given offset.          Each member of the return list is a prospect element containing         organizational information such as name and location.
Supports doing a search for prospects by city, reion, or country.          search_type should be one of 'city' 'region' 'country'.          This method is intended to be called with one of the outputs from the         get_options_for_query method above.
Gets the current user from the request and prepares and connects a signal receiver with the user already         attached to it.
Disconnects the signal receiver to prevent it from staying active.
Disconnects the signal receiver to prevent it from staying active in case of an exception.
Signal receiver with an extra, required 'user' kwarg. This method becomes a real (valid) signal receiver when         it is curried with the actor.
Returns whether the given field should be tracked by Auditlog.      Untracked fields are many-to-many relations and relations to the Auditlog LogEntry model.      :param field: The field to check.     :type field: Field     :return: Whether the given field should be tracked.     :rtype: bool
Returns the list of fields in the given model instance. Checks whether to use the official _meta API or use the raw     data. This method excludes many to many fields.      :param instance: The model instance to get the fields for     :type instance: Model     :return: The list of fields for the given model (instance)     :rtype: list
Calculates the differences between two model instances. One of the instances may be ``None`` (i.e., a newly     created model or deleted model). This will cause all fields with a value to have changed (from ``None``).      :param old: The old state of the model instance.     :type old: Model     :param new: The new state of the model instance.     :type new: Model     :return: A dictionary with the names of the changed fields as keys and a two tuple of the old and new field values              as value.     :rtype: dict
Return whether or not a User is authenticated.      Function provides compatibility following deprecation of method call to     `is_authenticated()` in Django 2.0.      This is *only* required to support Django < v1.10 (i.e. v1.9 and earlier),     as `is_authenticated` was introduced as a property in v1.10.s
Signal receiver that creates a log entry when a model instance is first saved to the database.      Direct use is discouraged, connect your model through :py:func:`auditlog.registry.register` instead.
Signal receiver that creates a log entry when a model instance is changed and saved to the database.      Direct use is discouraged, connect your model through :py:func:`auditlog.registry.register` instead.
Signal receiver that creates a log entry when a model instance is deleted from the database.      Direct use is discouraged, connect your model through :py:func:`auditlog.registry.register` instead.
Register a model with auditlog. Auditlog will then track mutations on this model's instances.          :param model: The model to register.         :type model: Model         :param include_fields: The fields to include. Implicitly excludes all other fields.         :type include_fields: list         :param exclude_fields: The fields to exclude. Overrides the fields to include.         :type exclude_fields: list
Connect signals for the model.
Disconnect signals for the model.
Helper method to create a new log entry. This method automatically populates some fields when no explicit value         is given.          :param instance: The model instance to log a change for.         :type instance: Model         :param kwargs: Field overrides for the :py:class:`LogEntry` object.         :return: The new log entry or `None` if there were no changes.         :rtype: LogEntry
Get log entries for the specified model instance.          :param instance: The model instance to get log entries for.         :type instance: Model         :return: QuerySet of log entries for the given model instance.         :rtype: QuerySet
Get log entries for the objects in the specified queryset.          :param queryset: The queryset to get the log entries for.         :type queryset: QuerySet         :return: The LogEntry objects for the objects in the given queryset.         :rtype: QuerySet
Get log entries for all objects of a specified type.          :param model: The model to get log entries for.         :type model: class         :return: QuerySet of log entries for the given model.         :rtype: QuerySet
Get the primary key field value for a model instance.          :param instance: The model instance to get the primary key for.         :type instance: Model         :return: The primary key value of the given model instance.
Return all objects related to ``objs`` via this ``GenericRelation``.
This is a proxy to the main fetch function to cache     the result based on the chart range parameter.
Get stock data (key stats and previous) from IEX.     Just deal with IEX's 100 stocks limit per request.
Compute a lifetimes matrix from our AssetFinder, then drop columns that         didn't exist at all during the query dates.          Parameters         ----------         start_date : pd.Timestamp             Base start date for the matrix.         end_date : pd.Timestamp             End date for the matrix.         extra_rows : int             Number of extra rows to compute before `start_date`.             Extra rows are needed by terms like moving averages that require a             trailing window of data.          Returns         -------         lifetimes : pd.DataFrame             Frame of dtype `bool` containing dates from `extra_rows` days             before `start_date`, continuing through to `end_date`.  The             returned frame contains as columns all assets in our AssetFinder             that existed for at least one day between `start_date` and             `end_date`.
Compute inputs for the given term.          This is mostly complicated by the fact that for each input we store as         many rows as will be necessary to serve **any** computation requiring         that input.
Compute the Pipeline terms in the graph for the requested start and end         dates.          Parameters         ----------         graph : zipline.pipeline.graph.TermGraph         dates : pd.DatetimeIndex             Row labels for our root mask.         symbols : list             Column labels for our root mask.         initial_workspace : dict             Map from term -> output.             Must contain at least entry for `self._root_mask_term` whose shape             is `(len(dates), len(symbols))`, but may contain additional             pre-computed terms for testing or optimization purposes.          Returns         -------         results : dict             Dictionary mapping requested results to outputs.
Convert raw computed pipeline results into a DataFrame for public APIs.          Parameters         ----------         terms : dict[str -> Term]             Dict mapping column names to terms.         data : dict[str -> ndarray[ndim=2]]             Dict mapping column names to computed results for those names.         mask : ndarray[bool, ndim=2]             Mask array of values to keep.         dates : ndarray[datetime64, ndim=1]             Row index for arrays `data` and `mask`         symbols : list             Column index          Returns         -------         results : pd.DataFrame             The indices of `results` are as follows:              index : two-tiered MultiIndex of (date, asset).                 Contains an entry for each (date, asset) pair corresponding to                 a `True` value in `mask`.             columns : Index of str                 One column per entry in `data`.          If mask[date, asset] is True, then result.loc[(date, asset), colname]         will contain the value of data[colname][date, asset].
Verify that the values passed to compute_chunk are well-formed.
preemphasising on the signal.      Args:         signal (array): The input signal.         shift (int): The shift step.         cof (float): The preemphasising coefficient. 0 equals to no filtering.      Returns:            array: The pre-emphasized signal.
Frame a signal into overlapping frames.      Args:         sig (array): The audio signal to frame of size (N,).         sampling_frequency (int): The sampling frequency of the signal.         frame_length (float): The length of the frame in second.         frame_stride (float): The stride between frames.         filter (array): The time-domain filter for applying to each frame.             By default it is one so nothing will be changed.         zero_padding (bool): If the samples is not a multiple of             frame_length(number of frames sample), zero padding will             be done for generating last frame.      Returns:             array: Stacked_frames-Array of frames of size (number_of_frames x frame_len).
This function computes the one-dimensional n-point discrete Fourier     Transform (DFT) of a real-valued array by means of an efficient algorithm     called the Fast Fourier Transform (FFT). Please refer to     https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.rfft.html     for further details.      Args:         frames (array): The frame array in which each row is a frame.         fft_points (int): The length of FFT. If fft_length is greater than frame_len, the frames will be zero-padded.      Returns:             array: The fft spectrum.             If frames is an num_frames x sample_per_frame matrix, output             will be num_frames x FFT_LENGTH.
Log power spectrum of each frame in frames.      Args:         frames (array): The frame array in which each row is a frame.         fft_points (int): The length of FFT. If fft_length is greater than             frame_len, the frames will be zero-padded.         normalize (bool): If normalize=True, the log power spectrum             will be normalized.      Returns:            array: The power spectrum - If frames is an            num_frames x sample_per_frame matrix, output will be            num_frames x fft_length.
This function the derivative features.      Args:         feat (array): The main feature vector(For returning the second              order derivative it can be first-order derivative).         DeltaWindows (int): The value of  DeltaWindows is set using             the configuration parameter DELTAWINDOW.      Returns:            array: Derivative feature vector - A NUMFRAMESxNUMFEATURES numpy            array which is the derivative features along the features.
This function is aimed to perform global cepstral mean and         variance normalization (CMVN) on input feature vector "vec".         The code assumes that there is one observation per row.      Args:         vec (array): input feature matrix             (size:(num_observation,num_features))         variance_normalization (bool): If the variance             normilization should be performed or not.      Return:           array: The mean(or mean+variance) normalized feature vector.
This function is aimed to perform local cepstral mean and     variance normalization on a sliding window. The code assumes that     there is one observation per row.      Args:         vec (array): input feature matrix             (size:(num_observation,num_features))         win_size (int): The size of sliding window for local normalization.             Default=301 which is around 3s if 100 Hz rate is             considered(== 10ms frame stide)         variance_normalization (bool): If the variance normilization should             be performed or not.      Return:           array: The mean(or mean+variance) normalized feature vector.
This function handle the issue with zero values if the are exposed     to become an argument for any log function.     :param x: The vector.     :return: The vector with zeros substituted with epsilon values.
Compute the Mel-filterbanks. Each filter will be stored in one rows.     The columns correspond to fft bins.      Args:         num_filter (int): the number of filters in the filterbank, default 20.         coefficients (int): (fftpoints//2 + 1). Default is 257.         sampling_freq (float): the samplerate of the signal we are working             with. It affects mel spacing.         low_freq (float): lowest band edge of mel filters, default 0 Hz         high_freq (float): highest band edge of mel filters,             default samplerate/2      Returns:            array: A numpy array of size num_filter x (fftpoints//2 + 1)                which are filterbank
Compute MFCC features from an audio signal.      Args:           signal (array): the audio signal from which to compute features.              Should be an N x 1 array          sampling_frequency (int): the sampling frequency of the signal              we are working with.          frame_length (float): the length of each frame in seconds.              Default is 0.020s          frame_stride (float): the step between successive frames in seconds.              Default is 0.02s (means no overlap)          num_filters (int): the number of filters in the filterbank,              default 40.          fft_length (int): number of FFT points. Default is 512.          low_frequency (float): lowest band edge of mel filters.              In Hz, default is 0.          high_frequency (float): highest band edge of mel filters.              In Hz, default is samplerate/2          num_cepstral (int): Number of cepstral coefficients.          dc_elimination (bool): hIf the first dc component should              be eliminated or not.      Returns:         array: A numpy array of size (num_frames x num_cepstral) containing mfcc features.
Compute Mel-filterbank energy features from an audio signal.          Args:          signal (array): the audio signal from which to compute features.              Should be an N x 1 array          sampling_frequency (int): the sampling frequency of the signal              we are working with.          frame_length (float): the length of each frame in seconds.              Default is 0.020s          frame_stride (float): the step between successive frames in seconds.              Default is 0.02s (means no overlap)          num_filters (int): the number of filters in the filterbank,              default 40.          fft_length (int): number of FFT points. Default is 512.          low_frequency (float): lowest band edge of mel filters.              In Hz, default is 0.          high_frequency (float): highest band edge of mel filters.              In Hz, default is samplerate/2      Returns:               array: features - the energy of fiterbank of size num_frames x num_filters. The energy of each frame: num_frames x 1
Compute log Mel-filterbank energy features from an audio signal.       Args:          signal (array): the audio signal from which to compute features.              Should be an N x 1 array          sampling_frequency (int): the sampling frequency of the signal              we are working with.          frame_length (float): the length of each frame in seconds.              Default is 0.020s          frame_stride (float): the step between successive frames in seconds.              Default is 0.02s (means no overlap)          num_filters (int): the number of filters in the filterbank,              default 40.          fft_length (int): number of FFT points. Default is 512.          low_frequency (float): lowest band edge of mel filters.              In Hz, default is 0.          high_frequency (float): highest band edge of mel filters.              In Hz, default is samplerate/2      Returns:               array: Features - The log energy of fiterbank of size num_frames x num_filters frame_log_energies. The log energy of each frame num_frames x 1
This function extracts temporal derivative features which are         first and second derivatives.      Args:         feature (array): The feature vector which its size is: N x M      Return:           array: The feature cube vector which contains the static, first and second derivative features of size: N x M x 3
removes the specified nodes from the cluster and returns     the remaining nodes
Create slices to split a UMI into approximately equal size substrings     Returns a list of tuples that can be passed to slice function
Build a dictionary of nearest neighbours using substrings, can be used     to reduce the number of pairwise comparisons.
Added by Matt 06/05/17     use substring dict to get (approximately) all the nearest neighbours to     each in a set of umis.
return the min UMI(s) need to account for cluster
return all UMIs with counts >1% of the         median counts in the cluster
identify all umis within hamming distance threshold
return groups for unique method
return groups for directional method
return groups for adjacency method
return groups for cluster or directional methods
Return "groups" for the the percentile method. Note         that grouping isn't really compatible with the percentile         method. This just returns the retained UMIs in a structure similar         to other methods
identify all umis within the hamming distance threshold         and where the counts of the first umi is > (2 * second umi counts)-1
find the connected UMIs within an adjacency dictionary
output short help (only command line options).
open file in *filename* with mode *mode*.      If *create* is set, the directory containing filename     will be created if it does not exist.      gzip - compressed files are recognized by the     suffix ``.gz`` and opened transparently.      Note that there are differences in the file     like objects returned, for example in the     ability to seek.      returns a file or file-like object.
return a header string with command line options and timestamp
return a string containing script parameters.      Parameters are all variables that start with ``param_``.
return a header string with command line options and     timestamp.
set up an experiment.      The :py:func:`Start` method will set up a file logger and add some     default and some optional options to the command line parser.  It     will then parse the command line and set up input/output     redirection and start a timer for benchmarking purposes.      The default options added by this method are:      ``-v/--verbose``         the :term:`loglevel`      ``timeit``         turn on benchmarking information and save to file      ``timeit-name``          name to use for timing information,      ``timeit-header``          output header for timing information.      ``seed``          the random seed. If given, the python random          number generator will be initialized with this          seed.      Optional options added are:      Arguments     ---------      param parser : :py:class:`U.OptionParser`        instance with command line options.      argv : list         command line options to parse. Defaults to         :py:data:`sys.argv`      quiet : bool         set :term:`loglevel` to 0 - no logging      return_parser : bool         return the parser object, no parsing. Useful for inspecting         the command line options of a script without running it.      add_pipe_options : bool         add common options for redirecting input/output      add_extract_options : bool         add options for extracting barcodes      add_sam_options : bool         add options for SAM/BAM input      add_umi_grouping_options : bool         add options for barcode grouping      add_group_dedup_options : bool         add options for UMI grouping and deduping      Returns     -------     tuple        (:py:class:`U.OptionParser` object, list of positional        arguments)
Check the validity of the option combinations for barcode extraction
Check the validity of the option combinations for sam/bam input
stop the experiment.      This method performs final book-keeping, closes the output streams     and writes the final log messages indicating script completion.
get a temporary file.      The file is created and the caller needs to close and delete     the temporary file once it is not used any more.      Arguments     ---------     dir : string         Directory of the temporary file and if not given is set to the         default temporary location in the global configuration dictionary.     shared : bool         If set, the tempory file will be in a shared temporary         location (given by the global configuration directory).     suffix : string         Filename suffix      Returns     -------     file : File         A file object of the temporary file.
return a temporary filename.      The file is created and the caller needs to delete the temporary     file once it is not used any more.      Arguments     ---------     dir : string         Directory of the temporary file and if not given is set to the         default temporary location in the global configuration dictionary.     shared : bool         If set, the tempory file will be in a shared temporary         location.     suffix : string         Filename suffix      Returns     -------     filename : string         Absolute pathname of temporary file.
extract the umi +/- cell barcode from the read id using the     specified separator
extract the umi +/- cell barcode from the specified tag
extract the umi and cell barcode from the read id (input as a     string) using the specified separator
extract the umi +/- cell barcode from the read name where the barcodes     were extracted using umis
Yields the counts per umi for each gene      bc_getter: method to get umi (plus optionally, cell barcode) from     read, e.g get_umi_read_id or get_umi_tag       TODO: ADD FOLLOWING OPTION      skip_regex: skip genes matching this regex. Useful to ignore                 unassigned reads (as per get_bundles class above)
return reads in order of metacontigs
Takes a cigar string and finds the first splice position as     an offset from the start. To find the 5' end (read coords) of     the junction for a reverse read, pass in the reversed cigar tuple
get the read position (taking account of clipping)
Check if chromosome has changed since last time. If it has, scan         for mates. Write the read to outfile and save the identity for paired         end retrieval
Scan the current chromosome for matches to any of the reads stored         in the read1s buffer
Write mates for remaining chromsome. Search for matches to any         unmatched reads
estimate the number of "true" cell barcodes using a gaussian     density-based method      input:          cell_barcode_counts = dict(key = barcode, value = count)          expect_cells (optional) = define the expected number of cells          cell_number (optional) = define number of cell barcodes to accept          plotfile_prefix = (optional) prefix for plots      returns:          List of true barcodes
estimate the number of "true" cell barcodes via a knee method     which finds the point with maximum distance      input:          cell_barcode_counts = dict(key = barcode, value = count)          cell_number (optional) = define number of cell barcodes to accept          plotfile_prefix = (optional) prefix for plots      returns:          List of true barcodes
Find the mappings between true and false cell barcodes based     on an edit distance threshold.      Any cell barcode within the threshold to more than one whitelist     barcode will be excluded
Check for errors (substitutions, insertions, deletions) between a barcode     and a set of whitelist barcodes.      Returns the whitelist barcodes which match the input barcode     allowing for errors. Returns as soon as two are identified.
Unzip filename to a temporary directory, set to the cwd.      The unzipped target is cleaned up after.
script main.      parses command line options in sys.argv, unless *argv* is given.
script main.      parses command line options in sys.argv, unless *argv* is given.
iterate over contents of fastq file.
This will return an iterator that returns tuples of fastq records.     At each step it will confirm that the first field of the read name     (before the first whitespace character) is identical between the     two reads. The response if it is not depends on the value of     :param:`strict`. If strict is true an error is returned. If strict     is `False` the second file is advanced until a read that matches     is found.      This allows for protocols where read one contains cell barcodes, and these     reads have been filtered and corrected before processing without regard     to read2
return quality score format -         might return several if ambiguous.
refill the list of random_umis
parse the BAM to obtain the frequency for each UMI
return n umis from the random_umis atr.
extract the identifier from a read and append the UMI and     cell barcode before the first space
Remove selected bases from seq and quals
test whether the umi_quals are below the threshold
return true if any of the umi quals is below the threshold
Mask all positions where quals < threshold with "N"
Extract the cell and umi barcodes using a regex.match object      inputs:      - read 1 and read2 = Record objects     - match = regex.match object     - extract_umi and extract_cell = switches to determine whether these                                      barcodes should be extracted     - discard = is there a region(s) of the sequence which should be       discarded entirely?     - retain_umi = Should UMI sequence be retained on the read sequence      returns:          - cell_barcode = Cell barcode string         - cell_barcode_quals = Cell barcode quality scores         - umi = UMI barcode string.         - umi_quals = UMI barcode quality scores         - new_seq = Read1 sequence after extraction         - new_quals = Read1 qualities after extraction      Barcodes and qualities default to empty strings where extract_cell     or extract_umi are false.
mask low quality bases and return masked umi
Filter out cell barcodes not in the whitelist, with         optional cell barcode error correction
read the first n entries in the bam file and identify the tags     available detecting multimapping
return a dataframe with aggregated counts per UMI
script main.      parses command line options in sys.argv, unless *argv* is given.
script main.      parses command line options in sys.argv, unless *argv* is given.
script main.      parses command line options in sys.argv, unless *argv* is given.
script main.      parses command line options in sys.argv, unless *argv* is given.
r"""     Calculate the diffusive conductance of conduits in network, where a     conduit is ( 1/2 pore - full throat - 1/2 pore ). See the notes section.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_diffusivity : string         Dictionary key of the pore diffusivity values      throat_diffusivity : string         Dictionary key of the throat diffusivity values      pore_area : string         Dictionary key of the pore area values      throat_area : string         Dictionary key of the throat area values      conduit_shape_factors : string         Dictionary key of the conduit DIFFUSION shape factor values      Returns     -------     g : ndarray         Array containing diffusive conductance values for conduits in the         geometry attached to the given physics object.      Notes     -----     (1) This function requires that all the necessary phase properties already     be calculated.      (2) This function calculates the specified property for the *entire*     network then extracts the values for the appropriate throats at the end.      (3) This function assumes cylindrical throats with constant cross-section     area. Corrections for different shapes and variable cross-section area can     be imposed by passing the proper flow_shape_factor argument.
r"""         Convert the Network (and optionally Phase) data to Pandas DataFrames.          Parameters         ----------         network: OpenPNM Network Object             The network containing the data to be stored          phases : list of OpenPNM Phase Objects             The data on each supplied phase will be added to DataFrame          join : boolean             If ``False`` (default), two DataFrames are returned with *pore*             data in one, and *throat* data in the other.  If ``True`` the pore             and throat data are combined into a single DataFrame.  This can be             problematic as it will put NaNs into all the *pore* columns which             are shorter than the *throat* columns.          Returns         -------         Pandas ``DataFrame`` object containing property and label data in each         column.  If ``join`` was False (default) the two DataFrames are         returned i a named tuple, or else a single DataFrame with pore and         throat data in the same file, despite the column length being         different.
r"""     Mortensen et al. have shown that the Hagen-Poiseuille hydraluic resistance     is linearly dependent on the compactness. Defined as perimeter^2/area.     The dependence is not universal as shapes with sharp corners provide more     resistance than those that are more elliptical. Count the number of     vertices and apply the right correction.      References     ----------     Mortensen N.A, Okkels F., and Bruus H. Reexamination of Hagen-Poiseuille     flow: Shape dependence of the hydraulic resistance in microchannels.     Physical Review E, v.71, pp.057301 (2005).
r"""     Mason and Morrow relate the capillary pressure to the shaped factor in a     similar way to Mortensen but for triangles.      References     ----------     Mason, G. and Morrow, N.R.. Capillary behavior of a perfectly wetting     liquid in irregular triangular tubes. Journal of Colloid and Interface     Science, 141(1), pp.262-274 (1991).
r"""     Jenkins and Rao relate the capillary pressure in an eliptical throat to     the aspect ratio      References     ----------     Jenkins, R.G. and Rao, M.B., The effect of elliptical pores on     mercury porosimetry results. Powder technology, 38(2), pp.177-180. (1984)
r"""
r"""         Adds outflow boundary condition to the selected pores.          Outflow condition simply means that the gradient of the solved         quantity does not change, i.e. is 0.
r"""         This method creates the petsc sparse coefficients matrix from the         OpenPNM scipy one. The method also equally decomposes the matrix at         certain rows into different blocks (each block contains all the         columns) and distributes them over the pre-assigned cores for parallel         computing. The method can be used in serial.
r"""         This method creates the petsc sparse linear solver.
r"""         Initialize the solution vector (self.petsc_x), which is a dense         matrix (1D vector) and defines the rhs vector (self.petsc_b) from         the existing data.
r"""         This method solves the sparse linear system, converts the         solution vector from a PETSc.Vec instance to a numpy array,         and finally destroys all the petsc objects to free memory.          Parameters         ----------         solver_type : string, optional             Default is the iterative solver 'cg' based on the             Conjugate Gradient method.         preconditioner_type : string, optional             Default is the 'jacobi' preconditioner, i.e., diagonal             scaling preconditioning. The preconditioner is used with             iterative solvers. When a direct solver is used, this             parameter is ignored.         factorization_type : string, optional             The factorization type used with the direct solver.             Default is 'lu'. This parameter is ignored when an             iterative solver is used.          Returns         -------         Returns a numpy array corresponding to the solution of         the linear sparse system Ax = b.          Notes         -----         Certain combinations of iterative solvers and precondioners         or direct solvers and factorization types are not supported.         The summary table of the different possibilities         can be found here:         https://www.mcs.anl.gov/petsc/documentation/linearsolvertable.html
r"""         This calculates the effective permeability in this linear transport         algorithm.          Parameters         ----------         inlets : array_like             The pores where the inlet pressure boundary conditions were             applied.  If not given an attempt is made to infer them from the             algorithm.          outlets : array_like             The pores where the outlet pressure boundary conditions were             applied.  If not given an attempt is made to infer them from the             algorithm.          domain_area : scalar, optional             The area of the inlet (and outlet) boundary faces.  If not given             then an attempt is made to estimate it, but it is usually             underestimated.          domain_length : scalar, optional             The length of the domain between the inlet and outlet boundary             faces.  If not given then an attempt is made to estimate it, but it             is usually underestimated.          Notes         -----         The area and length of the domain are found using the bounding box         around the inlet and outlet pores which do not necessarily lie on the         edge of the domain, resulting in underestimation of sizes.
r"""     Calculate the filling angle (alpha) for a given capillary pressure      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      mode : string (Default is 'max')         Determines what information to send back. Options are:         'max' : the maximum capillary pressure along the throat axis         'touch' : the maximum capillary pressure a meniscus can sustain before                   touching a solid feature         'men' : return the meniscus info for a target pressure      r_toroid : float or array_like         The radius of the toroid surrounding the pore      target_Pc : float         The target capillary pressure      num_points : float (Default 100)         The number of divisions to make along the profile length to assess the         meniscus properties in order to find target pressures, touch lengths,         minima and maxima.      surface_tension : dict key (string)         The dictionary key containing the surface tension values to be used. If         a pore property is given, it is interpolated to a throat list.      contact_angle : dict key (string)         The dictionary key containing the contact angle values to be used. If         a pore property is given, it is interpolated to a throat list.      throat_diameter : dict key (string)         The dictionary key containing the throat diameter values to be used.      touch_length : dict key (string)         The dictionary key containing the maximum length that a meniscus can         protrude into the connecting pore before touching a solid feature and         therfore invading      Notes     -----     This approach accounts for the converging-diverging nature of many throat     types.  Advancing the meniscus beyond the apex of the toroid requires an     increase in capillary pressure beyond that for a cylindical tube of the     same radius. The details of this equation are described by Mason and     Morrow [1]_, and explored by Gostick [2]_ in the context of a pore network     model.      References     ----------      .. [1] G. Mason, N. R. Morrow, Effect of contact angle on capillary            displacement curvatures in pore throats formed by spheres. J.            Colloid Interface Sci. 168, 130 (1994).     .. [2] J. Gostick, Random pore network modeling of fibrous PEMFC gas            diffusion media using Voronoi and Delaunay tessellations. J.            Electrochem. Soc. 160, F731 (2013).
r"""         Set up the required parameters for the algorithm          Parameters         ----------         phase : OpenPNM Phase object             The phase to be injected into the Network.  The Phase must have the             capillary entry pressure values for the system.          entry_pressure : string             The dictionary key to the capillary entry pressure.  If none is             supplied then the current value is retained. The default is             'throat.capillary_pressure'.          pore_volume : string             The dictionary key to the pore volume.  If none is supplied then             the current value is retained. The default is 'pore.volume'.          throat_volume : string             The dictionary key to the throat volume.  If none is supplied then             the current value is retained. The default is 'throat.volume'.
r"""          Parameters         ----------         pores : array_like             The list of inlet pores from which the Phase can enter the Network
r"""         Perform the algorithm          Parameters         ----------         n_steps : int             The number of throats to invaded during this step
r"""         Returns the phase configuration at the specified non-wetting phase         (invading phase) saturation.          Parameters         ----------         Snwp : scalar, between 0 and 1             The network saturation for which the phase configuration is             desired.          Returns         -------         Two dictionary containing arrays that describe the pore and throat         distribution at the given saturation.  Specifically, these are:          **'pore.occupancy'** : 1 indicates the pores is invaded and 0         otherwise.          **'throat.occupancy'** : Same as described above but for throats.
Apply trapping based on algorithm described by Y. Masson [1].         It is applied as a post-process and runs the percolation algorithm in         reverse assessing the occupancy of pore neighbors. Consider the         following scenario when running standard IP without trapping,         3 situations can happen after each invasion step:             The number of defending clusters stays the same and clusters can             shrink             A cluster of size one is suppressed             A cluster is split into multiple clusters         In reverse the following opposite situations can happen:             The number of defending clusters stays the same and clusters can             grow             A cluster of size one is created             Mutliple clusters merge into one cluster         With trapping the reversed rules are adjusted so that:             Only clusters that do not connect to a sink can grow and merge.             At the point that a neighbor connected to a sink is touched the             trapped cluster stops growing as this is the point of trapping in             forward invasion time.          Logger info displays the invasion sequence and pore index and a message         with condition number based on the modified trapping rules and the         assignment of the pore to a given cluster.          Initially all invaded pores are given cluster label -1         Outlets / Sinks are given -2         New clusters that grow into fully trapped clusters are either         identified at the point of breakthrough or grow from nothing if the         full invasion sequence is run, they are assigned numbers from 0 up.          Ref:         [1] Masson, Y., 2016. A fast two-step algorithm for invasion         percolation with trapping. Computers & Geosciences, 90, pp.41-48          Parameters         ----------         outlets : list or array of pore indices for defending fluid to escape         through          Returns         -------         Creates a throat array called 'pore.clusters' in the Algorithm         dictionary. Any positive number is a trapped cluster          Also creates 2 boolean arrays Np and Nt long called '<element>.trapped'
r"""     Calculate the hydraulic conductance of conduits in network, where a     conduit is ( 1/2 pore - full throat - 1/2 pore ). See the notes section.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_viscosity : string         Dictionary key of the pore viscosity values      throat_viscosity : string         Dictionary key of the throat viscosity values      pore_area : string         Dictionary key of the pore area values      throat_area : string         Dictionary key of the throat area values      conduit_shape_factors : string         Dictionary key of the conduit FLOW shape factor values      Returns     -------     g : ndarray         Array containing hydraulic conductance values for conduits in the         geometry attached to the given physics object.      Notes     -----     (1) This function requires that all the necessary phase properties already     be calculated.      (2) This function calculates the specified property for the *entire*     network then extracts the values for the appropriate throats at the end.      (3) This function assumes cylindrical throats with constant cross-section     area. Corrections for different shapes and variable cross-section area can     be imposed by passing the proper flow_shape_factor argument.
r"""     Calculate the generic conductance (could be mass, thermal, electrical,     or hydraylic) of conduits in the network, where a conduit is     ( 1/2 pore - full throat - 1/2 pore ).      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_diffusivity : string         Dictionary key of the pore diffusivity values.      throat_diffusivity : string         Dictionary key of the throat diffusivity values.      pore_area : string         Dictionary key of the pore area values.      throat_area : string         Dictionary key of the throat area values.      shape_factor : string         Dictionary key of the conduit shape factor values.      Returns     -------     g : ndarray         Array containing conductance values for conduits in the geometry         attached to the given physics object.      Notes     -----     (1) This function requires that all the necessary phase properties already     be calculated.      (2) This function calculates the specified property for the *entire*     network then extracts the values for the appropriate throats at the end.      (3) This function assumes cylindrical throats with constant cross-section     area. Corrections for different shapes and variable cross-section area can     be imposed by passing the proper shape factor.      (4) shape_factor depends on the physics of the problem, i.e. diffusion-like     processes and fluid flow need different shape factors.
r"""         This function is used to add objects to the project.  Arguments can         be single OpenPNM objects, an OpenPNM project list, or a plain list of         OpenPNM objects.
r"""         The object at the given index is removed from the list and returned.          Notes         -----         This method uses ``purge_object`` to perform the actual removal of the         object. It is reommended to just use that directly instead.          See Also         --------         purge_object
r"""         Clears objects from the project entirely or selectively, depdening on         the received arguments.          Parameters         ----------         objtype : list of strings             A list containing the object type(s) to be removed.  If no types             are specified, then all objects are removed.  To clear only objects             of a specific type, use *'network'*, *'geometry'*, *'phase'*,             *'physics'*, or *'algorithm'*.  It's also possible to use             abbreviations, like *'geom'*.
r"""         Creates a deep copy of the current project          A deep copy means that new, unique versions of all the objects are         created but with identical data and properties.          Parameters         ----------         name : string             The name to give to the new project.  If not supplied, a name             is automatically generated.          Returns         -------         A new Project object containing copies of all objects
r"""         Find the Phase associated with a given object.          Parameters         ----------         obj : OpenPNM Object             Can either be a Physics or Algorithm object          Returns         -------         An OpenPNM Phase object.          Raises         ------         If no Phase object can be found, then an Exception is raised.
r"""         Find the Geometry associated with a given Physics          Parameters         ----------         physics : OpenPNM Physics Object             Must be a Physics object          Returns         -------         An OpenPNM Geometry object          Raises         ------         If no Geometry object can be found, then an Exception is raised.
r"""         Find the Physics object(s) associated with a given Geometry, Phase,         or combination.          Parameters         ----------         geometry : OpenPNM Geometry Object             The Geometry object for which the Physics object(s) are sought          phase : OpenPNM Phase Object             The Phase object for which the Physics object(s) are sought          Returns         -------         A list containing the Physics object(s).  If only a ``geometry`` is         specified the the Physics for all Phases is returned.  If only a         ``phase`` is specified, then the Physics for all Geometries is         returned.  If both ``geometry`` and ``phase`` is specified then         the list only contains a single Physics.  If no Physics is found, the         the list will be empty.  See the Notes section for more information.          See Also         --------         grid          Notes         -----         The Project has an ``grid`` attribute that shows the association of         all objects.  If each Geometry represents a row and each Phase is a         column, then each row/col intersection represents a Physics. This         method finds the PHysics' at each intersection
r"""         Find the full domain object associated with a given object.         For geometry the network is found, for physics the phase is found and         for all other objects which are defined for for the full domain,         themselves are found.          Parameters         ----------         obj : OpenPNM Object             Can be any object          Returns         -------         An OpenPNM object
r"""         Remove an object from the Project.  This removes all references to         the object from all other objects (i.e. removes labels)          Parameters         ----------         obj : OpenPNM Object             The object to purge          deep : boolean             A flag that indicates whether to remove associated objects.             If ``True``, then removing a Geometry or Phase also removes             the associated Physics objects.  If ``False`` (default) then             only the given object is removed, along with its labels in all             associated objects.  Removing a Physics always keeps associated             Geometry and Phases since they might also be associated with other             Physics objects.          Raises         ------         An Exception is raised if the object is a Network.          Notes         -----         For a clearer picture of this logic, type ``print(project.grid)`` at         the console.  A deep purge of a Geometry is like removing a row, while         a Phase is like removing a column.
r"""         Saves the given object to a file          Parameters         ----------         obj : OpenPNM object             The file to be saved.  Depending on the object type, the file             extension will be one of 'net', 'geo', 'phase', 'phys' or 'alg'.
r"""         Loads a single object from a file          Parameters         ----------
r"""
r"""         Export the pore and throat data from the given object(s) into the         specified file and format.          Parameters         ----------         phases : list of OpenPNM Phase Objects             The data on each supplied phase will be added to file          filename : string             The file name to use.  If none is supplied then one will be             automatically generated based on the name of the project             containing the supplied Network, with the date and time appended.          filetype : string             Which file format to store the data.  If a valid extension is             included in the ``filename``, this is ignored.  Option are:              **'vtk'** : (default) The Visualization Toolkit format, used by             various softwares such as Paraview.  This actually produces a 'vtp'             file.  NOTE: This can be quite slow since all the data is written             to a simple text file.  For large data simulations consider             'xdmf'.              **'csv'** : The comma-separated values format, which is easily             openned in any spreadsheet program.  The column names represent             the property name, including the type and name of the object to             which they belonged, all separated by the pipe character.              **'xmf'** : The extensible data markup format, is a very efficient             format for large data sets.  This actually results in the creation             of two files, the *xmf* file and an associated *hdf* file.  The             *xmf* file contains instructions for looking into the *hdf* file             where the data is stored. Paraview opens the *xmf* format natively,             and is very fast.              **'mat'** : Matlab 'mat-file', which can be openned in Matlab.          Notes         -----         This is a helper function for the actual functions in the IO module.         For more control over the format of the output, and more information         about the format refer to ``openpnm.io``.
r"""         Dump data from all objects in project to an HDF5 file.  Note that         'pore.coords', 'throat.conns', 'pore.all', 'throat.all', and all         labels pertaining to the linking of objects are kept.          Parameters         ----------         mode : string or list of strings             The type of data to be dumped to the HDF5 file.  Options are:              **'props'** : Numerical data such as 'pore.diameter'.  The default             is only 'props'.              **'labels'** : Boolean data that are used as labels.  Since this             is boolean data it does not consume large amounts of memory and             probably does not need to be dumped.          See Also         --------         _fetch_data          Notes         -----         In principle, after data is fetched from an HDF5 file, it should         physically stay there until it's called upon.  This let users manage         the data as if it's in memory, even though it isn't.  This behavior         has not been confirmed yet, which is why these functions are hidden.
r"""         Retrieve data from an HDF5 file and place onto correct objects in the         project          See Also         --------         _dump_data          Notes         -----         In principle, after data is fetched from and HDF5 file, it should         physically stay there until it's called upon.  This let users manage         the data as if it's in memory, even though it isn't.  This behavior         has not been confirmed yet, which is why these functions are hidden.
r"""         Perform a check to find pores with overlapping or undefined Geometries          Returns         -------         A HealthDict
r"""         Perform a check to find pores which have overlapping or missing Physics          Parameters         ----------         phase : OpenPNM Phase object             The Phase whose Physics should be checked          Returns         -------         A HealthDict
r"""         Can be used to regenerate models across all objects in the project.          Parameters         ----------         objs : list of OpenPNM objects             Can be used to specify which specific objects to regenerate.  The             default is to regenerate all objects.  If a subset of objects is             given, this function ensure they are generated in a sensible order             such as any phases are done before any physics objects.          propnames : list of strings, or string             The specific model to regenerate.  If none are given then ALL             models on all objects are regenerated.  If a subset is given,             then only object that have a corresponding model are regenerated,             to avoid any problems.  This means that a single model can be             given, without specifying the objects.
r"""         Used to specify necessary arguments to the simulation.  This method is         useful for resetting the algorithm or applying more explicit control.          Parameters         ----------         phase : OpenPNM Phase object             The Phase object containing the physical properties of the invading             fluid.          quantity : string             The name of the quantity calculated by this algorithm.  This is             used for instance, by the late pore and throat filling models             to indicate the prevailing fluid pressure in the invading phase             for calculating the extent of filling.  The default is             'pressure'.  Note that there is no need to specify 'pore' and/or             'throat' with this as the given value will apply to both.          throat_entry_pressure : string             The dictionary key on the Phase object where the throat entry             pressure values are stored.  The default is             'throat.entry_pressure'.          pore_volume : string             The dictionary key containing the pore volume information. The             default is 'pore.volume'.          throat_volume : string             The dictionary key containing the throat volume information.  The             default is 'throat.volume'.          pore_partial_filling : string             The name of the model used to determine partial pore filling as             a function of applied pressure.          throat_partial_filling : string             The name of the model used to determine partial throat filling as             a function of applied pressure.
r"""         Define which pore filling model to apply.          Parameters         ----------         propname : string             Dictionary key on the physics object(s) containing the pore             filling model(s) to apply.          Notes         -----         It is assumed that these models are functions of the `quantity`         specified in the algorithms settings.  This values is applied to the         corresponding phase just prior to regenerating the given pore-scale         model(s).
r"""
r"""     Calculate conduit shape factors for hydraulic conductance, assuming     pores and throats are spheres (balls) and constant cross-section     cylinders (sticks).      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_area : string         Dictionary key of the pore area values      throat_area : string         Dictionary key of the throat area values      pore_diameter : string         Dictionary key of the pore diameter values      throat_diameter : string         Dictionary key of the throat diameter values      conduit_lengths : string         Dictionary key of the conduit lengths' values      Returns     -------     SF : dictionary         Dictionary containing conduit shape factors to be used in hagen-         poiseuille hydraulic conductance model. Shape factors are accessible         via the keys: 'pore1', 'pore2' and 'throat'.      Notes     -----     (1) This model accounts for the variable cross-section area in spheres.      (2) WARNING: This model could break if `conduit_lengths` does not     correspond to an actual ball and stick! Example: pore length is greater     than pore radius --> :(      References     ----------     Akbari, M., Sinton, D., & Bahrami, M. (2011). Viscous flow in variable     cross-section microchannels of arbitrary shapes. International Journal of     Heat and Mass Transfer, 54(17-18), 3970-3978.
r"""     Runs an arbitrary function on the given data      This allows users to place a customized calculation into the automatated     model regeneration pipeline.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      prop : string         The dictionary key containing the array to be operated on      func : Numpy function         A handle to the function to apply      kwargs : keyward arguments         All arguments required by the specific Numpy function      Examples     --------     The following example shows how to use a Numpy function, but any function     can be used, as long as it returns an array object:      >>> import openpnm as op     >>> import numpy as np     >>> pn = op.network.Cubic(shape=[5, 5, 5])     >>> geo = op.geometry.GenericGeometry(network=pn, pores=pn.Ps, throats=pn.Ts)     >>> geo['pore.rand'] = np.random.rand(geo.Np)     >>> geo.add_model(propname='pore.cos',     ...               model=op.models.misc.generic_function,     ...               func=np.cos,     ...               prop='pore.rand')
r"""     Calculates the product of multiple property values      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      prop1 : string         The name of the first argument      prop2 : string         The name of the second argument      Notes     -----     Additional properties can be specified beyond just ``prop1`` and ``prop2``     by including additional arguments in the function call (i.e. ``prop3 =     'pore.foo'``).
r"""     Create an array of random numbers of a specified size.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      seed : int         The starting seed value to send to Scipy's random number generator.         The default is None, which means different distribution is returned         each time the model is run.      num_range : list         A two element list indicating the low and high end of the returned         numbers.
r"""     Calculates a property as a linear function of a given property      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      m, b : floats         Slope and intercept of the linear corelation      prop : string         The dictionary key containing the independent variable or phase         property to be used in the correlation.
r"""     Calculates a property as a polynomial function of a given property      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      a : array_like         A list containing the polynomial coefficients, where element 0 in the         list corresponds to a0 and so on.  Note that no entries can be skipped         so 0 coefficients must be sent as 0.      prop : string         The dictionary key containing the independent variable or phase         property to be used in the polynomial.
r"""     Produces values from a Weibull distribution given a set of random numbers.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      seeds : string, optional         The dictionary key on the Geometry object containing random seed values         (between 0 and 1) to use in the statistical distribution.      shape : float         This controls the skewness of the distribution, with 'shape' < 1 giving         values clustered on the low end of the range with a long tail, and         'shape' > 1 giving a more symmetrical distribution.      scale : float         This controls the width of the distribution with most of values falling         below this number.      loc : float         Applies an offset to the distribution such that the smallest values are         above this number.      Examples     --------     The following code illustrates the inner workings of this function,     which uses the 'weibull_min' method of the scipy.stats module.  This can     be used to find suitable values of 'shape', 'scale'` and 'loc'.  Note that     'shape' is represented by 'c' in the actual function call.      >>> import scipy     >>> func = scipy.stats.weibull_min(c=1.5, scale=0.0001, loc=0)     >>> import matplotlib.pyplot as plt     >>> fig = plt.hist(func.ppf(q=scipy.rand(10000)), bins=50)
r"""     Produces values from a Weibull distribution given a set of random numbers.      Parameters     ----------     target : OpenPNM Object         The object with which this function as associated.  This argument         is required to (1) set number of values to generate (geom.Np or         geom.Nt) and (2) provide access to other necessary values         (i.e. geom['pore.seed']).      seeds : string, optional         The dictionary key on the Geometry object containing random seed values         (between 0 and 1) to use in the statistical distribution.      scale : float         The standard deviation of the Normal distribution      loc : float         The mean of the Normal distribution      Examples     --------     The following code illustrates the inner workings of this function,     which uses the 'norm' method of the scipy.stats module.  This can     be used to find suitable values of 'scale' and 'loc'.      >>> import scipy     >>> func = scipy.stats.norm(scale=.0001, loc=0.001)     >>> import matplotlib.pyplot as plt     >>> fig = plt.hist(func.ppf(q=scipy.rand(10000)), bins=50)
r"""     Accepts an 'rv_frozen' object from the Scipy.stats submodule and returns     values from the distribution for the given seeds      This uses the ``ppf`` method of the stats object      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      seeds : string, optional         The dictionary key on the Geometry object containing random seed values         (between 0 and 1) to use in the statistical distribution.      func : object         An 'rv_frozen' object from the Scipy.stats library with all of the         parameters pre-specified.      Examples     --------     The following code illustrates the process of obtaining a 'frozen' Scipy     stats object and adding it as a model:      >>> import scipy     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[3, 3, 3])     >>> geo = op.geometry.GenericGeometry(network=pn, pores=pn.Ps, throats=pn.Ts)     >>> geo.add_model(propname='pore.seed',     ...               model=op.models.geometry.pore_seed.random)      Now retrieve the stats distribution and add to ``geo`` as a model:      >>> stats_obj = scipy.stats.weibull_min(c=2, scale=.0001, loc=0)     >>> geo.add_model(propname='pore.size',     ...               model=op.models.geometry.pore_size.generic_distribution,     ...               seeds='pore.seed',     ...               func=stats_obj)       >>> import matplotlib.pyplot as plt     >>> fig = plt.hist(stats_obj.ppf(q=scipy.rand(1000)), bins=50)
r"""     Adopt a value from the values found in neighboring throats      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_prop : string         The dictionary key of the array containing the throat property to be         used in the calculation.  The default is 'throat.seed'.      mode : string         Controls how the pore property is calculated.  Options are 'min',         'max' and 'mean'.
r"""     Adopt a value based on the values in neighboring pores      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_prop : string         The dictionary key to the array containing the pore property to be         used in the calculation.  Default is 'pore.seed'.      mode : string         Controls how the throat property is calculated.  Options are 'min',         'max' and 'mean'.
r"""     Generates pore seeds that are spatailly correlated with their neighbors.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      weights : list of ints, optional         The [Nx,Ny,Nz] distances (in number of pores) in each direction that         should be correlated.      strel : array_like, optional (in place of weights)         The option allows full control over the spatial correlation pattern by         specifying the structuring element to be used in the convolution.          The array should be a 3D array containing the strength of correlations         in each direction.  Nonzero values indicate the strength, direction         and extent of correlations.  The following would achieve a basic         correlation in the z-direction:          strel = sp.array([[[0, 0, 0], [0, 0, 0], [0, 0, 0]], \                           [[0, 0, 0], [1, 1, 1], [0, 0, 0]], \                           [[0, 0, 0], [0, 0, 0], [0, 0, 0]]])      Notes     -----     This approach uses image convolution to replace each pore seed in the     geoemtry with a weighted average of those around it.  It then converts the     new seeds back to a random distribution by assuming they new seeds are     normally distributed.      Because is uses image analysis tools, it only works on Cubic networks.      This is the appproached used by Gostick et al [2]_ to create an anistropic     gas diffusion layer for fuel cell electrodes.      References     ----------     .. [2] J. Gostick et al, Pore network modeling of fibrous gas diffusion            layers for polymer electrolyte membrane fuel cells. J Power Sources            v173, pp277–290 (2007)      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[10, 10, 10])     >>> Ps, Ts = pn.Ps, pn.Ts     >>> geom = op.geometry.GenericGeometry(network=pn, pores=Ps, throats=Ts)     >>> mod = op.models.geometry.pore_seed.spatially_correlated     >>> geom.add_model(propname='pore.seed', model=mod, weights=[2, 2, 2])
r"""     Calculate throat cross-sectional area for a cylindrical throat      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_diameter : string         Dictionary key of the throat diameter values
r"""         Update the throat normals from the voronoi vertices
r'''         Function to calculate the centroid as the mean of a set of vertices.         Used for pore and throat.
r"""         Calculate an indiameter by distance transforming sections of the         fiber image. By definition the maximum value will be the largest radius         of an inscribed sphere inside the fibrous hull
r"""         Calculate the center to center distance from centroid of pore1 to         centroid of throat to centroid of pore2.
r"""         Use the Voronoi vertices and perform image analysis to obtain throat         properties
r"""         Tests whether points lie within a convex hull or not.         Computes a tesselation of the hull works out the normals of the facets.         Then tests whether dot(x.normals) < dot(a.normals) where a is the the         first vertex of the facets
r"""         Work out the voxels inside the convex hull of the voronoi vertices of         each pore
r'''         Function to count the number of voxels in the pore and fiber space         Which are assigned to each hull volume
r'''         A Bresenham line function to generate points to fill in for the fibers
r"""         Produce image by filling in voxels along throat edges using Bresenham         line then performing distance transform on fiber voxels to erode the         pore space
r"""         Plot an image of a slice through the fiber image         plane contains percentage values of the length of the image in each         axis          Parameters         ----------         plane : array_like         List of 3 values, [x,y,z], 2 must be zero and the other must be between         zero and one representing the fraction of the domain to slice along         the non-zero axis          index : array_like         similar to plane but instead of the fraction an index of the image is         used
r"""         Plot one slice from the fiber image          Parameters         ----------         plane : array_like         List of 3 values, [x,y,z], 2 must be zero and the other must be between         zero and one representing the fraction of the domain to slice along         the non-zero axis          index : array_like         similar to plane but instead of the fraction an index of the image is         used
r"""         Return a porosity profile in all orthogonal directions by summing         the voxel volumes in consectutive slices.
r"""         Plot a throat or list of throats in 2D showing key data          Parameters         ----------         throats : list or array containing throat indices tp include in figure          fig : matplotlib figure object to place plot in
r"""         Plot all throats around a given pore or list of pores in 3D          Parameters         ----------         pores : list or array containing pore indices tp include in figure          fig : matplotlib figure object to place plot in          axis_bounds : list of [xmin, xmax, ymin, ymax, zmin, zmax] values             to limit axes to          include_points : bool             Determines whether to scatter pore and throat centroids
r"""         Method to rotate a set of vertices (or coords) to align with an axis         points must be coplanar and normal must be given         Chops axis coord to give vertices back in 2D         Used to prepare verts for printing or calculating convex hull in order         to arrange them in hull order for calculations and printing
r"""         Return the domain extent based on the vertices          This function is better than using the pore coords as they may be far         away from the original domain size.  And will alter the effective         properties which should be based on the original domain sizes. Takes         one or two sets of pores and works out different geometric properties         if "length" is specified and two lists are given the planarity is         determined and the appropriate length (x,y,z) is returned.          Parameters         ----------         face1 : list or array containing pore indices for a face to include in             calculations.          parm : string             Determines what information is returned:                 volume, area (_xy, _xz, _yz), length (_x, _y, _z), minmax.             Default volume.
r'''         Helper Function to calculate the throat normal vectors
r"""     Calculates throat vector as straight path between connected pores.      Parameters     ----------     geometry : OpenPNM Geometry object         The object containing the geometrical properties of the throats      Notes     -----     There is an important impicit assumption here: the positive direction is     taken as the direction from the pore with the lower index to the higher.     This corresponds to the pores in the 1st and 2nd columns of the     'throat.conns' array as stored on the etwork.
r"""         This method takes several arguments that are essential to running the         algorithm and adds them to the settings.          Parameters         ----------         phase : OpenPNM Phase object             The phase on which the algorithm is to be run.          quantity : string             The name of the physical quantity to be calculated.          conductance : string             The name of the pore-scale transport conductance values.  These             are typically calculated by a model attached to a *Physics* object             associated with the given *Phase*.          solver : string             To use the default scipy solver, set this value to `spsolve` or             `umfpack`.  To use an iterative solver or a non-scipy solver,             additional arguments are required as described next.          solver_family : string             The solver package to use.  OpenPNM currently supports ``scipy``,             ``pyamg`` and ``petsc`` (if you have it installed).  The default is             ``scipy``.          solver_type : string             The specific solver to use.  For instance, if ``solver_family`` is             ``scipy`` then you can specify any of the iterative solvers such as             ``cg`` or ``gmres``.  [More info here]             (https://docs.scipy.org/doc/scipy/reference/sparse.linalg.html)          solver_preconditioner : string             This is used by the PETSc solver to specify which preconditioner             to use.  The default is ``jacobi``.          solver_atol : scalar             Used to control the accuracy to which the iterative solver aims.             The default is 1e-6.          solver_rtol : scalar             Used by PETSc as an additional tolerance control.  The default is             1e-6.          solver_maxiter : scalar             Limits the number of iterations to attempt before quiting when             aiming for the specified tolerance. The default is 5000.
r"""         Apply constant value boundary conditons to the specified pore         locations. These are sometimes referred to as Dirichlet conditions.          Parameters         ----------         pores : array_like             The pore indices where the condition should be applied          values : scalar or array_like             The value to of the boundary condition.  If a scalar is supplied             it is assigne to all locations, and if a vector is applied it             corresponds directy to the locations given in ``pores``.          Notes         -----         The definition of ``quantity`` is specified in the algorithm's         ``settings``, e.g. ``alg.settings['quentity'] = 'pore.pressure'``.
r"""         Apply constant rate boundary conditons to the specified pore         locations. This is similar to a Neumann boundary condition, but is         slightly different since it's the conductance multiplied by the         gradient, while Neumann conditions specify just the gradient.          Parameters         ----------         pores : array_like             The pore indices where the condition should be applied          values : scalar or array_like             The value to of the boundary condition.  If a scalar is supplied             it is assigne to all locations, and if a vector is applied it             corresponds directy to the locations given in ``pores``.          Notes         -----         The definition of ``quantity`` is specified in the algorithm's         ``settings``, e.g. ``alg.settings['quentity'] = 'pore.pressure'``.
r"""         Apply boundary conditions to specified pores          Parameters         ----------         pores : array_like             The pores where the boundary conditions should be applied          bctype : string             Specifies the type or the name of boundary condition to apply. The             types can be one one of the following:              - *'value'* : Specify the value of the quantity in each location             - *'rate'* : Specify the flow rate into each location          bcvalues : int or array_like             The boundary value to apply, such as concentration or rate.  If             a single value is given, it's assumed to apply to all locations.             Different values can be applied to all pores in the form of an             array of the same length as ``pores``.          mode : string, optional             Controls how the conditions are applied.  Options are:              *'merge'*: (Default) Adds supplied boundary conditions to already             existing conditions.              *'overwrite'*: Deletes all boundary condition on object then add             the given ones          Notes         -----         It is not possible to have multiple boundary conditions for a         specified location in one algorithm. Use ``remove_BCs`` to         clear existing BCs before applying new ones or ``mode='overwrite'``         which removes all existing BC's before applying the new ones.
r"""         Removes all boundary conditions from the specified pores          Parameters         ----------         pores : array_like             The pores from which boundary conditions are to be removed.  If no             pores are specified, then BCs are removed from all pores. No error             is thrown if the provided pores do not have any BCs assigned.
r"""         Builds the coefficient matrix based on conductances between pores.         The conductance to use is specified in the algorithm's ``settings``         under ``conductance``.  In subclasses (e.g. ``FickianDiffusion``)         this is set by default, though it can be overwritten.          Parameters         ----------         force : Boolean (default is ``False``)             If set to ``True`` then the A matrix is built from new.  If             ``False`` (the default), a cached version of A is returned.  The             cached version is *clean* in the sense that no boundary conditions             or sources terms have been added to it.
r"""         Builds the RHS matrix, without applying any boundary conditions or         source terms. This method is trivial an basically creates a column         vector of 0's.          Parameters         ----------         force : Boolean (default is ``False``)             If set to ``True`` then the b matrix is built from new.  If             ``False`` (the default), a cached version of b is returned.  The             cached version is *clean* in the sense that no boundary conditions             or sources terms have been added to it.
r"""         Applies all the boundary conditions that have been specified, by         adding values to the *A* and *b* matrices.
r"""         Sends the A and b matrices to the specified solver, and solves for *x*         given the boundary conditions, and source terms based on the present         value of *x*.  This method does NOT iterate to solve for non-linear         source terms or march time steps.          Parameters         ----------         A : sparse matrix             The coefficient matrix in sparse format. If not specified, then             it uses  the ``A`` matrix attached to the object.          b : ND-array             The RHS matrix in any format.  If not specified, then it uses             the ``b`` matrix attached to the object.          Notes         -----         The solver used here is specified in the ``settings`` attribute of the         algorithm.
r"""         Fetches the calculated quantity from the algorithm and returns it as         an array.          Parameters         ----------         times : scalar or list             Time steps to be returned. The default value is 'all' which results             in returning all time steps. If a scalar is given, only the             corresponding time step is returned. If a range is given             (e.g., 'range(0, 1, 1e-3)'), time steps in this range are returned.          t_precision : integer             The time precision (number of decimal places). Default value is 12.          Notes         -----         The keyword steps is interpreted in the same way as times.
r"""         Calculates the net rate of material moving into a given set of pores or         throats          Parameters         ----------         pores : array_like             The pores for which the rate should be calculated          throats : array_like             The throats through which the rate should be calculated          mode : string, optional             Controls how to return the rate.  Options are:              *'group'*: (default) Returns the cumulative rate of material             moving into the given set of pores              *'single'* : Calculates the rate for each pore individually          Returns         -------         If ``pores`` are specified, then the returned values indicate the         net rate of material exiting the pore or pores.  Thus a positive         rate indicates material is leaving the pores, and negative values         mean material is entering.          If ``throats`` are specified the rate is calculated in the direction of         the gradient, thus is always positive.          If ``mode`` is 'single' then the cumulative rate through the given         pores (or throats) are returned as a vector, if ``mode`` is 'group'         then the individual rates are summed and returned as a scalar.
r"""         Calculate the effective transport through the network          Parameters         ----------         inlets : array_like             The pores where the inlet boundary conditions were applied.  If             not given an attempt is made to infer them from the algorithm.          outlets : array_like             The pores where the outlet boundary conditions were applied.  If             not given an attempt is made to infer them from the algorithm.          domain_area : scalar             The area of the inlet and/or outlet face (which shold match)          domain_length : scalar             The length of the domain between the inlet and outlet faces          Returns         -------         The effective transport property through the network
r"""     Calculates viscosity of pure water or seawater at atmospheric pressure     using Eq. (22) given by Sharqawy et. al [1]. Values at temperature higher     than the normal boiling temperature are calculated at the saturation     pressure.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      temperature : string         The dictionary key containing the temperature values.  Temperature must         be in Kelvin for this emperical equation to work.  Can be either a pore         or throat array.      salinity : string         The dictionary key containing the salinity values.  Salinity must be         expressed in g of salt per kg of solution (ppt).  Can be either a         pore or throat array, but must be consistent with ``temperature``.      Returns     -------     mu_sw, the viscosity of water/seawater in [kg/m.s]      Notes     -----      T must be in K, and S in g of salt per kg of phase, or ppt (parts per         thousand)     VALIDITY: 273 < T < 453 K; 0 < S < 150 g/kg;     ACCURACY: 1.5 %      References     ----------     [1] Sharqawy M. H., Lienhard J. H., and Zubair, S. M., Desalination and         Water Treatment, 2010.
r"""     Uses exponential model by Reynolds [1] for the temperature dependance of     shear viscosity      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      u0, b : float, array_like             Coefficients of the viscosity exponential model (mu = u0*Exp(-b*T)             where T is the temperature in Kelvin      temperature : string         The dictionary key containing the temperature values (K).  Can be         either a pore or throat array.      [1] Reynolds O. (1886). Phil Trans Royal Soc London, v. 177, p.157.
r"""     Uses Chung et al. [1] model to estimate viscosity for gases at low     pressure (much less than the critical pressure) at conditions of interest.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      temperatre: string         The dictionary key containing the temperature values (K)      critical_temperature : string         The dictionary key containing the temperature values (K)      mol_weight: string         The dictionary key containing the molecular weight values (kg/mol)      critical_volume : string         The dictionary key containing the critical volume values (m3/kmol)      [1] Chung, T.H., Lee, L.L., and Starling, K.E., Applications of Kinetic Gas         Theories and Multiparameter Correlation for Prediction of Dilute Gas         Viscosity and Thermal Conductivity”, Ind. Eng. Chem. Fundam.23:8, 1984.
r"""         A subclassed version of the standard dict's clear method.  This can be         used to selectively clear certain data from the object, including         properties and/or labels.  Importantly, it does NOT clear items that         are required to maintain the integrity of the simulation.  These are         arrays that define the topology (ie. 'pore.all', 'pore.coords',         'throat.all', 'throat.conns'), as well as arrays that indicate         associations bewteen objects (ie. 'pore.geo_01').          Parameters         ----------         element : string or list of strings             Can be either 'pore' or 'throat', which specifies whether 'pore'             and/or 'throat' arrays should be cleared.  The default is both.          mode : string or list of strings             This controls what is cleared from the object.  Options are:              **'props'** : Removes all numerical property values from the object             dictionary              **'model_data'** : Removes only numerical data that were produced             by an associated model              **'labels'** : Removes all labels from the object dictionary,             except those relating to the pore and throat locations of             associated objects              **'all'** : Removes both 'props' and 'labels'          Notes         -----         If you wish to selectively remove some properties but not others, use         something like ``del object['pore.blah']`` at the Python prompt. This         can also be done in a for-loop to remove a list of items.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> len(pn.labels())  # There are 10 total labels on the network         12         >>> pn.clear(mode='labels')         >>> len(pn.labels())  # Kept only 'pore.all' and 'throat.all'         2         >>> geom = op.geometry.GenericGeometry(network=pn, pores=pn.Ps,         ...                                    throats=pn.Ts, name='geo1')         >>> len(pn.labels())  # 2 new labels were added for geometry locations         4         >>> pn.clear(mode='labels')         >>> 'pore.'+geom.name in pn.keys()  # The geometry labels were kept         True         >>> len(pn.props())  # The network has two properties         2         >>> pn.clear(element='pore', mode='props')         >>> 'pore.coords' in pn.keys()  # The pore property was removed         True         >>> pn.clear()  # Remove everything except protected labels and arrays         >>> print(sorted(list(pn.keys(element='pore', mode='all'))))         ['pore.all', 'pore.coords', 'pore.geo1']
r"""         This subclass works exactly like ``keys`` when no arguments are passed,         but optionally accepts an ``element`` and/or a ``mode``, which filters         the output to only the requested keys.          The default behavior is exactly equivalent to the normal ``keys``         method.          Parameters         ----------         element : string             Can be either 'pore' or 'throat', which limits the returned list of             keys to only 'pore' or 'throat' keys.  If neither is given, then             both are assumed.          mode : string (optional, default is 'skip')             Controls which keys are returned.  Options are:              **``None``** : This mode (default) bypasses this subclassed method             and just returns the normal KeysView object.              **'labels'** : Limits the returned list of keys to only 'labels'             (boolean arrays)              **'props'** : Limits he return list of keys to only 'props'             (numerical arrays).              **'all'** : Returns both 'labels' and 'props'.  This is equivalent             to sending a list of both 'labels' and 'props'.          See Also         --------         props         labels          Notes         -----         This subclass can be used to get dictionary keys of specific kinds of         data.  It's use augments ``props`` and ``labels`` by returning a list         containing both types, but possibly limited by element type ('pores'         or 'throats'.)          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic([5, 5, 5])         >>> pn.keys(mode='props')  # Get all props         ['pore.coords', 'throat.conns']         >>> pn.keys(mode='props', element='pore')  # Get only pore props         ['pore.coords']
r"""         This subclassed method can be used to obtain a dictionary containing         subset of data on the object          Parameters         ----------         keys : string or list of strings             The item or items to retrieve.          default : any object             The value to return in the event that the requested key(s) is not             found.  The default is ``None``.          Returns         -------         If a single string is given in ``keys``, this method behaves exactly         as the ``dict's`` native ``get`` method and returns just the item         requested (or the ``default`` if not found).  If, however, a list of         strings is received, then a dictionary containing each of the         requested items is returned.          Notes         -----         This is useful for creating Pandas Dataframes of a specific subset of         data.  Note that a Dataframe can be initialized with a ``dict``, but         all columns must be the same length.  (e.g. ``df = pd.Dataframe(d)``)          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> pore_props = pn.props(element='pore')         >>> subset = pn.get(keys=pore_props)         >>> print(len(subset))  # Only pore.coords, so gives dict with 1 array         1         >>> subset = pn.get(['pore.top', 'pore.bottom'])         >>> print(len(subset))  # Returns a dict with the 2 requested array         2          It behaves exactly as normal with a dict key string is supplied:          >>> array = pn.get('pore.coords')         >>> print(array.shape)  # Returns requested array         (125, 3)
r"""         Returns a list containing the names of all defined pore or throat         properties.          Parameters         ----------         element : string, optional             Can be either 'pore' or 'throat' to specify what properties are             returned.  If no element is given, both are returned          mode : string, optional             Controls what type of properties are returned.  Options are:              **'all'** : Returns all properties on the object (default)              **'models'** : Returns only properties that are associated with a             model              **'constants'** : returns data values that were *not* generated by             a model, but manaully created.          deep : boolean             If ``True`` this will also return the data on any associated             subdomain objects          Returns         -------         A an alphabetically sorted list containing the string name of all         pore or throat properties currently defined.  This list is an iterable,         so is useful for scanning through properties.          See Also         --------         labels         keys          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[3, 3, 3])         >>> pn.props('pore')         ['pore.coords']         >>> pn.props('throat')         ['throat.conns']         >>> pn.props()         ['pore.coords', 'throat.conns']
r"""         This is the actual label getter method, but it should not be called         directly.  Use ``labels`` instead.
r"""         Returns a list of labels present on the object          Additionally, this function can return labels applied to a specified         set of pores or throats          Parameters         ----------         element : string             Controls whether pore or throat labels are returned.  If empty then             both are returned (default).          pores (or throats) : array_like             The pores (or throats) whose labels are sought.  If left empty a             list containing all pore and throat labels is returned.          mode : string, optional             Controls how the query should be performed.  Only applicable             when ``pores`` or ``throats`` are specified:              **'or', 'union', 'any'**: (default) Returns the labels that are             assigned to *any* of the given locations.              **'and', 'intersection', 'all'**: Labels that are present on *all*             the given locations.              **'xor', 'exclusive_or'** : Labels that are present on *only one*             of the given locations.              **'nor', 'none', 'not'**: Labels that are *not* present on any of             the given locations.              **'nand'**: Labels that are present on *all but one* of the given             locations              **'xnor'**: Labels that are present on *more than one* of the given             locations.  'nxor' is also accepted.          Returns         -------         A list containing the labels on the object.  If ``pores`` or         ``throats`` are given, the results are filtered according to the         specified ``mode``.          See Also         --------         props         keys          Notes         -----         Technically, *'nand'* and *'xnor'* should also return pores with *none*         of the labels but these are not included.  This makes the returned list         more useful.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> pn.labels(pores=[11, 12])         ['pore.all', 'pore.front', 'pore.internal', 'pore.surface']
r"""         This is the actual method for getting indices, but should not be called         directly.  Use ``pores`` or ``throats`` instead.
r"""         Returns pore indicies where given labels exist, according to the logic         specified by the ``mode`` argument.          Parameters         ----------         labels : string or list of strings             The label(s) whose pores locations are requested.  This argument             also accepts '*' for wildcard searches.          mode : string             Specifies how the query should be performed.  The options are:              **'or', 'union', 'any'** : (default) Pores with *one or more* of             the given labels are returned.              **'and', 'intersection', 'all'** : Pores with *all* of the given             labels are returned.              **'xor', 'exclusive_or'** : Pores with *only one* of the given             labels are returned.              **'nor', 'none', 'not'** : Pores with *none* of the given labels             are returned.              **'nand'** : Pores with *not all* of the given labels are             returned.              **'xnor'** : Pores with *more than one* of the given labels are             returned.          asmask : boolean             If ``True`` then a boolean array of length Np is returned with             ``True`` values indicating the pores that satisfy the query.          Returns         -------         A Numpy array containing pore indices filtered by the logic specified         in ``mode``.          See Also         --------         throats          Notes         -----         Technically, *nand* and *xnor* should also return pores with *none* of         the labels but these are not included.  This makes the returned list         more useful.          To perform more complex or compound queries, you can opt to receive         the result a a boolean mask (``asmask=True``), then manipulate the         arrays manually.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> Ps = pn.pores(labels=['top', 'front'], mode='union')         >>> Ps[:5]  # Look at first 5 pore indices         array([0, 1, 2, 3, 4])         >>> pn.pores(labels=['top', 'front'], mode='xnor')         array([ 4,  9, 14, 19, 24])
r"""         Given a list of pore on a target object, finds indices of those pores         on the calling object          Parameters         ----------         pores : array_like             The indices of the pores on the object specifiedin ``origin``          origin : OpenPNM Base object             The object corresponding to the indices given in ``pores``          filtered : boolean (default is ``True``)             If ``True`` then a ND-array of indices is returned with missing             indices removed, otherwise a named-tuple containing both the             ``indices`` and a boolean ``mask`` with ``False`` indicating             which locations were not found.          Returns         -------         Pore indices on the calling object corresponding to the same pores         on the ``origin`` object.  Can be an array or a tuple containing an         array and a mask, depending on the value of ``filtered``.
r"""         Given a list of throats on a target object, finds indices of         those throats on the calling object          Parameters         ----------         throats : array_like             The indices of the throats on the object specified in ``origin``          origin : OpenPNM Base object             The object corresponding to the indices given in ``throats``          filtered : boolean (default is ``True``)             If ``True`` then a ND-array of indices is returned with missing             indices removed, otherwise a named-tuple containing both the             ``indices`` and a boolean ``mask`` with ``False`` indicating             which locations were not found.          Returns         -------         Throat indices on the calling object corresponding to the same throats         on the target object.  Can be an array or a tuple containing an array         and a mask, depending on the value of ``filtered``.
r"""         This is a generalized version of tomask that accepts a string of         'pore' or 'throat' for programmatic access.
r"""         Convert a list of pore or throat indices into a boolean mask of the         correct length          Parameters         ----------         pores or throats : array_like             List of pore or throat indices.  Only one of these can be specified             at a time, and the returned result will be of the corresponding             length.          Returns         -------         A boolean mask of length Np or Nt with True in the specified pore or         throat locations.          See Also         --------         toindices          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> mask = pn.tomask(pores=[0, 10, 20])         >>> sum(mask)  # 3 non-zero elements exist in the mask (0, 10 and 20)         3         >>> len(mask)  # Mask size is equal to the number of pores in network         125         >>> mask = pn.tomask(throats=[0, 10, 20])         >>> len(mask)  # Mask is now equal to number of throats in network         300
r"""         Convert a boolean mask to a list of pore or throat indices          Parameters         ----------         mask : array_like booleans             A boolean array with True at locations where indices are desired.             The appropriate indices are returned based an the length of mask,             which must be either Np or Nt long.          Returns         -------         A list of pore or throat indices corresponding the locations where         the received mask was True.          See Also         --------         tomask          Notes         -----         This behavior could just as easily be accomplished by using the mask         in ``pn.pores()[mask]`` or ``pn.throats()[mask]``.  This method is         just a convenience function and is a complement to ``tomask``.
r"""         Retrieves requested property from associated objects, to produce a full         Np or Nt length array.          Parameters         ----------         prop : string             The property name to be retrieved          Returns         -------         A full length (Np or Nt) array of requested property values.          Notes         -----         This makes an effort to maintain the data 'type' when possible; however         when data are missing this can be tricky.  Data can be missing in two         different ways: A set of pores is not assisgned to a geometry or the         network contains multiple geometries and data does not exist on all.         Float and boolean data is fine, but missing ints are converted to float         when nans are inserted.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[2, 2, 2])         >>> Ps = pn['pore.top']         >>> Ts = pn.find_neighbor_throats(pores=Ps)         >>> g1 = op.geometry.GenericGeometry(network=pn, pores=Ps, throats=Ts)         >>> Ts = ~pn.tomask(throats=Ts)         >>> g2 = op.geometry.GenericGeometry(network=pn, pores=~Ps, throats=Ts)         >>> g1['pore.value'] = 1         >>> print(g1['pore.value'])         [1 1 1 1]         >>> print(g2['pore.value'])  # 'pore.value' is defined on g1, not g2         [nan nan nan nan]         >>> print(pn['pore.value'])         [nan  1. nan  1. nan  1. nan  1.]         >>> g2['pore.value'] = 20         >>> print(pn['pore.value'])         [20  1 20  1 20  1 20  1]         >>> pn['pore.label'] = False         >>> print(g1['pore.label'])  # 'pore.label' is defined on pn, not g1         [False False False False]
r"""         Determines a pore (or throat) property as the average of it's         neighboring throats (or pores)          Parameters         ----------         propname: string             The dictionary key to the values to be interpolated.          Returns         -------         An array containing interpolated pore (or throat) data          Notes         -----         This uses an unweighted average, without attempting to account for         distances or sizes of pores and throats.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[3, 1, 1])         >>> pn['pore.value'] = [1, 2, 3]         >>> pn.interpolate_data('pore.value')         array([1.5, 2.5])
r"""         Returns which of the supplied pores (or throats) has the specified         label          Parameters         ----------         pores, or throats : array_like             List of pores or throats to be filtered          labels : list of strings             The labels to apply as a filter          mode : string              Controls how the filter is applied.  Options include:              **'or', 'union', 'any'**: (default) Returns a list of the given             locations where *any* of the given labels exist.              **'and', 'intersection', 'all'**: Only locations where *all* the             given labels are found.              **'xor', 'exclusive_or'**: Only locations where exactly *one* of             the given labels are found.              **'nor', 'none', 'not'**: Only locations where *none* of the given             labels are found.              **'nand'** : Only locations with *some but not all* of the given             labels are returned.              **'xnor'** : Only locations with *more than one* of the given             labels are returned.          Returns         -------         A list of pores (or throats) that have been filtered according the         given criteria.  The returned list is a subset of the received list of         pores (or throats).          See Also         --------         pores         throats          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> pn.filter_by_label(pores=[0, 1, 5, 6], labels='left')         array([0, 1])         >>> Ps = pn.pores(['top', 'bottom', 'front'], mode='or')         >>> pn.filter_by_label(pores=Ps, labels=['top', 'front'],         ...                    mode='and')         array([ 4,  9, 14, 19, 24])
r"""         Returns the number of pores of the specified labels          Parameters         ----------         labels : list of strings, optional             The pore labels that should be included in the count.             If not supplied, all pores are counted.          labels : list of strings             Label of pores to be returned          mode : string, optional             Specifies how the count should be performed.  The options are:              **'or', 'union', 'any'** : (default) Pores with *one or more* of             the given labels are counted.              **'and', 'intersection', 'all'** : Pores with *all* of the given             labels are counted.              **'xor', 'exclusive_or'** : Pores with *only one* of the given             labels are counted.              **'nor', 'none', 'not'** : Pores with *none* of the given labels             are counted.              **'nand'** : Pores with *some but not all* of the given labels are             counted.              **'xnor'** : Pores with *more than one* of the given labels are             counted.          Returns         -------         Np : int             Number of pores with the specified labels          See Also         --------         num_throats         count          Notes         -----         Technically, *'nand'* and *'xnor'* should also count pores with *none*         of the labels, however, to make the count more useful these are not         included.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> pn.num_pores()         125         >>> pn.num_pores(labels=['top'])         25         >>> pn.num_pores(labels=['top', 'front'], mode='or')         45         >>> pn.num_pores(labels=['top', 'front'], mode='xnor')         5
r"""         Return the number of throats of the specified labels          Parameters         ----------         labels : list of strings, optional             The throat labels that should be included in the count.             If not supplied, all throats are counted.          mode : string, optional             Specifies how the count should be performed.  The options are:              **'or', 'union', 'any'** : (default) Throats with *one or more* of             the given labels are counted.              **'and', 'intersection', 'all'** : Throats with *all* of the given             labels are counted.              **'xor', 'exclusive_or'** : Throats with *only one* of the given             labels are counted.              **'nor', 'none', 'not'** : Throats with *none* of the given labels             are counted.              **'nand'** : Throats with *some but not all* of the given labels             are counted.              **'xnor'** : Throats with *more than one* of the given labels are             counted.          Returns         -------         Nt : int             Number of throats with the specified labels          See Also         --------         num_pores         count          Notes         -----         Technically, *'nand'* and *'xnor'* should also count throats with         *none* of the labels, however, to make the count more useful these are         not included.
r"""         Returns a dictionary containing the number of pores and throats in         the network, stored under the keys 'pore' or 'throat'          Parameters         ----------         element : string, optional             Can be either 'pore' , 'pores', 'throat' or 'throats', which             specifies which count to return.          Returns         -------         A dictionary containing the number of pores and throats under the         'pore' and 'throat' key respectively.          See Also         --------         num_pores         num_throats          Notes         -----         The ability to send plurals is useful for some types of 'programmatic'         access.  For instance, the standard argument for locations is pores         or throats.  If these are bundled up in a **kwargs dict then you can         just use the dict key in count() without removing the 's'.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> pn._count('pore')         125         >>> pn._count('throat')         300
r"""         Show a quick plot of key property distributions.          Parameters         ----------         props : string or list of strings             The pore and/or throat properties to be plotted as histograms          bins : int or array_like             The number of bins to use when generating the histogram.  If an             array is given they are used as the bin spacing instead.          Notes         -----         Other keyword arguments are passed to the ``matplotlib.pyplot.hist``         function.
r"""         Check the health of pore and throat data arrays.          Parameters         ----------         element : string, optional             Can be either 'pore' or 'throat', which will limit the checks to             only those data arrays.          props : list of pore (or throat) properties, optional             If given, will limit the health checks to only the specfied             properties.  Also useful for checking existance.          Returns         -------         Returns a HealthDict object which a basic dictionary with an added         ``health`` attribute that is True is all entries in the dict are         deemed healthy (empty lists), or False otherwise.          Examples         --------         >>> import openpnm         >>> pn = openpnm.network.Cubic(shape=[5, 5, 5])         >>> h = pn.check_data_health()         >>> h.health         True
r"""         This private method accepts a list of pores or throats and returns a         properly structured Numpy array of indices.          Parameters         ----------         indices : multiple options             This argument can accept numerous different data types including             boolean masks, integers and arrays.          Returns         -------         A Numpy array of indices.          Notes         -----         This method should only be called by the method that is actually using         the locations, to avoid calling it multiple times.
r"""         This private method is used to parse the keyword \'element\' in many         of the above methods.          Parameters         ----------         element : string or list of strings             The element argument to check.  If is None is recieved, then a list             containing both \'pore\' and \'throat\' is returned.          single : boolean (default is False)             When set to True only a single element is allowed and it will also             return a string containing the element.          Returns         -------         When ``single`` is False (default) a list contain the element(s) is         returned.  When ``single`` is True a bare string containing the element         is returned.
r"""         This private method is used for converting \'labels\' to a proper         format, including dealing with wildcards (\*).          Parameters         ----------         labels : string or list of strings             The label or list of labels to be parsed. Note that the \* can be             used as a wildcard.          Returns         -------         A list of label strings, with all wildcard matches included if         applicable.
r"""         This private method is for checking the \'mode\' used in the calling         method.          Parameters         ----------         mode : string or list of strings             The mode(s) to be parsed          allowed : list of strings             A list containing the allowed modes.  This list is defined by the             calling method.  If any of the received modes are not in the             allowed list an exception is raised.          single : boolean (default is False)             Indicates if only a single mode is allowed.  If this argument is             True than a string is returned rather than a list of strings, which             makes it easier to work with in the caller method.          Returns         -------         A list containing the received modes as strings, checked to ensure they         are all within the allowed set (if provoided).  Also, if the ``single``         argument was True, then a string is returned.
r"""
r"""     Calculates internal surface area of pore bodies assuming they are spherical     then subtracts the area of the neighboring throats in a crude way, by     simply considering the throat cross-sectional area, thus not accounting     for the actual curvature of the intersection.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      pore_diameter : string         The dictionary key to the pore diameter array.      throat_area : string         The dictioanry key to the throat area array.  Throat areas are needed         since their insection with the pore are removed from the computation.
r"""     Calculates internal surface area of pore bodies assuming they are cubes     then subtracts the area of the neighboring throats.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      pore_diameter : string         The dictionary key to the pore diameter array.      throat_area : string         The dictioanry key to the throat area array.  Throat areas are needed         since their insection with the pore are removed from the computation.
r"""         Used to specify necessary arguments to the simulation.  This method is         useful for resetting the algorithm or applying more explicit control.          Parameters         ----------         phase : OpenPNM Phase object             The Phase object containing the physical properties of the invading             fluid.          pore_entry_pressure : string             The dictionary key on the Phase object where the pore entry             pressure values are stored.  The default is             'pore.entry_pressure'.          throat_entry_pressure : string             The dictionary key on the Phase object where the throat entry             pressure values are stored.  The default is             'throat.entry_pressure'.          snap_off : string             The dictionary key on the Phase object where the throat snap-off             pressure values are stored.          invade_isolated_Ts : boolean             If True, isolated throats are invaded at the higher invasion             pressure of their connected pores.          late_pore_filling : string             The name of the model used to determine late pore filling as             a function of applied pressure.          late_throat_filling : string             The name of the model used to determine late throat filling as             a function of applied pressure.          cooperative_pore_filling : string             The name of the model used to determine the meniscus properties             required for assessing cooperative pore filling.
r"""         Resets the various data arrays on the object back to their original         state. This is useful for repeating a simulation at different inlet         conditions, or invasion points for instance.
r"""          Parameters         ----------         pores : array_like             The list of inlet pores from which the Phase can enter the Network          clusters : list of lists - can be just one list but each list defines             a cluster of pores that share a common invasion pressure.          Like Basic Invasion Percolation a queue of
r"""         Set the locations through which defender exits the network.         This is only necessary if 'trapping' was set to True when ``setup``         was called.          Parameters         ----------         pores : array_like             Locations where the defender can exit the network.  Any defender             that does not have access to these sites will be trapped.          overwrite : boolean             If ``True`` then all existing outlet locations will be removed and             then the supplied locations will be added.  If ``False`` (default),             then supplied locations are added to any already existing outlet             locations.
Helper method to add throats to the cluster queue
Helper method to add pores to the cluster queue
r"""         Perform the algorithm          Parameters         ----------         max_pressure : float             The maximum pressure applied to the invading cluster. Any pores and             throats with entry pressure above this value will not be invaded.
r"""         Little helper function to merger clusters but only add the uninvaded         elements
r"""         Places the results of the IP simulation into the Phase object.          Parameters         ----------         Pc : float             Capillary Pressure at which phase configuration was reached
r"""         Convert the invaded sequence into an invaded time for a given flow rate         considering the volume of invaded pores and throats.          Parameters         ----------         flowrate : float             The flow rate of the injected fluid          Returns         -------         Creates a throat array called 'invasion_time' in the Algorithm         dictionary
r"""         Plot a simple drainage curve
r"""         Plot a simple drainage curve
r"""         Add all the throats to the queue with snap off pressure         This is probably wrong!!!! Each one needs to start a new cluster.
r"""         Method to start invasion in a network w. residual saturation.         Called after inlets are set.          Parameters         ----------         pores : array_like             The pores locations that are to be filled with invader at the             beginning of the simulation.          overwrite : boolean             If ``True`` then all existing inlet locations will be removed and             then the supplied locations will be added.  If ``False``, then             supplied locations are added to any already existing locations.          Notes         -----         Currently works for pores only and treats inner throats, i.e.         those that connect two pores in the cluster as invaded and outer ones         as uninvaded. Uninvaded throats are added to a new residual cluster         queue but do not start invading independently if not connected to an         inlet.          Step 1. Identify clusters in the phase occupancy.         Step 2. Look for clusters that are connected or contain an inlet         Step 3. For those that are merge into inlet cluster. May be connected         to more than one - run should sort this out         Step 4. For those that are isolated set the queue to not invading.         Step 5. (in run) When isolated cluster is met my invading cluster it         merges in and starts invading
r"""         Throats that are uninvaded connected to pores that are both invaded         should be invaded too.
r'''         Find whether 3 spheres intersect
r'''         Generate an array of pores with all connected throats and pairs of         throats that connect to the same pore
r'''         Take the pore center and throat center and work out which way         the throat normal is pointing relative to the vector between centers.         Offset the meniscus center along the throat vector in the correct         direction
r"""         Evaluate the cooperative pore filling condition that the combined         filling angle in next neighbor throats cannot exceed the geometric         angle between their throat planes.         This is used when the invading fluid has access to multiple throats         connected to a pore          Parameters         ----------         inv_points : array_like             The invasion pressures at which to assess coopertive pore filling.
r"""         Method run in loop after every pore invasion. All connecting throats         are now given access to the invading phase. Two throats with access to         the invading phase can cooperatively fill any pores that they are both         connected to, common pores.         The invasion of theses throats connected to the common pore is handled         elsewhere.
r"""     Calculate surface area for a cylindrical throat      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_diameter : string         Dictionary key to the throat diameter array.  Default is         'throat.diameter'.      throat_length : string         Dictionary key to the throat length array.  Default is 'throat.length'.
r"""     Calculate surface area for a cuboid throat      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_diameter : string         Dictionary key to the throat diameter array.  Default is         'throat.diameter'.      throat_length : string         Dictionary key to the throat length array.  Default is 'throat.length'.
r"""     Calculate surface area for an arbitrary shaped throat give the perimeter     and length.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_perimeter : string         Dictionary key to the throat perimeter array.  Default is         'throat.perimeter'.      throat_length : string         Dictionary key to the throat length array.  Default is 'throat.length'.
r"""     Calculate coordinates of throat endpoints, assuming throats don't overlap     with their adjacent pores. This model could be applied to conduits such as     cuboids or cylinders in series.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_diameter : string         Dictionary key of the pore diameter values      Returns     -------     EP : dictionary         Coordinates of throat endpoints stored in Dict form. Can be accessed         via the dict keys 'head' and 'tail'.      Notes     -----     This model is only accurate for cubic networks without diagonal     connections.
r"""     Calculate the coordinates of throat endpoints, assuming spherical pores.     This model accounts for the overlapping lens between pores and throats.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_diameter : string         Dictionary key of the pore diameter values.      throat_diameter : string         Dictionary key of the throat diameter values.      throat_centroid : string, optional         Dictionary key of the throat centroid values. See the notes.      Returns     -------     EP : dictionary         Coordinates of throat endpoints stored in Dict form. Can be accessed         via the dict keys 'head' and 'tail'.      Notes     -----     (1) This model should not be applied to true 2D networks. Use     `circular_pores` model instead.      (2) By default, this model assumes that throat centroid and pore     coordinates are colinear. If that's not the case, such as in extracted     networks, `throat_centroid` could be passed as an optional argument, and     the model takes care of the rest.
r"""     Calculate the coordinates of throat endpoints, assuming circular pores.     This model accounts for the overlapping lens between pores and throats.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_diameter : string         Dictionary key of the pore diameter values.      throat_diameter : string         Dictionary key of the throat diameter values.      throat_centroid : string, optional         Dictionary key of the throat centroid values. See the notes.      Returns     -------     EP : dictionary         Coordinates of throat endpoints stored in Dict form. Can be accessed         via the dict keys 'head' and 'tail'.      Notes     -----     (1) This model should only be applied to ture 2D networks.      (2) By default, this model assumes that throat centroid and pore     coordinates are colinear. If that's not the case, such as in extracted     networks, `throat_centroid` could be passed as an optional argument, and     the model takes care of the rest.
r"""     Calculate the coordinates of throat endpoints given a central coordinate,     unit vector along the throat direction and a length.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_centroid : string         Dictionary key of the throat center coordinates.      throat_vector : string         Dictionary key of the throat vector pointing along the length of the         throats.      throat_length : string         Dictionary key of the throat length.      Returns     -------     EP : dictionary         Coordinates of throat endpoints stored in Dict form. Can be accessed         via the dict keys 'head' and 'tail'.
r"""         Add pores to the faces of the network for use as boundary pores.          Pores are offset from the faces by 1/2 a lattice spacing such that         they lie directly on the boundaries.          Parameters         ----------         labels : string or list of strings             The labels indicating the pores defining each face where boundary             pores are to be added (e.g. 'left' or ['left', 'right'])          spacing : scalar or array_like             The spacing of the network (e.g. [1, 1, 1]).  This should be given             since it can be quite difficult to infer from the network, for             instance if boundary pores have already added to other faces.
r"""         Converts the values to a rectangular array with the same shape as the         network          Parameters         ----------         values : array_like             An Np-long array of values to convert to          Notes         -----         This method can break on networks that have had boundaries added.  It         will usually work IF the given values came only from 'internal'         pores.
r"""         Apply data to the network based on a rectangular array filled with         values.  Each array location corresponds to a pore in the network.          Parameters         ----------         array : array_like             The rectangular array containing the values to be added to the             network. This array must be the same shape as the original network.          propname : string             The name of the pore property being added.
r"""         Returns an adjacency matrix in the specified sparse format, with 1's         indicating the non-zero values.          Parameters         ----------         fmt : string, optional             The sparse storage format to return.  Options are:              **'coo'** : (default) This is the native format of OpenPNM data              **'lil'** : Enables row-wise slice of the matrix              **'csr'** : Favored by most linear algebra routines              **'dok'** : Enables subscript access of locations          Notes         -----         This method will only create the requested matrix in the specified         format if one is not already saved on the object.  If not present,         this method will create and return the matrix, as well as store it         for future use.          To obtain a matrix with weights other than ones at each non-zero         location use ``create_adjacency_matrix``.
r"""         Returns an incidence matrix in the specified sparse format, with 1's         indicating the non-zero values.          Parameters         ----------         fmt : string, optional             The sparse storage format to return.  Options are:              **'coo'** : (default) This is the native format of OpenPNM data              **'lil'** : Enables row-wise slice of the matrix              **'csr'** : Favored by most linear algebra routines              **'dok'** : Enables subscript access of locations          Notes         -----         This method will only create the requested matrix in the specified         format if one is not already saved on the object.  If not present,         this method will create and return the matrix, as well as store it         for future use.          To obtain a matrix with weights other than ones at each non-zero         location use ``create_incidence_matrix``.
r"""         Generates a weighted adjacency matrix in the desired sparse format          Parameters         ----------         weights : array_like, optional             An array containing the throat values to enter into the matrix             (in graph theory these are known as the 'weights').              If the array is Nt-long, it implies that the matrix is symmetric,             so the upper and lower triangular regions are mirror images.  If             it is 2*Nt-long then it is assumed that the first Nt elements are             for the upper triangle, and the last Nt element are for the lower             triangular.              If omitted, ones are used to create a standard adjacency matrix             representing connectivity only.          fmt : string, optional             The sparse storage format to return.  Options are:              **'coo'** : (default) This is the native format of OpenPNM data              **'lil'** : Enables row-wise slice of the matrix              **'csr'** : Favored by most linear algebra routines              **'dok'** : Enables subscript access of locations          triu : boolean (default is ``False``)             If ``True``, the returned sparse matrix only contains the upper-             triangular elements.  This argument is ignored if the ``weights``             array is 2*Nt-long.          drop_zeros : boolean (default is ``False``)             If ``True``, applies the ``eliminate_zeros`` method of the sparse             array to remove all zero locations.          Returns         -------         An adjacency matrix in the specified Scipy sparse format.          Notes         -----         The adjacency matrix is used by OpenPNM for finding the pores         connected to a give pore or set of pores.  Specifically, an adjacency         matrix has Np rows and Np columns.  Each row represents a pore,         containing non-zero values at the locations corresponding to the         indices of the pores connected to that pore.  The ``weights`` argument         indicates what value to place at each location, with the default         being 1's to simply indicate connections. Another useful option is         throat indices, such that the data values on each row indicate which         throats are connected to the pore.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> weights = sp.rand(pn.num_throats(), ) < 0.5         >>> am = pn.create_adjacency_matrix(weights=weights, fmt='csr')
r"""         Creates a weighted incidence matrix in the desired sparse format          Parameters         ----------         weights : array_like, optional             An array containing the throat values to enter into the matrix (In             graph theory these are known as the 'weights').  If omitted, ones             are used to create a standard incidence matrix representing             connectivity only.          fmt : string, optional             The sparse storage format to return.  Options are:              **'coo'** : (default) This is the native format of OpenPNMs data              **'lil'** : Enables row-wise slice of the matrix              **'csr'** : Favored by most linear algebra routines              **'dok'** : Enables subscript access of locations          drop_zeros : boolean (default is ``False``)             If ``True``, applies the ``eliminate_zeros`` method of the sparse             array to remove all zero locations.          Returns         -------         An incidence matrix in the specified sparse format          Notes         -----         The incidence matrix is a cousin to the adjacency matrix, and used by         OpenPNM for finding the throats connected to a give pore or set of         pores.  Specifically, an incidence matrix has Np rows and Nt columns,         and each row represents a pore, containing non-zero values at the         locations corresponding to the indices of the throats connected to that         pore.  The ``weights`` argument indicates what value to place at each         location, with the default being 1's to simply indicate connections.         Another useful option is throat indices, such that the data values         on each row indicate which throats are connected to the pore, though         this is redundant as it is identical to the locations of non-zeros.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> weights = sp.rand(pn.num_throats(), ) < 0.5         >>> im = pn.create_incidence_matrix(weights=weights, fmt='csr')
r"""         Return a list of pores connected to the given list of throats          Parameters         ----------         throats : array_like             List of throats numbers          flatten : boolean, optional             If ``True`` (default) a 1D array of unique pore numbers is             returned. If ``False`` each location in the the returned array             contains a sub-arras of neighboring pores for each input throat,             in the order they were sent.          mode : string             Specifies logic to filter the resulting list.  Options are:              **'or'** : (default) All neighbors of the input throats.  This is             also known as the 'union' in set theory or 'any' in boolean logic.             Both keywords are accepted and treated as 'or'.              **'xor'** : Only neighbors of one and only one input throat.  This             is useful for finding the sites that are not shared by any of the             input throats.              **'xnor'** : Neighbors that are shared by two or more input             throats. This is equivalent to finding all neighbors with 'or',             minus those found with 'xor', and is useful for finding neighbors             that the inputs have in common.              **'and'** : Only neighbors shared by all input throats.  This is             also known as 'intersection' in set theory and (somtimes) as 'all'             in boolean logic.  Both keywords are accepted and treated as 'and'.          Returns         -------         1D array (if ``flatten`` is ``True``) or ndarray of arrays (if         ``flatten`` is ``False``)          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> Ps = pn.find_connected_pores(throats=[0, 1])         >>> print(Ps)         [[0 1]          [1 2]]         >>> Ps = pn.find_connected_pores(throats=[0, 1], flatten=True)         >>> print(Ps)         [0 1 2]
r"""         Return the throat index connecting pairs of pores          Parameters         ----------         P1 , P2 : array_like             The indices of the pores whose throats are sought.  These can be             vectors of indices, but must be the same length          Returns         -------         Returns a list the same length as P1 (and P2) with the each element         containing the throat index that connects the corresponding pores,         or `None`` if pores are not connected.          Notes         -----         The returned list can be converted to an ND-array, which will convert         the ``None`` values to ``nan``.  These can then be found using         ``scipy.isnan``.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> Ts = pn.find_connecting_throat([0, 1, 2], [2, 2, 2])         >>> print(Ts)         [None, 1, None]
r"""         Returns a list of pores that are direct neighbors to the given pore(s)          Parameters         ----------         pores : array_like             Indices of the pores whose neighbors are sought          flatten : boolean             If ``True`` (default) the returned result is a compressed array of             all neighbors.  If ``False``, a list of lists with each sub-list             containing the neighbors for each input site.  Note that an             *unflattened* list might be slow to generate since it is a Python             ``list`` rather than a Numpy ``array``.          include_input : bool             If ``False`` (default) then the input pores are not included in             the returned list(s). Note that since pores are not neighbors of             themselves, the neighbors of pore N will not include N, even if             this flag is ``True``.          mode : string             Specifies logic to filter the resulting list.  Options are:              **'or'** : (default) All neighbors of the input pores.  This is             also known as the 'union' in set theory or 'any' in boolean logic.             Both keywords are accepted and treated as 'or'.              **'xor'** : Only neighbors of one and only one input pore.  This             is useful for finding the pores that are not shared by any of the             input pores.  This is known as 'exclusive_or' in set theory, and             is an accepted input.              **'xnor'** : Neighbors that are shared by two or more input pores.             This is equivalent to finding all neighbors with 'or', minus those             found with 'xor', and is useful for finding neighbors that the             inputs have in common.              **'and'** : Only neighbors shared by all input pores.  This is also             known as 'intersection' in set theory and (somtimes) as 'all' in             boolean logic.  Both keywords are accepted and treated as 'and'.          Returns         -------         If ``flatten`` is ``True``, returns a 1D array of pore indices filtered         according to the specified mode.  If ``flatten`` is ``False``, returns         a list of lists, where each list contains the neighbors of the         corresponding input pores.          Notes         -----         The ``logic`` options are applied to neighboring pores only, thus it         is not possible to obtain pores that are part of the global set but         not neighbors. This is because (a) the list of global pores might be         very large, and (b) it is not possible to return a list of neighbors         for each input pores if global pores are considered.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> Ps = pn.find_neighbor_pores(pores=[0, 2])         >>> print(Ps)         [ 1  3  5  7 25 27]         >>> Ps = pn.find_neighbor_pores(pores=[0, 1])         >>> print(Ps)         [ 2  5  6 25 26]         >>> Ps = pn.find_neighbor_pores(pores=[0, 1], mode='union',         ...                             include_input=True)         >>> print(Ps)         [ 0  1  2  5  6 25 26]         >>> Ps = pn.find_neighbor_pores(pores=[0, 2], flatten=False)         >>> print(Ps)         [array([ 1,  5, 25]), array([ 1,  3,  7, 27])]         >>> Ps = pn.find_neighbor_pores(pores=[0, 2], mode='xnor')         >>> print(Ps)         [1]         >>> Ps = pn.find_neighbor_pores(pores=[0, 2], mode='xor')         >>> print(Ps)         [ 3  5  7 25 27]
r"""         Returns a list of throats neighboring the given pore(s)          Parameters         ----------         pores : array_like             Indices of pores whose neighbors are sought          flatten : boolean, optional             If ``True`` (default) a 1D array of unique throat indices is             returned. If ``False`` the returned array contains arrays of             neighboring throat indices for each input pore, in the order             they were sent.          mode : string             Specifies logic to filter the resulting list.  Options are:              **'or'** : (default) All neighbors of the input pores.  This is             also known as the 'union' in set theory or 'any' in boolean logic.             Both keywords are accepted and treated as 'or'.              **'xor'** : Only neighbors of one and only one input pore.  This             is useful for finding the thraots that are not shared by any of the             input pores.              **'xnor'** : Neighbors that are shared by two or more input pores.             This is equivalent to finding all neighbors with 'or', minus those             found with 'xor', and is useful for finding neighbors that the             inputs have in common.              **'and'** : Only neighbors shared by all input pores.  This is also             known as 'intersection' in set theory and (somtimes) as 'all' in             boolean logic.  Both keywords are accepted and treated as 'and'.          Returns         -------         If ``flatten`` is ``True``, returns a 1D array of throat indices         filtered according to the specified mode.  If ``flatten`` is ``False``,         returns a list of lists, where each list contains the neighbors of the         corresponding input pores.          Notes         -----         The ``logic`` options are applied to neighboring bonds only, thus it         is not possible to obtain bonds that are part of the global set but         not neighbors. This is because (a) the list of global bonds might be         very large, and (b) it is not possible to return a list of neighbors         for each input site if global sites are considered.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> Ts = pn.find_neighbor_throats(pores=[0, 1])         >>> print(Ts)         [  0   1 100 101 200 201]         >>> Ts = pn.find_neighbor_throats(pores=[0, 1], flatten=False)         >>> print(Ts)         [array([  0, 100, 200]), array([  0,   1, 101, 201])]
r"""         Returns the number of neigbhoring pores for each given input pore          Parameters         ----------         pores : array_like             Pores whose neighbors are to be counted          flatten : boolean (optional)             If ``False`` (default) the number of pores neighboring each input             pore as an array the same length as ``pores``.  If ``True`` the             sum total number of is counted.          mode : string             The logic to apply to the returned count of pores.              **'or'** : (default) All neighbors of the input pores.  This is             also known as the 'union' in set theory or 'any' in boolean logic.             Both keywords are accepted and treated as 'or'.              **'xor'** : Only neighbors of one and only one input pore.  This             is useful for counting the pores that are not shared by any of the             input pores.  This is known as 'exclusive_or' in set theory, and             is an accepted input.              **'xnor'** : Neighbors that are shared by two or more input pores.             This is equivalent to counting all neighbors with 'or', minus those             found with 'xor', and is useful for finding neighbors that the             inputs have in common.              **'and'** : Only neighbors shared by all input pores.  This is also             known as 'intersection' in set theory and (somtimes) as 'all' in             boolean logic.  Both keywords are accepted and treated as 'and'.          Returns         -------         If ``flatten`` is False, a 1D array with number of neighbors in each         element, otherwise a scalar value of the number of neighbors.          Notes         -----         This method literally just counts the number of elements in the array         returned by ``find_neighbor_pores`` using the same logic.  Explore         those methods if uncertain about the meaning of the ``mode`` argument         here.          See Also         --------         find_neighbor_pores         find_neighbor_throats          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[5, 5, 5])         >>> Np = pn.num_neighbors(pores=[0, 1], flatten=False)         >>> print(Np)         [3 4]         >>> Np = pn.num_neighbors(pores=[0, 2], flatten=True)         >>> print(Np)         6         >>> Np = pn.num_neighbors(pores=[0, 2], mode='and', flatten=True)         >>> print(Np)         1
r"""         Find all pores within a given radial distance of the input pore(s)         regardless of whether or not they are toplogically connected.          Parameters         ----------         pores : array_like             The list of pores for whom nearby neighbors are to be found          r : scalar             The maximum radius within which the search should be performed          include_input : bool             Controls whether the input pores should be included in the returned             list.  The default is ``False``.          flatten : bool             If true returns a single list of all pores that match the criteria,             otherwise returns an array containing a sub-array for each input             pore, where each sub-array contains the pores that are nearby to             each given input pore.  The default is False.          Returns         -------             A list of pores which are within the given spatial distance.  If a             list of N pores is supplied, then a an N-long list of such lists is             returned.  The returned lists each contain the pore for which the             neighbors were sought.          Examples         --------         >>> import openpnm as op         >>> pn = op.network.Cubic(shape=[3, 3, 3])         >>> Ps = pn.find_nearby_pores(pores=[0, 1], r=1)         >>> print(Ps)         [array([3, 9]), array([ 2,  4, 10])]         >>> Ps = pn.find_nearby_pores(pores=[0, 1], r=0.5)         >>> print(Ps)         [array([], dtype=int64), array([], dtype=int64)]         >>> Ps = pn.find_nearby_pores(pores=[0, 1], r=1, flatten=True)         >>> print(Ps)         [ 2  3  4  9 10]
r"""         This method check the network topological health by checking for:              (1) Isolated pores             (2) Islands or isolated clusters of pores             (3) Duplicate throats             (4) Bidirectional throats (ie. symmetrical adjacency matrix)             (5) Headless throats          Returns         -------         A dictionary containing the offending pores or throat numbers under         each named key.          It also returns a list of which pores and throats should be trimmed         from the network to restore health.  This list is a suggestion only,         and is based on keeping the largest cluster and trimming the others.          Notes         -----         - Does not yet check for duplicate pores         - Does not yet suggest which throats to remove         - This is just a 'check' and does not 'fix' the problems it finds
r"""     Uses Fuller model to estimate diffusion coefficient for gases from first     principles at conditions of interest      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      MA : float, array_like         Molecular weight of component A [kg/mol]      MB : float, array_like         Molecular weight of component B [kg/mol]      vA:  float, array_like         Sum of atomic diffusion volumes for component A      vB:  float, array_like         Sum of atomic diffusion volumes for component B      pressure : string         The dictionary key containing the pressure values in Pascals (Pa)      temperature : string         The dictionary key containing the temperature values in Kelvin (K)
r"""     Uses Fuller model to adjust a diffusion coefficient for gases from     reference conditions to conditions of interest      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      DABo : float, array_like         Diffusion coefficient at reference conditions      Po, To : float, array_like         Pressure & temperature at reference conditions, respectively      pressure : string         The dictionary key containing the pressure values in Pascals (Pa)      temperature : string         The dictionary key containing the temperature values in Kelvin (K)
r"""     Uses Tyn_Calus model to estimate diffusion coefficient in a dilute liquid     solution of A in B from first principles at conditions of interest      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      VA : float, array_like         Molar volume of component A at boiling temperature (m3/mol)      VB : float, array_like         Molar volume of component B at boiling temperature (m3/mol)      sigmaA:  float, array_like         Surface tension of component A at boiling temperature (N/m)      sigmaB:  float, array_like         Surface tension of component B at boiling temperature (N/m)      pressure : string         The dictionary key containing the pressure values in Pascals (Pa)      temperature : string         The dictionary key containing the temperature values in Kelvin (K)
r"""     Uses Tyn_Calus model to adjust a diffusion coeffciient for liquids from     reference conditions to conditions of interest      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      DABo : float, array_like         Diffusion coefficient at reference conditions      mu_o, To : float, array_like         Viscosity & temperature at reference conditions, respectively      pressure : string         The dictionary key containing the pressure values in Pascals (Pa)      temperature : string         The dictionary key containing the temperature values in Kelvin (K)
r"""     Calculates surface tension of pure water or seawater at atmospheric     pressure using Eq. (28) given by Sharqawy et al. Values at     temperature higher than the normal boiling temperature are calculated at     the saturation pressure.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      temperature : string         The dictionary key containing the temperature values.  Temperature must         be in Kelvin for this emperical equation to work      salinity : string         The dictionary key containing the salinity values.  Salinity must be         expressed in g of salt per kg of solution (ppt).      Returns     -------     sigma_sw, the surface tension of seawater in [N/m]      Notes     -----      T must be in K, and S in g of salt per kg of phase, or ppt (parts per         thousand)     VALIDITY: 273 < T < 313 K; 0 < S < 40 g/kg;     ACCURACY: 0.2 %      References     ----------     Sharqawy M. H., Lienhard J. H., and Zubair, S. M., Desalination and     Water Treatment, 2010.
r"""     Missing description      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      k : float         Constant parameter specific to fluid      temperature : string         The dictionary key containing the temperature values (K)      critical_temperature : string         The dictionary key containing the critical temperature values (K)      molar_density : string         The dictionary key containing the molar density values (K)       TODO: Needs description, and improve definition of k
r"""     Missing description      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      K2 : scalar         Fluid specific constant      n : scalar         Fluid specific constant      temperature : string         The dictionary key containing the temperature values (K)      critical_temperature : string         The dictionary key containing the critical temperature values (K)      critical_pressure : string         The dictionary key containing the critical pressure values (K)
r"""     Uses Brock_Bird model to adjust surface tension from it's value at a given     reference temperature to temperature of interest      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      To : float         Reference temperature (K)      sigma_o : float         Surface tension at reference temperature (N/m)      temperature : string         The dictionary key containing the temperature values (K)      critical_temperature : string         The dictionary key containing the critical temperature values (K)
r"""     Calculates thermal conductivity of pure water or seawater at atmospheric     pressure using the correlation given by Jamieson and Tudhope. Values at     temperature higher  the normal boiling temperature are calculated at the     saturation pressure.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      temperature : string         The dictionary key containing the temperature values.  Temperature must         be in Kelvin for this emperical equation to work      salinity : string         The dictionary key containing the salinity values.  Salinity must be         expressed in g of salt per kg of solution (ppt).      Returns     -------     The thermal conductivity of water/seawater in [W/m.K]      Notes     -----     T must be in K, and S in g of salt per kg of phase, or ppt (parts per     thousand)      VALIDITY: 273 < T < 453 K; 0 < S < 160 g/kg;     ACCURACY: 3 %      References     ----------     D. T. Jamieson, and J. S. Tudhope, Desalination, 8, 393-401, 1970.
r"""     Uses Chung et al. model to estimate thermal conductivity for gases with     low pressure(<10 bar) from first principles at conditions of interest      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      acentric_factor : string         Dictionary key containing the acentric factor of the component      Cv : string         Dictionary key containing the heat capacity at constant volume         (J/(mol.K))      mol_weight : string         Dictionary key containing the molecular weight of the component         (kg/mol)      viscosity : string         The dictionary key containing the viscosity values (Pa.s)      temperature : string         The dictionary key containing the temperature values (K)      critical_temperatre: string         The dictionary key containing the critical temperature values (K)
r"""     Uses Sato et al. model to estimate thermal conductivity for pure liquids     from first principles at conditions of interest      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      boiling_temperature :  string         Dictionary key containing the toiling temperature of the component (K)      mol_weight : string         Dictionary key containing the molecular weight of the component         (kg/mol)      temperature : string         The dictionary key containing the temperature values (K)      critical_temperature : string         The dictionary key containing the critical temperature values (K)
r"""     Finds the maximum diameter pore that can be placed in each location without     overlapping any neighbors.      This method iteratively expands pores by increasing their diameter to     encompass half of the distance to the nearest neighbor.  If the neighbor     is not growing because it's already touching a different neighbor, then     the given pore will never quite touch this neighbor.  Increating the value     of ``iters`` will get it closer, but it's case of     [Zeno's paradox](https://en.wikipedia.org/wiki/Zeno%27s_paradoxes) with     each step cutting the remaining distance in half      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      fixed_diameter : string         The dictionary key containing the pore diameter values already         assigned to network, if any.  If not provided a starting value is         assumed as half-way to the nearest neighbor.      iters : integer         The number of iterations to perform when searching for maximum         diameter.  This function iteratively grows pores until they touch         their nearest neighbor, which is also growing, so this parameter limits         the maximum number of iterations.  The default is 10, but 5 is usally         enough.      Notes     -----     This model looks into all pores in the network when finding the diameter.     This means that when multiple Geometry objects are defined, it will     consider the diameter of pores on adjacent Geometries. If no diameters     have been assigned to these neighboring pores it will assume 0.  If     diameter value are assigned to the neighboring pores AFTER this model is     run, the pores will overlap.  This can be remedied by running this model     again.
r"""     Calculates the diameter of a sphere or edge-length of a cube with same     volume as the pore.      Parameters     ----------     target : OpenPNM Geometry Object         The Geometry object which this model is associated with. This controls         the length of the calculated array, and also provides access to other         necessary geometric properties.      pore_volume : string         The dictionary key containing the pore volume values      pore_shape : string         The shape of the pore body to assume when back-calculating from         volume.  Options are 'sphere' (default) or 'cube'.
r"""         Save network and phase data to a single vtp file for visualizing in         Paraview          Parameters         ----------         network : OpenPNM Network Object             The Network containing the data to be written          phases : list, optional             A list containing OpenPNM Phase object(s) containing data to be             written          filename : string, optional             Filename to write data.  If no name is given the file is named             after the network          delim : string             Specify which character is used to delimit the data names.  The             default is ' | ' which creates a nice clean output in the Paraview             pipeline viewer (e.g. net | property | pore | diameter)          fill_nans : scalar             The value to use to replace NaNs with.  The VTK file format does             not work with NaNs, so they must be dealt with.  The default is             `None` which means property arrays with NaNs are not written to the             file.  Other useful options might be 0 or -1, but the user must             be aware that these are not real values, only place holders.
r"""         Read in pore and throat data from a saved VTK file.          Parameters         ----------         filename : string (optional)             The name of the file containing the data to import.  The formatting             of this file is outlined below.          project : OpenPNM Project object             A GenericNetwork is created and added to the specified Project.             If no Project is supplied then one will be created and returned.
r"""     Calculates the molar density from the molecular weight and mass density      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      pore_MW : string         The dictionary key containing the molecular weight in kg/mol      pore_temperature : string         The dictionary key containing the density in kg/m3
r"""     Uses ideal gas law to calculate the molar density of an ideal gas      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      temperature : string         The dictionary key containing the density in kg/m3      pressure : string         The dictionary key containing the pressure values in Pascals (Pa)      Returns     -------     rho, the density in [mol/m3]      Notes     -----     This method uses the SI value for the ideal gas constant, hence the need to     provide the temperature and pressure in SI.  In general, OpenPNM use SI     throughout for consistency.
r"""     Uses Van der Waals equation of state to calculate the density of a real gas      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      pressure : string         The dictionary key containing the pressure values in Pascals (Pa)      temperature : string         The dictionary key containing the temperature values in Kelvin (K)      critical_pressure : string         The dictionary key containing the critical pressure values in Pascals         (Pa)      critical_temperature : string         The dictionary key containing the critical temperature values in Kelvin         (K)      Returns     -------     rho, the density in [mol/m3]
r"""         Save all the pore and throat property data on the Network (and         optionally on any Phases objects) to CSV files.          Parameters         ----------         network : OpenPNM Network             The Network containing the data to be stored          phases : list of OpenPNM Phases (optional)             The Phases whose data should be stored.          filename : string or path object             The name of the file to store the data          Notes         -----         The data from all Geometry objects is added to the file automatically.
r"""         Opens a 'csv' file, reads in the data, and adds it to the **Network**          Parameters         ----------         filename : string (optional)             The name of the file containing the data to import.  The formatting             of this file is outlined below.          project : OpenPNM Project object             A GenericNetwork is created and added to the specified Project.             If no Project object is supplied then one will be created and             returned.
r'''
r"""         Finds the indicies of the Voronoi nodes that define the facet or         ridge between the Delaunay nodes connected by the given throat.          Parameters         ----------         throats : array_like             The throats whose facets are sought.  The given throats should be             from the 'delaunay' network. If no throats are specified, all             'delaunay' throats are assumed.          Notes         -----         The method is not well optimized as it scans through each given throat         inside a for-loop, so it could be slow for large networks.
r"""         Finds the indices of the Voronoi nodes that define the convex hull         around the given Delaunay nodes.          Parameters         ----------         pores : array_like             The pores whose convex hull are sought.  The given pores should be             from the 'delaunay' network.  If no pores are given, then the hull             is found for all 'delaunay' pores.          Notes         -----         This metod is not fully optimized as it scans through each pore in a         for-loop, so could be slow for large networks.
r'''         Label the pores sitting on the faces of the domain in accordance with         the conventions used for cubic etc.
r"""         Add boundary pores to the specified faces of the network          Pores are offset from the faces of the domain.          Parameters         ----------         labels : string or list of strings             The labels indicating the pores defining each face where boundary             pores are to be added (e.g. 'left' or ['left', 'right'])          offset : scalar or array_like             The spacing of the network (e.g. [1, 1, 1]).  This must be given             since it can be quite difficult to infer from the network,             for instance if boundary pores have already added to other faces.
r"""         Write Network to a Mat file for exporting to Matlab.          Parameters         ----------         network : OpenPNM Network Object          filename : string             Desired file name, defaults to network name if not given          phases : list of phase objects ([])             Phases that have properties we want to write to file
r"""         Loads data onto the given network from an appropriately formatted         'mat' file (i.e. MatLAB output).          Parameters         ----------         filename : string (optional)             The name of the file containing the data to import.  The formatting             of this file is outlined below.          project : OpenPNM Project object             A GenericNetwork is created and added to the specified Project.             If no Project object is supplied then one will be created and             returned.          Returns         -------         If no project object is supplied then one will be created and returned.
r"""         Adds all objects in the given project to the console as variables         with handle names taken from each object's name.
r"""         Saves all the current Projects to a 'pnm' file          Parameters         ----------         filename : string, optional             If no filename is given, a name is genrated using the current             time and date. See Notes for more information on valid file names.          See Also         --------         save_project          Notes         -----         The filename can be a string such as 'saved_file.pnm'.  The string can         include absolute path such as 'C:\networks\saved_file.pnm', or can         be a relative path such as '..\..\saved_file.pnm', which will look         2 directories above the current working directory.  It can also be a         path object object such as that produced by ``pathlib`` or         ``os.path`` in the Python standard library.
r"""         Saves given Project to a 'pnm' file          This will include all of associated objects, including algorithms.          Parameters         ----------         project : OpenPNM Project             The project to save.          filename : string, optional             If no filename is given, the given project name is used. See Notes             for more information.          See Also         --------         save_workspace          Notes         -----         The filename can be a string such as 'saved_file.pnm'.  The string can         include absolute path such as 'C:\networks\saved_file.pnm', or can         be a relative path such as '..\..\saved_file.pnm', which will look         2 directories above the current working directory.  Can also be a         path object object such as that produced by ``pathlib`` or         ``os.path`` in the Python standard library.
r"""         Loads a Project from the specified 'pnm' file          The loaded project is added to the Workspace . This will *not* delete         any existing Projects in the Workspace and will rename any Projects         being loaded if necessary.          Parameters         ----------         filename : string or path object             The name of the file to open.  See Notes for more information.          See Also         --------         load_workspace          Notes         -----         The filename can be a string such as 'saved_file.pnm'.  The string can         include absolute path such as 'C:\networks\saved_file.pnm', or can         be a relative path such as '..\..\saved_file.pnm', which will look         2 directories above the current working directory.  Can also be a         path object object such as that produced by ``pathlib`` or         ``os.path`` in the Python standard library.
r"""         Creates a new empty Project object          Parameters         ----------         name : string (optional)             The unique name to give to the project.  If none is given, one             will be automatically generated (e.g. 'sim_01`)          Returns         -------         An empty project object, suitable for passing into a Network         generator
r"""         Generates a valid name for projects
r"""         Generates a sequence of integers of the given ``size``, starting at 1         greater than the last produced value.          The Workspace object keeps track of the most recent value, which         persists until the current python session is restarted, so the         returned array contains unique values for the given session.          Parameters         ----------         size : int             The number of values to generate.          Returns         -------         A Numpy array of the specified size, containing integer values starting         from the last used values.          Notes         -----         When a new Workspace is created the
r"""         Saves data from the given objects into the specified file.          Parameters         ----------         network : OpenPNM Network Object             The network containing the desired data          phases : list of OpenPNM Phase Objects (optional, default is none)             A list of phase objects whose data are to be included          Notes         -----         This method only saves the data, not any of the pore-scale models or         other attributes.  To save an actual OpenPNM Project use the         ``Workspace`` object.
r"""     Given a symmetric adjacency matrix, finds all sites that are connected     to the input sites.      Parameters     ----------     am : scipy.sparse matrix         The adjacency matrix of the network.  Must be symmetrical such that if         sites *i* and *j* are connected, the matrix contains non-zero values         at locations (i, j) and (j, i).      flatten : boolean         If ``True`` (default) the returned result is a compressed array of all         neighbors, or a list of lists with each sub-list containing the         neighbors for each input site.  Note that an *unflattened* list might         be slow to generate since it is a Python ``list`` rather than a Numpy         array.      include_input : boolean         If ``False`` (default) the input sites will be removed from the result.      logic : string         Specifies logic to filter the resulting list.  Options are:          **'or'** : (default) All neighbors of the input sites.  This is also         known as the 'union' in set theory or 'any' in boolean logic.  Both         keywords are accepted and treated as 'or'.          **'xor'** : Only neighbors of one and only one input site.  This is         useful for finding the sites that are not shared by any of the input         sites.  'exclusive_or' is also accepted.          **'xnor'** : Neighbors that are shared by two or more input sites. This         is equivalent to finding all neighbors with 'or', minus those found         with 'xor', and is useful for finding neighbors that the inputs have         in common.  'nxor' is also accepted.          **'and'** : Only neighbors shared by all input sites.  This is also         known as 'intersection' in set theory and (somtimes) as 'all' in         boolean logic.  Both keywords are accepted and treated as 'and'.      Returns     -------     An array containing the neighboring sites filtered by the given logic.  If     ``flatten`` is ``False`` then the result is a list of lists containing the     neighbors of each input site.      See Also     --------     find_complement      Notes     -----     The ``logic`` options are applied to neighboring sites only, thus it is not     possible to obtain sites that are part of the global set but not neighbors.     This is because (a) the list global sites might be very large, and (b) it     is not possible to return a list of neighbors for each input site if global     sites are considered.
r"""     Given an adjacency matrix, finds which sites are connected to the input     bonds.      Parameters     ----------     am : scipy.sparse matrix         The adjacency matrix of the network.  Must be symmetrical such that if         sites *i* and *j* are connected, the matrix contains non-zero values         at locations (i, j) and (j, i).      flatten : boolean (default is ``True``)         Indicates whether the returned result is a compressed array of all         neighbors, or a list of lists with each sub-list containing the         neighbors for each input site.  Note that an *unflattened* list might         be slow to generate since it is a Python ``list`` rather than a Numpy         array.      logic : string         Specifies logic to filter the resulting list.  Options are:          **'or'** : (default) All neighbors of the input bonds.  This is also         known as the 'union' in set theory or (sometimes) 'any' in boolean         logic.  Both keywords are accepted and treated as 'or'.          **'xor'** : Only neighbors of one and only one input bond.  This is         useful for finding the sites that are not shared by any of the input         bonds.  'exclusive_or' is also accepted.          **'xnor'** : Neighbors that are shared by two or more input bonds. This         is equivalent to finding all neighbors with 'or', minus those found         with 'xor', and is useful for finding neighbors that the inputs have         in common.  'nxor' is also accepted.          **'and'** : Only neighbors shared by all input bonds.  This is also         known as 'intersection' in set theory and (somtimes) as 'all' in         boolean logic.  Both keywords are accepted and treated as 'and'.      Returns     -------     An array containing the connected sites, filtered by the given logic.  If     ``flatten`` is ``False`` then the result is a list of lists containing the     neighbors of each given input bond.  In this latter case, sites that     have been removed by the given logic are indicated by ``nans``, thus the     array is of type ``float`` and is not suitable for indexing.      See Also     --------     find_complement
r"""     Given pairs of sites, finds the bonds which connects each pair.      Parameters     ----------     sites : array_like         A 2-column vector containing pairs of site indices on each row.      am : scipy.sparse matrix         The adjacency matrix of the network.  Must be symmetrical such that if         sites *i* and *j* are connected, the matrix contains non-zero values         at locations (i, j) and (j, i).      Returns     -------     Returns a list the same length as P1 (and P2) with each element     containing the throat number that connects the corresponding pores,     or `None`` if pores are not connected.      Notes     -----     The returned list can be converted to an ND-array, which will convert     the ``None`` values to ``nan``.  These can then be found using     ``scipy.isnan``.
r"""     Finds the complementary sites (or bonds) to a given set of inputs      Parameters     ----------     am : scipy.sparse matrix         The adjacency matrix of the network.      sites : array_like (optional)         The set of sites for which the complement is sought      bonds : array_like (optional)         The set of bonds for which the complement is sought      asmask : boolean         If set to ``True`` the result is returned as a boolean mask of the         correct length with ``True`` values indicate the complements.  The         default is ``False`` which returns a list of indices instead.      Returns     -------     An array containing indices of the sites (or bonds) that are not part of     the input list.      Notes     -----     Either ``sites`` or ``bonds`` must be specified
r"""     Returns ``True`` is the sparse adjacency matrix is upper triangular
r"""     Returns ``True`` is the sparse adjacency matrix is either upper or lower     triangular
r"""     A method to check if a square matrix is symmetric     Returns ``True`` if the sparse adjacency matrix is symmetric
r"""     Convert an adjacency matrix into an incidence matrix
r"""     Convert an incidence matrix into an adjacency matrix
r"""     Given a Delaunay Triangulation object from Scipy's ``spatial`` module,     converts to a sparse adjacency matrix network representation.      Parameters     ----------     tri : Delaunay Triangulation Object         This object is produced by ``scipy.spatial.Delaunay``      Returns     -------     A sparse adjacency matrix in COO format.  The network is undirected     and unweighted, so the adjacency matrix is upper-triangular and all the     weights are set to 1.
r"""     Given a Voronoi tessellation object from Scipy's ``spatial`` module,     converts to a sparse adjacency matrix network representation in COO format.      Parameters     ----------     vor : Voronoi Tessellation object         This object is produced by ``scipy.spatial.Voronoi``      Returns     -------     A sparse adjacency matrix in COO format.  The network is undirected     and unweighted, so the adjacency matrix is upper-triangular and all the     weights are set to 1.
r"""     Converts a list of connections into a Scipy sparse adjacency matrix      Parameters     ----------     conns : array_like, N x 2         The list of site-to-site connections      shape : list, optional         The shape of the array.  If none is given then it is inferred from the         maximum value in ``conns`` array.      force_triu : boolean         If True (default), then all connections are assumed undirected, and         moved to the upper triangular portion of the array      drop_diag : boolean         If True (default), then connections from a site and itself are removed.      drop_dupes : boolean         If True (default), then all pairs of sites sharing multiple connections         are reduced to a single connection.      drop_negs : boolean         If True (default), then all connections with one or both ends pointing         to a negative number are removed.
r"""     Identifies points that lie outside the specified region.      Parameters     ----------     domain_size : array_like         The size and shape of the domain beyond which points should be         trimmed. The argument is treated as follows:          **sphere** : If a scalar or single element list is received, it's         treated as the radius [r] of a sphere centered on [0, 0, 0].          **cylinder** : If a two-element list is received it's treated as         the radius and height of a cylinder [r, z] whose central axis         starts at [0, 0, 0] and extends in the positive z-direction.          **rectangle** : If a three element list is received, it's treated         as the outer corner of rectangle [x, y, z] whose opposite corner         lies at [0, 0, 0].      Returns     -------     An Np-long mask of True values indicating pores that lie outside the     domain.
r"""     Determines if a percolating clusters exists in the network spanning     the given inlet and outlet sites      Parameters     ----------     am : adjacency_matrix         The adjacency matrix with the ``data`` attribute indicating         if a bond is occupied or not      inlets : array_like         An array of indices indicating which sites are part of the inlets      outlets : array_like         An array of indices indicating which sites are part of the outlets      mode : string         Indicates which type of percolation to apply, either `'site'` or         `'bond'`
r"""     Finds cluster labels not attached to the inlets, and sets them to     unoccupied (-1)      Parameters     ----------     labels : tuple of site and bond labels         This information is provided by the ``site_percolation`` or         ``bond_percolation`` functions      inlets : array_like         A list of which sites are inlets.  Can be a boolean mask or an         array of indices.      Returns     -------     A tuple containing a list of site and bond labels, with all clusters     not connected to the inlet sites set to not occupied.
r"""     Calculates the site and bond occupancy status for a site percolation     process given a list of occupied sites.      Parameters     ----------     ij : array_like         An N x 2 array of [site_A, site_B] connections.  If two connected         sites are both occupied they are part of the same cluster, as it         the bond connecting them.      occupied_sites : boolean         A list indicating whether sites are occupied or not      Returns     -------     A tuple containing a list of site and bond labels, indicating which     cluster each belongs to.  A value of -1 indicates unoccupied.      Notes     -----     The ``connected_components`` function of scipy.csgraph will give ALL     sites a cluster number whether they are occupied or not, so this     function essentially adjusts the cluster numbers to represent a     percolation process.
r"""     Calculates the site and bond occupancy status for a bond percolation     process given a list of occupied bonds.      Parameters     ----------     ij : array_like         An N x 2 array of [site_A, site_B] connections.  A site is         considered occupied if any of it's connecting bonds are occupied.      occupied_bonds: boolean         A list indicating whether a bond is occupied or not      Returns     -------     A tuple contain a list of site and bond labels, indicating which     cluster each belongs to.  A value of -1 indicates uninvaded.      Notes     -----     The ``connected_components`` function of scipy.csgraph will give ALL     sites a cluster number whether they are occupied or not, so this     function essentially adjusts the cluster numbers to represent a     percolation process.
Remove pores or throats from the network.      Parameters     ----------     network : OpenPNM Network Object         The Network from which pores or throats should be removed      pores (or throats) : array_like         The indices of the of the pores or throats to be removed from the         network.      Notes     -----     This is an in-place operation, meaning the received Network object will     be altered directly.       Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[5, 5, 5])     >>> pn.Np     125     >>> pn.Nt     300     >>> op.topotools.trim(network=pn, pores=[1])     >>> pn.Np     124     >>> pn.Nt     296
r'''     Add individual pores and/or throats to the network from a list of coords     or conns.      Parameters     ----------     network : OpenPNM Network Object         The Network to which pores or throats should be added      pore_coords : array_like         The coordinates of the pores to add      throat_conns : array_like         The throat connections to add      labels : string, or list of strings, optional         A list of labels to apply to the new pores and throats      Notes     -----     This needs to be enhanced so that it increases the size of all pore     and throat props and labels on ALL associated Phase objects.  At the     moment it throws an error is there are any associated Phases.      This is an in-place operation, meaning the received Network object will     be altered directly.
r"""
r"""     Finds pores on the surface of the network and labels them according to     whether they are on the *top*, *bottom*, etc.  This function assumes the     network is cubic in shape (i.e. with six flat sides)      Parameters     ----------     network : OpenPNM Network object         The network to apply the labels      tol : scalar         The tolerance for defining what counts as a surface pore, which is         specifically meant for random networks.  All pores with ``tol`` of         the maximum or minimum along each axis are counts as pores.  The         default is 0.      label : string         An identifying label to isolate the pores on the faces of the network.         default is 'surface'.
r"""     Find the pores on the surface of the domain by performing a Delaunay     triangulation between the network pores and some external ``markers``. All     pores connected to these external marker points are considered surface     pores.      Parameters     ----------     network: OpenPNM Network Object         The network for which the surface pores are to be found      markers: array_like         3 x N array of the marker coordinates to use in the triangulation.  The         labeling is performed in one step, so all points are added, and then         any pores connected to at least one marker is given the provided label.         By default, this function will automatically generate 6 points outside         each axis of the network domain.          Users may wish to specify a single external marker point and provide an         appropriate label in order to identify specific faces.  For instance,         the marker may be *above* the domain, and the label might be         'top_surface'.      label : string         The label to apply to the pores.  The default is 'surface'.      Notes     -----     This function does not check whether the given markers actually lie outside     the domain, allowing the labeling of *internal* sufaces.      If this method fails to mark some surface pores, consider sending more     markers on each face.      Examples     --------     >>> import openpnm as op     >>> net = op.network.Cubic(shape=[5, 5, 5])     >>> op.topotools.find_surface_pores(network=net)     >>> net.num_pores('surface')     98      When cubic networks are created, the surfaces are already labeled:      >>> net.num_pores(['top','bottom', 'left', 'right', 'front','back'])     98      This function is mostly useful for unique networks such as spheres, random     topology, or networks that have been subdivied.
r'''     Clones the specified pores and adds them to the network      Parameters     ----------     network : OpenPNM Network Object         The Network object to which the new pores are to be added      pores : array_like         List of pores to clone      labels : string, or list of strings         The labels to apply to the clones, default is 'clone'      mode : string         Controls the connections between parents and clones.  Options are:          - 'parents': (Default) Each clone is connected only to its parent         - 'siblings': Clones are only connected to each other in the same                       manner as parents were connected         - 'isolated': No connections between parents or siblings
r"""     Combine multiple networks into one without doing any topological     manipulations (such as stiching nearby pores to each other).      Parameters     ----------     network : OpenPNM Network Object         The network to which all the other networks should be added.      donor : OpenPNM Network Object or list of Objects         The network object(s) to add to the given network      Notes     -----     This methods does *not* attempt to stitch the networks topologically.      See Also     --------     extend     trim     stitch
r'''     Stitches a second a network to the current network.      Parameters     ----------     networK : OpenPNM Network Object         The Network to which to donor Network will be attached      donor : OpenPNM Network Object         The Network to stitch on to the current Network      P_network : array_like         The pores on the current Network      P_donor : array_like         The pores on the donor Network      label_suffix : string or None         Some text to append to each label in the donor Network before         inserting them into the recipient.  The default is to append no         text, but a common option would be to append the donor Network's         name. To insert none of the donor labels, use None.      len_max : float         Set a length limit on length of new throats      method : string (default = 'delaunay')         The method to use when making pore to pore connections. Options are:          - 'delaunay' : Use a Delaunay tessellation         - 'nearest' : Connects each pore on the receptor network to its nearest                       pore on the donor network      Notes     -----     Before stitching it is necessary to translate the pore coordinates of     one of the Networks so that it is positioned correctly relative to the     other.      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[5, 5, 5])     >>> pn2 = op.network.Cubic(shape=[5, 5, 5])     >>> [pn.Np, pn.Nt]     [125, 300]     >>> [pn2.Np, pn2.Nt]     [125, 300]     >>> pn2['pore.coords'][:, 2] += 5.0     >>> op.topotools.stitch(network=pn, donor=pn2, P_network=pn.pores('top'),     ...                     P_donor=pn2.pores('bottom'), method='nearest',     ...                     len_max=1.0)     >>> [pn.Np, pn.Nt]     [250, 625]
r'''     Returns the possible connections between two group of pores, and optionally     makes the connections.      See ``Notes`` for advanced usage.      Parameters     ----------     network : OpenPNM Network Object      pores1 : array_like         The first group of pores on the network      pores2 : array_like         The second group of pores on the network      labels : list of strings         The labels to apply to the new throats.  This argument is only needed         if ``add_conns`` is True.      add_conns : bool         Indicates whether the connections should be added to the supplied         network (default is True).  Otherwise, the connections are returned         as an Nt x 2 array that can be passed directly to ``extend``.      Notes     -----     (1) The method also works if ``pores1`` and ``pores2`` are list of lists,     in which case it consecutively connects corresponding members of the two     lists in a 1-to-1 fashion. Example: pores1 = [[0, 1], [2, 3]] and     pores2 = [[5], [7, 9]] leads to creation of the following connections:         0 --> 5     2 --> 7     3 --> 7         1 --> 5     2 --> 9     3 --> 9      (2) If you want to use the batch functionality, make sure that each element     within ``pores1`` and ``pores2`` are of type list or ndarray.      (3) It creates the connections in a format which is acceptable by     the default OpenPNM connection ('throat.conns') and either adds them to     the network or returns them.      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[5, 5, 5])     >>> pn.Nt     300     >>> op.topotools.connect_pores(network=pn, pores1=[22, 32],     ...                            pores2=[16, 80, 68])     >>> pn.Nt     306     >>> pn['throat.conns'][300:306]     array([[16, 22],            [22, 80],            [22, 68],            [16, 32],            [32, 80],            [32, 68]])
r'''     Find the distance between all pores on set one to each pore in set 2      Parameters     ----------     network : OpenPNM Network Object         The network object containing the pore coordinates      pores1 : array_like         The pore indices of the first set      pores2 : array_Like         The pore indices of the second set.  It's OK if these indices are         partially or completely duplicating ``pores``.      Returns     -------     A distance matrix with ``len(pores1)`` rows and ``len(pores2)`` columns.     The distance between pore *i* in ``pores1`` and *j* in ``pores2`` is     located at *(i, j)* and *(j, i)* in the distance matrix.
r'''     It trim the pores and replace them by cubic networks with the sent shape.      Parameters     ----------     network : OpenPNM Network Object      pores : array_like         The first group of pores to be replaced      shape : array_like         The shape of cubic networks in the target locations      Notes     -----     - It works only for cubic networks.      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[5, 6, 5], spacing=0.001)     >>> pn.Np     150     >>> nano_pores = [2, 13, 14, 15]     >>> op.topotools.subdivide(network=pn, pores=nano_pores, shape=[4, 7, 3],     ...                        labels='nano')     >>> pn.Np     482
r"""     Remove throats with zero area from the network and also remove     pores that are isolated (as a result or otherwise)      Parameters     ----------     network : OpenPNM Network Object      mask : string         Applies routine only to pores and throats with this label
r"""     Combines a selection of pores into a new single pore located at the     centroid of the selected pores and connected to all of their neighbors.      Parameters     ----------     network : OpenPNM Network Object      pores : array_like         The list of pores which are to be combined into a new single pore      labels : string or list of strings         The labels to apply to the new pore and new throat connections      Notes     -----     (1) The method also works if a list of lists is passed, in which case     it consecutively merges the given selections of pores.      (2) The selection of pores should be chosen carefully, preferrable so that     they all form a continuous cluster.  For instance, it is recommended     to use the ``find_nearby_pores`` method to find all pores within a     certain distance of a given pore, and these can then be merged without     causing any abnormal connections.      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[20, 20, 1])     >>> Ps = pn.find_nearby_pores(pores=111, r=5, flatten=True)     >>> op.topotools.merge_pores(network=pn, pores=Ps, labels=['merged'])     >>> print(pn.Np)     321     >>> pn.pores('merged')     array([320])     >>> pn.num_throats('merged')     32
r"""     This private method generates an image array of a sphere/shell-disc/ring.     It is useful for passing to Cubic networks as a ``template`` to make     networks with desired shapes.      Parameters     ----------     dim : int         Network dimension      outer_radius : int         Number of the nodes in the outer radius of the network      inner_radius : int         Number of the nodes in the inner radius of the network      Returns     -------     A Numpy array containing 1's to demarcate the desired shape, and 0's     elsewhere.
r"""     This method generates an image array of a sphere-shell. It is useful for     passing to Cubic networks as a ``template`` to make spherical shaped     networks.      Parameters     ----------     outer_radius : int         Number of nodes in the outer radius of the sphere.      inner_radius : int         Number of nodes in the inner radius of the shell.  a value of 0 will         result in a solid sphere.      Returns     -------     A Numpy array containing 1's to demarcate the sphere-shell, and 0's     elsewhere.
r"""     This method generates an image array of a disc-ring.  It is useful for     passing to Cubic networks as a ``template`` to make circular-shaped 2D     networks.      Parameters     ----------     height : int         The height of the cylinder      outer_radius : int         Number of nodes in the outer radius of the cylinder      inner_radius : int         Number of the nodes in the inner radius of the annulus.  A value of 0         will result in a solid cylinder.      Returns     -------     A Numpy array containing 1's to demarcate the disc-ring, and 0's     elsewhere.
r"""     Produces a 3D plot of the network topology showing how throats connect     for quick visualization without having to export data to veiw in Paraview.      Parameters     ----------     network : OpenPNM Network Object         The network whose topological connections to plot      throats : array_like (optional)         The list of throats to plot if only a sub-sample is desired.  This is         useful for inspecting a small region of the network.  If no throats are         specified then all throats are shown.      fig : Matplotlib figure handle and line property arguments         If a ``fig`` is supplied, then the topology will be overlaid on this         plot.  This makes it possible to combine coordinates and connections,         and to color different throats differently (see ``kwargs``)      kwargs : other named arguments         By also in different line properties such as ``color`` it's possible to         plot several different sets of connections with unique colors.          For information on available line style options, visit the Matplotlib         documentation on the `web         <http://matplotlib.org/api/lines_api.html#matplotlib.lines.Line2D>`_      Notes     -----     The figure handle returned by this method can be passed into     ``plot_coordinates`` to create a plot that combines pore coordinates and     throat connections, and vice versa.      See Also     --------     plot_coordinates      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[10, 10, 3])     >>> pn.add_boundary_pores()     >>> Ts = pn.throats('*boundary', mode='nor')     >>> # Create figure showing boundary throats     >>> fig = op.topotools.plot_connections(network=pn, throats=Ts)     >>> Ts = pn.throats('*boundary')     >>> # Pass existing fig back into function to plot additional throats     >>> fig = op.topotools.plot_connections(network=pn, throats=Ts,     ...                                     fig=fig, color='r')
r"""     Produces a 3D plot showing specified pore coordinates as markers      Parameters     ----------     network : OpenPNM Network Object         The network whose topological connections to plot      pores : array_like (optional)         The list of pores to plot if only a sub-sample is desired.  This is         useful for inspecting a small region of the network.  If no pores are         specified then all are shown.      fig : Matplotlib figure handle         If a ``fig`` is supplied, then the coordinates will be overlaid.  This         enables the plotting of multiple different sets of pores as well as         throat connections from ``plot_connections``.      kwargs : dict         By also  in different marker properties such as size (``s``) and color         (``c``).          For information on available marker style options, visit the Matplotlib         documentation on the `web         <http://matplotlib.org/api/lines_api.html#matplotlib.lines.Line2D>`_      Notes     -----     The figure handle returned by this method can be passed into     ``plot_topology`` to create a plot that combines pore coordinates and     throat connections, and vice versa.      See Also     --------     plot_connections      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[10, 10, 3])     >>> pn.add_boundary_pores()     >>> Ps = pn.pores('internal')     >>> # Create figure showing internal pores     >>> fig = op.topotools.plot_coordinates(network=pn, pores=Ps, c='b')     >>> Ps = pn.pores('*boundary')     >>> # Pass existing fig back into function to plot boundary pores     >>> fig = op.topotools.plot_coordinates(network=pn, pores=Ps, fig=fig,     ...                                         c='r')
r'''     Returns a pretty 2d plot for 2d OpenPNM networks.      Parameters     ----------     network : OpenPNM Network object      plot_throats : boolean         Plots throats as well as pores, if True.      labels : list         List of OpenPNM labels      colors : list         List of corresponding colors to the given `labels`.      scale : float         Scale factor for size of pores.
r"""     Generates a set of base points for passing into the Tessellation-based     Network classes.  The points can be distributed in spherical, cylindrical,     or rectilinear patterns, as well as 2D and 3D (disks and squares).      Parameters     ----------     num_points : scalar         The number of base points that lie within the domain.  Note that the         actual number of points returned will be larger, with the extra points         lying outside the domain.      domain_size : list or array         Controls the size and shape of the domain, as follows:          **sphere** : If a single value is received, its treated as the radius         [r] of a sphere centered on [0, 0, 0].          **cylinder** : If a two-element list is received it's treated as the         radius and height of a cylinder [r, z] positioned at [0, 0, 0] and         extending in the positive z-direction.  If the z dimension is 0, a         disk of radius r is created.          **rectangle** : If a three element list is received, it's treated         as the outer corner of rectangle [x, y, z] whose opposite corner lies         at [0, 0, 0].  If the z dimension is 0, a rectangle of size X-by-Y is         created.      density_map : array, optional         A an array that contains fractional values (0 < i < 1) indicating the         liklihood that a point in that region should be kept.  The size of this         array can be anything, but the shape must match the ``domain_size``;         that is for a 3D network the shape of the ``density_map`` can be         [10, 10, 10] or [50, 50, 50], depending on how important the resolution         of the density distribution is.  For a 2D network the ``density_map``         should be [10, 10].          When specifying a custom probabiliy map is it recommended to also set         values outside the given domain to zero.  If not, then the correct         shape will still be returned, but with too few points in it.      reflect : boolean         If True, the the base points are generated as specified, the reflected         about each face of the domain.  This essentially tricks the         tessellation functions into creating smooth flat faces at the         boundaries once these excess pores are trimmed.      Notes     -----     The reflection approach tends to create larger pores near the surfaces, so     it might be necessary to use the ``density_map`` argument to specify a     slightly higher density of points near the surfaces.      The ``Voronoi``, ``Delaunay``, ``Gabriel``, and ``DelunayVoronoiDual``     classes can *techncially* handle base points with spherical or cylindrical     domains, but the reflection across round surfaces does not create perfect     Voronoi cells so the surfaces will not be smooth.       Examples     --------     The following generates a spherical array with higher values near the core.     It uses a distance transform to create a sphere of radius 10, then a     second distance transform to create larger values in the center away from     the sphere surface.  These distance values could be further skewed by     applying a power, with values higher than 1 resulting in higher values in     the core, and fractional values smoothinging them out a bit.      >>> import openpnm as op     >>> import scipy as sp     >>> import scipy.ndimage as spim     >>> im = sp.ones([21, 21, 21], dtype=int)     >>> im[10, 10, 10] = 0     >>> im = spim.distance_transform_edt(im) <= 20  # Create sphere of 1's     >>> prob = spim.distance_transform_edt(im)     >>> prob = prob / sp.amax(prob)  # Normalize between 0 and 1     >>> pts = op.topotools.generate_base_points(num_points=50,     ...                                         domain_size=[1, 1, 1],     ...                                         density_map=prob)     >>> net = op.network.DelaunayVoronoiDual(points=pts, shape=[1, 1, 1])
r'''     Helper function for relecting a set of points about the faces of a     given domain.      Parameters     ----------     base_pts : array_like         The coordinates of the base_pts to be reflected in the coordinate         system corresponding to the the domain as follows:          **spherical** : [r, theta, phi]         **cylindrical** or **circular** : [r, theta, z]         **rectangular** or **square** : [x, y, z]      domain_size : list or array         Controls the size and shape of the domain, as follows:          **sphere** : If a single value is received, its treated as the radius         [r] of a sphere centered on [0, 0, 0].          **cylinder** : If a two-element list is received it's treated as the         radius and height of a cylinder [r, z] positioned at [0, 0, 0] and         extending in the positive z-direction.  If the z dimension is 0, a         disk of radius r is created.          **rectangle** : If a three element list is received, it's treated         as the outer corner of rectangle [x, y, z] whose opposite corner lies         at [0, 0, 0].  If the z dimension is 0, a rectangle of size X-by-Y is         created.
r"""     Identify connected clusters of pores in the network.  This method can     also return a list of throat cluster numbers, which correspond to the     cluster numbers of the pores to which the throat is connected.  Either     site and bond percolation can be considered, see description of input     arguments for details.      Parameters     ----------     network : OpenPNM Network Object         The network      mask : array_like, boolean         A list of active bonds or sites (throats or pores).  If the mask is         Np long, then the method will perform a site percolation, and if         the mask is Nt long bond percolation will be performed.      Returns     -------     A tuple containing an Np long list of pore cluster labels, and an Nt-long     list of throat cluster labels.  The label numbers correspond such that     pores and throats with the same label are part of the same cluster.      Examples     --------     >>> import openpnm as op     >>> from scipy import rand     >>> pn = op.network.Cubic(shape=[25, 25, 1])     >>> pn['pore.seed'] = rand(pn.Np)     >>> pn['throat.seed'] = rand(pn.Nt)
r"""     This private method is called by 'find_clusters'
r"""     This private method is called by 'find_clusters'
r"""     This method uses ``clone_pores`` to clone the input pores, then shifts     them the specified amount and direction, then applies the given label.      Parameters     ----------     pores : array_like         List of pores to offset.  If no pores are specified, then it         assumes that all surface pores are to be cloned.      offset : 3 x 1 array         The distance in vector form which the cloned boundary pores should         be offset.      apply_label : string         This label is applied to the boundary pores.  Default is         'boundary'.      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[5, 5, 5])     >>> print(pn.Np)  # Confirm initial Network size     125     >>> Ps = pn.pores('top')  # Select pores on top face     >>> pn.add_boundary_pores(labels=['top'])     >>> print(pn.Np)  # Confirm addition of 25 new pores     150
r"""     Find the shortest path between pairs of pores.      Parameters     ----------     network : OpenPNM Network Object         The Network object on which the search should be performed      pore_pairs : array_like         An N x 2 array containing N pairs of pores for which the shortest         path is sought.      weights : array_like, optional         An Nt-long list of throat weights for the search.  Typically this         would be the throat lengths, but could also be used to represent         the phase configuration.  If no weights are given then the         standard topological connections of the Network are used.      Returns     -------     A dictionary containing both the pores and throats that define the     shortest path connecting each pair of input pores.      Notes     -----     The shortest path is found using Dijkstra's algorithm included in the     scipy.sparse.csgraph module      TODO: The returned throat path contains the correct values, but not     necessarily in the true order      Examples     --------     >>> import openpnm as op     >>> pn = op.network.Cubic(shape=[3, 3, 3])     >>> a = op.topotools.find_path(network=pn, pore_pairs=[[0, 4], [0, 10]])     >>> a['pores']     [array([0, 1, 4]), array([ 0,  1, 10])]     >>> a['throats']     [array([ 0, 19]), array([ 0, 37])]
r'''     Determines if given pores are coplanar with each other      Parameters     ----------     coords : array_like         List of pore coords to check for coplanarity.  At least 3 pores are         required.      Returns     -------     A boolean value of whether given points are coplanar (True) or not (False)
r"""         This calculates the effective electrical conductivity.          Parameters         ----------         inlets : array_like             The pores where the inlet voltage boundary conditions were             applied.  If not given an attempt is made to infer them from the             algorithm.          outlets : array_like             The pores where the outlet voltage boundary conditions were             applied.  If not given an attempt is made to infer them from the             algorithm.          domain_area : scalar, optional             The area of the inlet (and outlet) boundary faces.  If not given             then an attempt is made to estimate it, but it is usually             underestimated.          domain_length : scalar, optional             The length of the domain between the inlet and outlet boundary             faces.  If not given then an attempt is made to estimate it, but it             is usually underestimated.          Notes         -----         The area and length of the domain are found using the bounding box         around the inlet and outlet pores which do not necessarily lie on the         edge of the domain, resulting in underestimation of sizes.
r"""     Many of the methods are generic to pores and throats. Some information may     be stored on either the pore or throat and needs to be interpolated.     This is a helper method to return the properties in the correct format.     To do:         Check for method to convert throat to pore data
r"""     Computes the capillary entry pressure assuming the throat in a cylindrical     tube.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      surface_tension : string         The dictionary key containing the surface tension values to be used. If         a pore property is given, it is interpolated to a throat list.      contact_angle : string         The dictionary key containing the contact angle values to be used. If         a pore property is given, it is interpolated to a throat list.      diameter : string         The dictionary key containing the throat diameter values to be used.      Notes     -----     The Washburn equation is:      .. math::         P_c = -\frac{2\sigma(cos(\theta))}{r}      This is the most basic approach to calculating entry pressure and is     suitable for highly non-wetting invading phases in most materials.
r"""     Computes the throat capillary entry pressure assuming the throat is a     toroid.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      r_toroid : float or array_like         The radius of the toroid surrounding the pore      surface_tension : dict key (string)         The dictionary key containing the surface tension values to be used.         If a pore property is given, it is interpolated to a throat list.      contact_angle : dict key (string)         The dictionary key containing the contact angle values to be used.         If a pore property is given, it is interpolated to a throat list.      diameter : dict key (string)         The dictionary key containing the throat diameter values to be used.      Notes     -----     This approach accounts for the converging-diverging nature of many throat     types.  Advancing the meniscus beyond the apex of the toroid requires an     increase in capillary pressure beyond that for a cylindical tube of the     same radius. The details of this equation are described by Mason and     Morrow [1]_, and explored by Gostick [2]_ in the context of a pore network     model.      References     ----------      .. [1] G. Mason, N. R. Morrow, Effect of contact angle on capillary            displacement curvatures in pore throats formed by spheres. J.            Colloid Interface Sci. 168, 130 (1994).     .. [2] J. Gostick, Random pore network modeling of fibrous PEMFC gas            diffusion media using Voronoi and Delaunay tessellations. J.            Electrochem. Soc. 160, F731 (2013).
r"""     Computes the capillary snap-off pressure assuming the throat is cylindrical     with converging-diverging change in diamater - like the Purcell model.     The wavelength of the change in diamater is the fiber radius.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      shape_factor :         constant dependent on the shape of throat cross-section 1.75 - 2.0, see         Ref      wavelength : float or array like         The transverse interfacial radius of curvature at the neck         (fiber radius in fibrous media)      require_pair : bool         Controls whether snap-off requires a pair of arc meniscii to occur.      surface_tension : dict key (string)         The dictionary key containing the surface tension values to be used.         If a pore property is given, it is interpolated to a throat list.      contact_angle : dict key (string)         The dictionary key containing the contact angle values to be used.         If a pore property is given, it is interpolated to a throat list.      throat_diameter : dict key (string)         The dictionary key containing the throat diameter values to be used.      Notes     -----     This equation should be used to calculate the snap off capillary pressure     in fribrous media      References     ----------     [1]: Ransohoff, T.C., Gauglitz, P.A. and Radke, C.J., 1987. Snap‐off of gas     bubbles in smoothly constricted noncircular capillaries. AIChE Journal,     33(5), pp.753-765.
r"""     Computes the throat capillary entry pressure assuming the throat is a     toroid. Makes use of the toroidal meniscus model with mode touch.     This model accounts for mensicus protrusion into adjacent pores and     touching solid features.     It is bidirectional becauase the connected pores generally have different     sizes and this determines how far the meniscus can protrude.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      r_toroid : float or array_like         The radius of the toroid surrounding the pore      num_points : float (Default 100)         The number of divisions to make along the profile length to assess the         meniscus properties in order to find the touch length.      surface_tension : dict key (string)         The dictionary key containing the surface tension values to be used.         If a pore property is given, it is interpolated to a throat list.      contact_angle : dict key (string)         The dictionary key containing the contact angle values to be used.         If a pore property is given, it is interpolated to a throat list.      throat_diameter : dict key (string)         The dictionary key containing the throat diameter values to be used.      pore_diameter : dict key (string)         The dictionary key containing the pore diameter values to be used.     Notes
r"""     Computes the throat capillary entry pressure assuming the throat has a     sinusoisal profile.     Makes use of the toroidal meniscus model with mode touch.     This model accounts for mensicus protrusion into adjacent pores and     touching solid features.     It is bidirectional becauase the connected pores generally have different     sizes and this determines how far the meniscus can protrude.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties      num_points : float (Default 100)         The number of divisions to make along the profile length to assess the         meniscus properties in order to find the touch length.      surface_tension : dict key (string)         The dictionary key containing the surface tension values to be used.         If a pore property is given, it is interpolated to a throat list.      contact_angle : dict key (string)         The dictionary key containing the contact angle values to be used.         If a pore property is given, it is interpolated to a throat list.      throat_diameter : dict key (string)         The dictionary key containing the throat diameter values to be used.      throat_amplitude : dict key (string)         The dictionary key containing the amplitude of variation in the throat         diameter about the mean.      throat_length : dict key (string)         The dictionary key containing the throat length values to be used.      pore_diameter : dict key (string)         The dictionary key containing the pore diameter values to be used.     Notes
r"""         Add data to an OpenPNM Network from a undirected NetworkX graph object.          Parameters         ----------         G : networkx.classes.graph.Graph Object             The NetworkX graph. G should be undirected. The numbering of nodes             should be numeric (int's), zero-based and should not contain any             gaps, i.e. ``G.nodes() = [0,1,3,4,5]`` is not allowed and should be             mapped to ``G.nodes() = [0,1,2,3,4]``.          project : OpenPNM Project object             A GenericNetwork is created and added to the specified Project.             If no Project is supplied then one will be created and returned.          Returns         -------         An OpenPNM Project containing a GenericNetwork with all the data from         the NetworkX object.
r"""         Write OpenPNM Network to a NetworkX object.          Parameters         ----------         network : OpenPNM Network Object             The OpenPNM Network to be converted to a NetworkX object          Returns         -------         A NetworkX object with all pore/throat properties attached to it
r"""     Determines the conductance of a pore-throat-pore conduit based on the     invaded state of each element.      Parameters     ----------     target : OpenPNM Object         The OpenPNM object where the model is attached.  Should either be a         Physics or a Phase.      throat_conductance : string         The transport conductance of the phase associated with the ``target``         object at single-phase conditions.      pore_occupancy : string         The property name containing the occupancy of the phase associated         with the ``target`` object.  An occupancy of 1 means the pore         is completely filled with the phase and it fully conducts.      throat_occupancy : string         The property name containing the occupancy of the phase associated         with the ``target`` object.  An occupancy of 1 means the throat         is completely filled with the phase and it fully conducts.      mode : 'strict' or 'medium' or 'loose'         How agressive the method should be when determining if a conduit is         closed.          **'strict'** :  If any pore or throat in the conduit is unoccupied by          the given phase, the conduit is closed.          **'medium'** : If either the throat or both pores are unoccupied, the         conduit is closed          **'loose'** : Only close the conduit if the throat is unoccupied      factor : float (default is 1e-6)         The factor which becomes multiplied to the original conduit's         conductance to severely limit transport, but not set it to zero.
r"""     Calculates the fraction of a pore or throat filled with invading fluid     based on the capillary pressure in the invading phase.  The invading phase     volume is calculated from:          .. math::             S_{nwp} = 1 - S_{wp}^{*} (P^{*}/P_{c})^{\eta}      Parameters     ----------     pressure : string         The capillary pressure in the non-wetting phase (Pc > 0).      Pc_star : string         The minimum pressure required to create an interface within the pore         body or throat.  Typically this would be calculated using the Washburn         equation.      Swp_star : float         The residual wetting phase in an invaded pore or throat at a pressure         of ``pc_star``.      eta : float         Exponent controlling the rate at which wetting phase is displaced with         increasing pressure.      Returns     -------     An array containing the fraction of each pore or throat that would be     filled with non-wetting phase at the given phase pressure.  This does not     account for whether or not the element is actually invaded, which requires     a percolation algorithm of some sort.
r"""     The average of the pore coords
r"""         Load data from the \'dat\' files located in specified folder.          Parameters         ----------         path : string             The full path to the folder containing the set of \'dat\' files.          prefix : string             The file name prefix on each file. The data files are stored             as \<prefix\>_node1.dat.          network : OpenPNM Network Object             If given then the data will be loaded on it and returned.  If not             given, a Network will be created and returned.          Returns         -------         An OpenPNM Project containing a GenericNetwork holding all the data
r'''         Returns a list of dependencies in the order with which they should be         called to ensure data is calculated by one model before it's asked for         by another.          Notes         -----         This raises an exception if the graph has cycles which means the         dependencies are unresolvable (i.e. there is no order which the         models can be called that will work).  In this case it is possible         to visually inspect the graph using ``dependency_graph``.          See Also         --------         dependency_graph         dependency_map
r"""         Returns a NetworkX graph object of the dependencies          See Also         --------         dependency_list         dependency_map          Notes         -----         To visualize the dependencies, the following NetworkX function and         settings is helpful:          nx.draw_spectral(d, arrowsize=50, font_size=32, with_labels=True,                          node_size=2000, width=3.0, edge_color='lightgrey',                          font_weight='bold')
r"""         Create a graph of the dependency graph in a decent format          See Also         --------         dependency_graph         dependency_list
r"""         Adds a new model to the models dictionary (``object.models``)          Parameters         ----------         propname : string             The name of the property to be calculated by the model.          model : function             A reference (handle) to the function to be used.          regen_mode : string             Controls how/when the model is run (See Notes for more details).             Options are:              *'normal'* : The model is run directly upon being assiged, and             also run every time ``regenerate_models`` is called.              *'constant'* : The model is run directly upon being assigned, but             is not called again, thus making it's data act like a constant.             If, however, the data is deleted from the object it will be             regenerated again.              *'deferred'* Is not run upon being assigned, but is run the first             time that ``regenerate_models`` is called.
r"""         Re-runs the specified model or models.          Parameters         ----------         propnames : string or list of strings             The list of property names to be regenerated.  If None are given             then ALL models are re-run (except for those whose ``regen_mode``             is 'constant').          exclude : list of strings             Since the default behavior is to run ALL models, this can be used             to exclude specific models.  It may be more convenient to supply             as list of 2 models to exclude than to specify 8 models to include.          deep : boolean             Specifies whether or not to regenerate models on all associated             objects.  For instance, if ``True``, then all Physics models will             be regenerated when method is called on the corresponding Phase.             The default is ``False``.  The method does not work in reverse,             so regenerating models on a Physics will not update a Phase.
r"""         Removes model and data from object.          Parameters         ----------         propname : string or list of strings             The property or list of properties to remove          mode : list of strings             Controls what is removed.  Options are:              *'model'* : Removes the model but not any numerical data that may             already exist.              *'data'* : Removes the data but leaves the model.          The default is both.
r"""     Calculates the diameter of a cirlce or edge-length of a sqaure with same     area as the throat.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      thorat_area : string         The dictionary key to the throat area values      throat_shape : string         The shape cross-sectional shape of the throat to assume when         back-calculating from the area.  Options are 'circle' (default) or         'square'.
r"""         This method takes several arguments that are essential to running the         algorithm and adds them to the settings          Parameters         ----------         phase : OpenPNM Phase object             The phase on which the algorithm is to be run. If no value is             given, the existing value is kept.          quantity : string             The name of the physical quantity to be calcualted such as             ``'pore.xxx'``.          conductance : string             The name of the pore-scale transport conductance values. These             are typically calculated by a model attached to a *Physics* object             associated with the given *Phase*. Example; ``'throat.yyy'``.          t_initial : scalar, smaller than 't_final'             The simulation's start time. The default value is 0.          t_final : scalar, bigger than 't_initial'             The simulation's end time. The default value is 10.          t_step : scalar, between 't_initial' and 't_final'             The simulation's time step. The default value is 0.1.          t_output : scalar or list of scalars             When 't_output' is a scalar, it is considered as an output interval             to store transient solutions. The default value is 1e+08. Initial             and steady-state (if reached) fields are always stored. If             't_output' > 't_final', no transient data is stored. If 't_output'             is not a multiple of 't_step', 't_output' will be approximated.             When 't_output' is a list, transient solutions corresponding to             this list will be stored.          output_times : list             List of output times. The values in the list must be multiples of             the time step 't_step'.          t_tolerance : scalar             Transient solver tolerance. The simulation stops (before reaching             't_final') when the residual falls below 't_tolerance'. The             default value is 1e-06. The 'residual' measures the variation from             one time-step to another in the value of the 'quantity' solved for.          r_tolerance : scalar             Tolerance to achieve within each time step. The solver passes to             next time step when 'residual' falls below 'r_tolerance'. The             default value is 1e-04.          t_precision : integer             The time precision (number of decimal places).          t_scheme : string             The time discretization scheme. Three options available: 'steady'             to perform a steady-state simulation, and 'implicit' (fast, 1st             order accurate) and 'cranknicolson' (slow, 2nd order accurate) both             for transient simulations. The default value is 'implicit'.          Notes         -----         More settings can be adjusted in the presence of a non-linear source         term such as under-relaxation.         See the 'ReactiveTransport' class documentation for details.
r"""         A method to set simulation initial conditions          Parameters         ----------         values : ND-array or scalar             Set the initial conditions using an 'Np' long array. 'Np' being             the number of pores. If a scalar is given, the same value is             imposed to all pores.
r"""         A method to update 'A' matrix at each time step according to 't_scheme'
r"""         A method to update 'b' array at each time step according to         't_scheme' and the source term value
r"""         Builds 'A' matrix of the steady system of equations to be used at each         time step to build transient 'A' and 'b'. Imposes the initial         conditions and stores the initial field. Initialize transient 'A', 'b',         and source term (if present) and finally calls the transient solver.          Parameters         ----------         t : scalar             The time to start the simulation from. If no time is specified, the             simulation starts from 't_initial' defined in the settings.
r         Performs a transient simulation according to the specified settings         updating 'b' and calling '_t_run_reactive' at each time step.         Stops after reaching the end time 't_final' or after achieving the         specified tolerance 't_tolerance'. Stores the initial and steady-state         (if obtained) fields in addition to transient data (according to the         specified 't_output').          Parameters         ----------         t : scalar             The time to start the simulation from.          Notes         -----         Transient solutions are stored on the object under         ``pore.quantity_timeStepIndex`` where *quantity* is specified in the         ``settings`` attribute. Initial field is stored as         ``pore.quantity_initial``. Steady-state solution (if reached) is stored         as ``pore.quantity_steady``. Current solution is stored as         ``pore.quantity``.
r         Repeatedly updates transient 'A', 'b', and the solution guess within         each time step according to the applied source term then calls '_solve'         to solve the resulting system of linear equations. Stops when the         residual falls below 'r_tolerance'.          Parameters         ----------         x : ND-array             Initial guess of unknown variable          Returns         -------         x_new : ND-array             Solution array.          Notes         -----         Description of 'relaxation_quantity' and 'max_iter' settings can be         found in the parent class 'ReactiveTransport' documentation.
r"""
r"""     Calculates the rate, as well as slope and intercept of the following     function at the given value of `X`:          .. math::             r = A_{1}   X  +  A_{2}      Parameters     ----------     A1 -> A2 : string         The dictionary keys on the target object containing the coefficients         values to be used in the source term model      X : string         The dictionary key on the target objecxt containing the the quantity         of interest      Returns     -------     A dictionary containing the following three items:          **'rate'** - The value of the source term function at the given X.          **'S1'** - The slope of the source term function at the given X.          **'S2'** - The intercept of the source term function at the given X.      The slope and intercept provide a linearized source term equation about the     current value of X as follow:          .. math::             rate = S_{1}   X  +  S_{2}
r"""     Calculates the rate, as well as slope and intercept of the following     function at the given value of *X*:          .. math::             r = A_{1}   x^{A_{2}}  +  A_{3}      Parameters     ----------     A1 -> A3 : string         The dictionary keys on the target object containing the coefficients         values to be used in the source term model      X : string         The dictionary key on the target objecxt containing the the quantity         of interest      Returns     -------     A dictionary containing the following three items:          **'rate'** - The value of the source term function at the given X.          **'S1'** - The slope of the source term function at the given X.          **'S2'** - The intercept of the source term function at the given X.      The slope and intercept provide a linearized source term equation about the     current value of X as follow:          .. math::             rate = S_{1}   X  +  S_{2}
r"""     Calculates the rate, as well as slope and intercept of the following     function at the given value of `X`:          .. math::             r =  A_{1} A_{2}^{( A_{3} x^{ A_{4} } + A_{5})} + A_{6}      Parameters     ----------     A1 -> A6 : string         The dictionary keys on the target object containing the coefficients         values to be used in the source term model      X : string         The dictionary key on the target objecxt containing the the quantity         of interest      Returns     -------     A dictionary containing the following three items:          **'rate'** - The value of the source term function at the given X.          **'S1'** - The slope of the source term function at the given X.          **'S2'** - The intercept of the source term function at the given X.      The slope and intercept provide a linearized source term equation about the     current value of X as follow:          .. math::             rate = S_{1}   X  +  S_{2}
r'''     Take a symbolic equation and return the lambdified version plus the     linearization of form S1 * x + S2
r"""     Calculates the rate, as well as slope and intercept of the following     function at the given value of *x*:          .. math::             r = A_{1}   x  +  A_{2}      Parameters     ----------     A1 -> A2 : string         The dictionary keys on the target object containing the coefficients         values to be used in the source term model      X : string         The dictionary key on the target object containing the the quantity         of interest      Returns     -------     A dictionary containing the following three items:          **'rate'** - The value of the source term function at the given X.          **'S1'** - The slope of the source term function at the given X.          **'S2'** - The intercept of the source term function at the given X.      The slope and intercept provide a linearized source term equation about the     current value of X as follow:          .. math::             rate = S_{1}   X  +  S_{2}
r"""     Calculates the rate, as well as slope and intercept of the following     function at the given value of *x*:          .. math::             r =  A_{1} A_{2}^{( A_{3} x^{ A_{4} } + A_{5})} + A_{6}      Parameters     ----------     A1 -> A6 : string         The dictionary keys on the target object containing the coefficients         values to be used in the source term model      X : string or float/int or array/list         The dictionary key on the target objecxt containing the the quantity         of interest      Returns     -------     A dictionary containing the following three items:          **'rate'** - The value of the source term function at the given X.          **'S1'** - The slope of the source term function at the given X.          **'S2'** - The intercept of the source term function at the given X.      The slope and intercept provide a linearized source term equation about the     current value of X as follow:          .. math::             rate = S_{1}   X  +  S_{2}
r'''     A general function to interpret a sympy equation and evaluate the linear     components of the source term.      Parameters     ----------     target : OpenPNM object         The OpenPNM object where the result will be applied.      eqn : sympy symbolic expression for the source terms         e.g. y = a*x**b + c      arg_map : Dict mapping the symbols in the expression to OpenPNM data         on the target. Must contain 'x' which is the independent variable.         e.g. arg_map={'a':'pore.a', 'b':'pore.b', 'c':'pore.c', 'x':'pore.x'}      Example     ----------     >>> import openpnm as op     >>> from openpnm.models.physics import generic_source_term as gst     >>> import scipy as sp     >>> import sympy as _syp     >>> pn = op.network.Cubic(shape=[5, 5, 5], spacing=0.0001)     >>> water = op.phases.Water(network=pn)     >>> water['pore.a'] = 1     >>> water['pore.b'] = 2     >>> water['pore.c'] = 3     >>> water['pore.x'] = sp.random.random(water.Np)     >>> a, b, c, x = _syp.symbols('a,b,c,x')     >>> y = a*x**b + c     >>> arg_map = {'a':'pore.a', 'b':'pore.b', 'c':'pore.c', 'x':'pore.x'}     >>> water.add_model(propname='pore.general',     ...                 model=gst.general_symbolic,     ...                 eqn=y, arg_map=arg_map,     ...                 regen_mode='normal')     >>> assert 'pore.general.rate' in water.props()     >>> assert 'pore.general.S1' in water.props()     >>> assert 'pore.general.S1' in water.props()
r"""     Calculate the electrical conductance of conduits in network, where a     conduit is ( 1/2 pore - full throat - 1/2 pore ). See the notes section.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_thermal_conductivity : string         Dictionary key of the pore thermal conductivity values      throat_thermal_conductivity : string         Dictionary key of the throat thermal conductivity values      pore_area : string         Dictionary key of the pore area values      throat_area : string         Dictionary key of the throat area values      conduit_shape_factors : string         Dictionary key of the conduit DIFFUSION shape factor values      Returns     -------     g : ndarray         Array containing electrical conductance values for conduits in the         geometry attached to the given physics object.      Notes     -----     (1) This function requires that all the necessary phase properties already     be calculated.      (2) This function calculates the specified property for the *entire*     network then extracts the values for the appropriate throats at the end.      (3) This function assumes cylindrical throats with constant cross-section     area. Corrections for different shapes and variable cross-section area can     be imposed by passing the proper flow_shape_factor argument.
r"""     Homemade version of matlab tic and toc function, tic starts or resets     the clock, toc reports the time since the last call of tic.      Parameters     ----------     quiet : Boolean         If False (default) then a message is output to the console.  If True         the message is not displayed and the elapsed time is returned.      See Also     --------     tic
r"""     For a given list (of points) remove any duplicates
r"""     Given a list of nested lists of arbitrary depth, returns a single level or     'flat' list.
r"""     Given a nested dictionary, ensures that all nested dicts are normal     Python dicts.  This is necessary for pickling, or just converting     an 'auto-vivifying' dict to something that acts normal.
r"""
r"""     Converts a ModelsDict object to a ReST compatible table      Parameters     ----------     obj : OpenPNM object         Any object that has a ``models`` attribute      params : boolean         Indicates whether or not to include a list of parameter         values in the table.  Set to False for just a list of models, and         True for a more verbose table with all parameter values.
r"""     Return the respective lengths of the conduit components defined by the throat     conns P1 T P2     mode = 'pore' - uses pore coordinates     mode = 'centroid' uses pore and throat centroids
r"""         This calculates the effective diffusivity in this linear transport         algorithm.          Parameters         ----------         inlets : array_like             The pores where the inlet composition boundary conditions were             applied.  If not given an attempt is made to infer them from the             algorithm.          outlets : array_like             The pores where the outlet composition boundary conditions were             applied.  If not given an attempt is made to infer them from the             algorithm.          domain_area : scalar, optional             The area of the inlet (and outlet) boundary faces.  If not given             then an attempt is made to estimate it, but it is usually             underestimated.          domain_length : scalar, optional             The length of the domain between the inlet and outlet boundary             faces.  If not given then an attempt is made to estimate it, but it             is usually underestimated.          Notes         -----         The area and length of the domain are found using the bounding box         around the inlet and outlet pores which do not necessarily lie on the         edge of the domain, resulting in underestimation of sizes.
r"""         Loads network data from an iMorph processed image stack          Parameters         ----------         path : string             The path of the folder where the subfiles are held          node_file : string             The file that describes the pores and throats, the             default iMorph name is: throats_cellsThroatsGraph_Nodes.txt          graph_file : string             The file that describes the connectivity of the network, the             default iMorph name is: throats_cellsThroatsGraph.txt          network : OpenPNM Network Object             The OpenPNM Network onto which the data should be loaded.  If no             network is supplied then an empty import network is created and             returned.          voxel_size : float             Allows the user to define a voxel size different than what is             contained in the node_file. The value must be in meters.          return_geometry : Boolean             If True, then all geometrical related properties are removed from             the Network object and added to a GenericGeometry object.  In this             case the method returns a tuple containing (network, geometry). If             False (default) then the returned Network will contain all             properties that were in the original file.  In this case, the user             can call the ```split_geometry``` method explicitly to perform the             separation.          Returns         -------         If no Network object is supplied then one will be created and returned.          If return_geometry is True, then a tuple is returned containing both         the network and a geometry object.
r"""         Adds associations between an object and its boss object at the         given pore and/or throat locations.          Parameters         ----------         pores and throats : array_like             The pore and/or throat locations for which the association should             be added.  These indices are for the full domain.          Notes         -----         For *Physics* objects, the boss is the *Phase* with which it was         assigned, while for *Geometry* objects the boss is the *Network*.
r"""         Removes association between an objectx and its boss object at the         given pore and/or throat locations.          Parameters         ----------         pores and throats : array_like             The pore and/or throat locations from which the association should             be removed.  These indices refer to the full domain.          complete : boolean (default is ``False``)             If ``True`` then *all* pore and throat associations are removed             along with any trace that the objects were associated.          Notes         -----         For *Physics* objects, the boss is the *Phase* with which it was         assigned, while for *Geometry* objects the boss is the *Network*.
r"""         This private method is called by ``set_locations`` and         ``remove_locations`` as needed.
r"""     Calculate throat length assuming point-like pores, i.e. center-to-center     distance between pores. Also, this models assumes that pores and throat     centroids are colinear.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_diameter : string         Dictionary key of the pore diameter values
r"""     Calculate throat length from end points and optionally a centroid      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_endpoints : string         Dictionary key of the throat endpoint values.      throat_centroid : string         Dictionary key of the throat centroid values, optional.      Returns     -------     Lt : ndarray         Array containing throat lengths for the given geometry.      Notes     -----     (1) By default, the model assumes that the centroids of pores and the     connecting throat in each conduit are colinear.      (2) If `throat_centroid` is passed, the model accounts for the extra     length. This could be useful for Voronoi or extracted networks.
r"""     Calculate conduit lengths. A conduit is defined as half pore + throat     + half pore.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_endpoints : string         Dictionary key of the throat endpoint values.      throat_diameter : string         Dictionary key of the throat length values.      throat_length : string (optional)         Dictionary key of the throat length values.  If not given then the         direct distance bewteen the two throat end points is used.      Returns     -------     Dictionary containing conduit lengths, which can be accessed via the dict     keys 'pore1', 'pore2', and 'throat'.
r"""     Calculates the mass density from the molecular weight and molar density      Parameters     ----------     mol_weight : string         The dictionary key containing the molecular weight values      molar_density : string         The dictionary key containing the molar density values
r"""     Calculates density of pure water or seawater at atmospheric pressure     using Eq. (8) given by Sharqawy et. al [1]. Values at temperature higher     than the normal boiling temperature are calculated at the saturation     pressure.      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      temperature : string         The dictionary key containing the temperature values.  Temperature must         be in Kelvin for this emperical equation to work      salinity : string         The dictionary key containing the salinity values.  Salinity must be         expressed in g of salt per kg of solution (ppt).       Returns     -------     The density of water/seawater in [kg/m3]      Notes     -----      T must be in K, and S in g of salt per kg of phase, or ppt (parts per         thousand)     VALIDITY: 273 < T < 453 K; 0 < S < 160 g/kg;     ACCURACY: 0.1 %      References     ----------     [1] Sharqawy M. H., Lienhard J. H., and Zubair, S. M., Desalination and     Water Treatment, 2010.
r"""         This method converts a correctly formatted dictionary into OpenPNM         objects, and returns a handle to the *project* containing them.          Parameters         ----------         dct : dictionary             The Python dictionary containing the data.  The nesting and             labeling of the dictionary is used to create the appropriate             OpenPNM objects.          project : OpenPNM Project Object             The project with which the created objects should be associated.             If not supplied, one will be created.          Returns         -------         An OpenPNM Project containing the objects created to store the given         data.
r"""         Returns a single dictionary object containing data from the given         OpenPNM objects, with the keys organized differently depending on         optional arguments.          Parameters         ----------         network : OpenPNM Network Object (optional)             The network containing the desired data          phases : list of OpenPNM Phase Objects (optional, default is none)             A list of phase objects whose data are to be included          element : string or list of strings             An indication of whether 'pore' and/or 'throat' data are desired.             The default is both.          interleave : boolean (default is ``True``)             When ``True`` (default) the data from all Geometry objects (and             Physics objects if ``phases`` are given) is interleaved into             a single array and stored as a network property (or Phase             property for Physics data). When ``False``, the data for each             object are stored under their own dictionary key, the structuring             of which depends on the value of the ``flatten`` argument.          flatten : boolean (default is ``True``)             When ``True``, all objects are accessible from the top level             of the dictionary.  When ``False`` objects are nested under their             parent object.  If ``interleave`` is ``True`` this argument is             ignored.          categorize_by : string or list of strings             Indicates how the dictionaries should be organized.  The list can             contain any, all or none of the following strings:              **'object'** : If specified the dictionary keys will be stored             under a general level corresponding to their type (e.g.             'network/net_01/pore.all'). If  ``interleave`` is ``True`` then             only the only categories are *network* and *phase*, since             *geometry* and *physics* data get stored under their respective             *network* and *phase*.              **'data'** : If specified the data arrays are additionally             categorized by ``label`` and ``property`` to separate *boolean*             from *numeric* data.              **'element'** : If specified the data arrays are             additionally categorized by ``pore`` and ``throat``, meaning             that the propnames are no longer prepended by a 'pore.' or             'throat.'          Returns         -------         A dictionary with the data stored in a hierarchical data structure, the         actual format of which depends on the arguments to the function.          Notes         -----         There is a handy package called *flatdict* that can be used to         access this dictionary using a single key such that:          ``d[level_1][level_2] == d[level_1/level_2]``          Importantly, converting to a *flatdict* allows it be converted to an         *HDF5* file directly, since the hierarchy is dictated by the placement         of '/' characters.
r"""         Saves data from the given dictionary into the specified file.          Parameters         ----------         dct : dictionary             A dictionary to save to file, presumably obtained from the             ``to_dict`` method of this class.          filename : string or path object             The filename to store the dictionary.
r"""         Load data from the specified file into a Python dictionary          Parameters         ----------         filename : string             The path to the file to be opened          Notes         -----         This returns a Python dictionary which can be converted into OpenPNM         objects using the ``from_dict`` method of this class.
r"""         Used to specify necessary arguments to the simulation.  This method is         useful for resetting the algorithm or applying more explicit control.          Parameters         ----------         phase : OpenPNM Phase object             The Phase object containing the physical properties of the invading             fluid.          access_limited : boolean             If ``True`` the invading phase can only enter the network from the             invasion sites specified with ``set_inlets``.  Otherwise, invading             clusters can appear anywhere in the network.  This second case is             the normal *ordinary percolation* in the traditional sense, while             the first case is more physically representative of invading             fluids.          mode : string             Specifies the type of percolation process to simulate.  Options             are:              **'bond'** - The percolation process is controlled by bond entry             thresholds.              **'site'** - The percolation process is controlled by site entry             thresholds.          pore_entry_pressure : string             The dictionary key on the Phase object where the pore entry             pressure values are stored.  The default is             'pore.capillary_pressure'.  This is only accessed if the ``mode``             is set to site percolation.          throat_entry_pressure : string             The dictionary key on the Phase object where the throat entry             pressure values are stored.  The default is             'throat.capillary_pressure'.  This is only accessed if the ``mode``             is set to bond percolation.          'pore_volume' : string             The dictionary key containing the pore volume information.          'throat_volume' : string             The dictionary key containing the pore volume information.
r"""         Resets the various data arrays on the object back to their original         state. This is useful for repeating a simulation at different inlet         conditions, or invasion points for instance.
r"""         Set the locations from which the invader enters the network          Parameters         ----------         pores : array_like             Locations that are initially filled with invader, from which             clusters grow and invade into the network          overwrite : boolean             If ``True`` then all existing inlet locations will be removed and             then the supplied locations will be added.  If ``False`` (default),             then supplied locations are added to any already existing inlet             locations.
r"""         Specify locations of any residual invader.  These locations are set         to invaded at the start of the simulation.          Parameters         ----------         pores : array_like             The pores locations that are to be filled with invader at the             beginning of the simulation.          throats : array_like             The throat locations that are to be filled with invader at the             beginning of the simulation.          overwrite : boolean             If ``True`` then all existing inlet locations will be removed and             then the supplied locations will be added.  If ``False``, then             supplied locations are added to any already existing locations.
r"""         Find the invasion threshold at which a cluster spans from the inlet to         the outlet sites
r"""         Returns a True or False value to indicate if a percolating cluster         spans between the inlet and outlet pores that were specified at the         given applied pressure.          Parameters         ----------         applied_pressure : scalar, float             The pressure at which percolation should be checked          Returns         -------         A simple boolean True or False if percolation has occured or not.
r"""         Runs the percolation algorithm to determine which pores and throats         will be invaded at each given pressure point.          Parameters         ----------         points: int or array_like             An array containing the pressure points to apply.  If a scalar is             given then an array will be generated with the given number of             points spaced between the lowest and highest values of throat             entry pressures using logarithmic spacing.  To specify low and             high pressure points use the ``start`` and ``stop`` arguments.          start : int             The optional starting point to use when generating pressure points.          stop : int             The optional stopping point to use when generating pressure points.
r"""         Obtain the numerical values of the calculated intrusion curve          Returns         -------         A named-tuple containing arrays of applied capillary pressures and         invading phase saturation.
r"""         Plot the percolation curve as the invader volume or number fraction vs         the applied capillary pressure.
r"""         This method determines which pores and throats are filled with invading         phase at the specified capillary pressure, and creates several arrays         indicating the occupancy status of each pore and throat for the given         pressure.          Parameters         ----------         Pc : scalar             The capillary pressure for which an invading phase configuration             is desired.          Returns         -------         A dictionary containing an assortment of data about distribution         of the invading phase at the specified capillary pressure.  The data         include:          **'pore.occupancy'** : A value between 0 and 1 indicating the         fractional volume of each pore that is invaded.  If no late pore         filling model was applied, then this will only be integer values         (either filled or not).          **'throat.occupancy'** : The same as 'pore.occupancy' but for throats.          This dictionary can be passed directly to the ``update`` method of         the *Phase* object. These values can then be accessed by models         or algorithms.
r'''     Calculates the effective property of a continua using percolation theory      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      volume_fraction : string         The dictionary key in the Phase object containing the volume fraction         of the conducting component      bulk_property : string         The dictionary key in the Phase object containing the intrinsic         property of the conducting component      phi_crit : float         The volume fraction below which percolation does NOT occur      tau : float         The exponent of the percolation relationship      Notes     -----     This model uses the following standard percolation relationship:      .. math::          \sigma_{effective}=\sigma_{bulk}(\phi - \phi_{critical})^\lambda
r"""         This method takes several arguments that are essential to running the         algorithm and adds them to the settings          Parameters         ----------         phase : OpenPNM Phase object             The phase on which the algorithm is to be run. If no value is             given, the existing value is kept.          quantity : string             The name of the physical quantity to be calcualted such as             ``'pore.xxx'``.          conductance : string             The name of the pore-scale transport conductance values. These             are typically calculated by a model attached to a *Physics* object             associated with the given *Phase*. Example; ``'throat.yyy'``.          r_tolerance : scalar             Tolerance to achieve. The solver returns a solution when 'residual'             falls below 'r_tolerance'. The default value is 0.001.          max_iter : scalar             The maximum number of iterations the solver can perform to find             a solution. The default value is 5000.          relaxation_source : scalar, between 0 and 1             A relaxation factor to control under-relaxation of the source term.             Factor approaching 0 : improved stability but slow simulation.             Factor approaching 1 : fast simulation but may be unstable.             Default value is 1 (no under-relaxation).          relaxation_quantity :  scalar, between 0 and 1             A relaxation factor to control under-relaxation for the quantity             solving for.             Factor approaching 0 : improved stability but slow simulation.             Factor approaching 1 : fast simulation but may be unstable.             Default value is 1 (no under-relaxation).          Notes         -----         Under-relaxation is a technique used for improving stability of a         computation, particularly in the presence of highly non-linear terms.         Under-relaxation used here limits the change in a variable from one         iteration to the next. An optimum choice of the relaxation factor is         one that is small enough to ensure stable simulation and large enough         to speed up the computation.
r"""         Applies a given source term to the specified pores          Parameters         ----------         propname : string             The property name of the source term model to be applied          pores : array_like             The pore indices where the source term should be applied          Notes         -----         Source terms cannot be applied in pores where boundary conditions have         already been set. Attempting to do so will result in an error being         raised.
r"""         Apply boundary conditions to specified pores if no source terms are         already assigned to these pores. Otherwise, raise an error.          Parameters         ----------         pores : array_like             The pores where the boundary conditions should be applied          bctype : string             Specifies the type or the name of boundary condition to apply. The             types can be one one of the following:              - *'value'* : Specify the value of the quantity in each location             - *'rate'* : Specify the flow rate into each location          bcvalues : int or array_like             The boundary value to apply, such as concentration or rate.  If             a single value is given, it's assumed to apply to all locations.             Different values can be applied to all pores in the form of an             array of the same length as ``pores``.          mode : string, optional             Controls how the conditions are applied.  Options are:              *'merge'*: (Default) Adds supplied boundary conditions to already             existing conditions.              *'overwrite'*: Deletes all boundary condition on object then add             the given ones          Notes         -----         It is not possible to have multiple boundary conditions for a         specified location in one algorithm. Use ``remove_BCs`` to         clear existing BCs before applying new ones or ``mode='overwrite'``         which removes all existing BC's before applying the new ones.
r         Update physics using the current value of 'quantity'          Notes         -----         The algorithm directly writes the value of 'quantity' into the phase.         This method was implemented relaxing one of the OpenPNM rules of         algorithms not being able to write into phases.
r         Update 'A' and 'b' applying source terms to specified pores          Notes         -----         Applying source terms to 'A' and 'b' is performed after (optionally)         under-relaxing the source term to improve numerical stability. Physics         are also updated before applying source terms to ensure that source         terms values are associated with the current value of 'quantity'.         In the case of a transient simulation, the updates in 'A' and 'b'         also depend on the time scheme.
r"""         Builds the A and b matrices, and calls the solver specified in the         ``settings`` attribute.          Parameters         ----------         x : ND-array             Initial guess of unknown variable
r         Repeatedly updates 'A', 'b', and the solution guess within according         to the applied source term then calls '_solve' to solve the resulting         system of linear equations.         Stops when the residual falls below 'r_tolerance' or when the maximum         number of iterations is reached.          Parameters         ----------         x : ND-array             Initial guess of unknown variable          Returns         -------         x_new : ND-array             Solution array.
r"""     Calculate pore volume from diameter assuming a spherical pore body      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls         the length of the calculated array, and also provides access to other         necessary geometric properties.      pore_diameter : string         The dictionary key of the pore diameter values
r"""         Load data from a 3DMA-Rock extracted network.  This format consists of         two files: 'rockname.np2th' and 'rockname.th2pn'.  They should be         stored together in a folder which is referred to by the path argument.         These files are binary and therefore not human readable.          Parameters         ----------         path : string             The location of the 'np2th' and 'th2np' files. This can be an             absolute path or relative to the current working directory.          network : OpenPNM Network Object             If an Network object is recieved, this method will add new data to             it but NOT overwrite anything that already exists.  This can be             used to append data from different sources.          voxel_size : scalar             The resolution of the image on which 3DMA-Rock was run, in terms of             the linear length of eac voxel. The default is 1.  This is used to             scale the voxel counts to actual dimension. It is recommended that             this value be in SI units [m] to work well with OpenPNM.          project : OpenPNM Project object             A GenericNetwork is created and added to the specified Project.             If no Project is supplied then one will be created and returned.
r"""     Calculate the thermal conductance of conduits in network, where a     conduit is ( 1/2 pore - full throat - 1/2 pore ). See the notes section.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      pore_thermal_conductivity : string         Dictionary key of the pore thermal conductivity values      throat_thermal_conductivity : string         Dictionary key of the throat thermal conductivity values      pore_area : string         Dictionary key of the pore area values      throat_area : string         Dictionary key of the throat area values      conduit_shape_factors : string         Dictionary key of the conduit DIFFUSION shape factor values      Returns     -------     g : ndarray         Array containing thermal conductance values for conduits in the         geometry attached to the given physics object.      Notes     -----     (1) This function requires that all the necessary phase properties already     be calculated.      (2) This function calculates the specified property for the *entire*     network then extracts the values for the appropriate throats at the end.      (3) This function assumes cylindrical throats with constant cross-section     area. Corrections for different shapes and variable cross-section area can     be imposed by passing the proper flow_shape_factor argument.
r"""     Uses Antoine equation [1] to estimate vapor pressure of a pure component      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      A, B, C :  scalars         Antoine vapor pressure coefficients for pure compounds. Since virtually         all Antoine coefficients are reported for units of mmHg and C for         historical reaons, this method assumes these A, B and C values are for         mmHg and C, but converts all properties internally to return Pascals.      temperature : string         The dictionary key containing the phase temperature values in Kelvin         [K].  Can be either pore or throat values.      [1] Antoine, C. (1888), Vapor Pressure: a new relationship between pressure         and temperature, Comptes Rendus des Séances de l'Académie des Sciences         (in French) 107: 681–684, 778–780, 836–837
r"""     Calculates vapor pressure of pure water or seawater given by [1] based on     Raoult's law. The pure water vapor pressure is given by [2]      Parameters     ----------     target : OpenPNM Object         The object for which these values are being calculated.  This         controls the length of the calculated array, and also provides         access to other necessary thermofluid properties.      temperature : string         The dictionary key containing the phase temperature values      salinity : string         The dictionary key containing the phase salinity values      Returns     -------     The vapor pressure of water/seawater in [Pa]      Notes     -----      T must be in K, and S in g of salt per kg of phase, or ppt (parts per         thousand)     VALIDITY: 273 < T < 473 K; 0 < S < 240 g/kg;     ACCURACY: 0.5 %      References     ----------     [1] Sharqawy M. H., Lienhard J. H., and Zubair, S. M., Desalination and     Water Treatment, 2010.     [2] ASHRAE handbook: Fundamentals, ASHRAE; 2005.
r"""     Calculate throat volume assuing a cylindrical shape      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_length and throat_diameter : strings         The dictionary keys containing the arrays with the throat diameter and         length values.      Notes     -----     At present this models does NOT account for the volume reprsented by the     intersection of the throat with a spherical pore body.
r"""     Calculate throat volume assuing a square cross-section      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_length and throat_diameter : strings         The dictionary keys containing the arrays with the throat diameter and         length values.      Notes     -----     At present this models does NOT account for the volume reprsented by the     intersection of the throat with a spherical pore body.
r"""     Calculate throat volume from the throat area and the throat length. This     method is useful for abnormal shaped throats.      Parameters     ----------     target : OpenPNM Object         The object which this model is associated with. This controls the         length of the calculated array, and also provides access to other         necessary properties.      throat_length and throat_area : strings         The dictionary keys containing the arrays with the throat area and         length values.      Notes     -----     At present this models does NOT account for the volume reprsented by the     intersection of the throat with a spherical pore body.
r"""         Creates an HDF5 file containing data from the specified objects,         and categorized according to the given arguments.          Parameters         ----------         network : OpenPNM Network Object             The network containing the desired data          phases : list of OpenPNM Phase Objects (optional, default is none)             A list of phase objects whose data are to be included          element : string or list of strings             An indication of whether 'pore' and/or 'throat' data are desired.             The default is both.          interleave : boolean (default is ``True``)             When ``True`` (default) the data from all Geometry objects (and             Physics objects if ``phases`` are given) is interleaved into             a single array and stored as a network property (or Phase             property for Physics data). When ``False``, the data for each             object are stored under their own dictionary key, the structuring             of which depends on the value of the ``flatten`` argument.          flatten : boolean (default is ``True``)             When ``True``, all objects are accessible from the top level             of the dictionary.  When ``False`` objects are nested under their             parent object.  If ``interleave`` is ``True`` this argument is             ignored.          categorize_by : string or list of strings             Indicates how the dictionaries should be organized.  The list can             contain any, all or none of the following strings:              **'objects'** : If specified the dictionary keys will be stored             under a general level corresponding to their type (e.g.             'network/net_01/pore.all'). If  ``interleave`` is ``True`` then             only the only categories are *network* and *phase*, since             *geometry* and *physics* data get stored under their respective             *network* and *phase*.              **'data'** : If specified the data arrays are additionally             categorized by ``label`` and ``property`` to separate *boolean*             from *numeric* data.              **'elements'** : If specified the data arrays are additionally             categorized by ``pore`` and ``throat``, meaning that the propnames             are no longer prepended by a 'pore.' or 'throat.'
Return a list of items
Returns the backreferences
Returns the type id for the context passed in
Get the suffix of the ID, e.g. '-R01' or '-P05'      The current regex determines a pattern of a single uppercase character with     at most 2 numbers following at the end of the ID as the suffix.
Split off any suffix from ID      This mimics the old behavior of the Sample ID.
Returns the number of partitions of this AR
Returns the number of secondary ARs of this AR
Fetch the config dict from the Bika Setup for the given portal_type
Prepares a dictionary of key->value pairs usable for ID formatting
Slice out a segment of a string, which is splitted on both the wildcards     and the separator passed in, if any
Returns brains which share the same portal_type and ID prefix
Return a list of ids sharing the same portal type and prefix
Make a storage (dict-) key for the number generator
Return the sequence number of the given ID
Returns an Alphanumber that represents the number passed in, expressed     as defined in the template. Otherwise, returns the number
Compute the number for the sequence type "Counter"
Generate a new persistent number with the number generator for the     sequence type "Generated"
Generate pretty content IDs.
Rename the content after it was created/added
Object has been transitioned to an new state
Object has been modified
Object has been created
Return the title if possible, else return the Sample type.         Fall back on the instance's ID if there's no sample type or title.
Return a dictionary with the specification fields for each            service. The keys of the dictionary are the keywords of each            analysis service. Each service contains a dictionary in which            each key is the name of the spec field:            specs['keyword'] = {'min': value,                                'max': value,                                'warnmin': value,                                ... }
Return all sampletypes
Cache Key for object data
Get or create the audit log storage for the given object      :param obj: Content object     :returns: PersistentList
Returns the number of snapsots      :param obj: Content object     :returns: Current snapshots in the storage
Get a snapshot by version      Snapshot versions begin with `0`, because this is the first index of the     storage, which is a list.      :param obj: Content object     :param version: The index position of the snapshot in the storage     :returns: Snapshot at the given index position
Get object schema data      NOTE: We RAM cache this data because it should only change when the object     was modified!      XXX: We need to set at least the modification date when we set fields in     Ajax Listing when we take a snapshot there!      :param obj: Content object     :returns: Dictionary of extracted schema data
Get request header/form data      A typical request behind NGINX looks like this:      {         'CONNECTION_TYPE': 'close',         'CONTENT_LENGTH': '52',         'CONTENT_TYPE': 'application/x-www-form-urlencoded; charset=UTF-8',         'GATEWAY_INTERFACE': 'CGI/1.1',         'HTTP_ACCEPT': 'application/json, text/javascript, */*; q=0.01',         'HTTP_ACCEPT_ENCODING': 'gzip, deflate, br',         'HTTP_ACCEPT_LANGUAGE': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',         'HTTP_COOKIE': '_ga=GA1.2.1058345096.1522506452; ...',         'HTTP_HOST': 'senaite.ridingbytes.com',         'HTTP_ORIGIN': 'https://senaite.ridingbytes.com',         'HTTP_REFERER': 'https://senaite.ridingbytes.com/clients/client-1/H2O-0054',         'HTTP_USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36',         'HTTP_X_FORWARDED_FOR': '93.238.47.95',         'HTTP_X_REAL_IP': '93.238.47.95',         'HTTP_X_REQUESTED_WITH': 'XMLHttpRequest',         'PATH_INFO': '/VirtualHostBase/https/senaite.ridingbytes.com/senaite/VirtualHostRoot//@@API/update',         'PATH_TRANSLATED': '/VirtualHostBase/https/senaite.ridingbytes.com/senaite/VirtualHostRoot/@@API/update',         'QUERY_STRING': '',         'REMOTE_ADDR': '127.0.0.1',         'REQUEST_METHOD': 'POST',         'SCRIPT_NAME': '',         'SERVER_NAME': 'localhost',         'SERVER_PORT': '8081',         'SERVER_PROTOCOL': 'HTTP/1.0',         'SERVER_SOFTWARE': 'Zope/(2.13.28, python 2.7.12, linux2) ZServer/1.1',         'channel.creation_time': 1556086048     }      :param request: Request object     :returns: Dictionary of extracted request header/form data
Get object metadata      :param obj: Content object     :returns: Dictionary of extracted object metadata
Takes a snapshot of the passed in object      :param obj: Content object     :returns: New snapshot
Returns a diff of two given snapshots (dictionaries)      :param snapshot_a: First snapshot     :param snapshot_b: Second snapshot     :param raw: True to compare the raw values, e.g. UIDs     :returns: Dictionary of field/value pairs that differ
Helper to compare the last two snapshots directly
Returns a human-readable diff between two values      :param value_a: First value to compare     :param value_b: Second value to compare     :param raw: True to compare the raw values, e.g. UIDs     :returns a list of diff tuples
Convert the value into a human readable diff string
Returns the title or ID from the given UID
Return the Batch ID if title is not defined
Retrieves the Client for which the current Batch is attached to            Tries to retrieve the Client from the Schema property, but if not            found, searches for linked ARs and retrieve the Client from the            first one. If the Batch has no client, returns None.
Return all batch labels as a display list
Return all the Analysis Requests brains linked to the Batch         kargs are passed directly to the catalog.
Return all the Analysis Requests objects linked to the Batch kargs         are passed directly to the catalog.
Returns the certifications fully valid
Returns if the current instrument is not out for verification, calibration,         out-of-date regards to its certificates and if the latest QC succeed
Returns True if the results of the last batch of QC Analyses         performed against this instrument was within the valid range.          For a given Reference Sample, more than one Reference Analyses assigned         to this same instrument can be performed and the Results Capture Date         might slightly differ amongst them. Thus, this function gets the latest         QC Analysis performed, looks for siblings (through RefAnalysisGroupID)         and if the results for all them are valid, then returns True. If there         is one single Reference Analysis from the group with an out-of-range         result, the function returns False
Add reference analyses to reference
Return the current list of import data interfaces
Overwrite the Script (Python) `displayValue.py` located at            `Products.Archetypes.skins.archetypes` to handle the references            of our Picklist Widget (Methods) gracefully.            This method gets called by the `picklist.pt` template like this:             display python:context.displayValue(vocab, value, widget);"
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Various types need automation on edit.
Controls if the viewlet should be rendered
Called always before render()
Read Shimadzu GCMS-TQ8030 GC/MS/MS analysis results
Parses header lines              Header example:             [Header]             Data File Name,C:\GCMSsolution\Data\October\                     1-16-02249-001_CD_10172016_2.qgd             Output Date,10/18/2016             Output Time,12:04:11 PM
Parses quantitation result lines             Please see samples/GC-MS output.txt             [MS Quantitative Results] section
Read analysis results from an XML string
get the portal with the plone.api
get the counter storage
get the next consecutive number
set a key's value
Overrides jarn.jsi18n.view.jsi18n         See:             https://github.com/collective/jarn.jsi18n/issues/1             https://github.com/collective/jarn.jsi18n/pull/2          :param domain: translation domain         :param language: iso code for language         :return: dict, with the translations for the domain and language
Structure of values dict (dict entry for each analysis/field):              {'ALC': {'ALC': '13.55',                      'DefaultResult': 'ALC',                      'Remarks': ''},              'CO2': {'CO2': '0.66',                      'DefaultResult': 'CO2',                      'Remarks': ''},              'Date': {'Date': '21/11/2013',                       'DefaultResult': 'Date',                       'Remarks': ''},              'Malo': {'DefaultResult': 'Malo',                       'Malo': '0.26',                       'Remarks': ''},              'Meth': {'DefaultResult': 'Meth',                       'Meth': '0.58',              'Rep #': {'DefaultResult': 'Rep #',                       'Remarks': '',                       'Rep #': '1'}             }
It returns the registered stickers in the system.     :return: a DisplayList object
Try convert the MinimumVolume to 'ml' or 'g' so that JS has an         easier time working with it.  If conversion fails, return raw value.
Returns the vocabulary to be used in         AdmittedStickerTemplates.small_default          If the object has saved not AdmittedStickerTemplates.admitted stickers,         this method will return an empty DisplayList. Otherwise it returns         the stickers selected in admitted.          :return: A DisplayList
Set a JSON error object and a status to the response
Handle requests ajax routes
Set recipients to the reports w/o overwriting the old ones          :param reports: list of ARReports         :param recipients: list of name,email strings
Set status to prepublished/published/republished
parse an email to an unicode name, email tuple
Create a new MIME Attachment          The Content-Type: header is build from the maintype and subtype of the         guessed filename mimetype. Additional parameters for this header are         taken from the keyword arguments.
Prepare and send email to the recipients          :param recipients: a list of email or name,email strings         :param subject: the email subject         :param body: the email body         :param attachments: list of email attachments         :returns: True if all emails were sent, else false
Send the email via the MailHost tool
Set a portal status message
Report data to be used in the template
Attachments data
Recipients data to be used in the template
Responsibles data to be used in the template
Portal email
Portal email name
Calculate the total size of the given files
Return the objects from the UIDs given in the request
Return the objects from the UIDs given in the request
Get the object by UID
Return the filesize of the PDF as a float
Return the AR recipients in the same format like the AR Report         expects in the records field `Recipients`
Recalculate the total size of the selected attachments
/@@API/doActionFor: Perform workflow transition on values returned         by jsonapi "read" function.          Required parameters:              - action: The workflow transition to apply to found objects.          Parameters used to locate objects are the same as used for the "read"         method.
/@@API/doActionFor: Perform workflow transition on a list of objects.          required parameters:              - obj_paths: a json encoded list of objects to transition.             - action: the id of the transition
Returns whether the current context from the request is a Worksheet
Return whether the transition "assign" can be performed or not
Return whether the transition "submit" can be performed or not
Return whether the transition "multi_verify" can be performed or not     The transition multi_verify will only take place if multi-verification of     results is enabled.
Return whether the transition "retract" can be performed or not
Return whether the current belongs to superuser roles
Returns whether the current user was the last verifier or not
Returns whether all analyses can be transitioned or not
Returns whether the analysis is submittable or has already been submitted
Returns whether the analysis is verifiable or has already been verified
Checks if the date is beteween a validation period
Returns the days until the instrument returns from validation
Returns the UIDs of all the ancestors (Analysis Requests) this analysis     comes from
Check if contact has user
Vocabulary of available departments
Vocabulary of all departments
Adds a department          :param dep: UID or department object         :returns: True when the department was added
Removes a department          :param dep: UID or department object         :returns: True when the department was removed
Rename Bika Setup to just Setup to avoid naming confusions for new users
Checks if the result for the analysis passed in is out of range and/or     out of shoulders range.              min                                                   max             warn            min                   max             warn     ·········|---------------|=====================|---------------|·········     ----- out-of-range -----><----- in-range ------><----- out-of-range -----              <-- shoulder --><----- in-range ------><-- shoulder -->      :param brain_or_object: A single catalog brain or content object     :param result: Tentative result. If None, use the analysis result     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Tuple of two elements. The first value is `True` if the result is     out of range and `False` if it is in range. The second value is `True` if     the result is out of shoulder range and `False` if it is in shoulder range     :rtype: (bool, bool)
Returns a string representation of the interval defined by the results     range passed in     :param results_range: a dict or a ResultsRangeDict
Returns if 'submit' transition can be applied to the worksheet passed in.     By default, the target state for the 'submit' transition for a worksheet is     'to_be_verified', so this guard returns true if all the analyses assigned     to the worksheet have already been submitted. Those analyses that are in a     non-valid state (cancelled, inactive) are dismissed in the evaluation, but     at least one analysis must be in an active state (and submitted) for this     guard to return True. Otherwise, always returns False.     Note this guard depends entirely on the current status of the children.
Return whether the transition retract can be performed or not to the     worksheet passed in. Since the retract transition from worksheet is a     shortcut to retract transitions from all analyses the worksheet contains,     this guard only returns True if retract transition is allowed for all     analyses the worksheet contains
Returns True if 'verify' transition can be applied to the Worksheet     passed in. This is, returns true if all the analyses assigned     have already been verified. Those analyses that are in an inactive state     (cancelled, inactive) are dismissed, but at least one analysis must be in     an active state (and verified), otherwise always return False.     Note this guard depends entirely on the current status of the children     :returns: true or false
Return whether 'rollback_to_receive' transition can be performed or not
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Return a list of all containers of this type
Set the Dependent Services from the text of the calculation Formula
Recursively calculates all dependencies of this calculation.             The return value is dictionary of dictionaries (of dictionaries...)              {service_UID1:                 {service_UID2:                     {service_UID3: {},                      service_UID4: {},                     },                 },             }              set flat=True to get a simple list of AnalysisService objects
Return a flat list of services who depend on this calculation.          This refers only to services who's Calculation UIDReferenceField have         the value set to point to this calculation.          It has nothing to do with the services referenced in the calculation's         Formula.
Return the globals dictionary for the formula calculation
Get the member object of a module.          :param dotted_name: The dotted name of the module, e.g. 'scipy.special'         :type dotted_name: string         :param member: The name of the member function, e.g. 'gammaincinv'         :type member: string         :returns: member object or None
Removes the reference analysis from the system
Function triggered after a 'retract' transition for the reference     analysis passed in is performed. The reference analysis transitions to     "retracted" state and a new copy of the reference analysis is created
/@@API/update: Update existing object values          Required parameters:              - obj_path: path to the object, relative to plone site root.             - fields: json value, dict: key:value = fieldname:value.          {             runtime: Function running time.             error: true or string(message) if error. false if no error.             success: true or string(message) if success. false if no success.             <fieldname>: <current value>             ...         }          So.          >>> portal = layer['portal']         >>> portal_url = portal.absolute_url()         >>> from plone.app.testing import SITE_OWNER_NAME         >>> from plone.app.testing import SITE_OWNER_PASSWORD          Update a client's existing address:          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/update?", "&".join([         ... "obj_path=/clients/client-1",         ... "title=Test",         ... "PostalAddress={'address': '1 Wendy Way', 'city': 'Johannesburg', 'zip': '9000', 'state': 'Gauteng', 'district': '', 'country':'South Africa'}"         ... ]))         >>> browser.contents         '{..."success": true...}'          quickly check that it saved:          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/read?", "&".join([         ... "id=client-1",         ... "include_fields=PostalAddress",         ... ]))         >>> browser.contents         '{...1 Wendy Way...}'          Try the same with a nonsense fieldname:          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/update?", "&".join([         ... "obj_path=/clients/client-1",         ... "Thing=Fish",         ... ]))         >>> browser.contents         '{...The following request fields were not used: ...Thing...}'          Setting the value of a RefereceField to "" or None (null) should not cause         an error; setting an empty value should clear the field          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/update?", "&".join([         ... "obj_path=/clients/client-1",         ... 'DefaultCategories=',         ... ]))         >>> browser.contents         '{..."success": true...}'
fieldname is required
fieldname is used, remove from list of unused fields
/@@API/update_many: Update existing object values          This is a wrapper around the update method, allowing multiple updates         to be combined into a single request.          required parameters:              - input_values: A json-encoded dictionary.               Each key is an obj_path, and each value is a dictionary               containing key/value pairs to be set on the object.          Return value:          {             runtime: Function running time.             error: true or string(message) if error. false if no error.             success: true or string(message) if success. false if no success.             updates: return values from update         }
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Return a mapping of Analysis Service -> Reference Results
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Applies new properties to the item (Client) that is currently being         rendered as a row in the list          :param obj: client to be rendered as a row in the list         :param item: dict representation of the client, suitable for the list         :param index: current position of the item within the list         :type obj: ATContentType/DexterityContentType         :type item: dict         :type index: int         :return: the dict representation of the item         :rtype: dict
Returns all available instrument interfaces as a list of tuples. Each     tuple is (id_interface, adapter)
Returns whether the instrument interface passed in is for results import
Returns whether the instrument interface passed in is for results export
Returns the instrument interface for the exim_id passed in
Returns the parser to be used by default for the instrument id interface     and results file passed in.
Handle form submission
Check if edit is allowed
Check if manage is allowed
Get the current assigned services of this Worksheet
Get the current assigned services UIDs of this Worksheet
Get the supported services of the reference sample
Create choices for supported services
Create choices for available positions
Return a list of empty slot numbers
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Returns true if the current analysis to be rendered has a slot         assigned for the current layout.          :param obj: analysis to be rendered as a row in the list         :type obj: ATContentType/DexterityContentType         :return: True if the obj has an slot assigned. Otherwise, False.         :rtype: bool
Applies new properties to the item (analysis) that is currently         being rendered as a row in the list.          :param obj: analysis to be rendered as a row in the list         :param item: dict representation of the analysis, suitable for the list         :param index: current position of the item within the list         :type obj: ATContentType/DexterityContentType         :type item: dict         :type index: int         :return: the dict representation of the item         :rtype: dict
Returns an array of dictionaries, each dictionary represents an         analysis row to be rendered in the list. The array returned is sorted         in accordance with the layout positions set for the analyses this         worksheet contains when the analyses were added in the worksheet.          :returns: list of dicts with the items to be rendered in the list
Returns a dict with the positions of each analysis within the         current worksheet in accordance with the current layout. The key of the         dict is the uid of the analysis and the value is an string         representation of the full position of the analysis within the list:              {<analysis_uid>: '<slot_number>:<position_within_slot>',}          :returns: a dictionary with the full position within the worksheet of                   all analyses defined in the current layout.
Returns a list with the position for the analysis_uid passed in         within the current worksheet in accordance with the current layout,         where the first item from the list returned is the slot and the second         is the position of the analysis within the slot.          :param analysis_uid: uid of the analysis the position is requested         :return: the position (slot + position within slot) of the analysis         :rtype: list
Returns a list with the position number of the slots that are         occupied (if empty_uid=False) or are empty (if empty_uid=True)          :param empty_uid: True exclude occupied slots. False excludes empties         :return: sorted list with slot numbers
Append dicts to the items passed in for those slots that don't have         any analysis assigned but the row needs to be rendered still.          :param items: dictionary with the items to be rendered in the list
Generates the header cell for each slot. For each slot, the first         cell displays information about the parent all analyses within that         given slot have in common, such as the AR Id, SampleType, etc.          :param items: dictionary with items to be rendered in the list
Add the key to the item's "skip" list
Generates a slot header (the first cell of the row) for the item          :param item: the item for which the slot header is requested         :return: the html contents to be displayed in the first cell of a slot
Prepare the data for the slot header template
Renders a remarks image icon
Returns searchable data as Description
Compute default member discount if it applies
Returns an array with the number of analyses for the current AR in             different statuses, like follows:                 [verified, total, not_submitted, to_be_verified]
Return all manager info of responsible departments
Return all managers of responsible departments
Returns the earliest due date of the analyses this Analysis Request         contains.
Return True if there is at least one late analysis in this Request
returns "0", "1" or "2" to indicate Printed state.             0 -> Never printed.             1 -> Printed after last publish             2 -> Printed but republished afterwards.
Returns the items to be billed
It computes and returns the analysis service's discount amount         without VAT
It computes the VAT amount from (subtotal-discount.)*VAT/100, but         each analysis has its own VAT!          :returns: the analysis request VAT amount with the discount
It gets the discounted price from analyses and profiles to obtain the         total value with the VAT and the discount applied          :returns: analysis request's total price including VATs and discounts
Issue invoice
Print invoice
Returns the user that verified the whole Analysis Request. Since the         verification is done automatically as soon as all the analyses it         contains are verified, this function returns the user that verified the         last analysis pending.
Returns the ids from users that have verified at least one analysis         from this Analysis Request
Returns the list of lab contacts that have verified at least one         analysis from this Analysis Request
get the UID of the contact associated with the authenticated user
return the QC analyses performed in the worksheet in which, at         least, one sample of this AR is present.          Depending on qctype value, returns the analyses of:              - 'b': all Blank Reference Samples used in related worksheet/s             - 'c': all Control Reference Samples used in related worksheet/s             - 'd': duplicates only for samples contained in this AR          If qctype==None, returns all type of qc analyses mentioned above
Returns the valid result ranges for the analyses this Analysis         Request contains.          By default uses the result ranges defined in the Analysis Specification         set in "Specification" field if any. Values manually set through         `ResultsRange` field for any given analysis keyword have priority over         the result ranges defined in "Specification" field.          :return: A list of dictionaries, where each dictionary defines the             result range to use for any analysis contained in this Analysis             Request for the keyword specified. Each dictionary has, at least,                 the following keys: "keyword", "min", "max"         :rtype: dict
Returns True if the sample of this Analysis Request has to be         collected by the laboratory personnel
Returns a list of the departments assigned to the Analyses         from this Analysis Request
Returns the results interpretation for this Analysis Request            and department. If department not set, returns the results            interpretation tagged as 'General'.          :returns: a dict with the following keys:             {'uid': <department_uid> or 'general', 'richtext': <text/plain>}
Returns a dictionary with the settings for the analysis service that         match with the uid provided.          If there are no settings for the analysis service and         analysis requests:          1. looks for settings in AR's ARTemplate. If found, returns the            settings for the AnalysisService set in the Template         2. If no settings found, looks in AR's ARProfile. If found, returns the            settings for the AnalysisService from the AR Profile. Otherwise,            returns a one entry dictionary with only the key 'uid'
This functions returns the partitions from the analysis request's         analyses.          :returns: a list with the full partition objects
Checks if the analysis service that match with the uid provided must         be hidden in results. If no hidden assignment has been set for the         analysis in this request, returns the visibility set to the analysis         itself.          Raise a TypeError if the uid is empty or None          Raise a ValueError if there is no hidden assignment in this request or         no analysis service found for this uid.
If the Analysis Request has been rejected, returns the user who did the         rejection. If it was not rejected or the current user has not enough         privileges to access to this information, returns None.
Returns the key that will be used to sort the current Analysis         Request based on both its priority and creation date. On ASC sorting,         the oldest item with highest priority will be displayed.         :return: string used for sorting
This method is used as a metacolumn.         Returns a dictionary with the workflow id as key and workflow state as         value.         :returns: {'review_state':'active',...}
Returns the ancestor(s) of this Analysis Request         param all_ancestors: include all ancestors, not only the parent
Returns the descendant Analysis Requests          :param all_descendants: recursively include all descendants
Returns the UIDs of the descendant Analysis Requests          This method is used as metadata
Returns whether all analyses from this Analysis Request are open         (their status is either "assigned" or "unassigned")
Sets a parent analysis request, making the current a partition
Sets the date received to this analysis request and to secondary         analysis requests
Adds an attachment or a list of attachments to the Analysis Request
Handle form submission
Returns a mapping of container -> postition
Custom folderitems for Worksheet ARs
Get a preview of the next number
Returns an integer
Set a number of the number generator
Reset the number from which the next generated sequence start.             If you seed at 100, next seed will be 101
Return the 'bika.lims.dashboard_panels_visibility' values.     :return: A dictionary or None
Initializes the values for panels visibility in registry_records. By     default, only users with LabManager or Manager roles can see the panels.     :param section_name:     :return: An string like: "role1,yes,role2,no,rol3,no"
Return a list of pairs as values that represents the role-permission     view relation for the panel section passed in.     :param section_name: the panels section id.     :return: a list of tuples.
Checks if the user is allowed to see the panel     :param panel: panel ID as string     :param user: a MemberData object     :return: Boolean
Check if the dashboard cookie should exist through bikasetup         configuration.          If it should exist but doesn't exist yet, the function creates it         with all values as default.         If it should exist and already exists, it returns the value.         Otherwise, the function returns None.          :return: a dictionary of strings
Compares whether the 'selection_id' parameter value saved in the         cookie is the same value as the "value" parameter.          :param selection_id: a string as a dashboard_cookie key.         :param value: The value to compare against the value from         dashboard_cookie key.         :return: Boolean.
Checks if the user is the admin or a SiteAdmin user.         :return: Boolean
Gathers the different sections ids and creates a string as first         cookie data.          :return: A dictionary like:             {'analyses':'all','analysisrequest':'all','worksheets':'all'}
Returns a date range (date from, date to) that suits with the passed         in periodicity.          :param periodicity: string that represents the periodicity         :type periodicity: str         :return: A date range         :rtype: [(DateTime, DateTime)]
Returns an array with the sections to be displayed.             Every section is a dictionary with the following structure:                 {'id': <section_identifier>,                  'title': <section_title>,                 'panels': <array of panels>}
Returns the section dictionary related with Analysis             Requests, that contains some informative panels (like             ARs to be verified, ARs to be published, etc.)
Returns the section dictionary related with Worksheets,             that contains some informative panels (like             WS to be verified, WS with results pending, etc.)
Returns an array of dictionaries, where each dictionary contains the         amount of items created at a given date and grouped by review_state,         based on the passed in periodicity.          This is an expensive function that will not be called more than once         every 2 hours (note cache decorator with `time() // (60 * 60 * 2)
This method updates the 'query' dictionary with the criteria stored in         dashboard cookie.          :param query: A dictionary with search criteria.         :param section_name: The dashboard section name         :return: The 'query' dictionary
This function will get the partition info and then it'll write the container and preservation data         to the dictionary 'item'         :param item: a dict which contains the ARTeplate data columns         :param partition: a dict with some partition info         :returns: the item dict with the partition's data
Return a list with the services supported by this reference sample,         those for which there is a valid results range assigned in reference         results         :param only_uids: returns a list of uids or a list of objects         :return: list of uids or AnalysisService objects
return all analyses linked to this reference sample for a service
Creates a new Reference Analysis object based on this Sample         Reference, with the type passed in and associates the newly         created object to the Analysis Service passed in.          :param service: Object, brain or UID of the Analysis Service         :param reference_type: type of ReferenceAnalysis, where 'b' is is         Blank and 'c' is Control         :type reference_type: A String         :returns: the newly created Reference Analysis         :rtype: string
get all services for this Sample
Returns if the current Reference Sample is valid. This is, the sample         hasn't neither been expired nor disposed.
Function triggered before 'unassign' transition takes place
Function triggered before 'unassign' transition takes place
Method triggered after a 'submit' transition for the analysis passed in     is performed. Promotes the submit transition to the Worksheet to which the     analysis belongs to. Note that for the worksheet there is already a guard     that assures the transition to the worksheet will only be performed if all     analyses within the worksheet have already been transitioned.     This function is called automatically by     bika.lims.workfow.AfterTransitionEventHandler
Function triggered after a 'retract' transition for the analysis passed     in is performed. The analysis transitions to "retracted" state and a new     copy of the analysis is created. The copy initial state is "unassigned",     unless the the retracted analysis was assigned to a worksheet. In such case,     the copy is transitioned to 'assigned' state too
Function triggered after the "reject" transition for the analysis passed     in is performed.
Method triggered after a 'verify' transition for the analysis passed in     is performed. Promotes the transition to the Analysis Request and to     Worksheet (if the analysis is assigned to any)     This function is called automatically by     bika.lims.workfow.AfterTransitionEventHandler
Reindex the Analysis Request the analysis belongs to, as well as the     ancestors recursively
Removes the analysis passed in from the worksheet, if assigned to any
Removes the action Quality Control from Reports
Remove old portlets. Leave the Navigation portlet only
Setups the enhanced partitioning system
Adds the indexes for partitioning
Add metadata columns required for partitioning machinery
Removes sample_prep and sample_prep_complete transitions
Rebind calculations of active analyses. The analysis Calculation (an     HistoryAwareField) cannot resolve DependentServices
Reindex Multifiles to be searchable by the catalog
Adds the indexes for partitioning
Remove the view 'Not requested analyses" from inside AR
Returns the catalogs from the site
Add indexes for better worksheet handling
Remove all bika_listing resources
Removes samples views from everywhere, related indexes, etc.
Fixes inconsistencies between analyses and the ARs they belong to when     the AR is in a "cancelled", "invalidated" or "rejected state
Adds getProgressPercentage metadata to worksheets catalog
Removes getDepartmentUIDs indexes and metadata
Applies the system's Sample ID Formatting to Analysis Request
Sets the id formatting in setup for the format provided
Removes stale javascripts
Removes stale CSS
Removes stale indexes and metadata from bika_catalog. Most of these     indexes and metadata were used for Samples, but they are no longer used.
Walks through open worksheets and transition them to 'verified' or     'to_be_verified' if all their analyses are not in an open status
Walks trhough all AR-like partitions registered in the system and     applies the IAnalysisRequestPartition marker interface to them
Add Indexes/Metadata to bika_catalog_analysisrequest_listing
The name of the Setup field was NotifyOnARRetract, so it was     confusing. There was also two fields "NotifyOnRejection"
Sets the default id formatting for secondary ARs
Reindex submitted analyses
Creates the first version of all Calculations that hasn't been yet edited     See: https://github.com/senaite/senaite.core/pull/1260
Moves all existing invoices inside the client and removes the invoices     folder with the invoice batches
Returns a list with the workflow ids bound to the type of the object     passed in
Returns a list with the actions (transitions) supported by the workflows     the object pass in is bound to. Note it returns all actions, not only those     allowed for the object based on its current state and permissions.
Returns a list with the states supported by the workflows the object     passed in is bound to
Applies the review history for objects that are bound to new senaite_*     workflows
Restores the review history for the given brain or object
Returns the review history for the object passed in, but filtered     with the actions and states that match with the workflow currently bound     to the object plus those actions that are None (for initial state)
Fills the review_history_cache dict. The keys are the uids of the objects     to be bound to new workflow and the values are their current review_history
Returns the review history list for the given object. If there is no     review history for the object, it returns a default review history
Creates a new review history for the given object
Resorts client action views
Various types need automation on edit.
Initialize the contents for the audit log
Removes the old Log action from types
Reindex sortable_title from some catalogs
Returns the localized time in string format
Returns the localized time in string format, but in GMT+0
Returns the locale currency symbol
Formats the price with the set decimal mark and correct currency
Formats the max time record to a days, hours, minutes string
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Sort by Categories
Returns a list of SuperModel items
Returns an array with the templates of stickers available.          Each array item is a dictionary with the following structure:              {'id': <template_id>,             'title': <teamplate_title>,             'selected: True/False'}
Returns the id of the sticker template selected.          If no specific template found in the request (parameter template),         returns the default template set in Setup > Stickers.          If the template doesn't exist, uses the default template.          If no template selected but size param, get the sticker template set as         default in Bika Setup for the size set.
Looks for the CSS file from the selected template and return its            contents.          If the selected template is default.pt, looks for a file named         default.css in the stickers path and return its contents. If no CSS         file found, retrns an empty string
Renders the next available sticker.          Uses the template specified in the request ('template' parameter) by         default. If no template defined in the request, uses the default         template set by default in Setup > Stickers.          If the template specified doesn't exist, uses the default bika.lims'         Code_128_1x48mm.pt template (was sticker_small.pt).
Used in stickers_preview.pt
Returns the paths for the directory containing the css and pt files         for the stickers deppending on the filter_by_type.          :param resource_name: The name of the resource folder.         :type resource_name: string         :returns: a string as a path
Returns a pdf stream with the stickers
For the given objects generate as many copies as the desired number         of stickers.          :param items: list of objects whose stickers are going to be previewed.         :type items: list         :returns: list containing n copies of each object in the items list         :rtype: list
Return the copies_count number request parameter          :returns: the number of copies for each sticker as stated         in the request         :rtype: int
Return the list of duplicate analyses that share the same Request and         are included in the same Worksheet as the current analysis. The current         duplicate is excluded from the list.         :param retracted: If false, retracted/rejected siblings are dismissed         :type retracted: bool         :return: list of siblings for this analysis         :rtype: list of IAnalysis
Returns the valid result range for this analysis duplicate, based on         both on the result and duplicate variation set in the original analysis          A Duplicate will be out of range if its result does not match with the         result for the parent analysis plus the duplicate variation in % as the         margin error.         :return: A dictionary with the keys min and max         :rtype: dict
/@@API/getusers: Return users belonging to specified roles          Required parameters:              - roles: The role of which users to return          {             runtime: Function running time.             error: true or string(message) if error. false if no error.             success: true or string(message) if success. false if no success.             users: list of dictionaries: {fullname: fullname, userid: userid}         }                  >>> portal = layer['portal']         >>> portal_url = portal.absolute_url()         >>> from plone.app.testing import SITE_OWNER_NAME         >>> from plone.app.testing import SITE_OWNER_PASSWORD          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/getusers?roles:list=Manager&roles:list=Analyst")         >>> browser.contents         '{...test_labmanager1...}'         >>> browser.contents         '{...test_analyst1...}'                  >>> browser.open(portal_url+"/@@API/getusers")         >>> browser.contents         'No roles specified'
Returns a dictionary with catalogs definitions.
Returns the catalog that stores objects of instance passed in type.     If an object is indexed by more than one catalog, the first match     will be returned.      :param instance: A single object     :type instance: ATContentType     :returns: The first catalog that stores the type of object passed in
Setup the given catalogs. Redefines the map between content types and     catalogs and then checks the indexes and metacolumns, if one index/column     doesn't exist in the catalog_definition any more it will be     removed, otherwise, if a new index/column is found, it will be created.      :param portal: The Plone's Portal object     :param catalogs_definition: a dictionary with the following structure         {             CATALOG_ID: {                 'types':   ['ContentType', ...],                 'indexes': {                     'UID': 'FieldIndex',                     ...                 },                 'columns': [                     'Title',                     ...                 ]             }         }     :type catalogs_definition: dict     :param force_reindex: Force to reindex the catalogs even if there's no need     :type force_reindex: bool     :param force_no_reindex: Force reindexing NOT to happen.     :param catalog_extensions: An extension for the primary catalogs definition         Same dict structure as param catalogs_definition. Allows to add         columns and indexes required by Bika-specific add-ons.     :type catalog_extensions: dict
Merges two dictionaries that represent catalogs definitions. The first     dictionary contains the catalogs structure by default and the second dict     contains additional information. Usually, the former is the Bika LIMS     catalogs definition and the latter is the catalogs definition of an add-on     The structure of each dict as follows:         {             CATALOG_ID: {                 'types':   ['ContentType', ...],                 'indexes': {                     'UID': 'FieldIndex',                     ...                 },                 'columns': [                     'Title',                     ...                 ]             }         }      :param dict1: The dictionary to be used as the main template (defaults)     :type dict1: dict     :param dict2: The dictionary with additional information     :type dict2: dict     :returns: A merged dict with the same structure as the dicts passed in     :rtype: dict
Updates the mapping for content_types against catalogs     :archetype_tool: an archetype_tool object     :catalogs_definition: a dictionary like         {             CATALOG_ID: {                 'types':   ['ContentType', ...],                 'indexes': {                     'UID': 'FieldIndex',                     ...                 },                 'columns': [                     'Title',                     ...                 ]             }         }
Given a catalog definition it updates the indexes, columns and content_type     definitions of the catalog.     :portal: the Plone site object     :catalog_id: a string as the catalog id     :catalog_definition: a dictionary like         {             'types':   ['ContentType', ...],             'indexes': {                 'UID': 'FieldIndex',                 ...             },             'columns': [                 'Title',                 ...             ]         }
This function indexes the index element into the catalog if it isn't yet.     :catalog: a catalog object     :index: an index id as string     :indextype: the type of the index as string     :returns: a boolean as True if the element has been indexed and it returns     False otherwise.
This function adds a metadata column to the acatalog.     :cat: a catalog object     :col: a column id as string     :returns: a boolean as True if the element has been added and         False otherwise
This function desindexes the index element from the catalog.     :catalog: a catalog object     :index: an index id as string     :returns: a boolean as True if the element has been desindexed and it     returns False otherwise.
This function deletes a metadata column of the acatalog.     :cat: a catalog object     :col: a column id as string     :returns: a boolean as True if the element has been removed and         False otherwise
Rebuild the given catalogs.     :portal: the Plone portal object     :cleanrebuild: a list with catalog ids
Download the PDF
Decorator for updating the QuickInstaller of a upgrade
We must build headers carefully: there are multiple blank values         in the header row, and the instrument may just add more for all         we know.
Parses the data line into a dictionary for the importer
Update hook
TODO: Refactor to non-classic mode
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Return a list of dictionaries fit for AnalysisSpecsResultsField            consumption.          If neither hidemin nor hidemax are specified, only services which have         float()able entries in result,min and max field will be included. If         hidemin and/or hidemax specified, results might contain empty min         and/or max fields.
Only call during object initialization, this function sets fields     to schema defaults.  It's adapted from the original to support     IAcquireFieldDefaults adapters.  If IAcquireFieldDefaults adapter     does not find a suitable field, or that field's value is Falseish,     this function will not continue with the normal default machinery.
Promotes the transitiion passed in to ancestors, if any
Cascades the transition passed in to the descendant partitions
Cascades the transition to the analysis request analyses. If all_analyses     is set to True, the transition will be triggered for all analyses of this     analysis request, those from the descendant partitions included.
Returns a Display List with the active Analysis Services             available. The value is the keyword and the title is the             text to be displayed.
Generate a new id for 'portal_type'
return list of all analyses against this sample
Returns the date the retention period ends for this sample based on         the retention period from the Sample Type. If the sample hasn't been         collected yet, returns None
Convenience Classmethod which returns a Contact by a Username
Returns the linked Plone User or None
Link the user to the Contact          :returns: True if OK, False if the User could not be linked         :rtype: bool
Unlink the user to the Contact          :returns: True if OK, False if no User was unlinked         :rtype: bool
Set the UID of the current Contact in the User properties and update         all relevant own properties.
Remove the UID of the current Contact in the User properties and         update all relevant own properties.
Add user to the goup
Remove user from the group
Add local owner role from parent object
Remove local owner role from parent object
Reindex object security after user linking
https://jira.bikalabs.com/browse/LIMS-1818?focusedCommentId=16915&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16915         Only first 4 columns are important:          3/24/2015 7:55 AM BOG 651 (IND) - 16 0.10931925301288605 0.016803081793406938          Date: 3/24/2015 7:55 AM         Sample: BOG 651         Carbon: 0.10931925301288605         Sulphur: 0.016803081793406938
Flags a function as deprecated. A warning will be emitted.      :param comment: A human-friendly string, such as 'This  function                     will be removed soon'     :type comment: string     :param replacement: The function to be used instead     :type replacement: string or function
Returns "view" or "edit" modes, together with the place within where         this field has to be rendered, based on the permissions the current         user has for the context and the field passed in
Get the allowed instruments
Return the registered methods as DisplayList
Redirect with a message
Returns a uids list of the objects this action must be performed         against to. If no values for uids param found in the request, returns         the uid of the current context
Returns a list of uids from the request
Returns the action to be taken from the request. Returns None if no         action is found
Performs the workflow transition passed in and returns the list of         objects that have been successfully transitioned
Redirects the user to success page with informative message
Returns a value from the request's form for the given uid, if any
Returns the versioned references for the given instance
Retrieve the version of the object
Returns the backreferences for the given instance          :returns: list of UIDs
Preprocess the value for set
Link the current version of the target on the source
Unlink the current version of the target from the source
Add a new reference
Remove existing reference
Set (multi-)references
Get (multi-)references
Handle form submission
Return folderitems as brains
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Return WS Templates
Render Analyses Table
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Returns the title or ID from the given UID
Returns the metadata value for the given key
Get the actor's fullname of the snapshot
Fulltext search for the audit metadata
Snapshot created date
Checks if the current date is between a calibration period.
Return the current list of task types
Parse the data line. If an AS was selected it can distinguish between data rows and information rows.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or return the code error -1
Return the current used analyses positions
Person's Fullname
Get the currency Symbol
Get the log view of the requested analysis
Returns the value for the keyword and uid from the request
Convert ans string to a list removing all invalid characters.     Receive: a string as a number
Algorithm to check validity of NBI and NIF.     Receives string with a umber to validate.
Return all objects of the same type from the parent object
Return the objects of the same type from the parent object          :param query: Catalog query to narrow down the objects         :type query: dict         :returns: Content objects of the same portal type in the parent
Create a catalog query for the field
Validates the specs values from request for the service uid. Returns         a non-translated message if the validation failed.
Validates the specs values from request for the service uid. Returns         a non-translated message if the validation failed.
Return the name of the Organisation
Get an address for printing
Creates an object in Bika LIMS      This code uses most of the parts from the TypesTool     see: `Products.CMFCore.TypesTool._constructInstance`      :param container: container     :type container: ATContentType/DexterityContentType/CatalogBrain     :param portal_type: The portal type to create, e.g. "Client"     :type portal_type: string     :param title: The title for the new content object     :type title: string     :returns: The new created object
Get a portal tool by name      :param name: The name of the tool, e.g. `portal_catalog`     :type name: string     :param context: A portal object     :type context: ATContentType/DexterityContentType/CatalogBrain     :returns: Portal Tool
Check if the passed in object is a supported portal content object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: Portal Object     :returns: True if the passed in object is a valid portal content
Get the full content object      :param brain_object_uid: A catalog brain or content object or uid     :type brain_object_uid: PortalObject/ATContentType/DexterityContentType     /CatalogBrain/basestring     :returns: The full object
Checks if the passed in object is folderish      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: True if the object is folderish     :rtype: bool
Get the portal type for this object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Portal type     :rtype: string
Get the schema of the content      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Schema object
Get the list of fields from the object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: List of fields     :rtype: list
Get the Plone ID for this object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Plone ID     :rtype: string
Get the Title for this object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Title     :rtype: string
Get the Title for this object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Title     :rtype: string
Get the absolute URL for this object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Absolute URL     :rtype: string
Find an object by a given UID      :param uid: The UID of the object to find     :type uid: string     :returns: Found Object or None
Query a brain by a given UID      :param uid: The UID of the object to find     :type uid: string     :returns: ZCatalog brain or None
Find an object by a given physical path or absolute_url      :param path: The physical path of the object to find     :type path: string     :returns: Found Object or None
Calculate the physical path of this object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Physical path of the object     :rtype: string
Calculate the physical parent path of this object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Physical path of the parent object     :rtype: string
Locate the parent object of the content/catalog brain      The `catalog_search` switch uses the `portal_catalog` to do a search return     a brain instead of the full parent object. However, if the search returned     no results, it falls back to return the full parent object.      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :param catalog_search: Use a catalog query to find the parent object     :type catalog_search: bool     :returns: parent object     :rtype: ATContentType/DexterityContentType/PloneSite/CatalogBrain
Search for objects.      :param query: A suitable search query.     :type query: dict     :param catalog: A single catalog id or a list of catalog ids     :type catalog: str/list     :returns: Search results     :rtype: List of ZCatalog brains
Return the attribute value      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :param attr: Attribute name     :type attr: str     :returns: Attribute value     :rtype: obj
Get the revision history for the given brain or context.      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Workflow history     :rtype: obj
Get the assigned workflows for the given brain or context.      Note: This function supports also the portal_type as parameter.      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Assigned Workflows     :rtype: tuple
Get the current workflow status of the given brain or context.      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :param state_var: The name of the state variable     :type state_var: string     :returns: Status     :rtype: str
Get the creation date of the brain or object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Creation date     :rtype: DateTime
Get the modification date of the brain or object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Modification date     :rtype: DateTime
Get all registered catalogs for the given portal_type, catalog brain or     content object      :param brain_or_object: The portal_type, a catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: List of supported catalogs     :rtype: list
List available workflow transitions for all workflows      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: All possible available and allowed transitions     :rtype: list[dict]
Performs a workflow transition for the passed in object.      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: The object where the transtion was performed
Get a list of granted roles for the given permission on the object.      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Roles for the given Permission     :rtype: list
Checks if the passed in object is versionable.      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: True if the object is versionable     :rtype: bool
Get the version of the current object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: The current version of the object, or None if not available     :rtype: int or None
Get the view by name      :param name: The name of the view     :type name: str     :param context: The context to query the view     :type context: ATContentType/DexterityContentType/CatalogBrain     :param request: The request to query the view     :type request: HTTPRequest object     :returns: HTTP Request     :rtype: Products.Five.metaclass View object
Return Plone Group      :param group_or_groupname: Plone group or the name of the group     :type groupname:  GroupData/str     :returns: Plone GroupData
Return Plone User      :param user_or_username: Plone user or user id     :returns: Plone MemberData
Return User Properties      :param user_or_username: Plone group identifier     :returns: Plone MemberData
Search Plone users by their roles      :param roles: Plone role name or list of roles     :type roles:  list/str     :returns: List of Plone users having the role(s)
Returns the associated contact of a Plone user      If the user passed in has no contact associated, return None.     The `contact_types` parameter filter the portal types for the search.      :param: Plone user     :contact_types: List with the contact portal types to search     :returns: Contact associated to the Plone user or None
Returns the client of the contact of a Plone user      If the user passed in has no contact or does not belong to any client,     returns None.      :param: Plone user or contact     :returns: Client the contact of the Plone user belongs to
Generate a cache key for a common brain or object      :param brain_or_object: A single catalog brain or content object     :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain     :returns: Cache Key     :rtype: str
Normalize the id      :param string: A string to normalize     :type string: str     :returns: Normalized ID     :rtype: str
Normalize the filename      :param string: A string to normalize     :type string: str     :returns: Normalized ID     :rtype: str
Checks if the passed in uid is a valid UID      :param uid: The uid to check     :param validate: If False, checks if uid is a valid 23 alphanumeric uid. If     True, also verifies if a brain exists for the uid passed in     :type uid: string     :return: True if a valid uid     :rtype: bool
Tries to convert the passed in value to Zope's DateTime      :param value: The value to be converted to a valid DateTime     :type value: str, DateTime or datetime     :return: The DateTime representation of the value passed in or default
Returns the computed total number of minutes
Returns a representation of time in a string in xd yh zm format
Tries to convert the value to int.     Truncates at the decimal point if the value is a float      :param value: The value to be converted to an int     :return: The resulting int or default
Converts the passed in value to a float number      :param value: The value to be converted to a floatable number     :type value: str, float, int     :returns: The float number representation of the passed in value     :rtype: float
Parse the given metadata value to searchable text      :param value: The raw value of the metadata column     :returns: Searchable and translated unicode value or None
Create a Plone DisplayList from list items      :param pairs: list of key, value pairs     :param sort_by: Sort the items either by key or value     :param allow_empty: Allow to select an empty value     :returns: Plone DisplayList
Returns if the current user has rights for the permission passed in          :param permission: permission identifier         :param obj: object to check the permission against         :return: True if the user has rights for the permission passed in
Returns if the analysis passed in can be edited by the current user          :param analysis_brain: Brain that represents an analysis         :return: True if the user can edit the analysis, otherwise False
Checks if the edition of the result field is allowed          :param analysis_brain: Brain that represents an analysis         :return: True if the user can edit the result field, otherwise False
Checks if the edition of the uncertainty field is allowed          :param analysis_brain: Brain that represents an analysis         :return: True if the user can edit the result field, otherwise False
Return if the analysis has a valid instrument.          If the analysis passed in is from ReferenceAnalysis type or does not         have an instrument assigned, returns True          :param analysis_brain: Brain that represents an analysis         :return: True if the instrument assigned is valid or is None
Get the full content object. Returns None if the param passed in is         not a valid, not a valid object or not found          :param brain_or_object_or_uid: UID/Catalog brain/content object         :returns: content object
Returns a vocabulary with all the methods available for the passed in         analysis, either those assigned to an instrument that are capable to         perform the test (option "Allow Entry of Results") and those assigned         manually in the associated Analysis Service.          The vocabulary is a list of dictionaries. Each dictionary has the         following structure:              {'ResultValue': <method_UID>,              'ResultText': <method_Title>}          :param analysis_brain: A single Analysis brain         :type analysis_brain: CatalogBrain         :returns: A list of dicts
Returns a vocabulary with the valid and active instruments available         for the analysis passed in.          If the option "Allow instrument entry of results" for the Analysis         is disabled, the function returns an empty vocabulary.          If the analysis passed in is a Reference Analysis (Blank or Control),         the vocabulary, the invalid instruments will be included in the         vocabulary too.          The vocabulary is a list of dictionaries. Each dictionary has the         following structure:              {'ResultValue': <instrument_UID>,              'ResultText': <instrument_Title>}          :param analysis_brain: A single Analysis or ReferenceAnalysis         :type analysis_brain: Analysis or.ReferenceAnalysis         :return: A vocabulary with the instruments for the analysis         :rtype: A list of dicts: [{'ResultValue':UID, 'ResultText':Title}]
Checks if the passed in Analysis must be displayed in the list.         :param obj: A single Analysis brain or content object         :type obj: ATContentType/CatalogBrain         :returns: True if the item can be added to the list.         :rtype: bool
Prepare a data item for the listing.          :param obj: The catalog brain or content object         :param item: Listing item (dictionary)         :param index: Index of the listing item         :returns: Augmented listing data item
Sets the category to the item passed in          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Sets the suitable css class name(s) to `table_row_class` from the         item passed in, depending on the properties of the analysis object          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Set the analysis' due date to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Set the analysis' result to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Set the analysis' calculation and interims to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Fills the analysis' method to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Fills the analysis' instrument to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Fills the analysis' uncertainty to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Fills the analysis' detection limits to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Set the results range to the item passed in
Set the analysis' verification icons to the item passed in.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Adds an icon to the item dict if the analysis is assigned to a         worksheet and if the icon is suitable for the current context          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Adds an icon to the item dictionary if the analysis has been         automatically generated due to a reflex rule          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Set if the hidden field can be edited (enabled/disabled)          :analysis_brain: Brain that represents an analysis         :item: analysis' dictionary counterpart to be represented as a row
Resolves if field-specific icons must be displayed for the object         passed in.          :param analysis_brain: Brain that represents an analysis
Renders the Remarks field for the passed in analysis          If the edition of the analysis is permitted, adds the field into the         list of editable fields.          :param analysis_brain: Brain that represents an analysis         :param item: analysis' dictionary counterpart that represents a row
Appends an html value after or before the element in the item dict          :param item: dictionary that represents an analysis row         :param element: id of the element the html must be added thereafter         :param html: element to append         :param glue: glue to use for appending         :param after: if the html content must be added after or before
Uses the default Plone sortable_text index lower-case
Returns a sortable title as a mxin of sortkey + lowercase sortable_title
Update hook
Returns a mapping of UID -> setting
Returns a mapping of UID -> configuration
Formats the price with the set decimal mark and correct currency
TODO: Refactor to non-classic mode
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Return a list of dictionaries fit for ARTemplate/Analyses field            consumption.
parse the data
Before template render hook
Called before the listing renders
Applies new properties to the item (Batch) that is currently being         rendered as a row in the list          :param obj: client to be rendered as a row in the list         :param item: dict representation of the batch, suitable for the list         :param index: current position of the item within the list         :type obj: ATContentType/DexterityContentType         :type item: dict         :type index: int         :return: the dict representation of the item         :rtype: dict
Sort the analyses by AR ID ascending and subsorted by priority         sortkey within the AR they belong to
Return the current list of maintenance types
Removes the duplicate from the system
Function triggered after a 'retract' transition for the duplicate passed     in is performed. The duplicate transitions to "retracted" state and a new     copy of the duplicate is created.
Parses header lines              Header example:             Batch Info,2013-03-20T07:11:09.9053262-07:00,2013-03-20T07:12:55.5280967-07:00,2013-03-20T07:11:07.1047817-07:00,,,,,,,,,,,,,,             Batch Data Path,D:\MassHunter\Data\130129\QuantResults\130129LS.batch.bin,,,,,,,,,,,,,,,,             Analysis Time,3/20/2013 7:11 AM,Analyst Name,Administrator,,,,,,,,,,,,,,             Report Time,3/20/2013 7:12 AM,Reporter Name,Administrator,,,,,,,,,,,,,,             Last Calib Update,3/20/2013 7:11 AM,Batch State,Processed,,,,,,,,,,,,,,             ,,,,,,,,,,,,,,,,,
Parses sequence table lines              Sequence Table example:             Sequence Table,,,,,,,,,,,,,,,,,             Data File,Sample Name,Position,Inj Vol,Level,Sample Type,Acq Method File,,,,,,,,,,,             prerunrespchk.d,prerunrespchk,Vial 3,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             DSS_Nist_L1.d,DSS_Nist_L1,P1-A2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             DSS_Nist_L2.d,DSS_Nist_L2,P1-B2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             DSS_Nist_L3.d,DSS_Nist_L3,P1-C2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             UTAK_DS_L1.d,UTAK_DS_L1,P1-D2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             UTAK_DS_L2.d,UTAK_DS_L2,P1-E2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             mid_respchk.d,mid_respchk,Vial 3,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             UTAK_DS_low.d,UTAK_DS_Low,P1-F2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             FDBS_31.d,FDBS_31,P1-G2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             FDBS_32.d,FDBS_32,P1-H2,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             LS_60-r001.d,LS_60,P1-G12,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             LS_60-r002.d,LS_60,P1-G12,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             LS_61-r001.d,LS_61,P1-H12,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             LS_61-r002.d,LS_61,P1-H12,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             post_respchk.d,post_respchk,Vial 3,-1.00,,Sample,120824_VitD_MAPTAD_1D_MRM_practice.m,,,,,,,,,,,             ,,,,,,,,,,,,,,,,,
Parses quantitation result lines              Quantitation results example:             Quantitation Results,,,,,,,,,,,,,,,,,             Target Compound,25-OH D3+PTAD+MA,,,,,,,,,,,,,,,,             Data File,Compound,ISTD,Resp,ISTD Resp,Resp Ratio, Final Conc,Exp Conc,Accuracy,,,,,,,,,             prerunrespchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,5816,274638,0.0212,0.9145,,,,,,,,,,,             DSS_Nist_L1.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,6103,139562,0.0437,1.6912,,,,,,,,,,,             DSS_Nist_L2.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,11339,135726,0.0835,3.0510,,,,,,,,,,,             DSS_Nist_L3.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,15871,141710,0.1120,4.0144,,,,,,,,,,,             mid_respchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,4699,242798,0.0194,0.8514,,,,,,,,,,,             DSS_Nist_L3-r002.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,15659,129490,0.1209,4.3157,,,,,,,,,,,             UTAK_DS_L1-r001.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,29846,132264,0.2257,7.7965,,,,,,,,,,,             UTAK_DS_L1-r002.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,28696,141614,0.2026,7.0387,,,,,,,,,,,             post_respchk.d,25-OH D3+PTAD+MA,25-OH D3d3+PTAD+MA,5022,231748,0.0217,0.9315,,,,,,,,,,,             ,,,,,,,,,,,,,,,,,             Target Compound,25-OH D2+PTAD+MA,,,,,,,,,,,,,,,,             Data File,Compound,ISTD,Resp,ISTD Resp,Resp Ratio, Final Conc,Exp Conc,Accuracy,,,,,,,,,             prerunrespchk.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,6222,274638,0.0227,0.8835,,,,,,,,,,,             DSS_Nist_L1.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,1252,139562,0.0090,0.7909,,,,,,,,,,,             DSS_Nist_L2.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,3937,135726,0.0290,0.9265,,,,,,,,,,,             DSS_Nist_L3.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,826,141710,0.0058,0.7697,,,,,,,,,,,             mid_respchk.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,7864,242798,0.0324,0.9493,,,,,,,,,,,             DSS_Nist_L3-r002.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,853,129490,0.0066,0.7748,,,,,,,,,,,             UTAK_DS_L1-r001.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,127496,132264,0.9639,7.1558,,,,,,,,,,,             UTAK_DS_L1-r002.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,135738,141614,0.9585,7.1201,,,,,,,,,,,             post_respchk.d,25-OH D2+PTAD+MA,25-OH D3d3+PTAD+MA,6567,231748,0.0283,0.9219,,,,,,,,,,,             ,,,,,,,,,,,,,,,,,
Supplies a more detailed view of the Partitions for this         template.  It's built to mimic the partitions that are stored in the         ar_add form state variable, so that when a partition is chosen, there         is no further translation necessary.          It combines the Analyses and Partitions AT schema field values.          For some fields (separate, minvol) there is no information, when partitions         are specified in the AR Template.          :return a list of dictionaries like this:              container                      []             container_titles                      []             preservation                      []             preservation_titles                      []             separate                      false             minvol                      "0.0000 m3 "             services                      ["2fdc040e05bb42ca8b52e41761fdb795", 6 more...]             service_titles                      ["Copper", "Iron", "Magnesium", 4 more...]
A typed in value takes precedence over a selected value.
Search the catalog      Search terms can be passed in the REQUEST or as keyword     arguments.      The used argument is now deprecated and ignored
Possible redirects for an AR.         - If AR is sample_due: receive it before proceeding.         - If AR belongs to Batch, redirect to the BatchBook view.         - If AR does not belong to Batch:             - if permission/workflow permit: go to AR manage_results.         - For other ARs, just redirect to the view screen.
If this sample has a single AR, go there.         If the sample has 0 or >1 ARs, go to the sample's view URL.
Fills the admitted stickers and their default stickers to every sample     type.
Returns a list of Analyses assigned to this AR          Return a list of catalog brains unless `full_objects=True` is passed.         Other keyword arguments are passed to bika_analysis_catalog          :param instance: Analysis Request object         :param kwargs: Keyword arguments to inject in the search query         :returns: A list of Analysis Objects/Catalog Brains
Set/Assign Analyses to this AR          :param items: List of Analysis objects/brains, AnalysisService                       objects/brains and/or Analysis Service uids         :type items: list         :param prices: Mapping of AnalysisService UID -> price         :type prices: dict         :param specs: List of AnalysisService UID -> Result Range mappings         :type specs: list         :param hidden: List of AnalysisService UID -> Hidden mappings         :type hidden: list         :returns: list of new assigned Analyses
Fetch and return analysis service objects
Convert to Analysis Service          :param thing: UID/Catalog Brain/Object/Something         :returns: Analysis Service object or None
Update AR specifications          :param instance: Analysis Request         :param specs: List of Specification Records
Compute AnalysisProfileVATAmount
Computes the final price using the VATAmount and the subtotal price
Removes the service passed in from the services offered by the         current Profile. If the Analysis Service passed in is not assigned to         this Analysis Profile, returns False.         :param service: the service to be removed from this Analysis Profile         :type service: AnalysisService         :return: True if the AnalysisService has been removed successfully
Convert xlsx to easier format first, since we want to use the         convenience of the CSV library
parse the data
parse the data
Parses the data line and builds the dictionary.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or return the code error -1
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Returns a list of sample type titles
For the moment, we're manually trimming the sampletype<>samplepoint             relation to be equal on both sides, here.             It's done strangely, because it may be required to behave strangely.
Before template render hook
Returns an Alphanumber object that represents the number in accordance     with the format specified.     :param number: a number representation used to create the Alphanumber     :param format: the format to use. eg. '2a3d' for 2 chars and 3 digits     :param alphabet: alphabet to use     :type number: int, string, Alphanumber, float     :type format: string     :type alphabet: string
Converts an alphanumeric code (e.g AB12) to an integer     :param alpha_number: representation of an alphanumeric code     :param alphabet: alphabet to use when alpha_number is a non-int string     :type number: int, string, Alphanumber, float     :type alphabet: string
Returns the alphanumeric parts (chars + digits) of this Alphanum
These types will have their Add New... factories dropdown menu removed.
Compute the filesize of the PDF
Augment folder listing item
11/03/2014 14:46:46         PANalytical         Results quantitative - Omnian 2013,          Selected archive:,Omnian 2013         Number of results selected:,4
All lines come to this method.         :param line: a to parse         :returns: the number of rows to jump and parse the next data line or         return the code error -1
Parses the data line and adds to the dictionary.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or         return the code error -1
Adding current values as a Raw Result and Resetting everything.
This function builds the addRawResults dictionary using the header values of the labels section         as sample Ids.
Get all users of the portal
Return the properties of the User
Search Plone users which are not linked to a contact or lab contact
Link an existing user to the current Contact
Set a portal message
Create a new user
This function returns the key used to decide if select_state has to be recomputed
This function returns the key used to decide if method select_analysisservice has to be recomputed
This function returns the key used to decide if method select_analyst has to be recomputed
This function returns the key used to decide if method select_user has to be recomputed
This function returns the key used to decide if method select_daterange has to be recomputed
This function returns the key used to decide if method select_sample_type has to be recomputed
Returns the valid result range for this reference analysis based on         the results ranges defined in the Reference Sample from which this         analysis has been created.          A Reference Analysis (control or blank) will be considered out of range         if its results does not match with the result defined on its parent         Reference Sample, with the % error as the margin of error, that will be         used to set the range's min and max values         :return: A dictionary with the keys min and max         :rtype: dict
Accumulate a list of all AnalysisRequest objects contained in         this Batch, as well as those which are inherited.
TODO: Refactor to non-classic mode
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Return UIDs of the selected services
Render Analyses Services Listing Table
Get the user object      :param user: A user id, memberdata object or None for the current user     :returns: Plone User (PlonePAS) / Propertied User (PluggableAuthService)
Return the group      :param group: The group name/id     :returns: Group
Return the groups of the user      :param user: A user id, memberdata object or None for the current user     :returns: List of groups
Add the user to the group
Walks-through calculations associated to undergoing analyses and     resets the value for DependentServices field
Convert all ReferenceField's values into UIDReferenceFields.      These are not touched: HistoryAware to be removed:       - Analysis.Calculation: HistoryAwareReferenceField (rel=       AnalysisCalculation)       - DuplicateAnalysis.Calculation: HistoryAwareReferenceField (rel=       DuplicateAnalysisCalculation)       - RejectAnalysis.Calculation: HistoryAwareReferenceField (rel=       RejectAnalysisCalculation)      These are not touched: They are deprecated and will be removed:       - AnalysisRequest.Profile: ReferenceField (rel=AnalysisRequestProfile)       - LabContact.Department ReferenceField (rel=LabContactDepartment)      The remaining fields are listed below.
Convert an archetypes reference in src/src_relation to a UIDReference     in dst/fieldname.
Takes a brain or object and returns a valid UID.     In this case, the object may come from portal_archivist, so we will     need to do a catalog query to get the UID of the current version
This function walks over all attachment types and migrates their FileField     fields.
Apparently, some of Analysis Services remained without category after     migration.     Creating a new Category ('unknown') and assigning those AS'es to it.
Returns a list of objects coming from the "uids" request parameter
Returns a list of Samples data (dictionary)
Decorator for dead code removal
Decorator for functions which return JSON
Decorator to return standard content objects as SuperModels
cProfile decorator to profile a function      :param path: output file path     :type path: str     :return: Function
Decorator to log the execution time of a function
Form action enpoint to update the attachments
Form action to add a new attachment in a worksheet
Form action to add a new attachment          Code taken from bika.lims.content.addARAttachment.
Create an Attachment object in the given container
Delete attachment from the AR or Analysis          The attachment will be only deleted if it is not further referenced by         another AR/Analysis.
Get the human readable size of the attachment
Returns a dictionary of attachment information
Returns a list of attachments info dictionaries          Original code taken from bika.lims.analysisrequest.view
Returns a sorted list of analysis info dictionaries
Returns a list of available attachment types
Returns a list of analyses from the AR
Checks if the analysis
Checks if the current logged in user is allowed to add attachments
Checks if the current logged in user is allowed to update attachments
Checks if the current logged in user is allowed to delete attachments
A storage which keeps configuration settings for attachments
Remove the whole storage
Remember the attachments order
Endpoint for attachment delete in WS
Returns a dictionary with the settings for the analysis service that            match with the uid provided.          If there are no settings for the analysis service and template, returns         a dictionary with the key 'uid'
Removes the service passed in from the services offered by the         current Template. If the Analysis Service passed in is not assigned to         this Analysis Template, returns False.         :param service: the service to be removed from this AR Template         :type service: AnalysisService         :return: True if the AnalysisService has been removed successfully
Resolve a UID to an object.      :param context: context is the object containing the field's schema.     :type context: BaseContent     :param value: A UID.     :type value: string     :return: Returns a Content object or None.     :rtype: BaseContent
Return all objects which use a UIDReferenceField to reference context.      :param context: The object which is the target of references.     :param relationship: The relationship name of the UIDReferenceField.     :param as_brains: Requests that this function returns only catalog brains.         as_brains can only be used if a relationship has been specified.      This function can be called with or without specifying a relationship.      - If a relationship is provided, the return value will be a list of items       which reference the context using the provided relationship.        If relationship is provided, then you can request that the backrefs       should be returned as catalog brains.  If you do not specify as_brains,       the raw list of UIDs will be returned.      - If the relationship is not provided, then the entire set of       backreferences to the context object is returned (by reference) as a       dictionary.  This value can then be modified in-place, to edit the stored       backreferences.
Return the configured relationship key or generate a new one
Link the target to the source
Unlink the target from the source
Resolve a UID to an object.          :param context: context is the object containing the field's schema.         :type context: BaseContent         :param value: A UID.         :type value: string         :return: Returns a Content object.         :rtype: BaseContent
Takes a brain or object (or UID), and returns a UID.          :param context: context is the object who's schema contains this field.         :type context: BaseContent         :param value: Brain, object, or UID.         :type value: Any         :return: resolved UID.         :rtype: string
Grab the stored value, and resolve object(s) from UID catalog.          :param context: context is the object who's schema contains this field.         :type context: BaseContent         :param kwargs: kwargs are passed directly to the underlying get.         :type kwargs: dict         :return: object or list of objects for multiValued fields.         :rtype: BaseContent | list[BaseContent]
Grab the stored value, and return it directly as UIDs.          :param context: context is the object who's schema contains this field.         :type context: BaseContent         :param aslist: Forces a single-valued field to return a list type.         :type aslist: bool         :param kwargs: kwargs are passed directly to the underlying get.         :type kwargs: dict         :return: UID or list of UIDs for multiValued fields.         :rtype: string | list[string]
Set the back references on the linked items          This will set an annotation storage on the referenced items which point         to the current context.
Accepts a UID, brain, or an object (or a list of any of these),         and stores a UID or list of UIDS.          :param context: context is the object who's schema contains this field.         :type context: BaseContent         :param value: A UID, brain or object (or a sequence of these).         :type value: Any         :param kwargs: kwargs are passed directly to the underlying get.         :type kwargs: dict         :return: None
Returns whether the field is visible in a given mode
Removing special character from a keyword. Analysis Services must have     this kind of keywords. E.g. if assay name from the Instrument is     'HIV-1 2.0', an AS must be created on Bika with the keyword 'HIV120'
Returns whether the transition activate can be performed for the     analysis service passed in
Returns whether the transition deactivate can be performed for the     analysis service passed in
Called before the listing renders
Before template render hook
Returns the percentage of completeness of the Analysis Request
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Before template render hook
Applies new properties to the item (Client) that is currently being         rendered as a row in the list          :param obj: client to be rendered as a row in the list         :param item: dict representation of the client, suitable for the list         :param index: current position of the item within the list         :type obj: ATContentType/DexterityContentType         :type item: dict         :type index: int         :return: the dict representation of the item         :rtype: dict
Updates Reflex Rules' inactive_state, otherwise they don't have it by     default.     :param portal: Portal object     :return: None
Basic impl for form processing in a widget plus allowing empty         values to be saved
retrieves the value of the same named field on the proxy object
writes the value to the same named field on the proxy object
Since 500i and 1000i print the same results structure (https://jira.bikalabs.com/browse/LIMS-1571), this function     will be overwrote on i1000 importer to save code.     :param instrument_name: a string containing the instrument's name with the format: 'sysmex_xs_500i'     :param request: the request object     :returns: a dictionary with the requests results.
Sysmex XS - 500i analysis results
This is meant for general use and should do everything necessary to     create and initialise an AR and any other required auxilliary objects     (Sample, SamplePartition, Analysis...)     :param client:         The container (Client) in which the ARs will be created.     :param request:         The current Request object.     :param values:         a dict, where keys are AR|Sample schema field names.     :param analyses:         Analysis services list.  If specified, augments the values in         values['Analyses']. May consist of service objects, UIDs, or Keywords.     :param partitions:         A list of dictionaries, if specific partitions are required.  If not         specified, AR's sample is created with a single partition.     :param specifications:         These values augment those found in values['Specifications']     :param prices:         Allow different prices to be set for analyses.  If not set, prices         are read from the associated analysis service.
This function returns a list of UIDs from analyses services from its     parameters.     :param analyses_serv: A list (or one object) of service-related info items.         see _resolve_items_to_service_uids() docstring.     :type analyses_serv: list     :param values: a dict, where keys are AR|Sample schema field names.     :type values: dict     :returns: a list of analyses services UIDs
Returns a list of service uids without duplicates based on the items     :param items:         A list (or one object) of service-related info items. The list can be         heterogeneous and each item can be:         - Analysis Service instance         - Analysis instance         - Analysis Service title         - Analysis Service UID         - Analysis Service Keyword         If an item that doesn't match any of the criterias above is found, the         function will raise a RuntimeError
Notifies via email that a given Analysis Request has been rejected. The     notification is sent to the Client contacts assigned to the Analysis     Request.      :param analysisrequest: Analysis Request to which the notification refers     :returns: true if success
Creates a partition for the analysis_request (primary) passed in     :param analysis_request: uid/brain/object of IAnalysisRequest type     :param request: the current request object     :param analyses: uids/brains/objects of IAnalysis type     :param sampletype: uid/brain/object of SampleType     :param container: uid/brain/object of Container     :param preservation: uid/brain/object of Preservation     :param skip_fields: names of fields to be skipped on copy from primary     :param remove_primary_analyses: removes the analyses from the parent     :return: the new partition
Generates a dictionary with the field values of the object passed in, where     keys are the field names. Skips computed fields
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Maps and updates the permissions for rejected analysis requests so lab clerks, clients, owners and     RegulatoryInspector can see rejected analysis requests on lists.      :return: None
Migrates FileFields fields to blob ones for a given portal_type.     The wueries are done against 'portal_catalog', 'uid_catalog' and     'reference_catalog'      :param context: portal root object as context     :param query: an expression to filter the catalog by other filters than     the portal_type.     :param portal_type: The portal type name the migration is migrating *from*
generate a migrator for the given at-based portal type
Walks around and returns all objects which needs migration         It does exactly the same as the original method, but add some         progress loggers.          :return: objects (with acquisition wrapper) that needs migration         :rtype: generator
It reindexes the modified catalogs but, while cleanAndRebuildCatalogs         recatalogs all objects in the database, this method only reindexes over         the already cataloged objects.          If a metacolumn is added it refreshes the catalog, if only a new index         is added, it reindexes only those new indexes.
Code taken from Products.CMFPlone.WorkflowTool          This version adds some commits and loggins
Check if value is an actual date/time value. If not, attempt         to convert it to one; otherwise, set to None. Assign all         properties passed as kwargs to object.
Sets the worksheet layout, keeping it sorted by position         :param value: the layout to set
Adds a collection of analyses to the Worksheet at once
- add the analysis to self.Analyses().            - position is overruled if a slot for this analysis' parent exists            - if position is None, next available pos is used.
Unassigns the analysis passed in from the worksheet.         Delegates to 'unassign' transition for the analysis passed in
Adds the analysis passed in to the worksheet's layout
Purges the layout of not assigned analyses
This function returns the registered instruments in the system as a         vocabulary. The instruments are filtered by the selected method.
Creates and add reference analyses to the slot by using the         reference sample and service uids passed in.         If no destination slot is defined, the most suitable slot will be used,         typically a new slot at the end of the worksheet will be added.         :param reference: reference sample to which ref analyses belong         :param service_uids: he uid of the services to create analyses from         :param slot: slot where reference analyses must be stored         :return: the list of reference analyses added
Creates a reference analysis in the destination slot (dest_slot) passed         in, by using the reference and service_uid. If the analysis         passed in is not an IReferenceSample or has dependent services, returns         None. If no reference analyses group id (refgid) is set, the value will         be generated automatically.         :param reference: reference sample to create an analysis from         :param service: the service object to create an analysis from         :param slot: slot where the reference analysis must be stored         :param refgid: the reference analyses group id to be set         :return: the reference analysis or None
Returns the next ReferenceAnalysesGroupID for the given reference             sample. Gets the last reference analysis registered in the system             for the specified reference sample and increments in one unit the             suffix.
Creates and add duplicate analyes from the src_slot to the dest_slot         If no destination slot is defined, the most suitable slot will be used,         typically a new slot at the end of the worksheet will be added.         :param src_slot: slot that contains the analyses to duplicate         :param dest_slot: slot where the duplicate analyses must be stored         :return: the list of duplicate analyses added
Creates a duplicate of the src_analysis passed in. If the analysis         passed in is not an IRoutineAnalysis, is retracted or has dependent         services, returns None.If no reference analyses group id (ref_gid) is         set, the value will be generated automatically.         :param src_analysis: analysis to create a duplicate from         :param destination_slot: slot where duplicate analysis must be stored         :param ref_gid: the reference analysis group id to be set         :return: the duplicate analysis or None
Returns the suitable position for a duplicate analysis, taking into         account if there is a WorksheetTemplate assigned to this worksheet.          By default, returns a new slot at the end of the worksheet unless there         is a slot defined for a duplicate of the src_slot in the worksheet         template layout not yet used.          :param src_slot:         :return: suitable slot position for a duplicate of src_slot
Returns the suitable position for reference analyses, taking into         account if there is a WorksheetTemplate assigned to this worksheet.          By default, returns a new slot at the end of the worksheet unless there         is a slot defined for a reference of the same type (blank or control)         in the worksheet template's layout that hasn't been used yet.          :param reference: ReferenceSample the analyses will be created from         :return: suitable slot position for reference analyses
Returns the duplicates from the current worksheet that were created         by using the analysis passed in as the source          :param analysis: routine analyses used as the source for the duplicates         :return: a list of duplicates generated from the analysis passed in
Returns the list of analyses assigned to the slot passed in, sorted by         the positions they have within the slot.          :param slot: the slot where the analyses are located         :type slot: int         :return: a list of analyses
Returns the container object assigned to the slot passed in          :param slot: the slot where the analyses are located         :type slot: int         :return: the container (analysis request, reference sample, etc.)
Returns a list with the slots occupied for the type passed in.          Allowed type of analyses are:              'a'   (routine analysis)             'b'   (blank analysis)             'c'   (control)             'd'   (duplicate)             'all' (all analyses)          :param type: type of the analysis         :return: list of slot positions
Returns the slot where the analyses from the type and container passed         in are located within the worksheet.          :param container: the container in which the analyses are grouped         :param type: type of the analysis         :return: the slot position         :rtype: int
Returns the string used in slots to differentiate amongst analysis         types
Returns the container id used in slots to group analyses
Returns the slot where the instance passed in is located. If not         found, returns None
Returns the available slots from the current worksheet that fits         with the layout defined in the worksheet_template and type of analysis         passed in.          Allowed type of analyses are:              'a' (routine analysis)             'b' (blank analysis)             'c' (control)             'd' (duplicate)          :param worksheet_template: the worksheet template to match against         :param type: type of analyses to restrict that suit with the slots         :return: a list of slots positions
Add routine analyses to worksheet according to the worksheet template         layout passed in w/o overwriting slots that are already filled.          If the template passed in has an instrument assigned, only those         routine analyses that allows the instrument will be added.          If the template passed in has a method assigned, only those routine         analyses that allows the method will be added          :param wst: worksheet template used as the layout         :returns: None
Add duplicate analyses to worksheet according to the worksheet template         layout passed in w/o overwrite slots that are already filled.          If the slot where the duplicate must be located is available, but the         slot where the routine analysis should be found is empty, no duplicate         will be generated for that given slot.          :param wst: worksheet template used as the layout         :returns: None
Returns the reference sample from reference_samples passed in that fits         better with the service uid requirements. This is, the reference sample         that covers most (or all) of the service uids passed in and has less         number of remaining service_uids.          If no reference_samples are set, returns None          If no service_uids are set, returns the first reference_sample          :param reference_samples: list of reference samples         :param service_uids: list of service uids         :return: the reference sample that fits better with the service uids
Resolves the slots and reference samples in accordance with the         Worksheet Template passed in and the type passed in.         Returns a list of dictionaries         :param wst: Worksheet Template that defines the layout         :param type: type of analyses ('b' for blanks, 'c' for controls)         :return: list of dictionaries
Add reference analyses to worksheet according to the worksheet template         layout passed in. Does not overwrite slots that are already filled.         :param wst: worksheet template used as the layout
Add analyses to worksheet according to wst's layout.             Will not overwrite slots which are filled already.             If the selected template has an instrument assigned, it will             only be applied to those analyses for which the instrument             is allowed, the same happens with methods.
get list of analysis services present on this worksheet
Return the Quality Control analyses.         :returns: a list of QC analyses         :rtype: List of ReferenceAnalysis/DuplicateAnalysis
Return the duplicate analyses assigned to the current worksheet         :return: List of DuplicateAnalysis         :rtype: List of IDuplicateAnalysis objects
Return the reference analyses (controls) assigned to the current         worksheet         :return: List of reference analyses         :rtype: List of IReferenceAnalysis objects
Return the analyses assigned to the current worksheet that are directly         associated to an Analysis Request but are not QC analyses. This is all         analyses that implement IRoutineAnalysis         :return: List of regular analyses         :rtype: List of ReferenceAnalysis/DuplicateAnalysis
Returns the number of Quality Control samples.         :returns: number of QC samples         :rtype: integer
Returns the number of regular samples.         :returns: number of regular samples         :rtype: integer
Sets the specified instrument to the Analysis from the             Worksheet. Only sets the instrument if the Analysis             allows it, according to its Analysis Service and Method.             If an analysis has already assigned an instrument, it won't             be overriden.             The Analyses that don't allow the instrument specified will             not be modified.             Returns the number of analyses affected
Sets the specified method to the Analyses from the             Worksheet. Only sets the method if the Analysis             allows to keep the integrity.             If an analysis has already been assigned to a method, it won't             be overriden.             Returns the number of analyses affected.
Returns the name of the currently assigned analyst
Copy real analyses to RejectAnalysis, with link to real            create a new worksheet, with the original analyses, and new            duplicates and references to match the rejected            worksheet.
Checks if the current user has granted access to this worksheet             and if has also privileges for managing it.
Checks if the current user has granted access to this worksheet.             Returns False if the user has no access, otherwise returns True
Returns the analyses UIDs from the analyses assigned to this worksheet         :returns: a list of UIDs         :rtype: a list of strings
Returns the progress percentage of this worksheet
Parses the data line and builds the dictionary.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or return the code error -1
Applies new properties to the item (Client) that is currently being         rendered as a row in the list          :param obj: client to be rendered as a row in the list         :param item: dict representation of the client, suitable for the list         :param index: current position of the item within the list         :type obj: ATContentType/DexterityContentType         :type item: dict         :type index: int         :return: the dict representation of the item         :rtype: dict
Return the best possible title or id of an item, regardless        of whether obj is a catalog brain or an object, but returning an        empty title marker if the id is not set (i.e. it's auto-generated).
Returns an array with the Templates available in the Bika LIMS path         specified plus the templates from the resources directory specified and         available on each additional product (restype).          Each array item is a dictionary with the following structure:             {'id': <template_id>,              'title': <template_title>}          If the template lives outside the bika.lims add-on, both the template_id         and template_title include a prefix that matches with the add-on         identifier. template_title is the same name as the id, but with         whitespaces and without extension.          As an example, for a template from the my.product add-on located in         <restype> resource dir, and with a filename "My_cool_report.pt", the         dictionary will look like:             {'id': 'my.product:My_cool_report.pt',              'title': 'my.product: My cool report'}          :param bikalims_path: the path inside bika lims to find the stickers.         :type bikalims_path: an string as a path         :param restype: the resource directory type to search for inside             an addon.         :type restype: string         :param filter_by_type: the folder name to look for inside the         templates path         :type filter_by_type: string/boolean
Returns an array with the sticker templates available. Retrieves the         TAL templates saved in templates/stickers folder.          Each array item is a dictionary with the following structure:             {'id': <template_id>,              'title': <template_title>}          If the template lives outside the bika.lims add-on, both the template_id         and template_title include a prefix that matches with the add-on         identifier. template_title is the same name as the id, but with         whitespaces and without extension.          As an example, for a template from the my.product add-on located in         templates/stickers, and with a filename "EAN128_default_small.pt", the         dictionary will look like:             {'id': 'my.product:EAN128_default_small.pt',              'title': 'my.product: EAN128 default small'}         If filter by type is given in the request, only the templates under         the path with the type name will be rendered given as vocabulary.         Example: If filter_by_type=='worksheet', only *.tp files under a         folder with this name will be displayed.          :param filter_by_type:         :type filter_by_type: string/bool.         :returns: an array with the sticker templates available
Some special field handling for disabled fields, which don't         get submitted by the browser but still need to be written away.
Returns true if partitions can be created using the analysis request     passed in as the source.
Return whether the transition "submit" can be performed or not.     Returns True if there is at least one analysis in a non-detached state and     all analyses in a non-detached analyses have been submitted.
Returns whether 'prepublish' transition can be perform or not. Returns     True if the analysis request has at least one analysis in 'verified' or in     'to_be_verified' status. Otherwise, return False
Return whether 'rollback_to_receive' transition can be performed or not.     Returns True if the analysis request has at least one analysis in 'assigned'     or 'unassigned' status. Otherwise, returns False
Returns whether 'cancel' transition can be performed or not. Returns     True only if all analyses are in "unassigned" status
Returns whether 'reinstate" transition can be performed or not. Returns     True only if this is not a partition or the parent analysis request can be     reinstated or is not in a cancelled state
Returns whether 'sample' transition can be performed or not. Returns     True only if the analysis request has the DateSampled and Sampler set or if     the user belongs to the Samplers group
Returns `assigned` or `unassigned` depending on the state of the     analyses the analysisrequest contains. Return `unassigned` if the Analysis     Request has at least one analysis in `unassigned` state.     Otherwise, returns `assigned`
Retrieves all the values of metadata columns in the catalog for     wildcard searches     :return: all metadata values joined in a string
simple JSON error handler
Retrieve include_fields values from the request
Load values from the catalog metadata into a list of dictionaries
Load values from an AT object schema fields into a list of dictionaries
Retrieve include_methods values from the request
Search request for keys that match field names in obj,     and call field mutator with request value.      The list of fields for which schema mutators were found     is returned.
/@@API/remove: Remove existing object          Required parameters:              - UID: UID for the object.          {             runtime: Function running time.             error: true or string(message) if error. false if no error.             success: true or string(message) if success. false if no success.         }          So.          >>> portal = layer['portal']         >>> portal_url = portal.absolute_url()         >>> from plone.app.testing import SITE_OWNER_NAME         >>> from plone.app.testing import SITE_OWNER_PASSWORD         >>> blah = portal.portal_catalog(Type = "Contact")[-1]         >>> uid = blah.UID          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/remove?UID="+uid)         >>> browser.contents         '{..."success": true...}'
This function returns the visibility of the widget depending on whether         the rejection workflow is enabled or not.
Return a sorted list with the options defined in bikasetup
'd' is a dictionary with the stored data in the widget like:         {u'selected': [u'a', u'b'], u'checkbox': True, u'other': 'dsadas', u'checkbox_other': True}         Returns a string with the options both from selected and input items
Read Dimensional-CSV analysis results
This function is used to find keywords that are not on the analysis         but keywords that are on the interim fields.          This function and is is_keyword function should probably be in         resultsimport.py or somewhere central where it can be used by other         instrument interfaces.
This function is used to find keywords that are not on the analysis         but keywords that are on the interim fields.          This function and is is_keyword function should probably be in         resultsimport.py or somewhere central where it can be used by other         instrument interfaces.
This function is used to find keywords that are not on the analysis         but keywords that are on the interim fields.          This function and is is_keyword function should probably be in         resultsimport.py or somewhere central where it can be used by other         instrument interfaces.
Parses header lines              Keywords example:             Keyword1, Keyword2, Keyword3, ..., end
Parses result lines
Return whether the transition 'unassign' can be performed or not
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Gets the values from the ReflexRule section and returns them in the         correct way to be saved.         So the function will return a list of dictionaries:          [...,{             'conditions':[{                 'range1': 'X', 'range0': 'X',                 'cond_row_idx':'X'                 'and_or': 'and',                 'analysisservice': '<as_uid>',                 }, {...}],             'trigger': 'xxx',             'actions':[{'action':'<action_name>', 'act_row_idx':'X',                             'otherWS':'to_another', 'analyst': '<analyst_id>',                             'an_result_id':'rep-1',...},                     ]         },         {             'conditions':[{                 'range1': 'X', 'range0': 'X',                 'trigger': 'xxx',                 'cond_row_idx':'X'                 'and_or': 'and',                 'analysisservice': '<as_uid>',                 },                 {                 'discreteresult': 'X',                 'trigger': 'xxx',                 'cond_row_idx':'X'                 'and_or': 'and',                 'analysisservice': '<as_uid>',                 }, {...}],             'trigger': 'xxx',             'actions':[{'action':'<action_name>', 'act_row_idx':'X',                         'otherWS':to_another, 'analyst': '<analyst_id>',                         'an_result_id':'rep-1',...},                       {'action':'<action_name>', 'act_row_idx':'X',                         'otherWS':to_another, 'analyst': '<analyst_id>',                         'an_result_id':'rep-2'...},                 ]         }, ...]
This function gets a set of actions and conditionswith the following         format:           {'action-0': 'repeat',           'action-1': 'repeat',           'analysisservice-0': '30cd952b0bb04a05ac27b70ada7feab2',           'analysisservice-1': '30cd952b0bb04a05ac27b70ada7feab2',           'and_or-0': 'and',           'and_or-1': 'no',           'range0-0': '12',           'range0-1': '31',           'range1-0': '12',           'range1-1': '33',           'worksheettemplate-0': '70d48adfb34c4231a145f76a858e94cf',           'setresulton-0': 'original',           'setresulton-1': 'original',           'trigger': 'submit',           'value': '',           'an_result_id-0':'rep-1',           'an_result_id-1':'rep-2'}          and returns a formatted set with the conditions and actions sorted         like this one:         {         'conditions':[{             'range1': 'X', 'range0': 'X',             'cond_row_idx':'X'             'and_or': 'and',             'analysisservice': '<as_uid>',             },             {             'range1': 'X', 'range0': 'X',             'cond_row_idx':'X'             'and_or': 'and',             'analysisservice': '<as_uid>',             }, {...}],         'trigger': 'xxx',         'actions':[             {'action':'duplicate', 'act_row_idx':'0',                 'otherWS': to_another, 'analyst': 'sussan1',                 'setresultdiscrete': '1', 'setresultvalue': '2',                 'worksheettemplate-0': '70d48adfb34c4231a145f76a858e94cf',                 'setresulton': 'original','an_result_id-0':'rep-1'},             {'action':'repeat', 'act_row_idx':'1',                 'otherWS': current, 'analyst': '', ...},         ]         }
This returns a list of dictionaries with the conditions got in the         raw_set.         :raw_set: is the dict representing a set of rules and conditions.
This function returns only the elements starting with         'analysisservice-' in 'keys_list'. The returned list is sorted by the         index appended to the end of each element
This returns a list of dictionaries with the actions got in the         raw_set.         :raw_set: is the dict representing a set of rules and conditions.
This function returns only the elements starting with 'action-' in         'keys_list'. The returned list is sorted by the index appended to         the end of each element
Return a json dict with all the setup data necessary to build the         relations:         - Relations between methods and analysis services options.         - The current saved data         the functions returns:         {'<method_uid>': {             'analysisservices': {                 '<as_uid>': {'as_id': '<as_id>',                             'as_title':'<as_title>',                             'resultoptions': [,,]}                 '<as_uid>': {'as_id': '<as_id>',                             'as_title':'<as_title>',                             'resultoptions': [{                                 'ResultText': 'Failed',                                 'ResultValue': '1', 'value': ''},                                 ...                             ]}             },           'as_keys': ['<as_uid>', '<as_uid>'],           'method_id': '<method_id>',           'method_tile': '<method_tile>'           },         '<method_uid>': {             'analysisservices': {                 '<as_uid>': {'as_id': '<as_id>',                             'as_title':'<as_title>',                             'resultoptions': [,,]}                '<as_uid>': {'as_id': '<as_id>',                             'as_title':'<as_title>',                             'resultoptions': [,,]}             },           'as_keys': ['<as_uid>', '<as_uid>'],           'method_id': '<method_id>',           'method_tile': '<method_tile>'           },          'saved_actions': {'rules': [                     {'actions': [{'act_row_idx': 0,                                    'action': 'repeat',                                    'an_result_id': '',                                    'analyst': '',                                    'otherWS': current,                                    'setresultdiscrete': '',                                    'setresulton': 'original',                                    'setresultvalue': '',                                    'worksheettemplate': '70d48adfb34c4231a145f76a858e94cf',}],                       'conditions': [{'analysisservice': 'd802cdbf1f4742c094d45997b1038f9c',                                       'and_or': 'no',                                       'cond_row_idx': 0,                                       'discreteresult': '',                                       'range0': '12',                                       'range1': '12'}],                       'rulenumber': '1',                       'trigger': 'submit'},...],                    'method_id': '<method_uid>',                    'method_tile': '<method_tile>',                    'method_uid': '<method_uid>'                    }         }
Returns the expected value saved in the object.         :idx: it is an integer with the position of the reflex rules set in the         widget's list.         :element: a string with the name of the element to obtain:             'actions', 'trigger', 'conditions',          The widget is going to return a list like this:         [             {'conditions': [{'analysisservice': '<as-id>',                             'and_or': 'no',                             'cond_row_idx': 0,                             'discreteresult': '',                             'range0': '12',                             'range1': '12'}}]             'trigger': 'xxx',             'actions':[{'action':'<action_name>', 'act_row_idx':'X',                         'otherWS': 'to_another', 'analyst': '<analyst_id>',...},                       {'action':'<action_name>', 'act_row_idx':'X',                         'otherWS': 'to_another', 'analyst': '<analyst_id>'}                 ]             }, ...]         - The list is the abstraction of the rules section in a Reflex         Rule obj.         - Each dictionary inside the list is an abstraction of a set of         conditions and actions to be done if the conditions are met.         - The 'action' element from the dictionary is a list (its order is         important) with dictionaries where each dict represents a simple         action.         - The 'conditions' element from the dictionary is a list (its order is         important) with dictionaries where each dict represents a condition.         - act_row_idx: it is used to know the position numeber of the action         inside the list.         - cond_row_idx: it is used to know the position numeber of the         condition inside the list.
Returns the expected value saved in the action list object.         :set_idx: it is an integer with the position of the reflex rules set         in the widget's list.         :row_idx: is an integer with the numer of the row from the set         :element: a string with the name of the element of the action to             obtain: 'action', 'act_row_idx', 'otherWS', 'analyst'
Returns the expected value saved in the action list object.         :set_idx: it is an integer with the position of the reflex rules set         in the widget's list.         :row_idx: is an integer with the numer of the row from the set         :element: a string with the name of the element of the action to             obtain: 'analysisservice', 'cond_row_idx', 'range0', 'range1',                     'discreteresult', and_or
Event fired when BikaSetup object gets modified.         Since Sampling Round is a dexterity object we have to change the ID by "hand"         Then we have to redirect the user to the ar add form
Update hook
Returns a mapping of UID -> configuration
TODO: Refactor to non-classic mode
Return UIDs of the selected services for the AnalysisProfile reference field
Converts the given date to a localized time string
Formats the price with the set decimal mark and currency
Return a list of billable items
Custom setter method to calculate a `ValidTo` date based on         the `ValidFrom` and `ExpirationInterval` field values.
Vocabulary of date intervals to calculate the "To" field date based         from the "From" field date.
Returns if the current certificate is in a valid date range
Returns the days until this certificate expires          :returns: Days until the certificate expires         :rtype: int
Returns a mapping of id->workflow
Update the role mappings of the given object
Fix client permissions
Re-set the state of an AR, Sample and SamplePartition to match the     least-early state of all contained valid/current analyses. Ignores     retracted/rejected/cancelled analyses.
Removing sample preparation workflows from the system may have     left some samples ARs and Analyses in the state 'sample_prep'.  These     should be transitioned to 'sample_due'  so that they can be receieved     normally.     :param portal: portal object     :return: None
Method triggered before "sample" transition for the Analysis Request     passed in is performed
Method triggered after a 'reinstate' transition for the Analysis Request     passed in is performed. Sets its status to the last status before it was     cancelled. Reinstates the descendant partitions and all the analyses     associated to the analysis request as well.
Method triggered after "receive" transition for the Analysis Request     passed in is performed
Method triggered after "sample" transition for the Analysis Request     passed in is performed
Used to populate catalog values.         Returns the Title of the client for this analysis' AR.
Used to populate catalog values.         Returns the UID of the client for this analysis' AR.
This method is used to populate catalog values         Returns the URL of the client for this analysis' AR.
Used to populate catalog values.         Returns the date the Analysis Request this analysis belongs to was         received. If the analysis was created after, then returns the date         the analysis was created.
Used to populate getDueDate index and metadata.         This calculates the difference between the time the analysis processing         started and the maximum turnaround time. If the analysis has no         turnaround time set or is not yet ready for proces, returns None
Returns the valid result range for this routine analysis based on the         results ranges defined in the Analysis Request this routine analysis is         assigned to.          A routine analysis will be considered out of range if it result falls         out of the range defined in "min" and "max". If there are values set for         "warn_min" and "warn_max", these are used to compute the shoulders in         both ends of the range. Thus, an analysis can be out of range, but be         within shoulders still.         :return: A dictionary with keys "min", "max", "warn_min" and "warn_max"         :rtype: dict
Returns a list of siblings who depend on us to calculate their result.         :param retracted: If false, retracted/rejected dependents are dismissed         :type retracted: bool         :return: Analyses the current analysis depends on         :rtype: list of IAnalysis
Return a list of siblings who we depend on to calculate our result.         :param retracted: If false retracted/rejected dependencies are dismissed         :type retracted: bool         :return: Analyses the current analysis depends on         :rtype: list of IAnalysis
Returns the key that will be used to sort the current Analysis, from         most prioritary to less prioritary.         :return: string used for sorting
Returns whether if the analysis must be displayed in results         reports or not, as well as in analyses view when the user logged in         is a Client Contact.          If the value for the field HiddenManually is set to False, this function         will delegate the action to the method getAnalysisServiceSettings() from         the Analysis Request.          If the value for the field HiddenManually is set to True, this function         will return the value of the field Hidden.         :return: true or false         :rtype: bool
Sets if this analysis must be displayed or not in results report and         in manage analyses view if the user is a lab contact as well.          The value set by using this field will have priority over the visibility         criteria set at Analysis Request, Template or Profile levels (see         field AnalysisServiceSettings from Analysis Request. To achieve this         behavior, this setter also sets the value to HiddenManually to true.         :param hidden: true if the analysis must be hidden in report         :type hidden: bool
Sets the analysis that has been reflexed in order to create this         one, but if the analysis is the same as self, do nothing.         :param analysis: an analysis object or UID
This function adds a new item to the string field         ReflexRuleActionsTriggered. From the field: Reflex rule triggered         actions from which the current analysis is responsible of. Separated         by '|'         :param text: is a str object with the format '<UID>.<rulename>' ->         '123354.1'
This function does all the reflex rule process.         :param wf_action: is a string containing the workflow action triggered
compute total price
Compute VATAmount
get the UID of the contact associated with the authenticated             user
Compute Subtotal
Compute TotalPrice
return the uids of the products referenced by order items
Return a UID so that ReferenceField understands.
Return the field names to get values for
Returns a dict with the column values for the given brain
Returns the list of brains that match with the request criteria
Returns a list of dictionaries representing the values of each brain
Returns the json payload
Delete attachments where the Analysis was removed        https://github.com/senaite/senaite.core/issues/1025
https://github.com/senaite/senaite.core/issues/957 Reindex bika_setup     objects located in clients to give proper permissions to client contacts.
Migrate Attachments with the report option "a" (attach in report)        to the option to "i" (ignore in report)
Renames retract_ar transition to invalidate
Rebind the ARs automatically generated because of the retraction of their     parent to the new field 'Invalidated'. The field used until now     'ParentAnalysisRequest' will be used for partitioning
Recatalog the index and metadata field 'getDueDate'
Adds the permission 'Reject Analysis Request' and update the permission      mappings accordingly
Removes the metadata getLate from ar-catalog and adds the column     getDueDate
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Stores the analyses to be removed after partitions creation
Remove analyses relocated to partitions
Returns a list of AR data
Returns a list of SampleType data
Returns a list of Container data
Returns a list of Preservation data
Returns a list of objects coming from the "uids" request parameter
Returns the available SampleTypes of the system
Returns the available Containers of the system
Return the Analysis data for this AR
Return the Template data for this AR
Return the number of selected partitions
Extract the base info from the given object
return a list of all categories with accredited services
Before template render hook
Applies new properties to the item (Client) that is currently being         rendered as a row in the list          :param obj: client to be rendered as a row in the list         :param item: dict representation of the client, suitable for the list         :param index: current position of the item within the list         :type obj: ATContentType/DexterityContentType         :type item: dict         :type index: int         :return: the dict representation of the item         :rtype: dict
ARReport objects associated to the current Analysis request.         If the user is not a Manager or LabManager or Client, no items are         displayed.
Applies the filter by client to the search query
Applies the filter by Requested date to the search query
Applies the filter by review_state to the search query
Beckman Coulter Access 2 analysis results
Parses the data line and builds the dictionary.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or return the code error -1
Return the UID of the client
Lookup linked Analysis Requests          :returns: sorted list of ARs, where the latest AR comes first
Lookup linked Analyses          :returns: sorted list of ANs, where the latest AN comes first
Return a title for texts and listings
Return the primary AR this attachment is linked
Return the primary analysis this attachment is linked
Instruments capable to perform this method
Returns a list of roles for the current user
Configure a list of product versions from portal.quickinstaller
Return all the Analysis Request brains linked to the Sampling Round
This functions builds a list of tuples with the object AnalysisRequestTemplates' uids and names.         :returns: A list of tuples where the first value of the tuple is the AnalysisRequestTemplate name and the         second one is the AnalysisRequestTemplate UID. --> [(ART.title),(ART.UID),...]
Returns a dict with the department infomration         {'uid':'xxxx','id':'xxxx','title':'xxx','url':'xxx'}
Returns a dict with the SRTemplate infomration         {'uid':'xxxx','id':'xxxx','title':'xxx','url':'xxx'}
Returns info from the Client contact who coordinates with the lab
Checks if the current user has privileges to access to the editing view.         From Jira LIMS-1549:            - Creation/Edit: Lab manager, Client Contact, Lab Clerk, Client Contact (for Client-specific SRTs)         :returns: True/False
When the round is cancelled, all its associated Samples and ARs are cancelled by the system.
Return the current view url including request parameters
Returns the configured currency
Return the ar_count request paramteter
Create a temporary AR to fetch the fields from
Generate a new fieldname with a '-<arnum>' suffix
Returns a mapping of count -> specification
Get the field widget of the AR in column <arnum>          :param fieldname: The base fieldname         :type fieldname: string
Returns a mapping of UID index -> AR object
Get the default value of the field
Get the stored value of the field
Returns the Client
Returns the Batch
Returns the parent AR
Returns a mapping of '<fieldname>-<count>' to the default value         of the field or the field value of the source AR
Logic refactored from JavaScript:          * If client only has one contact, and the analysis request comes from         * a client, then Auto-complete first Contact field.         * If client only has one contect, and the analysis request comes from         * a batch, then Auto-complete all Contact field.          :returns: The default contact for the AR         :rtype: Client object or None
Check if the field is visible
Return the AR fields with the current visibility
Return all service categories in the right order          :param restricted: Client settings restrict categories         :type restricted: bool         :returns: Category catalog results         :rtype: brains
Return all Services          :param poc: Point of capture (lab/field)         :type poc: string         :returns: Mapping of category -> list of services         :rtype: dict
Return the service from the analysis
Checks if the given service is selected by one of the ARs.         This is used to make the whole line visible or not.
Return the sorted fields
Return the fields with visibility
Return the ISO representation of a date object
Returns a list of AR records          Fields coming from `request.form` have a number prefix, e.g. Contact-0.         Fields with the same suffix number are grouped together in a record.         Each record represents the data for one column in the AR Add form and         contains a mapping of the fieldName (w/o prefix) -> value.          Example:         [{"Contact": "Rita Mohale", ...}, {Contact: "Neil Standard"} ...]
Returns a list of parsed UIDs from a single form field identified by         the given key.          A form field ending with `_uid` can contain an empty value, a         single UID or multiple UIDs separated by a comma.          This method parses the UID value and returns a list of non-empty UIDs.
Returns a mapping of UID -> object
Returns the base info of an object
Returns the client info of an object
Returns the client info of an object
Returns the info for a Service
Returns the info for a Template
Returns the info for a Profile
Returns the info for a Method
Returns the info for a Calculation
Returns the info for a Sample Type
Returns the info for a Sample
Returns the info for a Specification
Returns the info for a Container
Returns the info for a Preservation
Returns the services information
Recalculate all AR records and dependencies              - samples             - templates             - profiles             - services             - dependecies          XXX: This function has grown too much and needs refactoring!
Recalculate prices for all ARs
Submit & create the ARs
If a line has only one column, then it is a Section or Subsection     header.     :param line: Line to check     :return: boolean -If line is header
Result CSV Line Parser.         :param line: a to parse         :returns: the number of rows to jump and parse the next data line or         return the code error -1
A function for lines with only ONE COLUMN.         If line has only one column, then it is either a Section or         Subsection name.
Parses the data line and adds to the dictionary.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or         return the code error -1
Adding current values as a Raw Result and Resetting everything.         Notice that we are not calculating final result of assay. We just set         NP and GP values and in Bika, AS will have a Calculation to generate         final result based on NP and GP values.
Removing special character from a keyword. Analysis Services must have         this kind of keywords. E.g. if assay name from GeneXpert Instrument is         'Ebola RUO', an AS must be created on Bika with the keyword 'EbolaRUO'
Return the current instrument title
Check if analyst assignment is allowed
Returns a dictionary with the analyses services from the current         worksheet which have at least one interim with 'Wide' attribute set to         true and that have not been yet submitted          The structure of the returned dictionary is the following:         <Analysis_keyword>: {             'analysis': <Analysis_name>,             'keyword': <Analysis_keyword>,             'interims': {                 <Interim_keyword>: {                     'value': <Interim_default_value>,                     'keyword': <Interim_key>,                     'title': <Interim_title>                 }             }         }
Checks the validity of the instruments used in the Analyses If an         analysis with an invalid instrument (out-of-date or with calibration         tests failed) is found, a warn message will be displayed.
Parses header lines              Analysis Time	7/13/2017 9:55             Analyst Name	MassHunter01\Agilent             Batch Data Path	D:\MassHunter\GCMS\Terpenes\2017\July\20170711\                     QuantResults\20170711 Sample Workup             Batch Name	20170711 Sample Workup             Batch State	Processed             Calibration Last Updated Time	6/29/2017 15:57             Report Generation Time	1/1/0001 12:00:00 AM             Report Generator Name	None             Report Results Data Path	None             SchemaVersion	65586             Quant Batch Version	B.08.00             Quant Report Version	B.08.00
Parses quantitation result lines             Please see samples/GC-MS output.txt             [MS Quantitative Results] section
Returns a list of editable fields for the given instance
decide if a field is visible in a given mode -> 'state'.
Returns True if the transition is to be SKIPPED          peek - True just checks the value, does not set.         unskip - remove skip key (for manual overrides).      called with only (instance, action_id), this will set the request variable preventing the     cascade's from re-transitioning the object and return None.
Tries to perform the transition to the instance.     Object is reindexed after the transition takes place, but only if succeeds.     If idxs is set, only these indexes will be reindexed. Otherwise, will try     to use the indexes defined in ACTIONS_TO_INDEX mapping if any.     :param instance: Object to be transitioned     :param action_id: transition id     :param idxs: indexes to be reindexed after the transition     :returns: True if the transition has been performed, together with message     :rtype: tuple (bool,str)
Calls the instance's workflow event
This event is executed after each transition and delegates further     actions to 'workflow.<portal_type>.events.after_<transition_id> function     if exists for the instance passed in.     :param instance: the instance that has been transitioned     :type instance: ATContentType     :param event: event that holds the transition performed     :type event: IObjectEvent
Compile a list of possible workflow transitions for this object
Checks if the object can perform the transition passed in.     :returns: True if transition can be performed     :rtype: bool
Returns a list with the transition ids that can be performed against     the instance passed in.     :param instance: A content object     :type instance: ATContentType     :returns: A list of transition/action ids     :rtype: list
Returns a list with the statuses of the instance from the review_history
Returns the previous status of the object. If status is set, returns the     previous status before the object reached the status passed in.     If instance has reached the status passed in more than once, only the last     one is considered.
Returns if the object passed matches with the states passed in
Returns the actor that performed a given transition. If transition has     not been perormed, or current user has no privileges, returns None     :return: the username of the user that performed the transition passed-in     :type: string
Returns date of action for object. Sometimes we need this date in Datetime     format and that's why added return_as_datetime param.
This function returns a list with the users who have done the transition.     :action_id: a sring as the transition id.     :last_user: a boolean to return only the last user triggering the         transition or all of them.     :returns: a list of user ids.
Generic workflow guard handler that returns true if the transition_id     passed in can be performed to the instance passed in.      This function is called automatically by a Script (Python) located at     bika/lims/skins/guard_handler.py, which in turn is fired by Zope when an     expression like "python:here.guard_handler('<transition_id>')" is set to     any given guard (used by default in all bika's DC Workflow guards).      Walks through bika.lims.workflow.<obj_type>.guards and looks for a function     that matches with 'guard_<transition_id>'. If found, calls the function and     returns its value (true or false). If not found, returns True by default.      :param instance: the object for which the transition_id has to be evaluated     :param transition_id: the id of the transition     :type instance: ATContentType     :type transition_id: string     :return: true if the transition can be performed to the passed in instance     :rtype: bool
Loads a python module based on the module relative name passed in.      At first, tries to get the module from sys.modules. If not found there, the     function tries to load it by using importlib. Returns None if no module     found or importlib is unable to load it because of errors.     Eg:         _load_wf_module('sample.events')      will try to load the module 'bika.lims.workflow.sample.events'      :param modrelname: relative name of the module to be loaded     :type modrelname: string     :return: the module     :rtype: module
Push a reindex job to the actions handler pool
Adds an instance into the pool, to be reindexed on resume
Returns if the task for the instance took place successfully
Resumes the pool and reindex all objects processed
Returns the names of the indexes to be reindexed for the object with         the uid passed in. If no indexes for this object have been specified         within the action pool job, returns an empty list (reindex all).         Otherwise, return all the indexes that have been specified for the         object within the action pool job.
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Overrides the workflow actions menu displayed top right in the         object's view. Displays the current state of the object, as well as a         list with the actions that can be performed.         The option "Advanced.." is not displayed and the list is populated with         all allowed transitions for the object.
/@@API/create: Create new object.          Required parameters:              - obj_type = portal_type of new object.             - obj_path = path of new object, from plone site root. - Not required for              obj_type=AnalysisRequest          Optionally:              - obj_id = ID of new object.          All other parameters in the request are matched against the object's         Schema.  If a matching field is found in the schema, then the value is         taken from the request and sent to the field's mutator.          Reference fields may have their target value(s) specified with a         delimited string query syntax, containing the portal_catalog search:              <FieldName>=index1:value1|index2:value2          eg to set the Client of a batch:              ...@@API/update?obj_path=<path>...             ...&Client=title:<client_title>&...          And, to set a multi-valued reference, these both work:              ...@@API/update?obj_path=<path>...             ...&InheritedObjects:list=title:AR1...             ...&InheritedObjects:list=title:AR2...              ...@@API/update?obj_path=<path>...             ...&InheritedObjects[]=title:AR1...             ...&InheritedObjects[]=title:AR2...          The Analysis_Specification parameter is special, it mimics         the format of the python dictionaries, and only service Keyword         can be used to reference services.  Even if the keyword is not         actively required, it must be supplied:              <service_keyword>:min:max:error tolerance          The function returns a dictionary as a json string:          {             runtime: Function running time.             error: true or string(message) if error. false if no error.             success: true or string(message) if success. false if no success.         }          >>> portal = layer['portal']         >>> portal_url = portal.absolute_url()         >>> from plone.app.testing import SITE_OWNER_NAME         >>> from plone.app.testing import SITE_OWNER_PASSWORD          Simple AR creation, no obj_path parameter is required:          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/create", "&".join([         ... "obj_type=AnalysisRequest",         ... "Client=portal_type:Client|id:client-1",         ... "SampleType=portal_type:SampleType|title:Apple Pulp",         ... "Contact=portal_type:Contact|getFullname:Rita Mohale",         ... "Services:list=portal_type:AnalysisService|title:Calcium",         ... "Services:list=portal_type:AnalysisService|title:Copper",         ... "Services:list=portal_type:AnalysisService|title:Magnesium",         ... "SamplingDate=2013-09-29",         ... "Specification=portal_type:AnalysisSpec|title:Apple Pulp",         ... ]))         >>> browser.contents         '{..."success": true...}'          If some parameters are specified and are not located as existing fields or properties         of the created instance, the create should fail:          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/create?", "&".join([         ... "obj_type=Batch",         ... "obj_path=/batches",         ... "title=Test",         ... "Thing=Fish"         ... ]))         >>> browser.contents         '{...The following request fields were not used: ...Thing...}'          Now we test that the AR create also fails if some fields are spelled wrong          >>> browser = layer['getBrowser'](portal, loggedIn=True, username=SITE_OWNER_NAME, password=SITE_OWNER_PASSWORD)         >>> browser.open(portal_url+"/@@API/create", "&".join([         ... "obj_type=AnalysisRequest",         ... "thing=Fish",         ... "Client=portal_type:Client|id:client-1",         ... "SampleType=portal_type:SampleType|title:Apple Pulp",         ... "Contact=portal_type:Contact|getFullname:Rita Mohale",         ... "Services:list=portal_type:AnalysisService|title:Calcium",         ... "Services:list=portal_type:AnalysisService|title:Copper",         ... "Services:list=portal_type:AnalysisService|title:Magnesium",         ... "SamplingDate=2013-09-29"         ... ]))         >>> browser.contents         '{...The following request fields were not used: ...thing...}'
Returns whether the passed in object needs to be partitioned
Sends an email notification to sample's client contact if the sample         passed in has a retest associated
Returns the email body text
Returns the laboratory email formatted
Returns a list with lab managers formatted emails
Returns a string with the formatted email for the given contact
Returns a list with the formatted emails from sample contacts
Returns an html formatted link for the given object
Updates the printed time of the last results report from the sample
Updates the Sampler and the Sample Date with the values provided in         the request. If neither Sampler nor SampleDate are present in the         request, returns False
Updates the Preserver and the Date Preserved with the values provided         in the request. If neither Preserver nor DatePreserved are present in         the request, returns False
Updates the scheduled Sampling sampler and the Sampling Date with the         values provided in the request. If neither Sampling sampler nor Sampling         Date are present in the request, returns False
Returns whether the request Hidden param for the given obj is True
Returns the analysis specs available in the request for the given uid
Parse the data line. If an AS was selected it can distinguish between data rows and information rows.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or return the code error -1
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Returns the object field names to search against
Returns the search term
Convert unicode values to strings even if they belong to lists or dicts.         :param data: an object.         :return: The object with all unicode values converted to string.
Returns the query inferred from the request
Returns the raw query to use for current search, based on the         base query + update query
Resolves the sorting criteria for the given query
Returns whether the index is sortable
Returns the index of the catalog for the given field_name, if any
Performs a search against the catalog and returns the brains
Get the mapped permissions for the given object      A mapped permission is one that is used in the object.      Each permission string, e.g. "senaite.core: Field: Edit Analysis Remarks" is     translated by the function `AccessControl.Permission.pname` to a valid     attribute name:      >>> from bika.lims.permissions import FieldEditAnalysisResult     >>> AccessControl.Permission import pname     >>> pname(FieldEditAnalysisResult)     _Field__Edit_Result_Permission      This attribute is looked up in the object by `getPermissionMapping`:      >>> from AccessControl.PermissionMapping import getPermissionMapping     >>> getPermissionMapping(FieldEditAnalysisResult, wrapper)     ("Manager", "Sampler")      Therefore, only those permissions which have roles mapped on the object     or by objects within the acquisition chain are considered.      Code extracted from `IRoleManager.manage_getUserRolesAndPermissions`      :param brain_or_object: Catalog brain or object     :returns: List of permissions
Get the allowed permissions for the given object      Code extracted from `IRoleManager.manage_getUserRolesAndPermissions`      :param brain_or_object: Catalog brain or object     :param user: A user ID, user object or None (for the current user)     :returns: List of allowed permissions
Get the disallowed permissions for the given object      Code extracted from `IRoleManager.manage_getUserRolesAndPermissions`      :brain_or_object: Catalog brain or object     :param user: A user ID, user object or None (for the current user)     :returns: List of disallowed permissions
Check whether the security context allows the given permission on        the given brain or object.      N.B.: This includes also acquired permissions      :param permission: Permission name     :brain_or_object: Catalog brain or object     :returns: True if the permission is granted
Return the permissions of the role which are granted on the object      Code extracted from `IRoleManager.permissionsOfRole`      :param role: The role to check the permission     :param brain_or_object: Catalog brain or object     :returns: List of permissions of the role
Return the roles of the permission that is granted on the object      Code extracted from `IRoleManager.rolesOfPermission`      :param permission: The permission to get the roles     :param brain_or_object: Catalog brain or object     :returns: List of roles having the permission
Get the local defined roles on the context      Code extracted from `IRoleManager.get_local_roles_for_userid`      :param brain_or_object: Catalog brain or object     :param user: A user ID, user object or None (for the current user)     :returns: List of granted local roles on the given object
Grant local roles for the object      Code extracted from `IRoleManager.manage_addLocalRoles`      :param brain_or_object: Catalog brain or object     :param user: A user ID, user object or None (for the current user)     :param roles: The local roles to grant for the current user
Revoke local roles for the object      Code extracted from `IRoleManager.manage_setLocalRoles`      :param brain_or_object: Catalog brain or object     :param roles: The local roles to revoke for the current user     :param user: A user ID, user object or None (for the current user)
Grant the permission for the object to the defined roles      Code extracted from `IRoleManager.manage_permission`      :param brain_or_object: Catalog brain or object     :param permission: The permission to be granted     :param roles: The roles the permission to be granted to     :param acquire: Flag to acquire the permission
Change the settings for the given permission.      Code extracted from `IRoleManager.manage_permission`      :param brain_or_object: Catalog brain or object     :param permission: The permission to be granted     :param roles: The roles the permission to be granted to     :param acquire: Flag to acquire the permission
Returns a DisplayList with the available templates found in         browser/samplinground/templates/
Returns the current samplinground rendered with the template             specified in the request (param 'template').             Moves the iterator to the next samplinground available.
Returns the css style to be used for the current template.             If the selected template is 'default.pt', this method will             return the content from 'default.css'. If no css file found             for the current template, returns empty string
Returns a lost of dicts with the analysis request templates infomration         [{'uid':'xxxx','id':'xxxx','title':'xxx','url':'xxx'}, ...]
Returns a list of dictionaries sorted by Sample Partition/Container         [{'requests and partition info'}, ...]
It returns the pdf for the sampling rounds printed
Tis function generates a pdf file from the html         :sr_html: the html to use to generate the pdf
Service triggered each time an item is iterated in folderitems.         The use of this service prevents the extra-loops in child objects.         :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Get all active Analysis Services from Bika Setup and return them as Display List.
Return the prefix for a portal_type.            If not found, simply uses the portal_type itself
Return true if the rejection workflow is enabled (its checkbox is set)
Return the list of predefined rejection reasons
Adds an analysis to be consumed by the Analyses Chart machinery (js)          :param analysis_object: analysis to be rendered in the chart
/@@API/allowedTransitionsFor_many: Returns a list of dictionaries. Each         dictionary has the following structure:             {'uid': <uid_object_passed_in>,              'transitions': [{'id': action_id, 'title': 'Action Title'},                               {...}]                  Required parameters:             - uid: uids of the objects to get the allowed transitions from
Returns a list with the rejection reasons as strings          :param keyword: set of rejection reasons to be retrieved.         Possible values are:             - 'selected': Get, amongst the set of predefined reasons, the ones selected             - 'other': Get the user free-typed reason for rejection             - None: Get all rejection reasons         :return: list of rejection reasons as strings or an empty list
Empties catalog, then finds all contentish objects (i.e. objects            with an indexObject method), and reindexes them.            This may take a long time.
Empties catalog, then finds all contentish objects quering over             uid_catalog and reindexes them.             This may take a long time and will not care about missing             objects in uid_catalog.
Handle form submission
Check if edit is allowed
Returns the text saved for the selected department
Get the UID of the user associated with the authenticated user
Return all available analysis categories
Return an array containing the contacts from this Client
Return the decimal mark to be used on reports for this client          If the client has DefaultDecimalMark selected, the Default value from         the LIMS Setup will be returned.          Otherwise, will return the value of DecimalMark.
Return the Country from the Physical or Postal Address
Return the Province from the Physical or Postal Address
Return the Province from the Physical or Postal Address
Overrides parent function. If the ids passed in are from Attachment         types, the function ignores the DeleteObjects permission. For the rest         of types, it works as usual (checks the permission)
Parses result lines
Augment folder listing item
Augment folder listing item
Augment folder listing item
Augment folder listing item
Augment folder listing item with additional data
Augment folder listing item with additional data
Parses result lines
This function adds a multifile object to the instrument folder     :param row_dict: the dictionary which contains the document information     :param folder: the instrument object
Returns a generator for all rows in a sheet.            Each row contains a dictionary where the key is the value of the            first row of the sheet for each column.            The data values are returned in utf-8 format.            Starts to consume data from startrow
Converts a sheet string value to a boolean value.             Needed because of utf-8 conversions
Fills the address fields for the specified object if allowed:             PhysicalAddress, PostalAddress, CountryState, BillingAddress
Fills the contact fields for the specified object if allowed:             EmailAddress, Phone, Fax, BusinessPhone, BusinessFax, HomePhone,             MobilePhone
This will return an object from the catalog.         Logs a message and returns None if no object or multiple objects found.         All keyword arguments are passed verbatim to the contentFilter
Return an array of objects of the specified type in accordance to             the object titles defined in the sheet specified in 'sheet_name' and             service set in the paramenter 'service_title'.             If a default_obj is set, it will be included in the returned array.
Replace target states from some workflow statuses
Prepare a data item for the listing.          :param obj: The catalog brain or content object         :param item: Listing item (dictionary)         :param index: Index of the listing item         :returns: Augmented listing data item
Parses header lines              Header example:             Date    2012/11/15    User    anonymous             Time    06:07:08PM    Software version: 4.0              Example laboratory             Arizona
Results log file line Parser. The parse method in         InstrumentCSVResultsFileParser calls this method for each line         in the results file that is to be parsed.         :param line: a to parse         :returns: the number of rows to jump and parse the next data line or         return the code error -1
Parses the data line and adds the results to the dictionary.         :param split_line: a split data line to parse         :returns: the current result id and the dictionary of values obtained from the results
Convert a string containing a date from results file to bika format         :param date_time: str with Date to convert         :param only_date: boolean value that specifies if there is only a date         to parse (true) or date plus time (false)         :return: datetime in bika format
Gets the default sticker for that content type depending on the         requested size.          :return: An sticker ID as string
Return a list of unique Identifier strings     This populates the Identifiers Keyword index, but with some     replacements to prevent the word-splitter etc from taking effect.
Return modified order of field schemats.
Returns the user ids of the users that verified this analysis
Return the uncertainty value, if the result falls within         specified ranges for the service from which this analysis was derived.
Returns the uncertainty for this analysis and result.         Returns the value from Schema's Uncertainty field if the Service has         the option 'Allow manual uncertainty'. Otherwise, do a callback to         getDefaultUncertainty(). Returns None if no result specified and the         current result for this analysis is below or above detections limits.
Sets the uncertainty for this analysis. If the result is a         Detection Limit or the value is below LDL or upper UDL, sets the         uncertainty value to 0
Set detection limit operand for this analysis         Allowed detection limit operands are `<` and `>`.
Returns the Lower Detection Limit (LDL) that applies to this         analysis in particular. If no value set or the analysis service         doesn't allow manual input of detection limits, returns the value set         by default in the Analysis Service
Returns the Upper Detection Limit (UDL) that applies to this         analysis in particular. If no value set or the analysis service         doesn't allow manual input of detection limits, returns the value set         by default in the Analysis Service
Returns True if the result is below the Lower Detection Limit or         if Lower Detection Limit has been manually set
Returns True if the result is above the Upper Detection Limit or         if Upper Detection Limit has been manually set
Validate and set a value into the Result field, taking into         account the Detection Limits.         :param value: is expected to be a string.
Calculates the result for the current analysis if it depends of         other analysis/interim fields. Otherwise, do nothing
The function obtains the analysis' price without VAT and without         member discount         :return: the price (without VAT or Member Discount) in decimal format
Compute the VAT amount without member discount.         :return: the result as a float
Returns the time in minutes taken for this analysis.         If the analysis is not yet 'ready to process', returns 0         If the analysis is still in progress (not yet verified),             duration = date_verified - date_start_process         Otherwise:             duration = current_datetime - date_start_process         :return: time in minutes taken for this analysis         :rtype: int
The remaining time in minutes for this analysis to be completed.         Returns zero if the analysis is neither 'ready to process' nor a         turnaround time is set.             earliness = duration - max_turnaround_time         The analysis is late if the earliness is negative         :return: the remaining time in minutes before the analysis reaches TAT         :rtype: int
Checks if the specified instrument can be set for this analysis,         either if the instrument was assigned directly (by using "Allows         instrument entry of results") or indirectly via Method ("Allows manual         entry of results") in Analysis Service Edit view.         Param instrument can be either an uid or an object         :param instrument: string,Instrument         :return: True if the assignment of the passed in instrument is allowed         :rtype: bool
Checks if the analysis can follow the method specified, either if         the method was assigned directly (by using "Allows manual entry of         results") or indirectly via Instrument ("Allows instrument entry of         results") in Analysis Service Edit view.         Param method can be either a uid or an object         :param method: string,Method         :return: True if the analysis can follow the method specified         :rtype: bool
Returns the allowed methods for this analysis, either if the method         was assigned directly (by using "Allows manual entry of results") or         indirectly via Instrument ("Allows instrument entry of results") in         Analysis Service Edit View.         :return: A list with the methods allowed for this analysis         :rtype: list of Methods
Returns the allowed instruments for this analysis, either if the         instrument was assigned directly (by using "Allows instrument entry of         results") or indirectly via Method (by using "Allows manual entry of         results") in Analysis Service edit view.         :return: A list of instruments allowed for this Analysis         :rtype: list of instruments
Returns the precision for the Analysis Service and result         provided. Results with a precision value above this exponential         format precision should be formatted as scientific notation.          If the Calculate Precision according to Uncertainty is not set,         the method will return the exponential precision value set in the         Schema. Otherwise, will calculate the precision value according to         the Uncertainty and the result.          If Calculate Precision from the Uncertainty is set but no result         provided neither uncertainty values are set, returns the fixed         exponential precision.          Will return positive values if the result is below 0 and will return         0 or positive values if the result is above 0.          Given an analysis service with fixed exponential format         precision of 4:         Result      Uncertainty     Returns         5.234           0.22           0         13.5            1.34           1         0.0077          0.008         -3         32092           0.81           4         456021          423            5          For further details, visit https://jira.bikalabs.com/browse/LIMS-1334          :param result: if provided and "Calculate Precision according to the         Uncertainty" is set, the result will be used to retrieve the         uncertainty from which the precision must be calculated. Otherwise,         the fixed-precision will be used.         :returns: the precision
Formatted result:         1. If the result is a detection limit, returns '< LDL' or '> UDL'         2. Print ResultText of matching ResultOptions         3. If the result is not floatable, return it without being formatted         4. If the analysis specs has hidemin or hidemax enabled and the            result is out of range, render result as '<min' or '>max'         5. If the result is below Lower Detection Limit, show '<LDL'         6. If the result is above Upper Detecion Limit, show '>UDL'         7. Otherwise, render numerical value         :param specs: Optional result specifications, a dictionary as follows:             {'min': <min_val>,              'max': <max_val>,              'error': <error>,              'hidemin': <hidemin_val>,              'hidemax': <hidemax_val>}         :param decimalmark: The string to be used as a decimal separator.             default is '.'         :param sciformat: 1. The sci notation has to be formatted as aE^+b                           2. The sci notation has to be formatted as a·10^b                           3. As 2, but with super html entity for exp                           4. The sci notation has to be formatted as a·10^b                           5. As 4, but with super html entity for exp                           By default 1         :param html: if true, returns an string with the special characters             escaped: e.g: '<' and '>' (LDL and UDL for results like < 23.4).
Returns the precision for the Analysis.          - If ManualUncertainty is set, calculates the precision of the result           in accordance with the manual uncertainty set.          - If Calculate Precision from Uncertainty is set in Analysis Service,           calculates the precision in accordance with the uncertainty infered           from uncertainties ranges.          - If neither Manual Uncertainty nor Calculate Precision from           Uncertainty are set, returns the precision from the Analysis Service          - If you have a number with zero uncertainty: If you roll a pair of         dice and observe five spots, the number of spots is 5. This is a raw         data point, with no uncertainty whatsoever. So just write down the         number. Similarly, the number of centimeters per inch is 2.54,         by definition, with no uncertainty whatsoever. Again: just write         down the number.          Further information at AbstractBaseAnalysis.getPrecision()
Returns the stored Analyst or the user who submitted the result
Returns the name of the currently assigned analyst
This method is used to populate catalog values         Returns a dictionary with the workflow id as key and workflow state as         value.         :return: {'review_state':'active',...}
Returns the Worksheet to which this analysis belongs to, or None
Used to populate metadata, so that we don't need full objects of         analyses when working with their attachments.
When this analysis is unassigned from a worksheet, this function         is responsible for deleting DuplicateAnalysis objects from the ws.
Sets a value to an interim of this analysis         :param keyword: the keyword of the interim         :param value: the value for the interim
Returns the value of an interim of this analysis
Checks if the current user has granted access to the worksheet.         If the user is an analyst without LabManager, LabClerk and         RegulatoryInspector roles and the option 'Allow analysts         only to access to the Worksheets on which they are assigned' is         ticked and the above condition is true, it will redirect to         the main Worksheets view.         Returns False if the user has no access, otherwise returns True
Checks if the current user has granted access to the worksheet         and if has also privileges for managing it. If the user has no         granted access and redirect's value is True, redirects to         /manage_results view. Otherwise, does nothing
Adds a portalMessage if         a) the worksheet has been rejected and replaced by another or         b) if the worksheet is the replacement of a rejected worksheet.         Otherwise, does nothing.
Tries to return a DateTime.DateTime object
This function gets ans string as time or a DateTime objects and returns a     string with the time formatted      :param time: The time to process     :type time: str/DateTime     :param long_format:  If True, return time in ling format     :type portal_type: boolean/null     :param time_only: If True, only returns time.     :type title: boolean/null     :param context: The current context     :type context: ATContentType     :param request: The current request     :type request: HTTPRequest object     :returns: The formatted date as string     :rtype: string
This convert bika domain date format msgstrs to Python         strftime format strings, by the same rules as ulocalized_time.         XXX i18nl10n.py may change, and that is where this code is taken from.
This function checks if the dict values are correct.     :instance: the object instance. Used for querying     :dic: is a dictionary with the following format:     {'actions': [{'act_row_idx': 0,                    'action': 'repeat',                    'an_result_id': 'rep-1',                    'analyst': '',                    'otherWS': 'current',                    'setresultdiscrete': '',                    'setresulton': 'original',                    'setresultvalue': '',                    'worksheettemplate': ''}],       'conditions': [{'analysisservice': '52853cf7d5114b5aa8c159afad2f3da1',                       'and_or': 'no',                       'cond_row_idx': 0,                       'discreteresult': '',                       'range0': '11',                       'range1': '12'}],       'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',       'rulenumber': '0',       'trigger': 'submit'},     These are the checking rules:         :range0/range1: string or number.     They are the numeric range within the action will be     carried on. It is needed to keep it as None or '' if the discreteresult     is going to be used instead.         :discreteresult: string     If discreteresult is not Null, ranges have to be Null.         :trigger: string.     So far there are only two options: 'submit'/'verify'. They are defined     in browser/widgets/reflexrulewidget.py/ReflexRuleWidget/getTriggerVoc.         :analysisservice: it is the uid of an analysis service         :actions: It is a list of dictionaries with the following format:     [{'action':'<action_name>', 'act_row_idx':'X',                 'otherWS':Bool, 'analyst': '<analyst_id>'},               {'action':'<action_name>', 'act_row_idx':'X',                 'otherWS':Bool, 'analyst': '<analyst_id>'},         ]         :'repetition_max': integer or string representing an integer.     <action_name> options are found in     browser/widgets/reflexrulewidget.py/ReflexRuleWidget/getActionVoc     so far.
Set the reflexrule field.         :rules_list: is a list of dictionaries with the following format:         [{'actions': [{'act_row_idx': 0,                        'action': 'repeat',                        'an_result_id': 'rep-1',                        'analyst': '',                        'otherWS': 'current',                        'setresultdiscrete': '',                        'setresulton': 'original',                        'setresultvalue': '',                        'worksheettemplate': ''}],           'conditions': [{'analysisservice': '52853cf7d5114b5aa8c159afad2f3da1',                           'and_or': 'no',                           'cond_row_idx': 0,                           'discreteresult': '',                           'range0': '11',                           'range1': '12'}],           'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',           'rulenumber': '0',           'trigger': 'submit'},          {'actions': [{'act_row_idx': 0,                        'action': 'repeat',                        'an_result_id': 'rep-2',                        'analyst': '',                        'otherWS': 'current',                        'setresultdiscrete': '',                        'setresulton': 'original',                        'setresultvalue': '',                        'worksheettemplate': ''},                       {'act_row_idx': 1,                        'action': 'repeat',                        'an_result_id': 'rep-4',                        'analyst': 'analyst1',                        'otherWS': 'to_another',                        'setresultdiscrete': '',                        'setresulton': 'original',                        'setresultvalue': '',                        'worksheettemplate': '70d48adfb34c4231a145f76a858e94cf'},                       {'act_row_idx': 2,                        'action': 'repeat',                        'an_result_id': 'rep-5',                        'analyst': '',                        'otherWS': 'create_another',                        'setresultdiscrete': '',                        'setresulton': 'original',                        'setresultvalue': '',                        'worksheettemplate': ''},                       {'act_row_idx': 3,                        'action': 'repeat',                        'an_result_id': 'rep-6',                        'analyst': '',                        'otherWS': 'no_ws',                        'setresultdiscrete': '',                        'setresulton': 'original',                        'setresultvalue': '',                        'worksheettemplate': ''}],           'conditions': [{'analysisservice': 'rep-1',                           'and_or': 'no',                           'cond_row_idx': 0,                           'discreteresult': '',                           'range0': '12',                           'range1': '12'}],           'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',           'rulenumber': '1',           'trigger': 'submit'},          {'actions': [{'act_row_idx': 0,                        'action': 'repeat',                        'an_result_id': 'rep-3',                        'analyst': '',                        'otherWS': 'current',                        'setresultdiscrete': '',                        'setresulton': 'original',                        'setresultvalue': '',                        'worksheettemplate': ''}],           'conditions': [{'analysisservice': 'rep-1',                           'and_or': 'and',                           'cond_row_idx': 0,                           'discreteresult': '',                           'range0': '12',                           'range1': '12'},                          {'analysisservice': 'rep-2',                           'and_or': 'or',                           'cond_row_idx': 1,                           'discreteresult': '',                           'range0': '115',                           'range1': '115'},                          {'analysisservice': 'rep-1',                           'and_or': 'no',                           'cond_row_idx': 2,                           'discreteresult': '',                           'range0': '14',                           'range1': '14'}],           'mother_service_uid': '52853cf7d5114b5aa8c159afad2f3da1',           'rulenumber': '2',           'trigger': 'submit'}]         This list of dictionaries is how the system will store the reflexrule         field info. This dictionaries must be in sync with the         browser/widgets/reflexrulewidget.py/process_form() dictionaries format.
Compute VAT Amount from the Price and system configured VAT
Compute discounted price excl. VAT
Compute discounted bulk discount excl. VAT
Compute total price including VAT
Compute total bulk price
Compute total discounted price
Compute total discounted corporate bulk price
A vocabulary listing available (and activated) categories.
Returns the Lower Detection Limit for this service as a floatable
Returns the Upper Detection Limit for this service as a floatable
Returns if the user that submitted a result for this analysis must         also be able to verify the result         :returns: true or false
Returns a DisplayList with the available options for the         self-verification list: 'system default', 'true', 'false'         :returns: DisplayList with the available options for the         self-verification list
Returns the number of required verifications a test for this         analysis requires before being transitioned to 'verified' state         :returns: number of required verifications
Returns a DisplayList with the available options for the         multi-verification list: 'system default', '1', '2', '3', '4'         :returns: DisplayList with the available options for the         multi-verification list
Returns the maximum turnaround time for this analysis. If no TAT is         set for this particular analysis, it returns the value set at setup         return: a dictionary with the keys "days", "hours" and "minutes"
Returns the list of analyses of the Analysis Request to which this         analysis belongs to, but with the current analysis excluded.         :param retracted: If false, retracted/rejected siblings are dismissed         :type retracted: bool         :return: list of siblings for this analysis         :rtype: list of IAnalysis
Returns a dictionary with the interims data
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Collect all services which depend on this service      :param service: Analysis Service Object/ZCatalog Brain     :returns: List of services that depend on this service
Calculation dependencies of this service and the calculation of each     dependent service (recursively).
Calculate the dependencies for the given service.
Find invalid instruments          - instruments who have failed QC tests         - instruments whose certificate is out of date         - instruments which are disposed until next calibration test          Return a dictionary with all info about expired/invalid instruments
Control availability of the viewlet
Render the viewlet
CSV Parser
Lookup the widget
Lookup the widget of the field and return the label
Converts the given date to a localized time string
Translate the given state string
Generate folderitems for each version
Adds a set of raw results for an object with id=resid             resid is usually an Analysis Request ID or Worksheet's Reference             Analysis ID. The values are a dictionary in which the keys are             analysis service keywords and the values, another dictionary with             the key,value results.             The column 'DefaultResult' must be provided, because is used to map             to the column from which the default result must be retrieved.              Example:             resid  = 'DU13162-001-R1'             values = {                 'D2': {'DefaultResult': 'Final Conc',                        'Remarks':       '',                        'Resp':          '5816',                        'ISTD Resp':     '274638',                        'Resp Ratio':    '0.0212',                        'Final Conc':    '0.9145',                        'Exp Conc':      '1.9531',                        'Accuracy':      '98.19' },                  'D3': {'DefaultResult': 'Final Conc',                        'Remarks':       '',                        'Resp':          '5816',                        'ISTD Resp':     '274638',                        'Resp Ratio':    '0.0212',                        'Final Conc':    '0.9145',                        'Exp Conc':      '1.9531',                        'Accuracy':      '98.19' }                 }
The total number of analysis results parsed
The analysis service keywords found
Given an input file read its contents, strip whitespace from the          beginning and end of each line and return a list of the preprocessed          lines read.          :param infile: file that contains the data to be read         :return: list of the read lines with stripped whitespace
Attach a file or a given set of files to an analysis          :param analysis: analysis where the files are to be attached         :param attachment: files to be attached. This can be either a         single file or a list of files         :return: None
Searches for analyses from ZODB to be filled with results.             objid can be either AR ID or Worksheet's Reference Sample IDs.             Only analyses that matches with getAnallowedAnalysisStates() will             be returned. If not a ReferenceAnalysis, getAllowedARStates() is             also checked.             Returns empty array if no analyses found
If an AR(objid) has an analysis that has a calculation         then check if param analysis is used on the calculations formula.         Here we are dealing with two types of analysis.         1. Calculated Analysis - Results are calculated.         2. Analysis - Results are captured and not calculated         :param objid: AR ID or Worksheet's Reference Sample IDs         :param analysis: Analysis Object
Duplicate an analysis consist on creating a new analysis with     the same analysis service for the same sample. It is used in     order to reduce the error procedure probability because both     results must be similar.     :base: the analysis object used as the creation base.
Create a new Analysis.  The source can be an Analysis Service or     an existing Analysis, and all possible field values will be set to the     values found in the source object.     :param context: The analysis will be created inside this object.     :param source: The schema of this object will be used to populate analysis.     :param kwargs: The values of any keys which match schema fieldnames will     be inserted into the corrosponding fields in the new analysis.     :returns: Analysis object that was created     :rtype: Analysis
Returns the precision for a given floatable value.     If value is None or not floatable, returns None.     Will return positive values if the result is below 1 and will     return 0 values if the result is above or equal to 1.     :param numeric_value: the value to get the precision from     :returns: the numeric_value's precision             Examples:             numeric_value     Returns             0               0             0.22            1             1.34            0             0.0021          3             0.013           2             2               0             22              0
Returns the formatted uncertainty according to the analysis, result     and decimal mark specified following these rules:      If the "Calculate precision from uncertainties" is enabled in     the Analysis service, and      a) If the the non-decimal number of digits of the result is above        the service's ExponentialFormatPrecision, the uncertainty will        be formatted in scientific notation. The uncertainty exponential        value used will be the same as the one used for the result. The        uncertainty will be rounded according to the same precision as        the result.         Example:        Given an Analysis with an uncertainty of 37 for a range of        results between 30000 and 40000, with an        ExponentialFormatPrecision equal to 4 and a result of 32092,        this method will return 0.004E+04      b) If the number of digits of the integer part of the result is        below the ExponentialFormatPrecision, the uncertainty will be        formatted as decimal notation and the uncertainty will be        rounded one position after reaching the last 0 (precision        calculated according to the uncertainty value).         Example:        Given an Analysis with an uncertainty of 0.22 for a range of        results between 1 and 10 with an ExponentialFormatPrecision        equal to 4 and a result of 5.234, this method will return 0.2      If the "Calculate precision from Uncertainties" is disabled in the     analysis service, the same rules described above applies, but the     precision used for rounding the uncertainty is not calculated from     the uncertainty neither the result. The fixed length precision is     used instead.      For further details, visit     https://jira.bikalabs.com/browse/LIMS-1334      If the result is not floatable or no uncertainty defined, returns     an empty string.      The default decimal mark '.' will be replaced by the decimalmark     specified.      :param analysis: the analysis from which the uncertainty, precision                      and other additional info have to be retrieved     :param result: result of the analysis. Used to retrieve and/or                    calculate the precision and/or uncertainty     :param decimalmark: decimal mark to use. By default '.'     :param sciformat: 1. The sci notation has to be formatted as aE^+b                   2. The sci notation has to be formatted as ax10^b                   3. As 2, but with super html entity for exp                   4. The sci notation has to be formatted as a·10^b                   5. As 4, but with super html entity for exp                   By default 1     :returns: the formatted uncertainty
Returns the formatted number part of a results value.  This is     responsible for deciding the precision, and notation of numeric     values in accordance to the uncertainty. If a non-numeric     result value is given, the value will be returned unchanged.      The following rules apply:      If the "Calculate precision from uncertainties" is enabled in     the Analysis service, and      a) If the non-decimal number of digits of the result is above        the service's ExponentialFormatPrecision, the result will        be formatted in scientific notation.         Example:        Given an Analysis with an uncertainty of 37 for a range of        results between 30000 and 40000, with an        ExponentialFormatPrecision equal to 4 and a result of 32092,        this method will return 3.2092E+04      b) If the number of digits of the integer part of the result is        below the ExponentialFormatPrecision, the result will be        formatted as decimal notation and the resulta will be rounded        in accordance to the precision (calculated from the uncertainty)         Example:        Given an Analysis with an uncertainty of 0.22 for a range of        results between 1 and 10 with an ExponentialFormatPrecision        equal to 4 and a result of 5.234, this method will return 5.2      If the "Calculate precision from Uncertainties" is disabled in the     analysis service, the same rules described above applies, but the     precision used for rounding the result is not calculated from     the uncertainty. The fixed length precision is used instead.      For further details, visit     https://jira.bikalabs.com/browse/LIMS-1334      The default decimal mark '.' will be replaced by the decimalmark     specified.      :param analysis: the analysis from which the uncertainty, precision                      and other additional info have to be retrieved     :param result: result to be formatted.     :param decimalmark: decimal mark to use. By default '.'     :param sciformat: 1. The sci notation has to be formatted as aE^+b                       2. The sci notation has to be formatted as ax10^b                       3. As 2, but with super html entity for exp                       4. The sci notation has to be formatted as a·10^b                       5. As 4, but with super html entity for exp                       By default 1     :result: should be a string to preserve the decimal precision.     :returns: the formatted result as string
Returns a dictionary with the constraints and rules for         methods, instruments and results to be applied to each of the         analyses specified in the param uids (an array of uids).         See docs/imm_results_entry_behaviour.png for further details
Update hook
TODO: Refactor to non-classic mode
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Return a list of dictionaries fit for ReferenceResultsField         consumption. Only services which have float()able entries in result,min         and max field will be included. If any of min, max, or result fields         are blank, the row value is ignored here.
Returns the value assigned to the passed in key for the analysis         service uid from the passed in form.          If check_floatable is true, will return the passed in default if the         obtained value is not floatable         :param form: form being submitted         :param uid: uid of the Analysis Service the specification relates         :param key: id of the specs param to get (e.g. 'min')         :param check_floatable: check if the value is floatable         :param default: fallback value that will be returned by default         :type default: str, None
Render Reference Results Table
Adds the value to the existing text stored in the field,         along with a small divider showing username and date of this entry.
Returns raw field value (possible wrapped in BaseUnit)
Runs before the first import step of the *default* profile      This handler is registered as a *pre_handler* in the generic setup profile      :param portal_setup: SetupTool
SENAITE setup handler
Remove default Plone contents
Hide root items in navigation
Reindex contents generated by Generic Setup
Setup roles and groups for BECHEM
Setup portal_type -> catalog mappings
Setup core catalogs
Setup auditlog catalog
This function returns an analysis when the derivative IDs conditions     are met.     :analysis: the analysis full object which we want to obtain the         rules for.     :ans_cond: the local id with the target derivative reflex rule id.
This functions executes the action against the analysis.     :base: a full analysis object. The new analyses will be cloned from it.     :action: a dictionary representing an action row.         [{'action': 'duplicate', ...}, {,}, ...]     :returns: the new analysis
This function creates a new worksheet takeing advantatge of the analyst     variable. If there isn't an analyst definet, the system will puck up the     the first one obtained in a query.
This function checks if the actions contains worksheet actions.      There is a selection list in each action section. This select has the     following options and consequence.      1) "To the current worksheet" (selected by default)     2) "To another worksheet"     3) "Create another worksheet"     4) "No worksheet"      - If option 1) is selected, the Analyst selection list will not be     displayed. Since the action doesn't require to add the new analysis to     another worksheet, the function will try to add the analysis to the same     worksheet as the base analysis. If the base analysis is not assigned in a     worksheet, no worksheet will be assigned to the new analysis.      - If option 2) is selected, the Analyst selection list will be displayed.      - If option 2) is selected and an analyst has also been selected, then the     system will search for the latest worksheet in status "open" for the     selected analyst and will add the analysis in that worksheet (the system     also searches for the worksheet template if defined).     If the system doesn't find any match, another worksheet assigned to the     selected analyst and with the analysis must be automatically created.      - If option 2) is selected but no analyst selected, then the system will     search for the latest worksheet in the status "open" regardless of the     analyst assigned and will add the analysis in that worksheet. If there     isn't any open worksheet available, then go to option 3)      - If option 3) is selected, a new worksheet with the defined analyst will     be created.     If no analyst is defined for the original analysis, the system     will create a new worksheet and assign the same analyst as the original     analysis to which the rule applies.     If the original analysis doesn't have assigned any analyst, the system will     assign the same analyst that was assigned to the latest worksheet available     in the system. If there isn't any worksheet created yet, use the first     active user with role "analyst" available.      - if option 4) the Analyst selection list will not be displayed. The     analysis (duplicate, repeat, whatever) will be created, but not assigned     to any worksheet, so it will stay "on queue", assigned to the same     Analysis Request as the original analysis for which the rule has been     triggered.
This function executes all the reflex rule actions inside action_row using     the object in the variable 'base' as the starting point     :base: a full analysis object     :action_row: a list of dictionaries containing the actions to do         [{'action': 'duplicate', ...}, {,}, ...]
This function returns a boolean as True if the conditions in the         action_set are met, and returns False otherwise.         :analysis: the analysis full object which we want to obtain the             rules for.         :action_set: a set of rules and actions as a dictionary.             {'actions': [{'act_row_idx': 0,                 'action': 'setresult',                 'an_result_id': 'set-4',                 'analyst': 'analyst1',                 'otherWS': 'current',                 'setresultdiscrete': '3',                 'setresulton': 'new',                 'setresultvalue': '',                 'worksheettemplate': ''}],             'conditions': [{'analysisservice': 'dup-2',                 'and_or': 'and',                 'cond_row_idx': 0,                 'discreteresult': '2',                 'range0': '',                 'range1': ''},                 {'analysisservice': 'dup-7',                 'and_or': 'no',                 'cond_row_idx': 1,                 'discreteresult': '2',                 'range0': '',                 'range1': ''}],             'mother_service_uid': 'ddaa2a7538bb4d188798498d6e675abd',             'rulenumber': '1',             'trigger': 'submit'}         :forceuid: a boolean used to get the analysis service's UID from the         analysis even if the analysis has been reflected and has a local_id.         :returns: a Boolean.
This function returns a list of dictionaries with the rules to be done         for the analysis service.         :analysis: the analysis full object which we want to obtain the             rules for.         :wf_action: it is the workflow action that the analysis is doing, we             have to act in consideration of the action_set 'trigger' variable         :returns: [{'action': 'duplicate', ...}, {,}, ...]
This function transposes values from the provided file into the         ARImport object's fields, and checks for invalid values.          If errors are found:             - Validation transition is aborted.             - Errors are stored on object and displayed to user.
Create objects from valid ARImport
Scrape the "Header" values from the original input file
Save values from the file's header row into their schema fields.
Read the rows specifying Samples and return a dictionary with         related data.          keys are:             headers - row with "Samples" in column 0.  These headers are                used as dictionary keys in the rows below.             prices - Row with "Analysis Price" in column 0.             total_analyses - Row with "Total analyses" in colmn 0             price_totals - Row with "Total price excl Tax" in column 0             samples - All other sample rows.
Save values from the file's header row into the DataGrid columns         after doing some very basic validation
Scrape the "Batch Header" values from the original input file
Save reference to batch, if existing batch specified         Create new batch, if possible with specified values
Validate headers fields from schema
Scan through the SampleData values and make sure         that each one is correct
Lookup an object of type (allowed_types).  kwargs is sent         directly to the catalog.
Return a list of services which are referenced in Analyses.         values may be UID, Title or Keyword.
Return a list of services which are referenced in profiles         values may be UID, Title or ProfileKey.
Return a sample container
Safely translate and convert to UTF8, any zope i18n msgid returned from     a bikaMessageFactory _
Present a DisplayList containing users in the specified         list of roles
Obtain and reformat the from and to dates         into a date query construct
Obtain and reformat the from and to dates         into a printable date parameter construct
Dummy method to replace decimal mark from an input string.         Assumes that 'value' uses '.' as decimal mark and ',' as         thousand mark.         ::value:: is a string         ::returns:: is a string with the decimal mark if needed
Will encode in quoted-printable encoding only if header     contains non latin characters
Convert title to sortable title
Change the workflow state of an object     @param content: Content obj which state will be changed     @param state_id: name of the state to put on content     @param kw: change the values of same name of the state mapping     @return: True if succeed. Otherwise, False
Uses plone.subrequest to fetch an internal image resource.      If the URL points to an external resource, the URL is handed     to weasyprint.default_url_fetcher.      Please see these links for details:          - https://github.com/plone/plone.subrequest         - https://pypi.python.org/pypi/plone.subrequest         - https://github.com/senaite/senaite.core/issues/538      :returns: A dict with the following keys:          * One of ``string`` (a byte string) or ``file_obj``           (a file-like object)         * Optionally: ``mime_type``, a MIME type extracted e.g. from a           *Content-Type* header. If not provided, the type is guessed from the           file extension in the URL.         * Optionally: ``encoding``, a character encoding extracted e.g. from a           *charset* parameter in a *Content-Type* header         * Optionally: ``redirected_url``, the actual URL of the resource           if there were e.g. HTTP redirects.         * Optionally: ``filename``, the filename of the resource. Usually           derived from the *filename* parameter in a *Content-Disposition*           header          If a ``file_obj`` key is given, it is the caller’s responsibility         to call ``file_obj.close()``.
create a PDF from some HTML.     htmlreport: rendered html     outfile: pdf filename; if supplied, caller is responsible for creating              and removing it.     css: remote URL of css file to download     images: A dictionary containing possible URLs (keys) and local filenames             (values) with which they may to be replaced during rendering.     # WeasyPrint will attempt to retrieve images directly from the URL     # referenced in the HTML report, which may refer back to a single-threaded     # (and currently occupied) zeoclient, hanging it.  All image source     # URL's referenced in htmlreport should be local files.
Convert a list of dictionaries into a dictionary of dictionaries.      key_subfieldname must exist in each Record's subfields and have a value,     which will be used as the key for the new dictionary. If a key is duplicated,     the earlier value will be overwritten.
Mainly used for Analysis Service's unit. Transform the text adding     sub and super html scripts:     For super-scripts, use ^ char     For sub-scripts, use _ char     The expression "cm^2" will be translated to "cm²" and the     expression "b_(n-1)" will be translated to "b n-1".     The expression "n_(fibras)/cm^3" will be translated as     "n fibras / cm³"     :param text: text to be formatted
Drops the trailinz zeros from decimal value.         Returns a string
Checks if a user has permissions for a given object.      Args:         permissions: The permissions the current user must be compliant with         obj: The object for which the permissions apply      Returns:         1 if the user complies with all the permissions for the given object.         Otherwise, it returns empty.
This decorator allows to measure the execution time     of a function and prints it to the console.     :param func_to_measure: function to be decorated
Returns a well-formed link. If href is None/empty, returns an empty string     :param href: value to be set for attribute href     :param value: the text to be displayed. If None, the href itself is used     :param kwargs: additional attributes and values     :return: a well-formed html anchor
Returns a well-formed link to an email address. If email is None/empty,     returns an empty string     :param email: email address     :param link_text: text to be displayed. If None, the email itself is used     :return: a well-formatted html anchor
Returns a well-formed image     :param name: file name of the image     :param kwargs: additional attributes and values     :return: a well-formed html img
Returns a string representation of attributes for html entities     :param kwargs: attributes and values     :return: a well-formed string representation of attributes
Gets the utility for IRegistry and returns the value for the key passed in.     If there is no value for the key passed in, returns default value     :param key: the key in the registry to look for     :param default: default value if the key is not registered     :return: value in the registry for the key passed in
Returns if the current user has rights for the permission passed in against     the obj passed in     :param permission: name of the permission     :param obj: the object to check the permission against for the current user     :return: 1 if the user has rights for this permission for the passed in obj
Tries to convert the value passed in as an int. If no success, returns the     default value passed in     :param value: the string to convert to integer     :param default: the default fallback     :return: int representation of the value passed in
Convert unicode values to strings even if they belong to lists or dicts.     :param data: an object.     :return: The object with all unicode values converted to string.
Convert string values to unicode even if they belong to lists or dicts.     :param data: an object.     :return: The object with all string values converted to unicode.
Returns a DisplayList with the items sorted by Title     :param brains_or_objects: list of brains or objects     :param none_item: adds an item with empty uid and text "Select.." in pos 0     :return: DisplayList (uid, title) sorted by title ascending     :rtype: DisplayList
Parses the data line and builds the dictionary.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or return the code error -1
A template helper to pick an Analysis identified by the name of the         current Analysis Service.          ar_data is the dictionary structure which is returned by _ws_data
Returns the current worksheet from the list. Returns None when             the iterator reaches the end of the array.
Splits a list to a n lists with chunksnum number of elements             each one.             For a list [3,4,5,6,7,8,9] with chunksunum 4, the method             will return the following list of groups:             [[3,4,5,6],[7,8,9]]
Returns a dictionary that represents the lab object             Keys: obj, title, url, address, confidence, accredited,                   accreditation_body, accreditation_logo, logo
Creates an ws dict, accessible from the view and from each             specific template.             Keys: obj, id, url, template_title, remarks, date_printed,                 ars, createdby, analyst, printedby, analyses_titles,                 portal, laboratory
Returns a dict that represents the user who created the ws             Keys: username, fullmame, email
Returns a dict that represent the analyst assigned to the             worksheet.             Keys: username, fullname, email
Returns a dict that represents the user who prints the ws             Keys: username, fullname, email
Returns a list of dicts. Each dict represents an analysis             assigned to the worksheet
Returns a dict that represents the analysis
Returns a dict that represents the sample             Keys: obj, id, url, client_sampleid, date_sampled,                   sampling_date, sampler, date_received, composite,                   date_expired, date_disposal, date_disposed, remarks
Returns a dict that represents the sample type assigned to             the sample specified             Keys: obj, id, title, url
Returns a dict that represents the sample point assigned to             the sample specified             Keys: obj, id, title, url
Returns a dict that represents the analysis request
Returns a dict that represents the client specified             Keys: obj, id, url, name
Containers vocabulary      This is a separate class so that it can be called from ajax to filter     the container list, as well as being used as the AT field vocabulary.      Returns a tuple of tuples: ((object_uid, object_title), ())      If the partition is flagged 'Separate', only containers are displayed.     If the Separate flag is false, displays container types.      XXX bsc = self.portal.bika_setup_catalog     XXX obj = bsc(getKeyword='Moist')[0].getObject()     XXX u'Container Type: Canvas bag' in obj.getContainers().values()     XXX True
Returns the methods available for this analysis.             If the service has the getInstrumentEntryOfResults(), returns             the methods available from the instruments capable to perform             the service, as well as the methods set manually for the             analysis on its edit view. If getInstrumentEntryOfResults()             is unset, only the methods assigned manually to that service             are returned.
Returns the instruments available for this service.             If the service has the getInstrumentEntryOfResults(), returns             the instruments capable to perform this service. Otherwhise,             returns an empty list.
Returns a DisplayList with the available Instruments             registered in Bika-Setup. Only active Instruments are             fetched. Used to fill the Instruments MultiSelectionWidget
This methods returns a list with the analyses services dependencies.         :return: a list of analysis services objects.
This methods returns a list with the service dependencies UIDs         :return: a list of uids
Method triggered after a 'deactivate' transition for the current         AnalysisService is performed. Removes this service from the Analysis         Profiles or Analysis Request Templates where is assigned.         This function is called automatically by         bika.lims.workflow.AfterTransitionEventHandler
Parses the data line and builds the dictionary.         :param sline: a split data line to parse         :returns: the number of rows to jump and parse the next data line or return the code error -1
Update hook
Get the results Range from the AR
XXX refactor if possible to non-classic mode
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Returns the currently authenticated member object     or the Anonymous User.  Never returns None.     This caches the value in the reqeust...
Before render hook of the listing base view
Check if the analyst can be assigned
Allow the Analyst reassignment
Returns the current selected state
Service triggered each time an item is iterated in folderitems.          The use of this service prevents the extra-loops in child objects.          :obj: the instance of the class to be foldered         :item: dict containing the properties of the object to be used by             the template         :index: current index of the item
Returns worksheet templates as JSON
Actions to be done when an analysis is added in an Analysis Request
Actions to be done when an analysis is removed from an Analysis Request
Before template render hook
Applies new properties to the item (Client) that is currently being         rendered as a row in the list          :param obj: client to be rendered as a row in the list         :param item: dict representation of the client, suitable for the list         :param index: current position of the item within the list         :type obj: ATContentType/DexterityContentType         :type item: dict         :type index: int         :return: the dict representation of the item         :rtype: dict
Ensures paths are correct for linux and windows
Read the requirements.txt file and parse into requirements for setup's   install_requirements option.
Identify platform.
Measure capnp serialization performance of a network containing a simple   python region that in-turn contains a Random instance.
Check that nupic.bindings extension libraries can be imported.    Throws ImportError on failure.
This script performs two checks. First it tries to import nupic.bindings   to check that it is correctly installed. Then it tries to import the C   extensions under nupic.bindings. Appropriate user-friendly status messages   are printed depend on the outcome.
Default implementation that return an attribute with the requested name.      This method provides a default implementation of     :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameter` that     simply returns an attribute with the parameter name. If the Region     conceptually contains multiple nodes with separate state, the ``index``     argument is used to request a parameter of a specific node inside the     region. In case of a region-level parameter the index should be ``-1``.      The implementation prevents accessing parameters names that start with     ``_``. It may be better to enforce this convention at the node spec level.      :param name: (string) name of requested parameter     :param index: (int) index of node inside the region (if relevant)
Default implementation that return the length of the attribute.      This default implementation goes hand in hand with     :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameterArray`.     If you override one of them in your subclass, you should probably override     both of them.      The implementation prevents accessing parameters names that start with     ``_``. It may be better to enforce this convention at the node spec level.      :param name: (string) name of requested parameter     :param index: (int) index of node inside the region (if relevant)     :raises: Exception if parameter starts with ``_``.
Default implementation that return an attribute with the requested name.      This method provides a default implementation of     :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameterArray`     that returns an attribute with the parameter name. If the Region     conceptually contains multiple nodes with separate state the ``index``     argument is used to request a parameter of a specific node inside the     region. The attribute value is written into the output array. No type or     sanity checks are performed for performance reasons. If something goes awry     it will result in a low-level exception. If you are unhappy about it you can     implement your own     :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getParameterArray`     method in the subclass.      The implementation prevents accessing parameters names that start with     ``_``. It may be better to enforce this convention at the node spec level.      :param name: (string) name of requested parameter     :param index: (int) index of node inside the region (if relevant)     :param array: output numpy array that the value is written to     :raises: Exception if parameter starts with ``_``.
Calls :meth:`~nupic.bindings.regions.PyRegion.PyRegion.writeToProto`     on subclass after converting proto to specific type using     :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSchema`.      :param proto: PyRegionProto capnproto object
Calls :meth:`~nupic.bindings.regions.PyRegion.PyRegion.readFromProto`     on subclass after converting proto to specific type using     :meth:`~nupic.bindings.regions.PyRegion.PyRegion.getSchema`.      :param proto: PyRegionProto capnproto object
Executes a method named ``methodName`` with the specified arguments.      This method is called when the user executes a command as defined in     the node spec. It provides a perfectly reasonble implementation     of the command mechanism. As a sub-class developer you just need to     implement a method for each command in the node spec. Note that due to     the command mechanism only unnamed argument are supported.      :param methodName: (string) the name of the method that correspond to a            command in the spec.     :param args: (list) of arguments that will be passed to the method
Set region sparse output value.      The region output memory is owned by the c++ caller and cannot be changed       directly from python. Use this method to update the sparse output fields in       the "outputs" array so it can be resized from the c++ code.      :param outputs: (dict) of numpy arrays. This is the original outputs dict             owned by the C++ caller, passed to region via the compute method to             be updated.     :param name: (string) name of an existing output to modify     :param value: (list) list of UInt32 indices of all the nonzero entries             representing the sparse array to be set
Measure capnp serialization performance of Random
Generate Steam 2FA code for timestamp      :param shared_secret: authenticator shared secret     :type shared_secret: bytes     :param timestamp: timestamp to use, if left out uses current time     :type timestamp: int     :return: steam two factor code     :rtype: str
Generate confirmation key for trades. Can only be used once.      :param identity_secret: authenticator identity secret     :type identity_secret: bytes     :param tag: tag identifies what the request, see list below     :type tag: str     :param timestamp: timestamp to use for generating key     :type timestamp: int     :return: confirmation key     :rtype: bytes      Tag choices:          * ``conf`` to load the confirmations page         * ``details`` to load details about a trade         * ``allow`` to confirm a trade         * ``cancel`` to cancel a trade
Get time offset from steam server time via WebAPI      :return: time offset (``None`` when Steam WebAPI fails to respond)     :rtype: :class:`int`, :class:`None`
Generate Android device id      :param steamid: Steam ID     :type steamid: :class:`.SteamID`, :class:`int`     :return: android device id     :rtype: str
Extract Steam Authenticator secrets from a rooted Android device      Prerequisite for this to work:          - rooted android device         - `adb binary <https://developer.android.com/studio/command-line/adb.html>`_         - device in debug mode, connected and paired      .. note::         If you know how to make this work, without requiring the device to be rooted,         please open a issue on github. Thanks      :param adb_path: path to adb binary     :type adb_path: str     :raises: When there is any problem     :return: all secrets from the device, steamid as key     :rtype: dict
:return: Steam aligned timestamp         :rtype: int
:param timestamp: time to use for code generation         :type timestamp: int         :return: two factor code         :rtype: str
:param tag: see :func:`generate_confirmation_key` for this value         :type tag: str         :param timestamp: time to use for code generation         :type timestamp: int         :return: trade confirmation key         :rtype: str
Add authenticator to an account.         The account's phone number will receive a SMS code required for :meth:`finalize`.          :raises: :class:`SteamAuthenticatorError`
Finalize authenticator with received SMS code          :param activation_code: SMS code         :type activation_code: str         :raises: :class:`SteamAuthenticatorError`
Remove authenticator          .. note::             After removing authenticator Steam Guard will be set to email codes          .. warning::             Doesn't work via :class:`.SteamClient`. Disabled by Valve          :raises: :class:`SteamAuthenticatorError`
Generate emergency codes          :param code: SMS code         :type code: str         :raises: :class:`SteamAuthenticatorError`         :return: list of codes         :rtype: list          .. note::             A confirmation code is required to generate emergency codes and this method needs             to be called twice as shown below.          .. code:: python              sa.create_emergency_codes()              # request a SMS code             sa.create_emergency_codes(code='12345')  # creates emergency codes
:return: authenticated web session         :rtype: :class:`requests.Session`         :raises: :class:`RuntimeError` when session is unavailable
Add phone number to account          Then confirm it via :meth:`confirm_phone_number()`          :param phone_number: phone number with country code         :type  phone_number: :class:`str`         :return: success (returns ``False`` on request fail/timeout)         :rtype: :class:`bool`
Confirm phone number with the recieved SMS code          :param sms_code: sms code         :type  sms_code: :class:`str`         :return: success (returns ``False`` on request fail/timeout)         :rtype: :class:`bool`
Check whether the account has a verified phone number          :return: result         :rtype: :class:`bool` or :class:`None`          .. note::             Retruns `None` if the web requests fails for any reason
Get rsa key for a given username          :param username: username         :type  username: :class:`str`         :return: json response         :rtype: :class:`dict`         :raises HTTPError: any problem with http request, timeouts, 5xx, 4xx etc
Attempts web login and returns on a session with cookies set          :param password: password, if it wasn't provided on instance init         :type  password: :class:`str`         :param captcha: text reponse for captcha challenge         :type  captcha: :class:`str`         :param email_code: email code for steam guard         :type  email_code: :class:`str`         :param twofactor_code: 2FA code for steam guard         :type  twofactor_code: :class:`str`         :param language: select language for steam web pages (sets language cookie)         :type  language: :class:`str`         :return: a session on success and :class:`None` otherwise         :rtype: :class:`requests.Session`, :class:`None`         :raises HTTPError: any problem with http request, timeouts, 5xx, 4xx etc         :raises LoginIncorrect: wrong username or password         :raises CaptchaRequired: when captcha is needed         :raises CaptchaRequiredLoginIncorrect: when captcha is needed and login is incorrect         :raises EmailCodeRequired: when email is needed         :raises TwoFactorCodeRequired: when 2FA is needed
Generates CLI prompts to perform the entire login process          :param password: password, if it wasn't provided on instance init         :type  password: :class:`str`         :param captcha: text reponse for captcha challenge         :type  captcha: :class:`str`         :param email_code: email code for steam guard         :type  email_code: :class:`str`         :param twofactor_code: 2FA code for steam guard         :type  twofactor_code: :class:`str`         :param language: select language for steam web pages (sets language cookie)         :type  language: :class:`str`         :return: a session on success and :class:`None` otherwise         :rtype: :class:`requests.Session`, :class:`None`          .. code:: python              In [3]: user.cli_login()             Enter password for 'steamuser':             Solve CAPTCHA at https://steamcommunity.com/login/rendercaptcha/?gid=1111111111111111111             CAPTCHA code: 123456             Invalid password for 'steamuser'. Enter password:             Solve CAPTCHA at https://steamcommunity.com/login/rendercaptcha/?gid=2222222222222222222             CAPTCHA code: abcdef             Enter 2FA code: AB123             Out[3]: <requests.sessions.Session at 0x6fffe56bef0>
.. versionadded:: 0.8.2          Find a leaderboard          :param app_id: application id         :type app_id: :class:`int`         :param name: leaderboard name         :type name: :class:`str`         :return: leaderboard instance         :rtype: :class:`SteamLeaderboard`         :raises: :class:`LookupError` on message timeout or error
Get leaderboard entries.          :param start: start entry, not index (e.g. rank 1 is ``start=1``)         :type start: :class:`int`         :param end: end entry, not index (e.g. only one entry then ``start=1,end=1``)         :type end: :class:`int`         :param data_request: data being requested         :type data_request: :class:`steam.enums.common.ELeaderboardDataRequest`         :param steam_ids: list of steam ids when using :prop:`.ELeaderboardDataRequest.Users`         :type steamids: :class:`list`         :return: a list of entries, see ``CMsgClientLBSGetLBEntriesResponse``         :rtype: :class:`list`         :raises: :class:`LookupError` on message timeout or error
Make a iterator over the entries          See :class:`steam.util.throttle.ConstantRateLimit` for ``times`` and ``seconds`` parameters.          :param chunk_size: number of entries per request         :type chunk_size: :class:`int`         :returns: generator object         :rtype: :class:`generator`          The iterator essentially buffers ``chuck_size`` number of entries, and ensures         we are not sending messages too fast.         For example, the ``__iter__`` method on this class uses ``get_iter(1, 1, 2000)``
Attempt to establish connection, see :meth:`.CMClient.connect`
Close connection, see :meth:`.CMClient.disconnect`
Implements explonential backoff delay before attempting to connect.         It is otherwise identical to calling :meth:`.CMClient.connect`.         The delay is reset upon a successful login.          :param maxdelay: maximum delay in seconds before connect (0-120s)         :type maxdelay: :class:`int`         :param retry: see :meth:`.CMClient.connect`         :type retry: :class:`int`         :return: successful connection         :rtype: :class:`bool`
Wait for a message, similiar to :meth:`.wait_event`          :param event: :class:`.EMsg' or job id         :param timeout: seconds to wait before timeout         :type timeout: :class:`int`         :param raises: On timeout when ``False` returns :class:`None`, else raise :class:`gevent.Timeout`         :type raises: :class:`bool`         :return: returns a message or :class:`None`         :rtype: :class:`None`, or `proto message`         :raises: ``gevent.Timeout``
.. versionchanged:: 0.8.4         Send a message to CM          :param message: a message instance         :type message: :class:`.Msg`, :class:`.MsgProto`         :param body_params: a dict with params to the body (only :class:`.MsgProto`)         :type body_params: dict
.. versionchanged:: 0.8.4         Send a message as a job          .. note::             Not all messages are jobs, you'll have to find out which are which          :param message: a message instance         :type message: :class:`.Msg`, :class:`.MsgProto`         :param body_params: a dict with params to the body (only :class:`.MsgProto`)         :type body_params: dict         :return: ``jobid`` event identifier         :rtype: :class:`str`          To catch the response just listen for the ``jobid`` event.          .. code:: python              jobid = steamclient.send_job(my_message)              resp = steamclient.wait_event(jobid, timeout=15)             if resp:                 (message,) = resp
.. versionchanged:: 0.8.4         Send a message as a job and wait for the response.          .. note::             Not all messages are jobs, you'll have to find out which are which          :param message: a message instance         :type message: :class:`.Msg`, :class:`.MsgProto`         :param body_params: a dict with params to the body (only :class:`.MsgProto`)         :type body_params: dict         :param timeout: (optional) seconds to wait         :type timeout: :class:`int`         :param raises: (optional) On timeout if ``False`` return ``None``, else raise ``gevent.Timeout``         :type raises: :class:`bool`         :return: response proto message         :rtype: :class:`.Msg`, :class:`.MsgProto`         :raises: ``gevent.Timeout``
.. versionchanged:: 0.8.4         Send a message to CM and wait for a defined answer.          :param message: a message instance         :type message: :class:`.Msg`, :class:`.MsgProto`         :param response_emsg: emsg to wait for         :type response_emsg: :class:`.EMsg`,:class:`int`         :param body_params: a dict with params to the body (only :class:`.MsgProto`)         :type body_params: dict         :param timeout: (optional) seconds to wait         :type timeout: :class:`int`         :param raises: (optional) On timeout if ``False`` return ``None``, else raise ``gevent.Timeout``         :type raises: :class:`bool`         :return: response proto message         :rtype: :class:`.Msg`, :class:`.MsgProto`         :raises: ``gevent.Timeout``
Returns contents of sentry file for the given username          .. note::             returns ``None`` if :attr:`credential_location` is not set, or file is not found/inaccessible          :param username: username         :type username: :class:`str`         :return: sentry file contents, or ``None``         :rtype: :class:`bytes`, :class:`None`
Store sentry bytes under a username          :param username: username         :type username: :class:`str`         :return: Whenver the operation succeed         :rtype: :class:`bool`
Login without needing credentials, essentially remember password.         The :attr:`login_key` is acquired after successful login and it will be         automatically acknowledged. Listen for the ``new_login_key`` event.         After that the client can relogin using this method.          .. note::             Only works when :attr:`relogin_available` is ``True``.          .. code:: python              if client.relogin_available: client.relogin()             else:                 client.login(user, pass)
Login as a specific user          :param username: username         :type username: :class:`str`         :param password: password         :type password: :class:`str`         :param login_key: login key, instead of password         :type login_key: :class:`str`         :param auth_code: email authentication code         :type auth_code: :class:`str`         :param two_factor_code: 2FA authentication code         :type two_factor_code: :class:`str`         :param login_id: number used for identifying logon session         :type login_id: :class:`int`         :return: logon result, see `CMsgClientLogonResponse.eresult <https://github.com/ValvePython/steam/blob/513c68ca081dc9409df932ad86c66100164380a6/protobufs/steammessages_clientserver.proto#L95-L118>`_         :rtype: :class:`.EResult`          .. note::             Failure to login will result in the server dropping the connection, ``error`` event is fired          ``auth_code_required`` event is fired when 2FA or Email code is needed.         Here is example code of how to handle the situation.          .. code:: python              @steamclient.on(steamclient.EVENT_AUTH_CODE_REQUIRED)             def auth_code_prompt(is_2fa, code_mismatch):                 if is_2fa:                     code = input("Enter 2FA Code: ")                     steamclient.login(username, password, two_factor_code=code)                 else:                     code = input("Enter Email Code: ")                     steamclient.login(username, password, auth_code=code)          Codes are required every time a user logins if sentry is not setup.         See :meth:`set_credential_location`
Login as anonymous user          :return: logon result, see `CMsgClientLogonResponse.eresult <https://github.com/ValvePython/steam/blob/513c68ca081dc9409df932ad86c66100164380a6/protobufs/steammessages_clientserver.proto#L95-L118>`_         :rtype: :class:`.EResult`
Logout from steam. Doesn't nothing if not logged on.          .. note::             The server will drop the connection immediatelly upon logout.
Generates CLI prompts to complete the login process          :param username: optionally provide username         :type  username: :class:`str`         :param password: optionally provide password         :type  password: :class:`str`         :return: logon result, see `CMsgClientLogonResponse.eresult <https://github.com/ValvePython/steam/blob/513c68ca081dc9409df932ad86c66100164380a6/protobufs/steammessages_clientserver.proto#L95-L118>`_         :rtype: :class:`.EResult`          Example console output after calling :meth:`cli_login`          .. code:: python              In [5]: client.cli_login()             Steam username: myusername             Password:             Steam is down. Keep retrying? [y/n]: y             Invalid password for 'myusername'. Enter password:             Enter email code: 123             Incorrect code. Enter email code: K6VKF             Out[5]: <EResult.OK: 1>
Initiate connection to CM. Blocks until connected unless ``retry`` is specified.          :param retry: number of retries before returning. Unlimited when set to ``None``         :type retry: :class:`int`         :param delay: delay in secnds before connection attempt         :type delay: :class:`int`         :return: successful connection         :rtype: :class:`bool`
Close connection
Send a message          :param message: a message instance         :type message: :class:`steam.core.msg.Msg`, :class:`steam.core.msg.MsgProto`
Clears the server list
Fetches CM server list from WebAPI and replaces the current one          :param cellid: cell id (0 = global)         :type cellid: :class:`int`         :return: booststrap success         :rtype: :class:`bool`
Reset status for all servers in the list
Mark server address as good          :param server_addr: (ip, port) tuple         :type server_addr: :class:`tuple`
Mark server address as bad, when unable to connect for example          :param server_addr: (ip, port) tuple         :type server_addr: :class:`tuple`
Add new CM servers to the list          :param new_list: a list of ``(ip, port)`` tuples         :type new_list: :class:`list`
Add/Accept a steam user to be your friend.         When someone sends you an invite, use this method to accept it.          :param steamid_or_accountname_or_email: steamid, account name, or email         :type steamid_or_accountname_or_email: :class:`int`, :class:`.SteamID`, :class:`.SteamUser`, :class:`str`          .. note::             Adding by email doesn't not work. It's only mentioned for the sake of completeness.
Remove a friend          :param steamid: their steamid         :type steamid: :class:`int`, :class:`.SteamID`, :class:`.SteamUser`
Low level function for calling Steam's WebAPI      .. versionchanged:: 0.8.3      :param url: request url (e.g. ``https://api.steampowered.com/A/B/v001/``)     :type url: :class:`str`     :param method: HTTP method (GET or POST)     :type method: :class:`str`     :param caller: caller reference, caller.last_response is set to the last response     :param params: dict of WebAPI and endpoint specific params     :type params: :class:`dict`     :param session: an instance requests session, or one is created per call     :type session: :class:`requests.Session`     :return: response based on paramers     :rtype: :class:`dict`, :class:`lxml.etree.Element`, :class:`str`
Send GET request to an API endpoint      .. versionadded:: 0.8.3      :param interface: interface name     :type interface: str     :param method: method name     :type method: str     :param version: method version     :type version: int     :param apihost: API hostname     :type apihost: str     :param https: whether to use HTTPS     :type https: bool     :param params: parameters for endpoint     :type params: dict     :return: endpoint response     :rtype: :class:`dict`, :class:`lxml.etree.Element`, :class:`str`
Send POST request to an API endpoint      .. versionadded:: 0.8.3      :param interface: interface name     :type interface: str     :param method: method name     :type method: str     :param version: method version     :type version: int     :param apihost: API hostname     :type apihost: str     :param https: whether to use HTTPS     :type https: bool     :param params: parameters for endpoint     :type params: dict     :return: endpoint response     :rtype: :class:`dict`, :class:`lxml.etree.Element`, :class:`str`
Returns a dict with the response from ``GetSupportedAPIList``          :return: :class:`dict` of all interfaces and methods          The returned value can passed to :meth:`load_interfaces`
Populates the namespace under the instance
Make an API call for specific method          :param method_path: format ``Interface.Method`` (e.g. ``ISteamWebAPIUtil.GetServerInfo``)         :type method_path: :class:`str`         :param kwargs: keyword arguments for the specific method         :return: response         :rtype: :class:`dict`, :class:`lxml.etree.Element` or :class:`str`
:return: Documentation for all interfaces and their methods         :rtype: str
Get request proto instance for given methed name          :param method_name: name for the method (e.g. ``Player.GetGameBadgeLevels#1``)         :type method_name: :class:`str`         :return: proto message instance, or :class:`None` if not found
Send service method request          :param message:             proto message instance (use :meth:`SteamUnifiedMessages.get`)             or method name (e.g. ``Player.GetGameBadgeLevels#1``)         :type message: :class:`str`, proto message instance         :param params: message parameters         :type params: :class:`dict`         :return: ``jobid`` event identifier         :rtype: :class:`str`          Listen for ``jobid`` on this object to catch the response.          .. note::             If you listen for ``jobid`` on the client instance you will get the encapsulated message
Send service method request and wait for response          :param message:             proto message instance (use :meth:`SteamUnifiedMessages.get`)             or method name (e.g. ``Player.GetGameBadgeLevels#1``)         :type message: :class:`str`, proto message instance         :param params: message parameters         :type params: :class:`dict`         :param timeout: (optional) seconds to wait         :type timeout: :class:`int`         :param raises: (optional) On timeout if :class:`False` return :class:`None`, else raise :class:`gevent.Timeout`         :type raises: :class:`bool`         :return: response proto message instance         :rtype: (proto message, :class:`.UnifiedMessageError`)         :raises: :class:`gevent.Timeout`
Return n bytes          :param n: number of bytes to return         :type  n: :class:`int`         :return: bytes         :rtype: :class:`bytes`
Reads a single null termianted string          :return: string without bytes         :rtype: :class:`bytes`
Unpack bytes using struct modules format          :param format_text: struct's module format         :type  format_text: :class:`str`         :return data: result from :func:`struct.unpack_from`         :rtype: :class:`tuple`
:param hmac_secret: optional HMAC     :type hmac_secret: :class:`bytes`     :return: (session_key, encrypted_session_key) tuple     :rtype: :class:`tuple`
:raises: :class:`RuntimeError` when HMAC verification fails
Converts protobuf message instance to dict      :param message: protobuf message instance     :return: parameters and their values     :rtype: dict     :raises: :class:`.TypeError` if ``message`` is not a proto message
Fills protobuf message parameters inplace from a :class:`dict`      :param message: protobuf message instance     :param data: parameters and values     :type data: dict     :param clear: whether clear exisiting values     :type clear: bool     :return: value of message paramater     :raises: incorrect types or values will raise
Splits a list into chunks      :param arr: list to split     :type arr: :class:`list`     :param size: number of elements in each chunk     :type size: :class:`int`     :return: generator object     :rtype: :class:`generator`
Send a message to GC          :param header: message header         :type header: :class:`steam.core.msg.GCMsgHdr`         :param body: serialized body of the message         :type body: :class:`bytes`
Get protobuf for given method name      :param method_name: full method name (e.g. ``Player.GetGameBadgeLevels#1``)     :type method_name: :class:`str`     :param response: whether to return proto for response or request     :type response: :class:`bool`     :return: protobuf message
Returns steam64 from various other representations.      .. code:: python          make_steam64()  # invalid steamid         make_steam64(12345)  # accountid         make_steam64('12345')         make_steam64(id=12345, type='Invalid', universe='Invalid', instance=0)         make_steam64(103582791429521412)  # steam64         make_steam64('103582791429521412')         make_steam64('STEAM_1:0:2')  # steam2         make_steam64('[g:1:4]')  # steam3
:param value: steam2 (e.g. ``STEAM_1:0:1234``)     :type value: :class:`str`     :return: (accountid, type, universe, instance)     :rtype: :class:`tuple` or :class:`None`      .. note::         The universe will be always set to ``1``. See :attr:`SteamID.as_steam2`
:param value: steam3 (e.g. ``[U:1:1234]``)     :type value: :class:`str`     :return: (accountid, type, universe, instance)     :rtype: :class:`tuple` or :class:`None`
Takes a Steam Community url and returns steam64 or None      .. note::         Each call makes a http request to ``steamcommunity.com``      .. note::         For a reliable resolving of vanity urls use ``ISteamUser.ResolveVanityURL`` web api      :param url: steam community url     :type url: :class:`str`     :param http_timeout: how long to wait on http request before turning ``None``     :type http_timeout: :class:`int`     :return: steam64, or ``None`` if ``steamcommunity.com`` is down     :rtype: :class:`int` or :class:`None`      Example URLs::          https://steamcommunity.com/gid/[g:1:4]         https://steamcommunity.com/gid/103582791429521412         https://steamcommunity.com/groups/Valve         https://steamcommunity.com/profiles/[U:1:12]         https://steamcommunity.com/profiles/76561197960265740         https://steamcommunity.com/id/johnc
:return: steam3 format (e.g ``[U:1:1234]``)         :rtype: :class:`str`
:return: e.g https://steamcommunity.com/profiles/123456789         :rtype: :class:`str`
Check whether this SteamID is valid          :rtype: :py:class:`bool`
r"""         Query game servers          https://developer.valvesoftware.com/wiki/Master_Server_Query_Protocol          .. note::             When specifying ``filter_text`` use *raw strings* otherwise python won't treat backslashes             as literal characters (e.g. ``query(r'\appid\730\white\1')``)          :param filter_text: filter for servers         :type  filter_text: str         :param max_servers: (optional) number of servers to return         :type  max_servers: int         :param timeout: (optional) timeout for request in seconds         :type  timeout: int         :param app_id: (optional) app id         :type  app_id: int         :param geo_location_ip: (optional) ip (e.g. '1.2.3.4')         :type  geo_location_ip: str         :returns: list of servers, see below. (``None`` is returned steam doesn't respond)         :rtype: :class:`list`, :class:`None`          Sample response:          .. code:: python              [{'auth_players': 0, 'server_ip': '1.2.3.4', 'server_port': 27015},              {'auth_players': 6, 'server_ip': '1.2.3.4', 'server_port': 27016},              ...             ]
Get list of servers. Works similiarly to :meth:`query`, but the response has more details.          :param filter_text: filter for servers         :type  filter_text: str         :param max_servers: (optional) number of servers to return         :type  max_servers: int         :param timeout: (optional) timeout for request in seconds         :type  timeout: int         :returns: list of servers, see below. (``None`` is returned steam doesn't respond)         :rtype: :class:`list`, :class:`None`         :raises: :class:`.UnifiedMessageError`           Sample response:          .. code:: python              [{'addr': '1.2.3.4:27067',               'appid': 730,               'bots': 0,               'dedicated': True,               'gamedir': 'csgo',               'gameport': 27067,               'gametype': 'valve_ds,empty,secure',               'map': 'de_dust2',               'max_players': 10,               'name': 'Valve CS:GO Asia Server (srcdsXXX.XXX.XXX)',               'os': 'l',               'players': 0,               'product': 'csgo',               'region': 5,               'secure': True,               'steamid': SteamID(id=3279818759, type='AnonGameServer', universe='Public', instance=7011),               'version': '1.35.4.0'}             ]
Resolve IPs from SteamIDs          :param server_steam_ids: a list of steamids         :type  server_steam_ids: list         :param timeout: (optional) timeout for request in seconds         :type  timeout: int         :return: map of ips to steamids         :rtype: dict         :raises: :class:`.UnifiedMessageError`          Sample response:          .. code:: python              {SteamID(id=123456, type='AnonGameServer', universe='Public', instance=1234): '1.2.3.4:27060'}
Resolve SteamIDs from IPs          :param steam_ids: a list of ips (e.g. ``['1.2.3.4:27015',...]``)         :type  steam_ids: list         :param timeout: (optional) timeout for request in seconds         :type  timeout: int         :return: map of steamids to ips         :rtype: dict         :raises: :class:`.UnifiedMessageError`          Sample response:          .. code:: python              {'1.2.3.4:27060': SteamID(id=123456, type='AnonGameServer', universe='Public', instance=1234)}
Request validation email          :return: result         :rtype: :class:`.EResult`
Request password change mail          :param password: current account password         :type  password: :class:`str`         :return: result         :rtype: :class:`.EResult`
Change account's password          :param password: current account password         :type  password: :class:`str`         :param new_password: new account password         :type  new_password: :class:`str`         :param email_code: confirmation code from email         :type  email_code: :class:`str`         :return: result         :rtype: :class:`.EResult`          .. note::             First request change mail via :meth:`request_password_change_mail()`             to get the email code
Get property from PersonaState          `See full list of available fields_names <https://github.com/ValvePython/steam/blob/fa8a5127e9bb23185483930da0b6ae85e93055a7/protobufs/steammessages_clientserver_friends.proto#L125-L153>`_
Personsa state (e.g. Online, Offline, Away, Busy, etc)          :rtype: :class:`.EPersonaState`
Contains Rich Presence key-values          :rtype: dict
Get URL to avatar picture          :param size: possible values are ``0``, ``1``, or ``2`` corresponding to small, medium, large         :type size: :class:`int`         :return: url to avatar         :rtype: :class:`str`
Send chat message to this steam user          :param message: message to send         :type message: str
Blocks until the rate is met
Get numbers of players for app id          :param app_id: app id         :type app_id: :class:`int`         :return: number of players         :rtype: :class:`int`, :class:`.EResult`
Get product info for apps and packages          :param apps: items in the list should be either just ``app_id``, or ``(app_id, access_token)``         :type apps: :class:`list`         :param packages: items in the list should be either just ``package_id``, or ``(package_id, access_token)``         :type packages: :class:`list`         :return: dict with ``apps`` and ``packages`` containing their info, see example below         :rtype: :class:`dict`, :class:`None`          .. code:: python              {'apps':     {570: {...}, ...},              'packages': {123: {...}, ...}             }
Get changes since a change number          :param change_number: change number to use as stating point         :type change_number: :class:`int`         :param app_changes: whether to inclued app changes         :type app_changes: :class:`bool`         :param package_changes: whether to inclued package changes         :type package_changes: :class:`bool`         :return: `CMsgClientPICSChangesSinceResponse <https://github.com/ValvePython/steam/blob/39627fe883feeed2206016bacd92cf0e4580ead6/protobufs/steammessages_clientserver.proto#L1171-L1191>`_         :rtype: proto message instance, or :class:`None` on timeout
Get app ownership ticket          :param app_id: app id         :type app_id: :class:`int`         :return: `CMsgClientGetAppOwnershipTicketResponse <https://github.com/ValvePython/steam/blob/39627fe883feeed2206016bacd92cf0e4580ead6/protobufs/steammessages_clientserver.proto#L158-L162>`_         :rtype: proto message
Get depot decryption key          :param depot_id: depot id         :type depot_id: :class:`int`         :param app_id: app id         :type app_id: :class:`int`         :return: `CMsgClientGetDepotDecryptionKeyResponse <https://github.com/ValvePython/steam/blob/39627fe883feeed2206016bacd92cf0e4580ead6/protobufs/steammessages_clientserver_2.proto#L533-L537>`_         :rtype: proto message
Get CDN authentication token          :param app_id: app id         :type app_id: :class:`int`         :param hostname: cdn hostname         :type hostname: :class:`str`         :return: `CMsgClientGetCDNAuthTokenResponse <https://github.com/ValvePython/steam/blob/39627fe883feeed2206016bacd92cf0e4580ead6/protobufs/steammessages_clientserver_2.proto#L585-L589>`_         :rtype: proto message
Get access tokens          :param app_ids: list of app ids         :type app_ids: :class:`list`         :param package_ids: list of package ids         :type package_ids: :class:`list`         :return: dict with ``apps`` and ``packages`` containing their access tokens, see example below         :rtype: :class:`dict`, :class:`None`          .. code:: python              {'apps':     {123: 'token', ...},              'packages': {456: 'token', ...}             }
Register/Redeem a CD-Key          :param key: CD-Key         :type  key: :class:`str`         :return: format ``(eresult, result_details, receipt_info)``         :rtype: :class:`tuple`          Example ``receipt_info``:          .. code:: python              {'BasePrice': 0,               'CurrencyCode': 0,               'ErrorHeadline': '',               'ErrorLinkText': '',               'ErrorLinkURL': '',               'ErrorString': '',               'LineItemCount': 1,               'PaymentMethod': 1,               'PurchaseStatus': 1,               'ResultDetail': 0,               'Shipping': 0,               'Tax': 0,               'TotalDiscount': 0,               'TransactionID': UINT_64(111111111111111111),               'TransactionTime': 1473000000,               'lineitems': {'0': {'ItemDescription': 'Half-Life 3',                 'TransactionID': UINT_64(11111111111111111),                 'packageid': 1234}},               'packageid': -1}
r"""Generator that returns (IP,port) pairs of servers      .. warning::         Valve's master servers seem to be heavily rate limited.         Queries that return a large numbers IPs will timeout before returning everything.         There is no way to resume the query.         Use :class:`SteamClient` to access game servers in a reliable way.      .. note::         When specifying ``filter_text`` use *raw strings* otherwise python won't treat backslashes         as literal characters (e.g. ``query(r'\appid\730\white\1')``)      :param filter_text: filter for servers     :type  filter_text: str     :param region: (optional) region code     :type  region: :class:`.MSRegion`     :param master: (optional) master server to query     :type  master: (:class:`str`, :class:`int`)     :raises: :class:`RuntimeError`, :class:`socket.timeout`     :returns: a generator yielding (ip, port) pairs     :rtype: :class:`generator`
Get information from a server      .. note::         All ``GoldSrc`` games have been updated to reply in ``Source`` format.         ``GoldSrc`` format is essentially DEPRECATED.         By default the function will prefer to return ``Source`` format, and will         automatically fallback to ``GoldSrc`` if available.      :param server_addr: (ip, port) for the server     :type  server_addr: tuple     :param force_goldsrc: (optional) only accept ``GoldSrc`` response format     :type  force_goldsrc: :class:`bool`     :param timeout: (optional) timeout in seconds     :type  timeout: float     :raises: :class:`RuntimeError`, :class:`socket.timeout`     :returns: a dict with information or `None` on timeout     :rtype: :class:`dict`
Get list of players and their info      :param server_addr: (ip, port) for the server     :type  server_addr: tuple     :param timeout: (optional) timeout in seconds     :type  timeout: float     :param challenge: (optional) challenge number     :type  challenge: int     :raises: :class:`RuntimeError`, :class:`socket.timeout`     :returns: a list of players     :rtype: :class:`list`
Get rules from server      :param server_addr: (ip, port) for the server     :type  server_addr: tuple     :param timeout: (optional) timeout in seconds     :type  timeout: float     :param challenge: (optional) challenge number     :type  challenge: int     :raises: :class:`RuntimeError`, :class:`socket.timeout`     :returns: a list of players     :rtype: :class:`list`
Ping a server      .. warning::         This method for pinging is considered deprecated and may not work on certian servers.         Use :func:`.a2s_info` instead.      :param server_addr: (ip, port) for the server     :type  server_addr: tuple     :param timeout: (optional) timeout in seconds     :type  timeout: float     :raises: :class:`RuntimeError`, :class:`socket.timeout`     :returns: ping response in milliseconds or `None` for timeout     :rtype: :class:`float`
Get protobuf for a given EMsg      :param emsg: EMsg     :type emsg: :class:`steam.enums.emsg.EMsg`, :class:`int`     :return: protobuf message
Get web authentication cookies via WebAPI's ``AuthenticateUser``          .. note::             The cookies are valid only while :class:`.SteamClient` instance is logged on.          :return: dict with authentication cookies         :rtype: :class:`dict`, :class:`None`
Get a :class:`requests.Session` that is ready for use          See :meth:`get_web_session_cookies`          .. note::             Auth cookies will only be send to ``(help|store).steampowered.com`` and ``steamcommunity.com`` domains          .. note::             The session is valid only while :class:`.SteamClient` instance is logged on.          :param language: localization language for steam pages         :type language: :class:`str`         :return: authenticated Session ready for use         :rtype: :class:`requests.Session`, :class:`None`
:returns: requests session     :rtype: :class:`requests.Session`
Set name, persona state, flags          .. note::             Changing persona state will also change :attr:`persona_state`          :param persona_state: persona state (Online/Offlane/Away/etc)         :type persona_state: :class:`.EPersonaState`         :param player_name: profile name         :type player_name: :class:`str`         :param persona_state_flags: persona state flags         :type persona_state_flags: :class:`.EPersonaStateFlag`
Request persona state data          :param steam_ids: list of steam ids         :type  steam_ids: :class:`list`         :param state_flags: client state flags         :type  state_flags: :class:`.EClientPersonaStateFlag`
Get :class:`.SteamUser` instance for ``steam id``          :param steam_id: steam id         :type steam_id: :class:`int`, :class:`.SteamID`         :param fetch_persona_state: whether to request person state when necessary         :type fetch_persona_state: :class:`bool`         :return: SteamUser instance         :rtype: :class:`.SteamUser`
Set the apps being played by the user          :param app_ids: a list of application ids         :type app_ids: :class:`list`          These app ids will be recorded in :attr:`current_games_played`.
Make new GlobalID          :param sequence_count: sequence count         :type sequence_count: :class:`int`         :param start_time: start date time of server (must be after 2005-01-01)         :type start_time: :class:`str`, :class:`datetime`         :param process_id: process id         :type process_id: :class:`int`         :param box_id: box id         :type box_id: :class:`int`         :return: Global ID integer         :rtype: :class:`int`
Retrieves all people that share their location with this account
Retrieves the person associated with this account
Retrieves a person by nickname
Retrieves a person by full name
Retrieves a person's coordinates by nickname
Retrieves a person's coordinates by full name
A datetime representation of the location retrieval
Cross-platform version of `os.path.isabs()`       Returns True if `filename` is absolute on       Linux, OS X or Windows.
Cross-platform version of os.path.normpath
Make relative path out of absolute by stripping       prefixes used on Linux, OS X and Windows.        This function is critical for security.
Parse patch file. If successful, returns       PatchSet() object. Otherwise returns False.
Parse text string and return PatchSet()       object (or False if parsing fails)
Parse patch from an URL, return False       if an error occured. Note that this also       can throw urlopen() exceptions.
Strip n leading components from the given path
parse unified diff         return True on success
detect and return type for the specified Patch object         analyzes header and filenames info          NOTE: must be run before filenames are normalized
sanitize filenames, normalizing paths, i.e.:         1. strip a/ and b/ prefixes from GIT and HG style patches         2. remove all references to parent directories (with warning)         3. translate any absolute paths to relative (with warning)          [x] always use forward slashes to be crossplatform             (diff/patch were born as a unix utility after all)          return None
calculate diffstat and return as a string         Notes:           - original diffstat ouputs target filename           - single + or - shouldn't escape histogram
return name of file to be patched or None
Apply parsed patch, optionally stripping leading components         from file paths. `root` parameter specifies working dir.         return True on success
reverse patch direction (this doesn't touch filenames)
apply patch in reverse order
Check if specified filename can be patched. Returns None if file can     not be found among source filenames. False if patch can not be applied     clearly. True otherwise.      :returns: True, False or None
Generator that yields stream patched with hunks iterable          Converts lineends in hunk lines to the best suitable format         autodetected from input
Gets the software name and returns the path of the binary.
Error handler for ``shutil.rmtree``.      If the error is due to an access error (read only file)     it attempts to add write permission and then retries.      If the error is for another reason it re-raises the error.      Usage : ``shutil.rmtree(path, onerror=onerror)``      # 2007/11/08     # Version 0.2.6     # pathutils.py     # Functions useful for working with files and paths.     # http://www.voidspace.org.uk/python/recipebook.shtml#utils      # Copyright Michael Foord 2004     # Released subject to the BSD License     # Please see http://www.voidspace.org.uk/python/license.shtml      # For information about bugfixes, updates and support, please join the Pythonutils mailing list.     # http://groups.google.com/group/pythonutils/     # Comments, suggestions and bug reports welcome.     # Scripts maintained at http://www.voidspace.org.uk/python/index.shtml     # E-mail fuzzyman@voidspace.org.uk
Changes into a given directory and cleans up after it is done      Args:         new_directory: The directory to change to         clean_up: A method to clean up the working directory once done
Creates a temporary directory
Puts the two whole widgets in the correct position depending on compound.
Function bound to event of selection in the Combobox, calls callback if callable                  :param event: Tkinter event
Callback for the press of the left mouse button.          Selects a new item and sets its highlightcolor.                  :param event: Tkinter event
Callback for the release of the left button.          :param event: Tkinter event
Callback for the B1-Motion event, or the dragging of an item.          Moves the item to the desired location, but limits its movement to a         place on the actual Canvas. The item cannot be moved outside of the Canvas.          :param event: Tkinter event
Callback for the right mouse button event to pop up the correct menu.                  :param event: Tkinter event
Add a new item on the Canvas.                  :param text: text to display         :type text: str         :param font: font of the text         :type font: tuple or :class:`~tkinter.font.Font`         :param backgroundcolor: background color         :type  backgroundcolor: str         :param textcolor: text color         :type  textcolor: str         :param highlightcolor: the color of the text when the item is selected         :type  highlightcolor: str
Delete the current item on the Canvas.
Set the background image of the Canvas.                  :param image: background image         :type image: PhotoImage         :param path: background image path         :type path: str         :param resize: whether to resize the image to the Canvas size         :type resize: bool
Query widget option.          :param key: option name         :type key: str         :return: value of the option          To get the list of options for this widget, call the method :meth:`~ItemsCanvas.keys`.
Configure resources of the widget.          To get the list of options for this widget, call the method :meth:`~ItemsCanvas.keys`.         See :meth:`~ItemsCanvas.__init__` for a description of the widget specific option.
Save widget content.
Restore previous stdout/stderr and destroy the window.
Convert the content to int between the limits of the variable.          If the content is not an integer between the limits, the value is         corrected and the corrected result is returned.
Set a new auto completion list                  :param completion_list: completion values         :type completion_list: list
Autocomplete the Entry.                  :param delta: 0, 1 or -1: how to cycle through possible hits         :type delta: int
Open a ColorPicker dialog and return the chosen color.      :return: the selected color in RGB(A) and hexadecimal #RRGGBB(AA) formats.              (None, None) is returned if the color selection is cancelled.      :param color: initially selected color (RGB(A), HEX or tkinter color name)     :type color: sequence[int] or str     :param parent: parent widget     :type parent: widget     :param title: dialog title     :type title: str     :param alpha: whether to display the alpha channel     :type alpha: bool
Unfocus palette items when click on bar or square.
Update color preview.
Respond to user click on a palette item.
Respond to user click on a palette item.
Respond to motion of the color selection cross.
Respond to motion of the hsv cursor.
Update display after a change in the HEX entry.
Update display after a change in the alpha spinbox.
Update display after a change in the HSV spinboxes.
Update display after a change in the RGB spinboxes.
Validate color selection and destroy dialog.
Draw the gradient and put the cursor on alpha.
Set cursor position on the color corresponding to the alpha value.          :param alpha: new alpha value (between 0 and 255)         :type alpha: int
Change gradient color and change cursor position if an alpha value is supplied.          :param color: new gradient color in RGB(A) format         :type color: tuple[int]
Event handler for the keyrelease event on this widget.                  :param event: Tkinter event
Puts all the child widgets in the correct position.
Callback if family is changed                  :param family: family name
Callback if properties are changed.                  :param properties: (bool bold, bool italic, bool underline, bool overstrike)
Callback if any of the values are changed.
Generate a font tuple for tkinter widgets based on the user's entries.                  :return: font tuple (family_name, size, *options)
Selected font.                  :return: font tuple (family_name, size, \*options), :class:`~font.Font` object
Change style on focus out events.
Change style on focus in events.
Draw the gradient and put the cursor on hue.
Move selection cursor on click.
Make selection cursor follow the cursor.
Puts all the widgets in the correct place.
Call callback if any property is changed.
Set the text color to the hover color.
Set the text color to either the normal color when not clicked or the clicked color when clicked.
Open the link in the web browser.
Query widget option.          :param key: option name         :type key: str         :return: value of the option          To get the list of options for this widget, call the method :meth:`~LinkLabel.keys`.
Configure resources of the widget.          To get the list of options for this widget, call the method :meth:`~LinkLabel.keys`.         See :meth:`~LinkLabel.__init__` for a description of the widget specific option.
Return a list of all resource names of this widget.
Swap dragged column with its side (=left/right) neighbor.
Insert dragged row at item's position.
Start dragging column/row on left click.
Start dragging a column
Start dragging a row
Stop dragging.
Drag around label if visible.
Continue dragging a column
Continue dragging a row
Sort a column by its values
Query widget option.          :param key: option name         :type key: str         :return: value of the option          To get the list of options for this widget, call the method :meth:`~Table.keys`.
Query or modify the options for the specified column.          If `kw` is not given, returns a dict of the column option values. If         `option` is specified then the value for that option is returned.         Otherwise, sets the options to the corresponding values.          :param id: the column's identifier (read-only option)         :param anchor: "n", "ne", "e", "se", "s", "sw", "w", "nw", or "center":                        alignment of the text in this column with respect to the cell         :param minwidth: minimum width of the column in pixels         :type minwidth: int         :param stretch: whether the column's width should be adjusted when the widget is resized         :type stretch: bool         :param width: width of the column in pixels         :type width: int         :param type: column's content type (for sorting), default type is `str`         :type type: type
Configure resources of the widget.          To get the list of options for this widget, call the method :meth:`~Table.keys`.         See :meth:`~Table.__init__` for a description of the widget specific option.
Apply options set in attributes to Treeview
Configure a new sortable state
Configure a new drag_cols state
Delete all specified items and all their descendants. The root item may not be deleted.          :param items: list of item identifiers         :type items: sequence[str]
Unlinks all of the specified items from the tree.          The items and all of their descendants are still present, and may be         reinserted at another point in the tree, but will not be displayed.         The root item may not be detached.          :param items: list of item identifiers         :type items: sequence[str]
Query or modify the heading options for the specified column.          If `kw` is not given, returns a dict of the heading option values. If         `option` is specified then the value for that option is returned.         Otherwise, sets the options to the corresponding values.          :param text: text to display in the column heading         :type text: str         :param image: image to display to the right of the column heading         :type image: PhotoImage         :param anchor: "n", "ne", "e", "se", "s", "sw", "w", "nw", or "center":                        alignement of the heading text         :type anchor: str         :param command: callback to be invoked when the heading label is pressed.         :type command: function
Creates a new item and return the item identifier of the newly created item.                  :param parent: identifier of the parent item         :type parent: str         :param index: where in the list of parent's children to insert the new item         :type index: int or "end"         :param iid: item identifier, iid must not already exist in the tree. If iid is None a new unique identifier is generated.         :type iid: None or str         :param kw: item's options: see :meth:`~Table.item`                  :return: the item identifier of the newly created item         :rtype: str
Query or modify the options for the specified item.          If no options are given, a dict with options/values for the item is returned.         If option is specified then the value for that option is returned.         Otherwise, sets the options to the corresponding values as given by `kw`.                  :param text: item's label         :type text: str         :param image: image to be displayed on the left of the item's label         :type image: PhotoImage         :param values: values to put in the columns         :type values: sequence         :param open: whether the item's children should be displayed         :type open: bool         :param tags: list of tags associated with this item         :type tags: sequence[str]
Moves item to position index in parent’s list of children.          It is illegal to move an item under one of its descendants. If index is         less than or equal to zero, item is moved to the beginning, if greater         than or equal to the number of children, it is moved to the end.         If item was detached it is reattached.          :param item: item's identifier         :type item: str         :param parent: new parent of item         :type parent: str         :param index: where in the list of parent’s children to insert item         :type index: int of "end"
Query or set the value of given item.          With one argument, return a dictionary of column/value pairs for the         specified item. With two arguments, return the current value of the         specified column. With three arguments, set the value of given column         in given item to the specified value.          :param item: item's identifier         :type item: str         :param column: column's identifier         :type column: str, int or None         :param value: new value
Replaces item’s children with newchildren.          Children present in item that are not present in newchildren are detached         from tree. No items in newchildren may be an ancestor of item.          :param newchildren: new item's children (list of item identifiers)         :type newchildren: sequence[str]
Put the widgets in the correct position based on self.__compound.
Callback for the Entry widget, sets the Scale variable to the appropriate value.                  :param event: Tkinter event
Callback for the Scale widget, inserts an int value into the Entry.          :param event: Tkinter event
Query widget option.          :param key: option name         :type key: str         :return: value of the option          To get the list of options for this widget, call the method :meth:`~ScaleEntry.keys`.
Configure resources of the widget.          To get the list of options for this widget, call the method :meth:`~ScaleEntry.keys`.         See :meth:`~ScaleEntry.__init__` for a description of the widget specific option.
Configure resources of the Scale widget.
Function bound to double click on Listbox that calls the callback if a valid callback object is passed                  :param args: Tkinter event
Selection property.                  :return: None if no font is selected and font family name if one is selected.         :rtype: None or str
Place the widgets in the Toplevel.
Creates a delayed callback for the :obj:`<Enter>` event.
Callback for the :obj:`<Leave>` event to destroy the Toplevel.
Create the Toplevel widget and its child widgets to show in the spot of the cursor.          This is the callback for the delayed :obj:`<Enter>` event (see :meth:`~Balloon._on_enter`).
Query widget option.          :param key: option name         :type key: str         :return: value of the option          To get the list of options for this widget, call the method :meth:`~Balloon.keys`.
Configure resources of the widget.          To get the list of options for this widget, call the method :meth:`~Balloon.keys`.         See :meth:`~Balloon.__init__` for a description of the widget specific option.
Place the widgets in the correct positions         :return: None
Handles clicks and calls callback.
Convert RGB color to HSV.
Convert RGB(A) color to hexadecimal.
Convert hexadecimal color to RGB.
Return hue value corresponding to given RGB color.
Return a checkered image of size width x height.      Arguments:         * width: image width         * height: image height         * c1: first color (RGBA)         * c2: second color (RGBA)         * s: size of the squares
Overlay a rectangle of color (RGBA) on the image and return the result.
Configure all widgets using the grid geometry manager          Automatically called by the :meth:`__init__` method.         Does not have to be called by the user except in extraordinary         cases.
Setup the event bindings for the widgets:         Configure for _timeline         Horizontal and Vertical scrolling for all widgets
Draw the contents of the whole TimeLine Canvas
Draw the time marker on the TimeLine Canvas
Draw the category labels on the Canvas
Setup the scroll region on the Canvas
Clear the contents of the TimeLine Canvas          Does not modify the actual markers dictionary and thus after         redrawing all markers are visible again.
Draw the time tick markers on the TimeLine Canvas
Draw the lines separating the categories on the Canvas
Draw all created markers on the TimeLine Canvas
Function from ScrolledFrame, adapted for the _timeline
Create a new marker in the TimeLine with the specified options          :param category: Category identifier, key as given in categories             dictionary upon initialization         :type category: Any         :param start: Start time for the marker         :type start: float         :param finish: Finish time for the marker         :type finish: float         :param marker: marker dictionary (replaces kwargs)         :type marker: dict[str, Any]          **Marker Options**                  Options can be given either in the marker dictionary argument,         or as keyword arguments. Given keyword arguments take precedence         over tag options, which take precedence over default options.          :param text: Text to show in the marker. Text may not be             displayed fully if the zoom level does not allow the marker             to be wide enough. Updates when resizing the marker.         :type text: str         :param background: Background color for the marker         :type background: str         :param foreground: Foreground (text) color for the marker         :type foreground: str         :param outline: Outline color for the marker         :type outline: str         :param border: The width of the border (for which outline is the             color)         :type border: int         :param font: Font tuple to set for the marker         :type font: tuple         :param iid: Unique marker identifier to use. A marker is             generated if not given, and its value is returned. Use this             option if keeping track of markers in a different manner             than with auto-generated iid's is necessary.         :type iid: str         :param tags: Set of tags to apply to this marker, allowing             callbacks to be set and other options to be configured. The             option precedence is from the first to the last item, so             the options of the last item overwrite those of the one             before, and those of the one before that, and so on.         :type tags: tuple[str]         :param move: Whether the marker is allowed to be moved         :type move: bool          Additionally, all the options with the ``marker_`` prefix from         :meth:`__init__`, but without the prefix, are supported. Active state         options are also available, with the ``active_`` prefix for         ``background``, ``foreground``, ``outline``, ``border``. These         options are also available for the hover state with the         ``hover_`` prefix.          :return: identifier of the created marker         :rtype: str         :raise ValueError: One of the specified arguments is invalid
Draw the text and shorten it if required
Change the options for a certain marker and redraw the marker          :param iid: identifier of the marker to change         :type iid: str         :param kwargs: Dictionary of options to update         :type kwargs: dict         :raises: ValueError
Delete a marker from the TimeLine          :param iid: marker identifier         :type iid: str
Increase zoom factor and redraw TimeLine
Decrease zoom factor and redraw TimeLine
Reset the zoom factor to default and redraw TimeLine
Set the time marker to a specific time          :param time: Time to set for the time marker on the TimeLine         :type time: float
Callback for <B1-Motion> Event: Move the selected marker
Callback for <B1-Release> Event: Hide time marker window
Show the time marker window
Create a marker tag          :param tag_name: Identifier for the tag          :param move_callback: Callback to be called upon moving a             marker. Arguments to callback:             ``(iid: str, (old_start: float, old_finish: float),             (new_start: float, new_finish: float))``         :type move_callback: callable         :param left_callback: Callback to be called upon left clicking             a marker. Arguments to callback:             ``(iid: str, x_coord: int, y_coord: int)``         :type left_callback: callable         :param right_callback: Callback to be called upon right clicking             a marker. Arguments to callback:             ``(iid: str, x_coord: int, y_coord: int)``         :type right_callback: callable         :param menu: A Menu widget to show upon right click. Can be             used with the right_callback option simultaneously.         :type menu: tk.Menu          In addition, supports all options supported by markers. Note         that tag options are applied to markers upon marker creation,         and thus is a tag is updated, the markers are not automatically         updated as well.
Generator for all the tags of a certain marker
Scroll both categories Canvas and scrolling container
Callback <Shift-MouseWheel> event for horizontal scrolling
Callback for <MouseWheel> event for vertical scrolling
Set horizontal scroll of scroll container and ticks Canvas
Get x-coordinate for given time          :param time: Time to determine x-coordinate on Canvas for         :type time: float         :return: X-coordinate for the given time         :rtype: int         :raises: ValueError
Get time for x-coordinate          :param position: X-coordinate position to determine time for         :type position: int         :return: Time for the given x-coordinate         :rtype: float
Create a properly formatted string given a time and unit          :param time: Time to format         :type time: float         :param unit: Unit to apply format of. Only supports hours ('h')             and minutes ('m').         :type unit: str         :return: A string in format '{whole}:{part}'         :rtype: str
Function bound to right click event for marker canvas
Function bound to left click event for marker canvas
Function bound to move event for marker canvas
Callback for :obj:`<Enter>` event on marker, to set hover options
Callback for :obj:`<Leave>` event on marker, to set normal options
Set a custom state of the marker          :param iid: identifier of the marker to set the state of         :type iid: str         :param state: supports "active", "hover", "normal"         :type state: str
Update the active marker on the marker Canvas
Proxy to called by after() in mainloop
Call the available callbacks for a certain marker          :param iid: marker identifier         :type iid: str         :param type: type of callback (key in tag dictionary)         :type type: str         :param args: arguments for the callback         :type args: tuple         :return: amount of callbacks called         :rtype: int
Current value the time marker is pointing to          :rtype: float
Currently active item on the _timeline Canvas          :rtype: str
Currently active item's iid          :rtype: str
Width of the whole TimeLine in pixels          :rtype: int
Update options of the TimeLine widget
Return the value of an option
Configure options of items drawn on the Canvas          Low-level access to the individual elements of markers and other         items drawn on the timeline Canvas. All modifications are         overwritten when the TimeLine is redrawn.
Calculate Canvas text coordinates based on rectangle coords
Like built-in :func:`~builtins.range`, but with float support
Check the type and values of keyword arguments to :meth:`__init__`          :param kwargs: Dictionary of keyword arguments         :type kwargs: dict[str, Any]         :raises: TypeError, ValueError
Check the types of the keyword arguments for marker creation          :param kwargs: dictionary of options for marker creation         :type kwargs: dict         :raises: TypeError, ValueError
Places all the child widgets in the appropriate positions.
Private function to configure the interior Frame.                  :param args: Tkinter event
Private function to configure the internal Canvas.                  Changes the width of the canvas to fit the interior Frame                  :param args: Tkinter event
Function for the user to resize the internal Canvas widget if desired.                  :param height: new height in pixels         :type height: int         :param width: new width in pixels         :type width: int
Set the fractional values of the slider position.                  :param lo: lower end of the scrollbar (between 0 and 1)         :type lo: float         :param hi: upper end of the scrollbar (between 0 and 1)         :type hi: float
Alternative to pack_info and place_info in case of bug.
Place a widget in the parent widget.                  :param in\_: master relative to which the widget is placed         :type in\_: widget         :param x: locate anchor of this widget at position x of master         :type x: int         :param y: locate anchor of this widget at positiony of master         :type y: int         :param relx: locate anchor of this widget between 0 and 1                       relative to width of master (1 is right edge)         :type relx: float         :param rely: locate anchor of this widget between 0 and 1                       relative to height of master (1 is bottom edge)         :type rely: float         :param anchor: position anchor according to given direction                          ("n", "s", "e", "w" or combinations)         :type anchor: str         :param width: width of this widget in pixel         :type width: int         :param height: height of this widget in pixel         :type height: int         :param relwidth: width of this widget between 0.0 and 1.0                           relative to width of master (1.0 is the same width                           as the master)         :type relwidth: float         :param relheight: height of this widget between 0.0 and 1.0                            relative to height of master (1.0 is the same                            height as the master)         :type relheight: float         :param bordermode: "inside" or "outside": whether to take border width of master widget into account         :type bordermode: str
Pack a widget in the parent widget.                  :param after: pack it after you have packed widget         :type after: widget         :param anchor: position anchor according to given direction                          ("n", "s", "e", "w" or combinations)         :type anchor: str         :param before: pack it before you will pack widget         :type before: widget         :param expand: expand widget if parent size grows         :type expand: bool         :param fill: "none" or "x" or "y" or "both": fill widget if widget grows         :type fill: str         :param in\_: widget to use as container         :type in\_: widget         :param ipadx: add internal padding in x direction         :type ipadx: int         :param ipady: add internal padding in y direction         :type ipady: int         :param padx: add padding in x direction         :type padx: int         :param pady: add padding in y irection         :type pady: int         :param side: "top" (default), "bottom", "left" or "right": where to add this widget         :type side: str
Position a widget in the parent widget in a grid.                   :param column: use cell identified with given column (starting with 0)         :type column: int         :param columnspan: this widget will span several columns         :type columnspan: int         :param in\_: widget to use as container         :type in\_: widget         :param ipadx: add internal padding in x direction         :type ipadx: int         :param ipady: add internal padding in y direction         :type ipady: int         :param padx: add padding in x direction         :type padx: int         :param pady: add padding in y irection         :type pady: int         :param row: use cell identified with given row (starting with 0)         :type row: int         :param rowspan: this widget will span several rows         :type rowspan: int         :param sticky: "n", "s", "e", "w" or combinations: if cell is                         larger on which sides will this widget stick to                         the cell boundary         :type sticky: str
Toggle :obj:`ToggledFrame.interior` opened or closed.
Function bound to event of selection in the Combobox, calls callback if callable                  :param args: Tkinter event
Selection property.                  :return: None if no font is selected and font family name if one is selected.         :rtype: None or str
Expand all items.
Collapse all items.
Modify or inquire widget state.                  :param statespec: Widget state is returned if `statespec` is None,                            otherwise it is set according to the statespec                            flags and then a new state spec is returned                            indicating which flags were changed.         :type statespec: None or sequence[str]
Replace the current state of the item.          i.e. replace the current state tag but keeps the other tags.                  :param item: item id         :type item: str         :param state: "checked", "unchecked" or "tristate": new state of the item          :type state: str
Add tag to the tags of item.                  :param item: item identifier         :type item: str         :param tag: tag name         :type tag: str
Remove tag from the tags of item.                  :param item: item identifier         :type item: str         :param tag: tag name         :type tag: str
Creates a new item and return the item identifier of the newly created item.                  :param parent: identifier of the parent item         :type parent: str         :param index: where in the list of parent's children to insert the new item         :type index: int or "end"         :param iid: item identifier, iid must not already exist in the tree. If iid is None a new unique identifier is generated.         :type iid: None or str         :param kw: other options to be passed on to the :meth:`ttk.Treeview.insert` method                  :return: the item identifier of the newly created item         :rtype: str          .. note:: Same method as for the standard :class:`ttk.Treeview` but                    add the tag for the box state accordingly to the parent                    state if no tag among                    ('checked', 'unchecked', 'tristate') is given.
Return the list of checked items that do not have any child.
Check the boxes of item's descendants.
Put the box of item in tristate and change the state of the boxes of         item's ancestors accordingly.
Uncheck the boxes of item's descendant.
Uncheck the box of item and change the state of the boxes of item's         ancestors accordingly.
Check or uncheck box when clicked.
Query widget option.          :param key: option name         :type key: str         :return: value of the option          To get the list of options for this widget, call the method :meth:`~TickScale.keys`.
Configure resources of the widget.          To get the list of options for this widget, call the method :meth:`~TickScale.keys`.         See :meth:`~TickScale.__init__` for a description of the widget specific option.
Convert value in the scale's unit into a position in pixels.                  :param value: value to convert         :type value: float                  :return: the corresponding position in pixels         :rtype: float
Measure the length of the slider and update the value of self._sliderlength.          self.scale.identify(x, y) is used to find the first and last pixels of         the slider. Indeed, self.scale.identify(x, y) returns the element         of the ttk.Scale to which the pixel (x, y) belongs. So, the length of         the slider is determined by scanning horizontally the pixels of the scale.
Measure the length of the slider and update the value of self._sliderlength.          self.scale.identify(x, y) is used to find the first and last pixels of         the slider. Indeed, self.scale.identify(x, y) returns the element         of the ttk.Scale to which the pixel (x, y) belongs. So, the length of         the slider is determined by scanning vertically the pixels of the scale.
Apply the scale style to the frame and labels.
Create and grid the widgets.
Create and grid the widgets for a vertical orientation.
Create and grid the widgets for a horizontal orientation.
Display the current value and update the label position.
Display the current value and update the label position.
Display the ticks for a horizontal scale.
Display the ticks for a vertical slider.
Move the slider only by increment given by resolution.
Redisplay the ticks and the label so that they adapt to the new size of the scale.
Create the gradient.
Draw the gradient and the selection cross on the canvas.
Change hue.          :param value: new hue value (between 0 and 360)         :type value: int
Move cross on click.
Make the cross follow the cursor.
Get selected color.          :return: color under cursor as a (RGB, HSV, HEX) tuple
Put cursor on sel_color given in HSV.          :param sel_color: color in HSV format         :type sel_color: sequence(int)
This returns the Adyen API endpoint based on the provided platform,         service and action.          Args:             platform (str): Adyen platform, ie 'live' or 'test'.             service (str): API service to place request through.             action (str): the API action to perform.
This returns the Adyen HPP endpoint based on the provided platform,         and action.          Args:             platform (str): Adyen platform, ie 'live' or 'test'.             action (str):   the HPP action to perform.             possible actions: select, pay, skipDetails, directory
This returns the Adyen API endpoint based on the provided platform,         service and action.          Args:             platform (str): Adyen platform, ie 'live' or 'test'.             action (str): the API action to perform.
This will call the adyen api. username, password, merchant_account,         and platform are pulled from root module level and or self object.         AdyenResult will be returned on 200 response. Otherwise, an exception         is raised.          Args:             request_data (dict): The dictionary of the request to place. This                 should be in the structure of the Adyen API.                 https://docs.adyen.com/manuals/api-manual             service (str): This is the API service to be called.             action (str): The specific action of the API service to be called             idempotency (bool, optional): Whether the transaction should be                 processed idempotently.                 https://docs.adyen.com/manuals/api-manual#apiidempotency         Returns:             AdyenResult: The AdyenResult is returned when a request was                 succesful.
This will call the adyen hpp. hmac_key and platform are pulled from         root module level and or self object.         AdyenResult will be returned on 200 response.         Otherwise, an exception is raised.          Args:             request_data (dict): The dictionary of the request to place. This                 should be in the structure of the Adyen API.                 https://docs.adyen.com/manuals/api-manual             service (str): This is the API service to be called.             action (str): The specific action of the API service to be called             idempotency (bool, optional): Whether the transaction should be                 processed idempotently.                 https://docs.adyen.com/manuals/api-manual#apiidempotency         Returns:             AdyenResult: The AdyenResult is returned when a request was                 succesful.                 :param message:                 :param hmac_key:
This will call the checkout adyen api. xapi key merchant_account,         and platform are pulled from root module level and or self object.         AdyenResult will be returned on 200 response. Otherwise, an exception         is raised.          Args:             request_data (dict): The dictionary of the request to place. This                 should be in the structure of the Adyen API.                 https://docs.adyen.com/developers/checkout/api-integration             service (str): This is the API service to be called.             action (str): The specific action of the API service to be called
This parses the content from raw communication, raising an error if         anything other than 200 was returned.          Args:             url (str): URL where request was made             raw_response (str): The raw communication sent to Adyen             raw_request (str): The raw response returned by Adyen             status_code (int): The HTTP status code             headers (dict): Key/Value of the headers.             request_dict (dict): The original request dictionary that was given                 to the HTTPClient.          Returns:             AdyenResult: Result object if successful.
This function handles the non 200 responses from Adyen, raising an         error that should provide more information.          Args:             url (str): url of the request             response_obj (dict): Dict containing the parsed JSON response from                 Adyen             status_code (int): HTTP status code of the request             psp_ref (str): Psp reference of the request attempt             raw_request (str): The raw request placed to Adyen             raw_response (str): The raw response(body) returned by Adyen             headers(dict): headers of the response          Returns:             None
This function will POST to the url endpoint using pycurl. returning         an AdyenResult object on 200 HTTP responce. Either json or data has to         be provided. If username and password are provided, basic auth will be         used.           Args:             url (str): url to send the POST             json (dict, optional): Dict of the JSON to POST             data (dict, optional): Dict, presumed flat structure                 of key/value of request to place             username (str, optional): Username for basic auth. Must be included                 as part of password.             password (str, optional): Password for basic auth. Must be included                 as part of username.             headers (dict, optional): Key/Value pairs of headers to include             timeout (int, optional): Default 30. Timeout for the request.          Returns:             str:    Raw response received             str:    Raw request placed             int:    HTTP status code, eg 200,404,401             dict:   Key/Value pairs of the headers received.
This function will POST to the url endpoint using requests.         Returning an AdyenResult object on 200 HTTP response.         Either json or data has to be provided.         If username and password are provided, basic auth will be used.           Args:             url (str): url to send the POST             json (dict, optional): Dict of the JSON to POST             data (dict, optional): Dict, presumed flat structure of key/value                 of request to place             username (str, optionl): Username for basic auth. Must be included                 as part of password.             password (str, optional): Password for basic auth. Must be included                 as part of username.             headers (dict, optional): Key/Value pairs of headers to include             timeout (int, optional): Default 30. Timeout for the request.          Returns:             str:    Raw response received             str:    Raw request placed             int:    HTTP status code, eg 200,404,401             dict:   Key/Value pairs of the headers received.
This function will POST to the url endpoint using urllib2. returning         an AdyenResult object on 200 HTTP responce. Either json or data has to         be provided. If username and password are provided, basic auth will be         used.          Args:             url (str):                  url to send the POST             json (dict, optional):      Dict of the JSON to POST             data (dict, optional):      Dict, presumed flat structure of                                         key/value of request to place as                                         www-form             username (str, optional):    Username for basic auth. Must be                                         uncluded as part of password.             password (str, optional):   Password for basic auth. Must be                                         included as part of username.             headers (dict, optional):   Key/Value pairs of headers to include             timeout (int, optional): Default 30. Timeout for the request.          Returns:             str:    Raw response received             str:    Raw request placed             int:    HTTP status code, eg 200,404,401             dict:   Key/Value pairs of the headers received.
This is overridden on module initialization. This function will make         an HTTP POST to a given url. Either json/data will be what is posted to         the end point. he HTTP request needs to be basicAuth when username and         password are provided. a headers dict maybe provided,         whatever the values are should be applied.          Args:             url (str):                  url to send the POST             json (dict, optional):      Dict of the JSON to POST             data (dict, optional):      Dict, presumed flat structure of                                         key/value of request to place as                                         www-form             username (str, optional):    Username for basic auth. Must be                                         uncluded as part of password.             password (str, optional):   Password for basic auth. Must be                                         included as part of username.             headers (dict, optional):   Key/Value pairs of headers to include         Returns:             str:    Raw request placed             str:    Raw response received             int:    HTTP status code, eg 200,404,401             dict:   Key/Value pairs of the headers received.             :param timout:
This function checks for missing properties in the request dict     for the corresponding action.
Resolve CLI option type name
List all available sensors
Get temperatures of all available sensors
Get temperature of a specific sensor
Change the precision for the sensor and persist it in the sensor's EEPROM
Load kernel modules needed by the temperature sensor     if they are not already loaded.     If the base directory then does not exist an exception is raised an the kernel module loading     should be treated as failed.      :raises KernelModuleLoadError: if the kernel module could not be loaded properly
Return all available sensors.              :param list types: the type of the sensor to look for.                                If types is None it will search for all available types.              :returns: a list of sensor instances.             :rtype: list
Reads the raw strings from the kernel module sysfs interface              :returns: raw strings containing all bytes from the sensor memory             :rtype: str              :raises NoSensorFoundError: if the sensor could not be found             :raises SensorNotReadyError: if the sensor is not ready yet
Returns the raw integer ADC count from the sensor              Note: Must be divided depending on the max. sensor resolution             to get floating point celsius              :returns: the raw value from the sensor ADC             :rtype: int              :raises NoSensorFoundError: if the sensor could not be found             :raises SensorNotReadyError: if the sensor is not ready yet
Returns the unit factor depending on the unit constant              :param int unit: the unit of the factor requested              :returns: a function to convert the raw sensor value to the given unit             :rtype: lambda function              :raises UnsupportedUnitError: if the unit is not supported
Returns the temperature in the specified unit              :param int unit: the unit of the temperature requested              :returns: the temperature in the given unit             :rtype: float              :raises UnsupportedUnitError: if the unit is not supported             :raises NoSensorFoundError: if the sensor could not be found             :raises SensorNotReadyError: if the sensor is not ready yet             :raises ResetValueError: if the sensor has still the initial value and no measurment
Returns the temperatures in the specified units              :param list units: the units for the sensor temperature              :returns: the sensor temperature in the given units. The order of             the temperatures matches the order of the given units.             :rtype: list              :raises UnsupportedUnitError: if the unit is not supported             :raises NoSensorFoundError: if the sensor could not be found             :raises SensorNotReadyError: if the sensor is not ready yet
Get the current precision from the sensor.              :returns: sensor resolution from 9-12 bits             :rtype: int
Set the precision of the sensor for the next readings.              If the ``persist`` argument is set to ``False`` this value             is "only" stored in the volatile SRAM, so it is reset when             the sensor gets power-cycled.              If the ``persist`` argument is set to ``True`` the current set             precision is stored into the EEPROM. Since the EEPROM has a limited             amount of writes (>50k), this command should be used wisely.              Note: root permissions are required to change the sensors precision.              Note: This function is supported since kernel 4.7.              :param int precision: the sensor precision in bits.                                   Valid values are between 9 and 12             :param bool persist: if the sensor precision should be written                                  to the EEPROM.              :returns: if the sensor precision could be set or not.             :rtype: bool
Round to sig figs
Shared axes limits without shared locators, ticks, etc.      By Joe Kington
Center ticklabels and hide any outside axes limits.      By Joe Kington
Unpacks lists in a list:          [1, 2, [3, 4], [5, [6, 7]]]      becomes          [1, 2, 3, 4, 5, 6, 7]      http://stackoverflow.com/a/12472564/3381305
Concatenate anything into a list.      Args:         a: the first thing         b: the second thing      Returns:         list. All the things in a list.
Grabs, renames and transforms stuff from a lasio object.      Args:         l (lasio): a lasio instance.         section (str): The LAS section to grab from, eg ``well``         item (str): The item in the LAS section to grab from, eg ``name``         attrib (str): The attribute of the item to grab, eg ``value``         default (str): What to return instead.         remap (dict): Optional. A dict of 'old': 'new' LAS field names.         funcs (dict): Optional. A dict of 'las field': function() for             implementing a transform before loading. Can be a lambda.      Returns:         The transformed item.
Interpolation. From ageobot, from somewhere else.
Find the array value, or index of the array value, closest to some given     value.      Args:         a (ndarray)         value (float)         index (bool): whether to return the index instead of the array value.      Returns:         float. The array value (or index, as int) nearest the specified value.
Find the nearest array value, or index of the array value, before some     given value. Optionally also return the fractional distance of the given     value from that previous value.      Args:         a (ndarray)         value (float)         index (bool): whether to return the index instead of the array value.             Default: False.         return_distance(bool): whether to return the fractional distance from             the nearest value to the specified value. Default: False.      Returns:         float. The array value (or index, as int) before the specified value.             If ``return_distance==True`` then a tuple is returned, where the             second value is the distance.
Return two arrays: one of the changes, and one of the values.      Returns:         tuple: Two ndarrays, tops and values.
From ``bruges``      Normalize an array to [0,1] or to arbitrary new min and max.      Args:         a (ndarray)         new_min (float): the new min, default 0.         new_max (float): the new max, default 1.      Returns:         ndarray. The normalized array.
From ``bruges``      Computes the mean in a moving window. Naive implementation.      Example:         >>> test = np.array([1,9,9,9,9,9,9,2,3,9,2,2,3,1,1,1,1,3,4,9,9,9,8,3])         >>> moving_average(test, 7, mode='same')         [ 4.42857143,  5.57142857,  6.71428571,  7.85714286,  8.        ,         7.14285714,  7.14285714,  6.14285714,  5.14285714,  4.28571429,         3.14285714,  3.        ,  2.71428571,  1.57142857,  1.71428571,         2.        ,  2.85714286,  4.        ,  5.14285714,  6.14285714,         6.42857143,  6.42857143,  6.28571429,  5.42857143]      TODO:         Other types of average.
From ``bruges``      Moving average via convolution. Seems slower than naive.
From ``bruges``      Extrapolate up and down an array from the first and last non-NaN samples.      E.g. Continue the first and last non-NaN values of a log up and down.
Remove the NaNs from the top and tail (only) of a well log.      Args:         a (ndarray): An array.     Returns:         ndarray: The top and tailed array.
Decimal degrees to DMS.      Args:         dd (float). Decimal degrees.      Return:         tuple. Degrees, minutes, and seconds.
A Ricker wavelet.      Args:         f (float): frequency in Haz, e.g. 25 Hz.         length (float): Length in s, e.g. 0.128.         dt (float): sample interval in s, e.g. 0.001.      Returns:         tuple. time basis, amplitude values.
Utility function to convert hex to (r,g,b) triples.     http://ageo.co/1CFxXpO      Args:         hexx (str): A hexadecimal colour, starting with '#'.      Returns:         tuple: The equivalent RGB triple, in the range 0 to 255.
Function to decide if a hex colour is dark.      Args:         hexx (str): A hexadecimal colour, starting with '#'.      Returns:         bool: The colour's brightness is less than the given percent.
Function to decide what colour to use for a given hex colour.      Args:         hexx (str): A hexadecimal colour, starting with '#'.      Returns:         bool: The colour's brightness is less than the given percent.
Get zero-indexed line from an open file-like.
A bit like grep. Finds a pattern, looking in path. Returns the filename.
Make a Location object from a lasio object. Assumes we're starting         with a lasio object, l.          Args:             l (lasio).             remap (dict): Optional. A dict of 'old': 'new' LAS field names.             funcs (dict): Optional. A dict of 'las field': function() for                 implementing a transform before loading. Can be a lambda.          Returns:             Location. An instance of this class.
Add a deviation survey to this instance, and try to compute a position         log from it.
Provides an transformation and interpolation function that converts         MD to TVD.          Args:             kind (str): The kind of interpolation to do, e.g. 'linear',                 'cubic', 'nearest'.          Returns:             function.
Args:             deviation (ndarray): A deviation survey with rows like MD, INC, AZI             td (Number): The TD of the well, if not the end of the deviation                 survey you're passing.             method (str):                 'aa': average angle                 'bt': balanced tangential                 'mc': minimum curvature             update_deviation: This function makes some adjustments to the dev-                 iation survey, to account for the surface and TD. If you do not                 want to change the stored deviation survey, set to False.          Returns:             ndarray. A position log with rows like X-offset, Y-offset, Z-offset
Jupyter Notebook magic repr function.
Constructor. Essentially just wraps ``Well.from_las()``, but is more         convenient for most purposes.          Args:             path (str): The path of the LAS files, e.g. ``./*.las`` (the                 default). It will attempt to load everything it finds, so                 make sure it only leads to LAS files.             remap (dict): Optional. A dict of 'old': 'new' LAS field names.             funcs (dict): Optional. A dict of 'las field': function() for                 implementing a transform before loading. Can be a lambda.             data (bool): Whether to load curves or not.             req (list): A list of alias names, giving all required curves. If                 not all of the aliases are present, the well is not loaded.             alias (dict): The alias dict, e.g. ``alias = {'gamma': ['GR', 'GR1'], 'density': ['RHOZ', 'RHOB'], 'pants': ['PANTS']}``          Returns:             project. The project object.
This may be too specific a method... just move it to the workflow.          Requires striplog.
Utility function to get all curve names from all wells, regardless         of data type or repetition.
Looks at all the wells in turn and returns the highest thing         in the alias table.          Args:             mnemonics (list)             alias (dict)          Returns:             list. A list of lists.
Counts the wells that have a given curve, given the mnemonic and an         alias dict.
Another version of the curve table.          Args:             uwis (list): Only these UWIs. List of ``str``.             keys (list): Only these names. List of ``str``.             alias (dict): Alias table, maps names to mnemomnics in order of                 preference.             tests (dict): Test table, maps names to lists of functions.             exclude (list): Except these names. List of ``str``. Ignored if                 you pass ``keys``.             limit (int): Curve must be present in at least this many wells.          Returns:             str. HTML representation of the table.
Plot KDEs for all curves with the given name.          Args:             menmonic (str): the name of the curve to look for.             alias (dict): a welly alias dictionary.             uwi_regex (str): a regex pattern. Only this part of the UWI will be displayed                 on the plot of KDEs.             return_fig (bool): whether to return the matplotlib figure object.          Returns:             None or figure.
Returns a new Project with only the wells which have the named curve.          Args:             menmonic (str): the name of the curve to look for.             alias (dict): a welly alias dictionary.                  Returns:             project.
Returns a new Project with only the wells which DO NOT have the named curve.          Args:             menmonic (str): the name of the curve to look for.             alias (dict): a welly alias dictionary.                  Returns:             project.
Returns a new Project with only the wells named by UWI.          Args:             uwis (list): list or tuple of UWI strings.                  Returns:             project.
Returns a new project where wells with specified uwis have been omitted          Args:              uwis (list): list or tuple of UWI strings.          Returns:              project
Returns a Well object identified by UWI          Args:             uwi (string): the UWI string for the well.                  Returns:             well
Returns a new Project object containing wells from self where         curves from the wells on the right have been added. Matching between         wells in self and right is based on uwi match and ony wells in self         are considered          Args:             uwi (string): the UWI string for the well.                  Returns:             project
Makes a pandas DataFrame containing Curve data for all the wells         in the Project. The DataFrame has a dual index of well UWI and         curve Depths. Requires `pandas`.          Args:             No arguments.          Returns:             `pandas.DataFrame`.
Make X.
Jupyter Notebook magic repr function.
The depth or time basis of the curve's points. Computed         on the fly from the start, stop and step.          Returns             ndarray. The array, the same length as the curve.
Return basic statistics about the curve.
Makes a curve object from a lasio curve object and either a depth         basis or start and step information.          Args:             curve (ndarray)             depth (ndarray)             basis (ndarray)             start (float)             stop (float)             step (float): default: 0.1524             run (int): default: -1             null (float): default: -999.25             service_company (str): Optional.             data (str): Optional.          Returns:             Curve. An instance of the class.
Given a mnemonic, get the alias name(s) it falls under. If there aren't         any, you get an empty list.
Plot a 2D curve.          Args:             ax (ax): A matplotlib axis.             width (int): The width of the image.             aspect (int): The aspect ratio (not quantitative at all).             cmap (str): The colourmap to use.             curve (bool): Whether to plot the curve as well.             ticks (tuple): The tick interval on the y-axis.             return_fig (bool): whether to return the matplotlib figure.                 Default False.          Returns:             ax. If you passed in an ax, otherwise None.
Plot a curve.          Args:             ax (ax): A matplotlib axis.             legend (striplog.legend): A legend. Optional.             return_fig (bool): whether to return the matplotlib figure.                 Default False.             kwargs: Arguments for ``ax.set()``          Returns:             ax. If you passed in an ax, otherwise None.
Interpolate across any missing zones.          TODO         Allow spline interpolation.
Remove then interpolate across
Make a new curve in a new basis, given an existing one. Wraps         ``to_basis()``.          Pass in a curve or the basis of a curve.          Args:             basis (ndarray): A basis, but can also be a Curve instance.          Returns:             Curve. The current instance in the new basis.
Make a new curve in a new basis, given a basis, or a new start, step,         and/or stop. You only need to set the parameters you want to change.         If the new extents go beyond the current extents, the curve is padded         with the ``undefined`` parameter.          Args:             basis (ndarray)             start (float)             stop (float)             step (float)             undefined (float)          Returns:             Curve. The current instance in the new basis.
Private function. Implements read_at() for a single depth.          Args:             d (float)             interpolation (str)             index(bool)             return_basis (bool)          Returns:             float
Read the log at a specific depth or an array of depths.          Args:             d (float or array-like)             interpolation (str)             index(bool)             return_basis (bool)          Returns:             float or ndarray.
Run a series of tests and return the corresponding results.          Args:             tests (list): a list of functions.             alias (dict): a dictionary mapping mnemonics to lists of mnemonics.          Returns:             list. The results. Stick to booleans (True = pass) or ints.
Run a series of tests and return the normalized score.             1.0:   Passed all tests.             (0-1): Passed a fraction of tests.             0.0:   Passed no tests.             -1.0:  Took no tests.          Args:             tests (list): a list of functions.             alias (dict): a dictionary mapping mnemonics to lists of mnemonics.          Returns:             float. The fraction of tests passed, or -1 for 'took no tests'.
Block a log based on number of bins, or on cutoffs.          Args:             cutoffs (array)             values (array): the values to map to. Defaults to [0, 1, 2,...]             n_bins (int)             right (bool)             function (function): transform the log if you want.          Returns:             Curve.
Private function. Smoother for other smoothing/conditioning functions.          Args:             window_length (int): the window length.             func1d (function): a function that takes a 1D array and returns a                 scalar.             step (int): if you want to skip samples in the shifted versions.                 Don't use this for smoothing, you will get strange results.          Returns:             ndarray: the resulting array.
Args:             window (int): window length in samples. Default 33 (or 5 m for                 most curves sampled at 0.1524 m intervals).             samples (bool): window length is in samples. Use False for a window                 length given in metres.             z (float): Z score          Returns:             Curve.
Runs any kind of function over a window.          Args:             window_length (int): the window length. Required.             samples (bool): window length is in samples. Use False for a window                 length given in metres.             func1d (function): a function that takes a 1D array and returns a                 scalar. Default: ``np.mean()``.          Returns:             Curve.
Plot a KDE for the curve. Very nice summary of KDEs:         https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/          Args:             ax (axis): Optional matplotlib (MPL) axis to plot into. Returned.             amax (float): Optional max value to permit.             amin (float): Optional min value to permit.             label (string): What to put on the y-axis. Defaults to curve name.             return_fig (bool): If you want to return the MPL figure object.          Returns:             None, axis, figure: depending on what you ask for.
Not implemented. Will provide a route from CSV file.
Args:         well (Well)         key (str): The key of the predicted Striplog in `well.data`.      Returns:         dict.
Processes a single row from the file.
Compute basis rather than storing it.
Get the synthetic as a Curve, in depth. Facilitates plotting along-         side other curve data.
Plot a synthetic.          Args:             ax (ax): A matplotlib axis.             legend (Legend): For now, only here to match API for other plot                 methods.             return_fig (bool): whether to return the matplotlib figure.                 Default False.          Returns:             ax. If you passed in an ax, otherwise None.
Check for gaps, after ignoring any NaNs at the top and bottom.
Arg ``tolerance`` is the number of spiky samples allowed.
Returns the fraction of the curve extents that are good (non-nan data).
Jupyter Notebook magic repr function.
Constructor. If you already have the lasio object, then this makes a         well object from it.          Args:             l (lasio object): a lasio object.             remap (dict): Optional. A dict of 'old': 'new' LAS field names.             funcs (dict): Optional. A dict of 'las field': function() for                 implementing a transform before loading. Can be a lambda.             data (bool): Whether to load curves or not.             req (dict): An alias list, giving all required curves. If not                 all of the aliases are present, the well is empty.          Returns:             well. The well object.
Constructor. Essentially just wraps ``from_lasio()``, but is more         convenient for most purposes.          Args:             fname (str): The path of the LAS file, or a URL to one.             remap (dict): Optional. A dict of 'old': 'new' LAS field names.             funcs (dict): Optional. A dict of 'las field': function() for                 implementing a transform before loading. Can be a lambda.             printfname (bool): prints filename before trying to load it, for                  debugging          Returns:             well. The well object.
Return current curve data as a ``pandas.DataFrame`` object.                  Everything has to have the same basis, because the depth         is going to become the index of the DataFrame. If you don't         provide one, ``welly`` will make one using ``survey_basis()``.          Args:             keys (list): List of strings: the keys of the data items to                 survey, if not all of them.             basis (array): A basis, if you want to enforce one, otherwise                 you'll get the result of ``survey_basis()``.             uwi (bool): Whether to add a 'UWI' column.              Returns:             pandas.DataFrame.
Makes a lasio object from the current well.          Args:             basis (ndarray): Optional. The basis to export the curves in. If                 you don't specify one, it will survey all the curves with                 ``survey_basis()``.             keys (list): List of strings: the keys of the data items to                 include, if not all of them. You can have nested lists, such                 as you might use for ``tracks`` in ``well.plot()``.          Returns:             lasio. The lasio object.
Writes the current well instance as a LAS file. Essentially just wraps         ``to_lasio()``, but is more convenient for most purposes.          Args:             fname (str): The path of the LAS file to create.             basis (ndarray): Optional. The basis to export the curves in. If                 you don't specify one, it will survey all the curves with                 ``survey_basis()``.             keys (list): List of strings: the keys of the data items to                 include, if not all of them. You can have nested lists, such                 as you might use for ``tracks`` in ``well.plot()``.          Other keyword args are passed to lasio.LASFile.write.          Returns:             None. Writes the file as a side-effect.
Given a LAS file, add curves from it to the current well instance.         Essentially just wraps ``add_curves_from_lasio()``.          Args:             fname (str): The path of the LAS file to read curves from.             remap (dict): Optional. A dict of 'old': 'new' LAS field names.             funcs (dict): Optional. A dict of 'las field': function() for                 implementing a transform before loading. Can be a lambda.          Returns:             None. Works in place.
Given a LAS file, add curves from it to the current well instance.         Essentially just wraps ``add_curves_from_lasio()``.          Args:             fname (str): The path of the LAS file to read curves from.             remap (dict): Optional. A dict of 'old': 'new' LAS field names.             funcs (dict): Optional. A dict of 'las field': function() for                 implementing a transform before loading. Can be a lambda.          Returns:             None. Works in place.
Private function. Depth track plotting.          Args:             ax (ax): A matplotlib axis.             md (ndarray): The measured depths of the track.             kind (str): The kind of track to plot.          Returns:             ax.
Plot multiple tracks.          Args:             legend (striplog.legend): A legend instance.             tracks (list): A list of strings and/or lists of strings. The                 tracks you want to plot from ``data``. Optional, but you will                 usually want to give it.             track_titles (list): Optional. A list of strings and/or lists of                 strings. The names to give the tracks, if you don't want welly                 to guess.             alias (dict): a dictionary mapping mnemonics to lists of mnemonics.             basis (ndarray): Optional. The basis of the plot, if you don't                 want welly to guess (probably the best idea).             return_fig (bool): Whether to return the matplotlig figure. Default                 False.             extents (str): What to use for the y limits:                 'td' — plot 0 to TD.                 'curves' — use a basis that accommodates all the curves.                 'all' — use a basis that accommodates everything.                 (tuple) — give the upper and lower explictly.          Returns:             None. The plot is a side-effect.
Look at the basis of all the curves in ``well.data`` and return a         basis with the minimum start, maximum depth, and minimum step.          Args:             keys (list): List of strings: the keys of the data items to                 survey, if not all of them.             alias (dict): a dictionary mapping mnemonics to lists of mnemonics.             step (float): a new step, if you want to change it.                      Returns:             ndarray. The most complete common basis.
Give everything, or everything in the list of keys, the same basis.         If you don't provide a basis, welly will try to get one using         ``survey_basis()``.          Args:             basis (ndarray): A basis: the regularly sampled depths at which                 you want the samples.             keys (list): List of strings: the keys of the data items to                 unify, if not all of them.          Returns:             None. Works in place.
Should probably integrate getting curves with regex, vs getting with         aliases, even though mixing them is probably confusing. For now I can't         think of another use case for these wildcards, so I'll just implement         for the curve table and we can worry about a nice solution later if we         ever come back to it.
Instead of picking curves by name directly from the data dict, you         can pick them up with this method, which takes account of the alias         dict you pass it. If you do not pass an alias dict, then you get the         curve you asked for, if it exists, or None. NB Wells do not have alias         dicts, but Projects do.          Args:             mnemonic (str): the name of the curve you want.             alias (dict): an alias dictionary, mapping mnemonics to lists of                 mnemonics.          Returns:             Curve.
Wraps get_mnemonic.          Instead of picking curves by name directly from the data dict, you         can pick them up with this method, which takes account of the alias         dict you pass it. If you do not pass an alias dict, then you get the         curve you asked for, if it exists, or None. NB Wells do not have alias         dicts, but Projects do.          Args:             mnemonic (str): the name of the curve you want.             alias (dict): an alias dictionary, mapping mnemonics to lists of                 mnemonics.          Returns:             Curve.
Counts the number of curves in the well that will be selected with the         given key list and the given alias dict. Used by Project's curve table.
Returns False if the well does not have one or more of the keys in its         data dictionary. Used by ``project.data_to_matrix()``.
Early hack. Use with extreme caution.          Hands-free. There'll be a more granualr version in synthetic.py.          Assumes DT is in µs/m and RHOB is kg/m3.          There is no handling yet for TVD.          The datum handling is probably sketchy.          TODO:             A lot.
Run tests on a cohort of curves.          Args:             alias (dict): an alias dictionary, mapping mnemonics to lists of                 mnemonics.          Returns:             dict.
Run a series of tests against the data and return the corresponding         results.          Args:             tests (list): a list of functions.          Returns:             list. The results. Stick to booleans (True = pass) or ints.
Makes a nice table out of ``qc_data()``          Returns:             str. An HTML string.
Make a Canstrat DAT (aka ASCII) file.          TODO:             The data part should probably belong to striplog, and only the             header should be written by the well.          Args:            filename (str)            key (str)            log (str): the log name, should be 6 characters.            lith_field (str) the name of the lithology field in the striplog's                Primary component. Must match the Canstrat definitions.            filename (str)            as_text (bool): if you don't want to write a file.
Provide a feature matrix, given a list of data items.          I think this will probably fail if there are striplogs in the data         dictionary for this well.          TODO:             Deal with striplogs and other data, if present.          Args:             keys (list): List of the logs to export from the data dictionary.             return_basis (bool): Whether or not to return the basis that was                 used.             basis (ndarray): The basis to use. Default is to survey all curves                 to find a common basis.             alias (dict): A mapping of alias names to lists of mnemonics.             start (float): Optionally override the start of whatever basis                 you find or (more likely) is surveyed.             stop (float): Optionally override the stop of whatever basis                 you find or (more likely) is surveyed.             step (float): Override the step in the basis from survey_basis.             window_length (int): The number of samples to return around each sample.                 This will provide one or more shifted versions of the features.             window_step (int): How much to step the offset versions.          Returns:             ndarray.             or             ndarray, ndarray if return_basis=True
Turn a PROJ.4 string into a mapping of parameters. Bare parameters         like "+no_defs" are given a value of ``True``. All keys are checked         against the ``all_proj_keys`` list.          Args:             prjs (str): A PROJ4 string.
Given an integer code, returns an EPSG-like mapping.         Note: the input code is not validated against an EPSG database.
Turn a CRS dict into a PROJ.4 string. Mapping keys are tested against         ``all_proj_keys`` list. Values of ``True`` are omitted, leaving the key         bare: {'no_defs': True} -> "+no_defs" and items where the value is         otherwise not a str, int, or float are omitted.          Args:             crs: A CRS dict as used in Location.          Returns:             str. The string representation.
Input:  {               Input for CK             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                (stdout)     - stdout, if available               (stderr)     - stderr, if available               (std)        - stdout+stderr             }
Input:  {               http - http object               type - content type               bin  - bytes to output             }      Output: {               return - 0             }
Note, that we can't create repos in parallel (recording to repo cache may fail).     However, for now, we do not expect such cases (i.e. repos are created rarely)      Input:  {               (repo_uoa)                 - repo UOA (where to create entry)               data_uoa                   - data UOA               (data_uid)                 - data UID (if uoa is an alias)               (data_name)                - user friendly data name                (cids[0])                  - as uoa or full CID                (path)                     - if !='' - create in this path or import from this path               (here)                     - if =='yes', create in current path               (use_default_path)         - if 'yes' create repository in the default path (CK_REPOS)                                            instead of the current path (default is 'yes')                (use_current_path)         - if 'yes' create repository in the current path                                            (default is 'no')                (default)                  - if 'yes', no path is used,                                             but the repository is taken either                                             from the CK directory or from CK_LOCAL_REPO                (import)                   - if 'yes', register repo in the current directory in CK                                            (when received from someone else)                (remote)                   - if 'yes', remote repository               (remote_repo_uoa)          - if !='' and type=='remote' repository UOA on the remote CK server                (shared)                   - if not remote and =='git', repo is shared/synced through GIT               (share)                    - (for user-friendly CMD) if 'yes', set shared=git                (allow_writing)            - if 'yes', allow writing                                             (useful when kernel is set to allow writing only to such repositories)                (url)                      - if type=='remote' or 'git', URL of remote repository or git repository               (hostname)                 - if !='', automatically form url above (add http:// + /ck?)               (port)                     - if !='', automatically add to url above               (hostext)                  - if !='', add to the end of above URL instead of '/ck?' -                                            useful when CK server is accessed via Apache2, IIS, Nginx or other web servers                (githubuser)               - if shared repo, use this GitHub user space instead of default "ctuning"               (sync)                     - if 'yes' and type=='git', sync repo after each write operation                (gitzip)                   - if 'yes', download as zip from GitHub               (zip)                      - path to zipfile (local or remote http/ftp)               (overwrite)                - if 'yes', overwrite files when unarchiving                (repo_deps)                - dict with dependencies on other shared repositories with following keys:                                              "repo_uoa"                                              ("repo_uid") - specific UID (version) of a repo                                              ("repo_url") - URL of the shared repository (if not from github.com/ctuning)                (quiet)                    - if 'yes', do not ask questions unless really needed                (skip_reusing_remote_info) - if 'yes', do not reuse remote .cmr.json description of a repository                (current_repos)            - if resolving dependencies on other repos, list of repos being updated (to avoid infinite recursion)                (describe)                 - describe repository for Artifact Evaluation (see http://cTuning.org/ae)                (stable)        - take stable version (highly experimental)               (version)       - checkout version (default - stable)               (branch)        - git branch               (checkout)      - git checkout             }      Output: {               return       - return code =  0, if successful                                            16, repository with a given path is already registered in CK                                          >  0, if error               (error)      - error text if return > 0             }
Update repository info      Input:  {               data_uoa                   - data UOA of the repo                (shared)                   - if not remote and =='git', shared through GIT                (url)                      - if type=='git', URL of remote repository or git repository               (hostname)                 - if !='', automatically form url above (add http:// + /ck?)               (port)                     - if !='', automatically add to url above               (hostext)                  - if !='', add to the end of above URL instead of '/ck?' -                                            useful when CK server is accessed via Apache2, IIS, Nginx or other web servers                (sync)                     - if 'yes' and type=='git', sync repo after each write operation               (allow_writing)            - if 'yes', allow writing                                             (useful when kernel is set to allow writing only to such repositories)                (repo_deps)                - dict with dependencies on other shared repositories with following keys:                                              "repo_uoa"                                              ("repo_uid") - specific UID (version) of a repo                                              ("repo_url") - URL of the shared repository (if not from github.com/ctuning)                (update)                   - if 'yes', force updating                (describe)                 - describe repository for Artifact Evaluation (see http://cTuning.org/ae)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (path)  - repo UOA (where to create entry)               (type)  - type               (url)   - URL                  or                (data_uoa)      - repo UOA                (clone)         - if 'yes', clone repo instead of update                (current_repos) - if resolving dependencies on other repos, list of repos being updated (to avoid infinite recursion)                (git)           - if 'yes', use git protocol instead of https                (ignore_pull)   - useful just for switching to another branch                (stable)        - take stable version (highly experimental)               (version)       - checkout version (default - stable)               (branch)        - git branch               (checkout)      - git checkout             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (path)  - repo UOA (where to create entry)               (type)  - type               (url)   - URL                  or                (data_uoa)  - repo UOA                (clone) - if 'yes', clone repo instead of update             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {}      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (repo_uoa)            - repo UOA (where to delete entry about repository)               uoa                   - data UOA               (force)               - if 'yes', force removal               (with_files) or (all) - if 'yes', remove files as well             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               data_uoa       - repo UOA                (archive_path) - if '' create inside repo path                (archive_name) - if !='' use it for zip name               (auto_name)    - if 'yes', generate name name from data_uoa: ckr-<repo_uoa>.zip               (bittorent)    - if 'yes', generate zip name for BitTorrent: ckr-<repo_uid>-YYYYMMDD.zip                (overwrite)    - if 'yes', overwrite zip file               (store)        - if 'yes', store files instead of packing                 (data)         - CID allowing to add only these entries with pattern (can be from another archive)                (all)          - archive all files             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (data_uoa)    - repo UOA where to unzip (default, if not specified)               zip           - path to zipfile (local or remote http/ftp)               (overwrite)   - if 'yes', overwrite files when unarchiving             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               zip              - zip filename or URL               path             - path to extract               (overwrite)      - if 'yes', overwrite files when unarchiving               (path_to_remove) - if !='', remove this part of the path from extracted archive             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (data_uoa)      - repo UOA                   or               (path)          - path to .cmr.json                (current_repos) - list of repos being updated (to avoid infinite recursion)                (how)           - 'pull' (default) or 'add'                (version)       - checkout version (default - stable)               (branch)        - git branch               (checkout)      - git checkout             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               data_uoa                   - data UOA of the repo                  or               repo_deps                  - dict with dependencies on other shared repos                (out_prefix)               - output prefix befor each string             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                repo_deps             }
Input:  {             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                repo_deps    - list with dependencies on other repositories ...             }
Input:  {               data_uoa                   - data UOA of the repo                (stable)                   - take stable version (highly experimental)               (checkout)                 - checkout (default - stable)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               data_uoa                   - data UOA of the repo             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (dict)  - dict with current repo description             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                (dict)  - updated dict with current repo description             }
Input:  {               (url)                      - if type=='remote' or 'git', URL of remote repository or git repository               (hostname)                 - if !='', automatically form url above (add http:// + /ck?)               (port)                     - if !='', automatically add to url above               (hostext)                  - if !='', add to the end of above URL instead of '/ck?' -                                            useful when CK server is accessed via Apache2, IIS or other web servers             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                url          - formed URL             }
Input:  {               (data_uoa) - repo UOA                (reset)    - if 'yes', reset repos                (stable)   - take stable version (highly experimental)               (version)  - checkout version (default - stable)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               data_uoa  - repo UOA                (new_data_uoa)                  or               xcids[0]           - {'data_uoa'} - new data UOA             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (kernel)  - if 'yes', pull kernel too (unless installed as a package)               (install) - if 'yes', install CK kernel as python module (via python setup.py install)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  s - unicode string to print      Output: Nothing
Input:  i - dictionary      Output: return = 0
Input:  s - unicode string to print      Output: Nothing
Input:  {               return - return code               error - error text             }      Output: Nothing; quits program
Input:  {               cmd       - command line               (timeout) - timeout in seconds (granularity 0.01 sec) - may cause overheads ...             }      Output: {               return       - return code =  0, if successful                                          >  0, if error                                          =  8, if timeout               (error)      - error text if return > 0                return_code  - return code from app             }
Input:  {             cmd       - list of command line arguments, starting with the command itself             (shell)   - if 'yes', reuse shell environment           }    Output: {             return       - return code =  0, if successful                                        >  0, if error                                        =  8, if timeout             (error)      - error text if return > 0              return_code  - return code from app              stdout       - string, standard output of the command           }
Input:  dict1         - first check in this dict (and remove if there)             key           - key in dict1             default_value - default value if not found             dict2         - then check from here      Output: value
Input:  {               iso_datetime - iso date time             }      Output: {                return         - return code =  0, if successful                                            >  0, if error               (error)        - error text if return > 0               (datetime_obj) - datetime object             }
Input:  {               text - text to print             }      Output: {               return       - return code =  0                string       - input string             }
Input:  {               dict             - dict with values being dicts with 'name' as string to display and 'sort' as int (for ordering)               (title)          - print title               (error_if_empty) - if 'yes' and Enter, make error               (skip_sort)      - if 'yes', do not sort array             }      Output: {               return       - return code =  0                string       - selected dictionary key             }
Input:  {               choices      - list from search function               (skip_enter) - if 'yes', do not select 0 when user presses Enter               (skip_sort)  - if 'yes', do not sort array             }      Output: {               return  - return code =  0, if successful                                     >  0, if error               (error) - error text if return > 0               choice  - data UOA             }
Input:  either a list, or a string of comma-separated tags.      Output: If i is a list, it's returned.             If i is a string, the list of tags it represents is returned              (each tag is stripped of leading and trailing whitespace).
Input:  {               (module_uoa)               (module_uid)                (repo_uoa)               (repo_uid)               (repo_dict)                (delete)     - if 'yes', check if global delete operation is allowed             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0               (repo_dict)  - repo cfg if available             }
Input:  {}      Output: {               return       - return code =  0                version      - list starting from major version number               version_str  - version string             }
Input:  {               (suffix)     - temp file suffix               (prefix)     - temp file prefix               (remove_dir) - if 'yes', remove dir             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                file_name    - temp file name              }
Input:  {               (bits)      - force OS bits             }      Output: {               return      - return code =  0               platform    - 'win' or 'linux'               bits        - OS bits in string: 32 or 64               python_bits - Python installation bits in string: 32 or 64              }
Input:  {}      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                data_uid     - UID in string format (16 characters 0..9,a..f)             }
Input:  string to check      Output: True if UID, otherwise False
Input:  string to check      Output: True if allowed UOA, False otherwise
Input:  {             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                dict         - dict with info             }
Input:  {               str                      - string (use ' instead of ", i.e. {'a':'b'}                                           to avoid issues in CMD in Windows and Linux!)                (skip_quote_replacement) - if 'yes', do not make above replacement             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                dict         - dict from json file             }
Input:  {               json_file - name of file with json             }      Output: {               return       - return code =  0, if successful                                          = 16, if file not found (may be warning)                                          >  0, if error               (error)  - error text if return > 0                dict     - dict from json file             }
Input:  {               yaml_file - name of YAML file             }      Output: {               return       - return code =  0, if successful                                          = 16, if file not found (may be warning)                                          >  0, if error               (error)  - error text if return > 0                dict     - dict from YAML file             }
Input:  {               text_file           - name of text file               (keep_as_bin)       - if 'yes', return only bin               (encoding)          - by default 'utf8', however sometimes we use utf16                (split_to_list)     - if 'yes', split to list                (convert_to_dict)   - if 'yes', split to list and convert to dict               (str_split)         - if !='', use as separator of keys/values when converting to dict               (remove_quotes)     - if 'yes', remove quotes from values when converting to dict                (delete_after_read) - if 'yes', delete file after read (useful when reading tmp files)             }      Output: {               return       - return code =  0, if successful                                          = 16, if file not found (may be warning)                                          >  0, if error               (error)  - error text if return > 0                bin      - bin               (string) - loaded text (with removed \r)               (lst)    - if split_to_list=='yes', return as list               (dict)   - if convert_to_dict=='yes', return as dict             }
Input:  {               filename - file               string1  - string to be replaced               string2  - replace string             }      Output: {               return       - return code =  0, if successful                                          = 16, if file not found                                          >  0, if error               (error)  - error text if return > 0             }
Input:  {               dict          - dictionary               (skip_indent) - if 'yes', skip indent               (sort_keys)   - if 'yes', sort keys             }      Output: {               return       - return code =  0, if successful                                          >  0, if error                string       - json string (in utf8)             }
Input:  {               json_file    - file name               dict         - dict to save               (sort_keys)  - if 'yes', sort keys               (safe)       - if 'yes', ignore non-JSON values (only for Debugging - changes original dict!)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               yaml_file   - file name               dict        - dict to save             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               text_file - name of text file               string    - string to write (with removed \r)               (append)  - if 'yes', append             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)  - error text if return > 0             }
Input:  {               string - string to copy      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               dict1 - merge this dict with dict2 (will be directly modified!)               dict2 - dict      Output: {               return       - return code =  0, if successful                dict1        - output dict             }
Input:  {               filename - file name to convert             }      Output: {               return              - return code =  0, if successful                                                 >  0, if error               (error)             - error text if return > 0               file_content_base64 - string that can be transmitted through Internet             }
Input:  {               file_content_base64 - string transmitted through Internet               (filename)          - file name to write (if empty, generate tmp file)             }      Output: {               return              - return code =  0, if successful                                                 >  0, if error               (error)             - error text if return > 0               filename            - filename with full path               filename_ext        - filename extension             }
Input:  {               text - text to print             }      Output: {               return              - return code =  0, if successful                                                 >  0, if error               (error)             - error text if return > 0                string               dict                - parsed JSON             }
Input:  [                CK list: see 'action' function from this kernel             ]      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                ck_dict      - {                                "action":action                                "cid":module_uoa or CID (x means that it may not be really CID                                         and has to be processed specially                                "cids":[cid1, cid2, cid3, ...]                                "key1":value1                                "key2":value2                                ...                                "key10":""                                "key11":value11                                keys/values from file_json; if file extension is .tmp,                                                             it will be deleted after read!                                keys/values from cmd_json                                "unparsed":unparsed_cmd                              }
Input:  {}      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               path               - top level path               (file_name)        - search for a specific file name               (pattern)          - return only files with this pattern               (path_ext)         - path extension (needed for recursion)               (limit)            - limit number of files (if directories with a large number of files)               (number)           - current number of files               (all)              - if 'yes' do not ignore special directories (like .cm)               (ignore_names)     - list of names to ignore               (ignore_symb_dirs) - if 'yes', ignore symbolically linked dirs                                     (to avoid recursion such as in LLVM)               (add_path)         - if 'yes', add path             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                list         - dictionary of all files:                                {"file_with_full_path":{"size":.., "path":..}               sizes        - sizes of files (the same order)               number       - number of files in a current directory (needed for recursion)             }
Input:  {               (force)      - if 'yes', force recaching             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {}      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               repo_uoa - repo_uoa             }      Output: {               return       - return code =  0, if successful                                            16, if repo not found (may be warning)                                          >  0, if error               (error)      - error text if return > 0                repo_uoa     - repo UOA               repo_uid     - repo UID               repo_alias   - repo alias                all other info from repo dict             }
Input:  {               path - path to repo             }      Output: {               return       - return code =  0, if successful                                            16, if repo not found (may be warning)                                          >  0, if error               (error)      - error text if return > 0                repo_uoa     - repo UOA               repo_uid     - repo UID               repo_alias   - repo alias             }
Input:  {               (repo_uoa) - repo UOA; if empty, get the default repo             }      Output: {               return       - return code =  0, if successful                                            16, if repo not found (may be warning)                                          >  0, if error               (error)      - error text if return > 0                dict         - dict from cache               path         - path to repo                repo_uoa     - repo UOA               repo_uid     - repo UID               repo_alias   - repo alias             }
Input:  {               (repo_uoa) - repo UOA               module_uoa - module UOA               data_uoa   - data UOA             }      Output: {               return       - return code =  0, if successful                                            16, if data not found (may be warning)                                          >  0, if error               (error)      - error text if return > 0               path         - path to data               path_module  - path to module entry with this entry               path_repo    - path to the repository of this entry               repo_uoa     - repo UOA                repo_uid     - repo UID               repo_alias   - repo alias               module_uoa   - module UOA                module_uid   - module UID               module_alias - module alias               uoa          - data UOA               uid          - data UID               alias        - data alias             }
Input:  {               path     - path to a repository               data_uoa - data UOA             }      Output: {               return       - return code =  0, if successful                                            16, if data not found (may be warning)                                          >  0, if error               (error)      - error text if return > 0                path         - path to data entry               data_uid     - data uid (from UOA)               data_alias   - data alias (from UOA)               data_uoa     - data alias or data uid, if data alias==''             }
Input:  {               path           - path to a data entry                (skip_updates) - if 'yes', do not load updates               (skip_desc)    - if 'yes', do not load descriptions             }      Output: {               return         - return code =  0, if successful                                            >  0, if error               (error)        - error text if return > 0                dict           - dict with meta description               path           - path to json file with meta description                (info)         - dict with info if exists               (path_info)    - path to json file with info                (updates)      - dict with updates if exists               (path_updates) - path to json file with updates               (path_desc)    - path to json file with API description             }
Input:  {               path             - module path               module_code_name - module name               (cfg)            - configuration of the module if exists ...               (skip_init)      - if 'yes', skip init               (data_uoa)       - module UOA (useful when printing error)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                code         - python code object               path         - full path to the module               cuid         - internal UID of the module             }
Input:  { See 'perform_action' function }     Output: { See 'perform_action' function }
Input:  {               all parameters from function 'access'                (web)         - if 'yes', called from the web                (common_func) - if 'yes', ignore search for modules                                          and call common func from the CK kernel                (local)       - if 'yes', run locally even if remote repo ...             }      Output: {               return  - return code =  0, if successful                                          >  0, if error               (error) - error text if return > 0                (out)   - if action change output, return it               Output from the module/action             }
Input:  {               (path)       - path to module, if comes from access function                 or               (module_uoa) - if comes from CMD                (func)       - func for API                (out)  - output             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                title        - title string               desc         - original description               module       - module name               api          - api as string               line         - line in found module             }
Input:  {               cid            - in format (REPO_UOA:)MODULE_UOA:DATA_UOA                (cur_cid)      - output of function 'detect_cid_in_current_path'               (ignore_error) - if 'yes', ignore wrong format             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                data_uoa     - data UOA               module_uoa   - module UOA               (repo_uoa)   - repo UOA             }
Input:  {               path       - path where to create an entry               (data_uoa) - data UOA               (data_uid) - if uoa is an alias, we can force data UID                (force)    - if 'yes', force creation even if directory already exists             }      Output: {               return       - return code =  0, if successful                                            16, if data entry already exists                                          >  0, if error               (error)      - error text if return > 0                path         - path to data entry               data_uid     - data UID (from UOA)               data_alias   - data alias (from UOA)               data_uoa     - data alias or data uid if data alias==''             }
Input:  {               path         - path to the entry               data_uid     - data UID               (data_alias) - data alias               (repo_dict)  - repo cfg if available to check sync               (share)      - if 'yes', try to rm via GIT             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               path - path to delete             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Any list item is converted to @number=value     Any dict item is converted to #key=value     # is always added at the beginning       Input:  {               dict         - python dictionary                (prefix)     - prefix (for recursion)                (prune_keys) - list of keys to prune (can have wildcards)             }      Output: {               return  - return code =  0, if successful                                     >  0, if error               (error) - error text if return > 0               dict    - flattened dictionary             }
Input:  {               dict  - dictionary               key   - flat key             }      Output: {               return  - return code =  0, if successful                                     >  0, if error               (error) - error text if return > 0               value   - value or None, if doesn't exist             }
Input:  {               dict - flattened dict             }      Output: {               return  - return code =  0, if successful                                     >  0, if error               (error) - error text if return > 0               dict    - restored dict             }
Input:  {               path               - path to be locked                (get_lock)         - if 'yes', lock this entry               (lock_retries)     - number of retries to aquire lock (default=11)               (lock_retry_delay) - delay in seconds before trying to aquire lock again (default=3)               (lock_expire_time) - number of seconds before lock expires (default=30)                (unlock_uid)       - UID of the lock to release it             }      Output: {               return       - return code =  0, if successful                                          = 32, couldn't acquire lock (still locked after all retries)                                          >  0, if error               (error)      - error text if return > 0                (lock_uid)   - lock UID, if locked successfully             }
Input:  {               path               - path to be locked               (unlock_uid)       - UID of the lock to release it             }      Output: {               return       - return code =  0, if successful                                          = 32, lock UID is not matching                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {}      Output: {               return       - return code =  0                array        - array with date and time               iso_datetime - date and time in ISO format             }
Input:  {               (path)     - path, otherwise current directory             }      Output: {               return         - return code =  0, if successful                                            >  0, if error               (error)        - error text if return > 0                repo_uoa       - repo UOA               repo_uid       - repo UID               repo_alias     - repo alias               (module_uoa)   - module UOA               (module_uid)   - module UID               (module_alias) - module alias               (data_uoa)     - data UOA               (data_uid)     - data UID               (data_alias)   - data alias             }
Input:  {}      Output: {               Output from 'gen_uid' function             }
Input:  {}      Output: {               output from function 'get_version'             }
Input:  {}      Output: {                version - sys.version                version_info - sys.version_info             }
Input:  {}      Output: {               outdated     - if 'yes', newer version exists                return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               version      - your version (string)             }      Output: {               return          - return code =  0                ok              - if 'yes', your CK kernel version is outdated               current_version - your CK kernel version             }
Input:  {                (repo_uoa)   - Repo UOA                (repo_uid)   - Repo UID                (module_uoa) - Module UOA                (module_uid) - Module UID                (data_uoa)   - Data UOA                (data_uid)   - Data UID             }      Output: {               return       - return code =  0                cuoa         - module_uoa:data_uoa           (substituted with ? if can't find)               cid          - module_uid:data_uid           (substituted with ? if can't find)               xcuoa        - repo_uoa:module_uoa:data_uoa  (substituted with ? if can't find)               xcid         - repo_uid:module_uid:data_uid  (substituted with ? if can't find)             }
Input:  { from access function }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {                 (repo_uoa)                (module_uoa)                (data_uoa)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  { from access function }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (template)   - use this web template               (repo_uoa)   -               (module_uoa) -                (data_uoa)   - view a given entry               (extra_url)  - extra URL             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {}      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                help         - help text             }
Input:  {             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                help         - help text             }
Input:  {             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                html         - input as JSON             }
Input:  {               (repo_uoa)               module_uoa               (data_uoa)             }      Output: {               Output of 'load' function             }
Input:  {}      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                Output from from 'detect_cid_in_current_path' function             }
Input:  {               (repo_uoa)   - repo UOA               (module_uoa) - module UOA               (data_uoa)   - data UOA                   If above is empty, detect in current path !              }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                Output from from 'detect_cid_in_current_path' function                data_uoa     - data UOA               module_uoa   - module UOA               (repo_uoa)   - repo UOA             }
Input:  {               (add_quotes) - if 'yes', add quotes             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (repo_uoa)              - repo UOA               module_uoa              - module UOA               data_uoa                - data UOA                (get_lock)              - if 'yes', lock this entry               (lock_retries)          - number of retries to aquire lock (default=5)               (lock_retry_delay)      - delay in seconds before trying to aquire lock again (default=10)               (lock_expire_time)      - number of seconds before lock expires (default=30)                (skip_updates)          - if 'yes', do not load updates               (skip_desc)             - if 'yes', do not load descriptions                (load_extra_json_files) - list of files to load from the entry                (unlock_uid)            - UID of the lock to release it                (min)                   - show minimum when output to console (i.e. meta and desc)                (create_if_not_found)   - if 'yes', create, if entry is not found - useful to create and lock entries             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                dict         - entry meta description               (info)       - entry info               (updates)    - entry updates               (desc)       - entry description                path         - path to data entry               path_module  - path to module entry with this entry               path_repo    - path to the repository of this entry               repo_uoa     - repo UOA                repo_uid     - repo UID               repo_alias   - repo alias               module_uoa   - module UOA                module_uid   - module UID               module_alias - module alias               data_uoa     - data UOA               data_uid     - data UID               data_alias   - data alias               data_name    - user friendly name                (extra_json_files) - dict with extra json files (key is the filename from 'load_extra_json_files')                (lock_uid)   - unlock UID, if locked successfully             }
Input:  {               (repo_uoa)  - repo UOA               module_uoa  - module UOA               data_uoa    - data UOA             }      Output: {                Output of the 'load' function                 number_of_entries - total number of found entries             }
Input:  {               (repo_uoa)  - repo UOA               module_uoa  - module UOA               data_uoa    - data UOA                  or               cid              }      Output: {                Output of the 'load' function                 string - prepared string 'cd {path to entry}'             }
Input:  {               (repo_uoa)  - repo UOA               module_uoa  - module UOA               data_uoa    - data UOA                  or               cid              }      Output: {                Output of the 'load' function              }
Input:  {               (repo_uoa)             - repo UOA               module_uoa             - module UOA               data_uoa               - data UOA               (data_uid)             - data UID (if uoa is an alias)               (data_name)            - user friendly data name                (dict_from_cid)        -               (dict_from_repo_uoa)   -                (dict_from_module_uoa) -                (dict_from_data_uoa)   - if present, pre-load dict                                         from this (module_uoa):data_uoa (analog of copy)                (update)               - if == 'yes' and entry exists, update it                (dict)                 - meta description to record               (substitute)           - if 'yes' and update=='yes' substitute dictionaries, otherwise merge!                (desc)                 - description of an entry (gradually adding API description in flat format)                (extra_json_files)     - dict with extra json files to save to entry (key is a filename)                (tags)                 - list or comma separated list of tags to add to entry                (info)                 - entry info to record - normally, should not use it!               (extra_info)           - enforce extra info such as                                           author                                           author_email                                           author_webpage                                           license                                           copyright                                        If not specified then taken from kernel (prefix 'default_')                (updates)              - entry updates info to record - normally, should not use it!               (ignore_update)        - if 'yes', do not add info about update                (ask)                  - if 'yes', ask questions, otherwise silent                (unlock_uid)           - unlock UID if was previously locked                (sort_keys)            - by default, 'yes'                (share)                - if 'yes', try to add via GIT             }      Output: {               return       - return code =  0, if successful                                            16, if entry already exists                                          >  0, if error               (error)      - error text if return > 0                Output from the 'create_entry' function             }
Input:  {               (repo_uoa)             - repo UOA               module_uoa             - module UOA               data_uoa               - data UOA               (data_uid)             - data UID (if uoa is an alias)               (data_name)            - user friendly data name                (dict_from_cid)        -               (dict_from_repo_uoa)   -                (dict_from_module_uoa) -                (dict_from_data_uoa)   - if present, pre-load dict                                         from this (module_uoa):data_uoa (analog of copy)                (dict)                 - meta description to record               (substitute)           - if 'yes', substitute dictionaries, otherwise merge!                (tags)                 - list or comma separated list of tags to add to entry                (info)                 - entry info to record - normally, should not use it!               (updates)              - entry updates info to record - normally, should not use it!               (ignore_update)        - if 'yes', do not add info about update                (ask)                  - if 'yes', ask questions, otherwise silent                (unlock_uid)           - unlock UID if was previously locked                (sort_keys)            - if 'yes', sort keys             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                Output from the 'add' function (the last one in case of wildcards)             }
Input:  {               (repo_uoa)             - repo UOA               module_uoa             - module UOA               data_uoa               - data UOA                (ignore_update)        - (default==yes) if 'yes', do not add info about update               (sort_keys)            - (default==yes) if 'yes', sort keys                (edit_desc)            - if 'yes', edit description rather than meta                                         (useful for compiler descriptions)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (repo_uoa)      - repo UOA    ; can be wild cards               module_uoa      - module UOA  ; can be wild cards               data_uoa        - data UOA    ; can be wild cards                (force)         - if 'yes', force deleting without questions                  or               (f)             - to be compatible with rm -f                 (share)         - if 'yes', try to remove via GIT                (tags)          - use these tags in format tags=x,y,z to prune rm                    or               (search_string) - prune entries with expression *?             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (repo_uoa)         - repo UOA               module_uoa         - module UOA               data_uoa           - old data UOA                new_data_uoa       - new data alias                  or               new_data_uid       - new data UID (leave empty to keep old one)                  or               xcids[0]           - {'data_uoa'} - new data UOA                (new_uid)          - generate new UID                (remove_alias)     - if 'yes', remove alias                (add_uid_to_alias) - if 'yes', add UID to alias                (share)            - if 'yes', try to remove old entry via GIT and add new one             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (repo_uoa)       - repo UOA               module_uoa       - module UOA               data_uoa         - data UOA                xcids[0]         - {'repo_uoa', 'module_uoa', 'data_uoa'} - new CID                  or               (new_repo_uoa)   - new repo UOA               (new_module_uoa) - new module UOA               new_data_uoa     - new data alias               (new_data_uid)   - new data UID (leave empty to generate new one)                (move)           - if 'yes', remove old               (keep_old_uid)   - if 'yes', keep old UID                (without_files)  - if 'yes', do not move/copy files             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                Output of 'add' function             }
Input:  {               (repo_uoa)    - repo UOA               module_uoa    - module UOA               data_uoa      - data UOA                xcids[0]         - {'repo_uoa', 'module_uoa', 'data_uoa'} - new CID                  or               (new_repo_uoa)   - new repo UOA               (new_module_uoa) - new module UOA               (new_data_uoa)   - new data alias               (new_data_uid)   - new data UID (leave empty to generate new one)              }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                Output of 'copy' function             }
Input:  {               (repo_uoa)  - repo UOA               module_uoa  - module UOA               data_uoa    - data UOA                filename    - filename to delete including relative path               (force)     - if 'yes', force deleting without questions             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               (repo_uoa)           - repo UOA               (module_uoa)         - module UOA               (data_uoa)           - data UOA                (repo_uoa_list)      - list of repos to search               (module_uoa_list)    - list of module to search               (data_uoa_list)      - list of data to search                (filter_func)        - name of filter function               (filter_func_addr)   - address of filter function                (add_if_date_before) - add only entries with date before this date                (add_if_date_after)  - add only entries with date after this date               (add_if_date)        - add only entries with this date                (ignore_update)      - if 'yes', do not add info about update (when updating in filter)                (search_by_name)     - search by name                (search_dict)        - search if this dict is a part of the entry                (ignore_case)        - ignore case when searching!                (print_time)         - if 'yes', print elapsed time at the end                (do_not_add_to_lst)  - if 'yes', do not add entries to lst                (time_out)           - in secs, default=30 (if -1, no timeout)                (limit_size)         - if !='' limit size                (print_full)         - if 'yes', show CID (repo_uoa:module_uoa:data_uoa)                   or               (all)                (print_uid)          - if 'yes', print UID in brackets                (print_name)         - if 'yes', print name (and add info to the list)                   or               (name)                (add_info)           - if 'yes', add info about entry to the list               (add_meta)           - if 'yes', add meta about entry to the list             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                lst          - [{'repo_uoa', 'repo_uid',                                'module_uoa', 'module_uid',                                 'data_uoa','data_uid',                                'path' (,info)                                 }]                elapsed_time - elapsed time in string                (timed_out)  - if 'yes', timed out or limited by size             }
Input:  {               (repo_uoa)           - repo UOA               (module_uoa)         - module UOA               (data_uoa)           - data UOA                (repo_uoa_list)      - list of repos to search               (module_uoa_list)    - list of module to search               (data_uoa_list)      - list of data to search                (add_if_date_before) - add only entries with date before this date                (add_if_date_after)  - add only entries with date after this date               (add_if_date)        - add only entries with this date                (search_by_name)     - search by name                (print_time)         - if 'yes', print elapsed time at the end                (search_flat_dict)   - search if these flat keys/values exist in entries               (search_dict)        - search if this dict is a part of the entry               (tags)               - add tags to search in format tags=x,y,z                    or               (search_string)      - search with expressions *?                (ignore_case)        - if 'yes', ignore case of letters                (time_out)           - in secs, default=30                (internal)           - if 'yes', use internal search even if indexing is on                (limit_size)         - by default 5000 or -1 if no limit                (print_full)         - if 'yes', show CID (repo_uoa:module_uoa:data_uoa)               (print_uid)          - if 'yes', print UID in brackets                (print_name)         - if 'yes', print name (and add info to the list)               (add_info)           - if 'yes', add info about entry to the list               (add_meta)           - if 'yes', add meta about entry to the list             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                lst          - [{'repo_uoa', 'repo_uid',                                'module_uoa', 'module_uid',                                 'data_uoa','data_uid',                                'path'}]               elapsed_time - elapsed time in string                (timed_out)  - if 'yes', timed out             }
Input:  {               repo_uoa             - repo UOA               module_uoa           - module UOA               data_uoa             - data UOA               path                 - path                  (search_dict)        - search if this dict is a part of the entry               (ignore_case)        - if 'yes', ignore case of letters             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                lst          - [{'repo_uoa', 'repo_uid',                                'module_uoa', 'module_uid',                                 'data_uoa','data_uid',                                'path'}]             }
Input:  {               dict1         - dictionary 1               dict2         - dictionary 2               (ignore_case) - ignore case of letters                Note that if dict1 and dict2 has lists, the results will be as follows:                * dict1={"key":['a','b','c']}                 dict2={"key":['a','b']}                 EQUAL                * dict1={"key":['a','b']}                 dict2={"key":['a','b','c']}                 NOT EQUAL             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                equal        - if 'yes' dictionaries are equal             }
Input:  {               dict1            - dictionary 1               dict2            - dictionary 2               (ignore_case)    - ignore case of letters               (space_as_none)  - if 'yes', consider "" as None               (keys_to_ignore) - list of keys to ignore (can be wildcards)             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                equal        - if 'yes' dictionaries are equal             }
Input:  {               dict            - dictionary 1               (search_string) - search string               (ignore_case)   - ignore case of letters             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                found        - if 'yes', string found             }
Input:  {               repo_uoa             - repo UOA               module_uoa           - module UOA               data_uoa             - data UOA               path                 - path                  (search_string)      - search with expressions *?             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                lst          - [{'repo_uoa', 'repo_uid',                                'module_uoa', 'module_uid',                                 'data_uoa','data_uid',                                'path'}]             }
Input:  {               request        - request type ('PUT', 'DELETE', 'TEST')               (path)         - path                 (dict)         - query as dict to send             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                dict         - returned dict             }
Input:  {               (repo_uoa)                  - repo UOA               module_uoa                  - normally should be 'module' already               data_uoa                    - UOA of the module to be created                func                        - action               (desc)                      - desc               (for_web)                   - if 'yes', can be used to output html                (skip_appending_dummy_code) - if 'yes', do not append code             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                 Output of 'update' function             }
Input:  {               (repo_uoa)  - repo UOA               module_uoa  - normally should be 'module' already               data_uoa    - UOA of the module to be created                func        - action             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                 Output of 'update' function             }
Input:  {               (repo_uoa)   - repo UOA               (module_uoa) - module_uoa, if =="", use kernel               (data_uoa)               }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                actions      - list of actions             }
Input:  {               (repo_uoa)      - repo UOA, if needed               module_uoa      - module UOA                data_uoa        - data UOA                (filename)      - filename (with path) (if empty, set archive to 'yes')                   or               (cid[0])                                 if empty, create an archive of the entry               (archive)       - if 'yes' pull whole entry as zip archive using filename or ck_archive.zip               (all)           - if 'yes' and archive, add even special directories (.cm, .svn, .git, etc)                 (out)           - if 'json' or 'json_file', encode file and return in r               (skip_writing)  - if 'yes', do not write file (not archive) to current directory                (pattern)       - return only files with this pattern               (patterns)      - multiple patterns (useful to pack mutiple points in experiments)                (encode_file)   - if 'yes', encode file                (skip_tmp)      - if 'yes', skip tmp files and directories             }      Output: {               return                - return code =  0, if successful                                                   >  0, if error               (error)               - error text if return > 0               (file_content_base64) - if i['to_json']=='yes', encoded file               (filename)            - filename to record locally             }
Input:  {               (repo_uoa)            - repo UOA, if needed               module_uoa            - module UOA                data_uoa              - data UOA                (filename)            - local filename                    or               (cid[0])                (extra_path)          - extra path inside entry (create if doesn't exist)                (file_content_base64) - if !='', take its content and record into filename                (archive)             - if 'yes' push to entry and unzip ...                (overwrite)           - if 'yes', overwrite files             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Input:  {               archive_file         - full path to zip file                 (path)               - path where unzip (current if empty)               (overwrite)          - if 'yes', overwrite               (delete_after_unzip) - if 'yes', delete original zip file after unzipping             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                skipped       - list of files which was not overwritten            }
Input:  {               (repo_uoa)                  (module_uoa)                (data_uoa)                  parameters for function 'list_all_files'             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                Output of list all files             }
Input:  Can be dictionary or string (string will be converted to dictionary)              {                action                 module_uoa or CID -> converted to cid                  or                (cid1)            -  if doesn't have = and doesn't start from -- or - or @ -> appended to cids[]                (cid2)            -  if doesn't have = and doesn't start from -- or - or @ -> appended to cids[]                (cid3)            -  if doesn't have = and doesn't start from -- or - or @ -> appended to cids[]                  or                (repo_uoa)                (module_uoa)                (data_uoa)                 (out=type)     Module output                               == ''              - none                               == 'con'           - console interaction (if from CMD, default)                               == 'json'          - return dict as json to console                               == 'json_with_sep' - separation line and return dict as json to console                               == 'json_file'     - return dict as json to file                 (out_file)     Output file if out=='json_file'                 (con_encoding) - force encoding for IO                (ck_profile)   - if 'yes', profile CK                 INPUT TO A GIVEN FUNCTION                  NOTE: If INPUT is a string and it will be converted to INPUT dictionary as follows (the same as CK command line):                       ck key1=value1    -> converted to {key1:value1}                          -key10         -> converted to {key10:"yes"}                         -key11=value11 -> converted to {key11:value11}                          --key12         -> converted to {key12:"yes"}                         --key13=value13 -> converted to {key13:value13}                        @file_json         -> JSON from this file will be merged with INPUT                      @@                 -> CK will ask user ot enter manually JSON from console and merge with INPUT                      @@key              -> Enter JSON manually from console and merge with INPUT under this key                       @@@cmd_json        -> convert string to JSON (special format) and merge with INPUT                       -- xyz             -> add everything after -- to "unparsed_cmd" key in INPUT                       When string is converted to INPUT dictionary, "cmd" variable is set to True             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                OUTPUT FROM A GIVEN FUNCTION             }
Input:  {               (repo_uoa)          - repo UOA               module_uoa          - normally should be 'module' already               data_uoa            - UOA of the module to be created                (desc)              - module description               (license)           - module license               (copyright)         - module copyright               (developer)         - module developer               (developer_email)   - module developer               (developer_webpage) - module developer               (actions)           - dict with actions {"func1":{}, "func2":{} ...}               (dict)              - other meta description to add to entry                (quiet)             - minimal interaction               (func)              - just add one dummy action             }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0                Output of the 'add' kernel function             }
Input:  {                (the same as list; can use wildcards)               }      Output: {               return       - return code =  0, if successful                                          >  0, if error               (error)      - error text if return > 0             }
Returns maximum path similarity between two senses.      :param sense1: A synset.     :param sense2: A synset.     :param option: String, one of ('path', 'wup', 'lch').     :return: A float, similarity measurement.
Returns similarity scores by information content.      :param sense1: A synset.     :param sense2: A synset.     :param option: String, one of ('res', 'jcn', 'lin').     :return: A float, similarity measurement.
Calculates similarity based on user's choice.      :param sense1: A synset.     :param sense2: A synset.     :param option: String, one of ('path', 'wup', 'lch', 'res', 'jcn', 'lin').     :return: A float, similarity measurement.
Perform WSD by maximizing the sum of maximum similarity between possible     synsets of all words in the context sentence and the possible synsets of the     ambiguous words (see http://goo.gl/XMq2BI):     {argmax}_{synset(a)}(\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}      :param context_sentence: String, a sentence.     :param ambiguous_word: String, a single word.     :return: If best, returns only the best Synset, else returns a dict.
Removes <tags> in angled brackets from text.
Converts SemCor sensekey IDs to synset offset.     >>> print semcor_to_offset('live%2:42:06::')     02614387-v
Tries to convert a surface word into lemma, and if lemmatize word is not in     wordnet then try and convert surface word into its stem.      This is to handle the case where users input a surface word as an ambiguous     word and the surface word is a not a lemma.
Converts tags from Penn format (input: single string) to Morphy.
Making from NLTK's WordNet Synset's properties to function.     Note: This is for compatibility with NLTK 2.x
Returns a list of synsets of a word after lemmatization.
Computes the label for each data point
Adds a column of ones to estimate the intercept term for          separation boundary
Evaluates the predicted outputs against the gold data.
Calculates cosine similarity between 2 sentences/documents.     Thanks to @vpekar, see http://goo.gl/ykibJY
Takes a Synset and returns its signature words.      :param ss: An instance of wn.Synset.     :return: A set of signature strings
Takes an ambiguous word and optionally its Part-Of-Speech and returns     a dictionary where keys are the synsets and values are sets of signatures.      :param ambiguous_word: String, a single word.     :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.     :return: dict(synset:{signatures}).
Calculate overlaps between the context sentence and the synset_signatures     and returns the synset with the highest overlap.      Note: Greedy algorithm only keeps the best sense,     see https://en.wikipedia.org/wiki/Greedy_algorithm      Only used by original_lesk(). Keeping greedy algorithm for documentary sake,     because original_lesks is greedy.      :param context: List of strings, tokenized sentence or document.     :param synsets_signatures: dict of Synsets and the set of their corresponding signatures.     :return: The Synset with the highest number of overlaps with its signatures.
Calculates overlaps between the context sentence and the synset_signture     and returns a ranked list of synsets from highest overlap to lowest.      :param context: List of strings, tokenized sentence or document.     :param synsets_signatures: dict of Synsets and the set of their corresponding signatures.     :return: The Synset with the highest number of overlaps with its signatures.
This function is the implementation of the original Lesk algorithm (1986).     It requires a dictionary which contains the definition of the different     sense of each word. See http://dl.acm.org/citation.cfm?id=318728      :param context_sentence: String, sentence or document.     :param ambiguous_word: String, a single word.     :return: A Synset for the estimated best sense.
Returns a synsets_signatures dictionary that includes signature words of a     sense from its:     (i)   definition     (ii)  example sentences     (iii) hypernyms and hyponyms      :param ambiguous_word: String, a single word.     :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.     :return: dict(synset:{signatures})
Simple Lesk is somewhere in between using more than the     original Lesk algorithm (1986) and using less signature     words than adapted Lesk (Banerjee and Pederson, 2002)      :param context_sentence: String, sentence or document.     :param ambiguous_word: String, a single word.     :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.     :return: A Synset for the estimated best sense.
This function is the implementation of the Adapted Lesk algorithm,     described in Banerjee and Pederson (2002). It makes use of the lexical     items from semantically related senses within the wordnet     hierarchies and to generate more lexical items for each sense.     see www.d.umn.edu/~tpederse/Pubs/cicling2002-b.pdf‎      :param context_sentence: String, sentence or document.     :param ambiguous_word: String, a single word.     :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.     :return: A Synset for the estimated best sense.
In line with vector space models, we can use cosine to calculate overlaps     instead of using raw overlap counts. Essentially, the idea of using     signatures (aka 'sense paraphrases') is lesk-like.      :param context_sentence: String, sentence or document.     :param ambiguous_word: String, a single word.     :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.     :return: A Synset for the estimated best sense.
Returns files from SemEval2007 Coarse-grain All-words WSD task.
Returns the file, line by line. Use test_file if no filename specified.
Returns a {(key,value), ...} dictionary of {(instance_id,Answer),...)}         >>> coarse_wsd = SemEval2007_Coarse_WSD()         >>> inst2ans = coarse_wsd.get_answers()         >>> for inst in inst2ans:         ...    print inst, inst2ans[inst         ...    break
Returns the instances by sentences, and yields a list of tokens,         similar to the pywsd.semcor.sentences.          >>> coarse_wsd = SemEval2007_Coarse_WSD()         >>> for sent in coarse_wsd.sentences():         >>>     for token in sent:         >>>         print token         >>>         break         >>>     break         word(id=None, text=u'Your', offset=None, sentid=0, paraid=u'd001', term=None)
Returns a random sense.      :param ambiguous_word: String, a single word.     :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.     :return: A random Synset.
Returns the first sense.      :param ambiguous_word: String, a single word.     :param pos: String, one of 'a', 'r', 's', 'n', 'v', or None.     :return: The first Synset in the wn.synsets(word) list.
Returns the sense with the highest lemma_name count.     The max_lemma_count() can be treated as a rough gauge for the     Most Frequent Sense (MFS), if no other sense annotated corpus is available.     NOTE: The lemma counts are from the Brown Corpus      :param ambiguous_word: String, a single word.     :return: The estimated most common Synset.
Returns the mean and the variance of a data set of X points assuming that      the points come from a gaussian distribution X.
Returns maximum value of a dictionary.
L2 normalize squared
Normalizes an array to sum to one, either column wise,      or row wise or the full array.          *Directions*         Column-wise - 0 default         Row-wise - 1 default         All - 2 default
Check whether a C/C++ library is available on the system to the compiler.      Parameters     ----------     library: str         The library we want to check for e.g. if we are interested in FFTW3, we         want to check for `fftw3.h`, so this parameter will be `fftw3`.     extension: str         If we want to check for a C library, the extension is `.c`, for C++         `.cc`, `.cpp` or `.cxx` are accepted.      Returns     -------     bool         Whether or not the library is available.
Calculate counts-per-million on data where the rows are genes.          Parameters     ----------     x : array_like     axis : int         Axis accross which to compute CPM. 0 for genes being in rows and 1 for         genes in columns.
Perform log transform log(x + 1).          Parameters     ----------     data : array_like
Check that the metric is supported by the KNNIndex instance.
Initialize an embedding using samples from an isotropic Gaussian.      Parameters     ----------     X: np.ndarray         The data matrix.      n_components: int         The dimension of the embedding space.      random_state: Union[int, RandomState]         If the value is an int, random_state is the seed used by the random         number generator. If the value is a RandomState instance, then it will         be used as the random number generator. If the value is None, the random         number generator is the RandomState instance used by `np.random`.      Returns     -------     initialization: np.ndarray
Initialize an embedding using the top principal components.      Parameters     ----------     X: np.ndarray         The data matrix.      n_components: int         The dimension of the embedding space.      random_state: Union[int, RandomState]         If the value is an int, random_state is the seed used by the random         number generator. If the value is a RandomState instance, then it will         be used as the random number generator. If the value is None, the random         number generator is the RandomState instance used by `np.random`.      Returns     -------     initialization: np.ndarray
Initialize points onto an existing embedding by placing them in the     weighted mean position of their nearest neighbors on the reference embedding.      Parameters     ----------     X: np.ndarray     embedding: TSNEEmbedding     neighbors: np.ndarray     distances: np.ndarray      Returns     -------     np.ndarray
Create a numba accelerated version of heap initialization for the     alternative k-neighbor graph algorithm. This approach builds two heaps     of neighbors simultaneously, one is a heap used to construct a very     approximate k-neighbor graph for searching; the other is the     initialization for searching.      Parameters     ----------     dist: function         A numba JITd distance function which, given two arrays computes a         dissimilarity between them.      dist_args: tuple         Any extra arguments that need to be passed to the distance function         beyond the two arrays to be compared.      Returns     -------     A numba JITd function for for heap initialization that is     specialised to the given metric.
Prune the k-neighbors graph back so that nodes have a maximum     degree of ``max_degree``.      Parameters     ----------     graph: sparse matrix         The adjacency matrix of the graph      max_degree: int (optional, default 20)         The maximum degree of any node in the pruned graph      Returns     -------     result: sparse matrix         The pruned graph.
Perform pruning on the graph so that there are fewer edges to     be followed. In practice this operates in two passes. The first pass     removes edges such that no node has degree more than ``3 * n_neighbors -     prune_level``. The second pass builds up a graph out of spanning trees;     each iteration constructs a minimum panning tree of a graph and then     removes those edges from the graph. The result is spanning trees that     take various paths through the graph. All these spanning trees are merged     into the resulting graph. In practice this prunes away a limited number     of edges as long as enough iterations are performed. By default we will     do ``n_neighbors - prune_level``iterations.      Parameters     ----------     graph: sparse matrix         The adjacency matrix of the graph      prune_level: int (optional default 0)         How aggressively to prune the graph, larger values perform more         aggressive pruning.      n_neighbors: int (optional 10)         The number of neighbors of the k-neighbor graph that was constructed.      Returns     -------     result: sparse matrix         The pruned graph
Query the training data for the k nearest neighbors          Parameters         ----------         query_data: array-like, last dimension self.dim             An array of points to query          k: integer (default = 10)             The number of nearest neighbors to return          queue_size: float (default 5.0)             The multiplier of the internal search queue. This controls the             speed/accuracy tradeoff. Low values will search faster but with             more approximate results. High values will search more             accurately, but will require more computation to do so. Values             should generally be in the range 1.0 to 10.0.          Returns         -------         indices, distances: array (n_query_points, k), array (n_query_points, k)             The first array, ``indices``, provides the indices of the data             points in the training set that are the nearest neighbors of             each query point. Thus ``indices[i, j]`` is the index into the             training data of the jth nearest neighbor of the ith query points.              Similarly ``distances`` provides the distances to the neighbors             of the query points such that ``distances[i, j]`` is the distance             from the ith query point to its jth nearest neighbor in the             training data.
Fit the PyNNDescent transformer to build KNN graphs with         neighbors given by the dataset X.          Parameters         ----------         X : array-like, shape (n_samples, n_features)             Sample data          Returns         -------         transformer : PyNNDescentTransformer             The trained transformer
Computes the (weighted) graph of Neighbors for points in X          Parameters         ----------         X : array-like, shape (n_samples_transform, n_features)             Sample data          Returns         -------         Xt : CSR sparse matrix, shape (n_samples_fit, n_samples_transform)             Xt[i, j] is assigned the weight of edge that connects i to j.             Only the neighbors have an explicit value.
A weighted version of Minkowski distance.      ..math::         D(x, y) = \left(\sum_i w_i |x_i - y_i|^p\right)^{\frac{1}{p}}      If weights w_i are inverse standard deviations of data in each dimension     then this represented a standardised Minkowski distance (and is     equivalent to standardised Euclidean distance for p=1).
Convert the user friendly params into something the optimizer can     understand.
Run optmization on the embedding for a given number of steps.          Parameters         ----------         n_iter: int             The number of optimization iterations.          learning_rate: float             The learning rate for t-SNE optimization. Typical values range             between 100 to 1000. Setting the learning rate too low or too high             may result in the points forming a "ball". This is also known as the             crowding problem.          exaggeration: float             The exaggeration factor is used to increase the attractive forces of             nearby points, producing more compact clusters.          momentum: float             Momentum accounts for gradient directions from previous iterations,             resulting in faster convergence.          negative_gradient_method: str             Specifies the negative gradient approximation method to use. For             smaller data sets, the Barnes-Hut approximation is appropriate and             can be set using one of the following aliases: ``bh``, ``BH`` or             ``barnes-hut``. For larger data sets, the FFT accelerated             interpolation method is more appropriate and can be set using one of             the following aliases: ``fft``, ``FFT`` or ``ìnterpolation``.          theta: float             This is the trade-off parameter between speed and accuracy of the             tree approximation method. Typical values range from 0.2 to 0.8. The             value 0 indicates that no approximation is to be made and produces             exact results also producing longer runtime.          n_interpolation_points: int             Only used when ``negative_gradient_method="fft"`` or its other             aliases. The number of interpolation points to use within each grid             cell for interpolation based t-SNE. It is highly recommended leaving             this value at the default 3.          min_num_intervals: int             Only used when ``negative_gradient_method="fft"`` or its other             aliases. The minimum number of grid cells to use, regardless of the             ``ints_in_interval`` parameter. Higher values provide more accurate             gradient estimations.          inplace: bool             Whether or not to create a copy of the embedding or to perform             updates inplace.          propagate_exception: bool             The optimization process can be interrupted using callbacks. This             flag indicates whether we should propagate that exception or to             simply stop optimization and return the resulting embedding.          random_state: Union[int, RandomState]             The random state parameter follows the convention used in             scikit-learn. If the value is an int, random_state is the seed used             by the random number generator. If the value is a RandomState             instance, then it will be used as the random number generator. If             the value is None, the random number generator is the RandomState             instance used by `np.random`.          n_jobs: int             The number of threads to use while running t-SNE. This follows the             scikit-learn convention, ``-1`` meaning all processors, ``-2``             meaning all but one, etc.          callbacks: Callable[[int, float, np.ndarray] -> bool]             Callbacks, which will be run every ``callbacks_every_iters``             iterations.          callbacks_every_iters: int             How many iterations should pass between each time the callbacks are             invoked.          Returns         -------         PartialTSNEEmbedding             An optimized partial t-SNE embedding.          Raises         ------         OptimizationInterrupt             If a callback stops the optimization and the ``propagate_exception``             flag is set, then an exception is raised.
Embed new points into the existing embedding.          This procedure optimizes each point only with respect to the existing         embedding i.e. it ignores any interactions between the points in ``X``         among themselves.          Please see the :ref:`parameter-guide` for more information.          Parameters         ----------         X: np.ndarray             The data matrix to be added to the existing embedding.          perplexity: float             Perplexity can be thought of as the continuous :math:`k` number of             nearest neighbors, for which t-SNE will attempt to preserve             distances. However, when transforming, we only consider neighbors in             the existing embedding i.e. each data point is placed into the             embedding, independently of other new data points.          initialization: Union[np.ndarray, str]             The initial point positions to be used in the embedding space. Can             be a precomputed numpy array, ``median``, ``weighted`` or             ``random``. In all cases, ``median`` of ``weighted`` should be             preferred.          k: int             The number of nearest neighbors to consider when initially placing             the point onto the embedding. This is different from ``perpelxity``             because perplexity affects optimization while this only affects the             initial point positions.          learning_rate: float             The learning rate for t-SNE optimization. Typical values range             between 100 to 1000. Setting the learning rate too low or too high             may result in the points forming a "ball". This is also known as the             crowding problem.          n_iter: int             The number of iterations to run in the normal optimization regime.             Typically, the number of iterations needed when adding new data             points is much lower than with regular optimization.          exaggeration: float             The exaggeration factor to use during the normal optimization phase.             This can be used to form more densely packed clusters and is useful             for large data sets.          momentum: float             The momentum to use during optimization phase.          Returns         -------         PartialTSNEEmbedding             The positions of the new points in the embedding space.
Prepare a partial embedding which can be optimized.          Parameters         ----------         X: np.ndarray             The data matrix to be added to the existing embedding.          initialization: Union[np.ndarray, str]             The initial point positions to be used in the embedding space. Can             be a precomputed numpy array, ``median``, ``weighted`` or             ``random``. In all cases, ``median`` of ``weighted`` should be             preferred.          k: int             The number of nearest neighbors to consider when initially placing             the point onto the embedding. This is different from ``perpelxity``             because perplexity affects optimization while this only affects the             initial point positions.          **affinity_params: dict             Additional params to be passed to the ``Affinities.to_new`` method.             Please see individual :class:`~openTSNE.affinity.Affinities`             implementations as the parameters differ between implementations.          Returns         -------         PartialTSNEEmbedding             An unoptimized :class:`PartialTSNEEmbedding` object, prepared for             optimization.
Fit a t-SNE embedding for a given data set.          Runs the standard t-SNE optimization, consisting of the early         exaggeration phase and a normal optimization phase.          Parameters         ----------         X: np.ndarray             The data matrix to be embedded.          Returns         -------         TSNEEmbedding             A fully optimized t-SNE embedding.
Prepare the initial embedding which can be optimized as needed.          Parameters         ----------         X: np.ndarray             The data matrix to be embedded.          Returns         -------         TSNEEmbedding             An unoptimized :class:`TSNEEmbedding` object, prepared for             optimization.
Compute the conditional probability matrix P_{j|i}.      This method computes an approximation to P using the nearest neighbors.      Parameters     ----------     neighbors: np.ndarray         A `n_samples * k_neighbors` matrix containing the indices to each         points" nearest neighbors in descending order.     distances: np.ndarray         A `n_samples * k_neighbors` matrix containing the distances to the         neighbors at indices defined in the neighbors parameter.     perplexities: double         The desired perplexity of the probability distribution.     symmetrize: bool         Whether to symmetrize the probability matrix or not. Symmetrizing is         used for typical t-SNE, but does not make sense when embedding new data         into an existing embedding.     normalization: str         The normalization scheme to use for the affinities. Standard t-SNE         considers interactions between all the data points, therefore the entire         affinity matrix is regarded as a probability distribution, and must sum         to 1. When embedding new points, we only consider interactions to         existing points, and treat each point separately. In this case, we         row-normalize the affinity matrix, meaning each point gets its own         probability distribution.     n_reference_samples: int         The number of samples in the existing (reference) embedding. Needed to         properly construct the sparse P matrix.     n_jobs: int         Number of threads.      Returns     -------     csr_matrix         A `n_samples * n_reference_samples` matrix containing the probabilities         that a new sample would appear as a neighbor of a reference point.
Change the perplexity of the affinity matrix.          Note that we only allow lowering the perplexity or restoring it to its         original value. This restriction exists because setting a higher         perplexity value requires recomputing all the nearest neighbors, which         can take a long time. To avoid potential confusion as to why execution         time is slow, this is not allowed. If you would like to increase the         perplexity above the initial value, simply create a new instance.          Parameters         ----------         new_perplexity: float             The new perplexity.
Compute the affinities of new samples to the initial samples.          This is necessary for embedding new data points into an existing         embedding.          Please see the :ref:`parameter-guide` for more information.          Parameters         ----------         data: np.ndarray             The data points to be added to the existing embedding.          perplexity: float             Perplexity can be thought of as the continuous :math:`k` number of             nearest neighbors, for which t-SNE will attempt to preserve             distances.          return_distances: bool             If needed, the function can return the indices of the nearest             neighbors and their corresponding distances.          Returns         -------         P: array_like             An :math:`N \\times M` affinity matrix expressing interactions             between :math:`N` new data points the initial :math:`M` data             samples.          indices: np.ndarray             Returned if ``return_distances=True``. The indices of the :math:`k`             nearest neighbors in the existing embedding for every new data             point.          distances: np.ndarray             Returned if ``return_distances=True``. The distances to the             :math:`k` nearest neighbors in the existing embedding for every new             data point.
Compute the affinities of new samples to the initial samples.          This is necessary for embedding new data points into an existing         embedding.          Parameters         ----------         data: np.ndarray             The data points to be added to the existing embedding.          k: int             The number of nearest neighbors to consider for each kernel.          sigma: float             The bandwidth to use for the Gaussian kernels in the ambient space.          return_distances: bool             If needed, the function can return the indices of the nearest             neighbors and their corresponding distances.          Returns         -------         P: array_like             An :math:`N \\times M` affinity matrix expressing interactions             between :math:`N` new data points the initial :math:`M` data             samples.          indices: np.ndarray             Returned if ``return_distances=True``. The indices of the :math:`k`             nearest neighbors in the existing embedding for every new data             point.          distances: np.ndarray             Returned if ``return_distances=True``. The distances to the             :math:`k` nearest neighbors in the existing embedding for every new             data point.
Change the perplexities of the affinity matrix.          Note that we only allow lowering the perplexities or restoring them to         their original maximum value. This restriction exists because setting a         higher perplexity value requires recomputing all the nearest neighbors,         which can take a long time. To avoid potential confusion as to why         execution time is slow, this is not allowed. If you would like to         increase the perplexity above the initial value, simply create a new         instance.          Parameters         ----------         new_perplexities: List[float]             The new list of perplexities.
Compute the affinities of new samples to the initial samples.          This is necessary for embedding new data points into an existing         embedding.          Please see the :ref:`parameter-guide` for more information.          Parameters         ----------         data: np.ndarray             The data points to be added to the existing embedding.          perplexities: List[float]             A list of perplexity values, which will be used in the multiscale             Gaussian kernel. Perplexity can be thought of as the continuous             :math:`k` number of nearest neighbors, for which t-SNE will attempt             to preserve distances.          return_distances: bool             If needed, the function can return the indices of the nearest             neighbors and their corresponding distances.          Returns         -------         P: array_like             An :math:`N \\times M` affinity matrix expressing interactions             between :math:`N` new data points the initial :math:`M` data             samples.          indices: np.ndarray             Returned if ``return_distances=True``. The indices of the :math:`k`             nearest neighbors in the existing embedding for every new data             point.          distances: np.ndarray             Returned if ``return_distances=True``. The distances to the             :math:`k` nearest neighbors in the existing embedding for every new             data point.
Check and correct/truncate perplexities.          If a perplexity is too large, it is corrected to the largest allowed         value. It is then inserted into the list of perplexities only if that         value doesn't already exist in the list.
Constructor for the numba enabled heap objects. The heaps are used     for approximate nearest neighbor search, maintaining a list of potential     neighbors sorted by their distance. We also flag if potential neighbors     are newly added to the list or not. Internally this is stored as     a single ndarray; the first axis determines whether we are looking at the     array of candidate indices, the array of distances, or the flag array for     whether elements are new or not. Each of these arrays are of shape     (``n_points``, ``size``)      Parameters     ----------     n_points: int         The number of data points to track in the heap.      size: int         The number of items to keep on the heap for each data point.      Returns     -------     heap: An ndarray suitable for passing to other numba enabled heap functions.
Push a new element onto the heap. The heap stores potential neighbors     for each data point. The ``row`` parameter determines which data point we     are addressing, the ``weight`` determines the distance (for heap sorting),     the ``index`` is the element to add, and the flag determines whether this     is to be considered a new addition.      Parameters     ----------     heap: ndarray generated by ``make_heap``         The heap object to push into      row: int         Which actual heap within the heap object to push to      weight: float         The priority value of the element to push onto the heap      index: int         The actual value to be pushed      flag: int         Whether to flag the newly added element or not.      Returns     -------     success: The number of new elements successfully pushed into the heap.
Given a set of ``indices`` for data points from ``data``, create     a random hyperplane to split the data, returning two arrays indices     that fall on either side of the hyperplane. This is the basis for a     random projection tree, which simply uses this splitting recursively.      This particular split uses euclidean distance to determine the hyperplane     and which side each data sample falls on.      Parameters     ----------     data: array of shape (n_samples, n_features)         The original data to be split      indices: array of shape (tree_node_size,)         The indices of the elements in the ``data`` array that are to         be split in the current operation.      rng_state: array of int64, shape (3,)         The internal state of the rng      Returns     -------     indices_left: array         The elements of ``indices`` that fall on the "left" side of the         random hyperplane.      indices_right: array         The elements of ``indices`` that fall on the "left" side of the         random hyperplane.
Estimate the autocorrelation function of a time series using the FFT.      :param x:         The time series. If multidimensional, set the time axis using the         ``axis`` keyword argument and the function will be computed for every         other axis.      :param axis: (optional)         The time axis of ``x``. Assumed to be the first axis if not specified.      :param fast: (optional)         If ``True``, only use the largest ``2^n`` entries for efficiency.         (default: False)
Estimate the integrated autocorrelation time of a time series.      See `Sokal's notes <http://www.stat.unc.edu/faculty/cji/Sokal.pdf>`_ on     MCMC and sample estimators for autocorrelation times.      :param x:         The time series. If multidimensional, set the time axis using the         ``axis`` keyword argument and the function will be computed for every         other axis.      :param axis: (optional)         The time axis of ``x``. Assumed to be the first axis if not specified.      :param window: (optional)         The size of the window to use. (default: 50)      :param fast: (optional)         If ``True``, only use the largest ``2^n`` entries for efficiency.         (default: False)
Thermodynamic integration estimate of the evidence.      :param betas: The inverse temperatures to use for the quadrature.      :param logls:  The mean log-likelihoods corresponding to ``betas`` to use for         computing the thermodynamic evidence.      :return ``(logZ, dlogZ)``: Returns an estimate of the         log-evidence and the error associated with the finite         number of temperatures at which the posterior has been         sampled.      The evidence is the integral of the un-normalized posterior     over all of parameter space:      .. math::          Z \\equiv \\int d\\theta \\, l(\\theta) p(\\theta)      Thermodymanic integration is a technique for estimating the     evidence integral using information from the chains at various     temperatures.  Let      .. math::          Z(\\beta) = \\int d\\theta \\, l^\\beta(\\theta) p(\\theta)      Then      .. math::          \\frac{d \\log Z}{d \\beta}         = \\frac{1}{Z(\\beta)} \\int d\\theta l^\\beta p \\log l         = \\left \\langle \\log l \\right \\rangle_\\beta      so      .. math::          \\log Z(1) - \\log Z(0)         = \\int_0^1 d\\beta \\left \\langle \\log l \\right\\rangle_\\beta      By computing the average of the log-likelihood at the     difference temperatures, the sampler can approximate the above     integral.
Return request's 'Authorization:' header, as a bytestring.     From: https://github.com/tomchristie/django-rest-framework/blob/master/rest_framework/authentication.py
Returns an active user that matches the payload's user id and email.
Given a set of transactions, find the patterns in it     over the specified support threshold.
Given a set of frequent itemsets, return a dict     of association rules in the form     {(left): ((right), confidence)}
Check if node has a particular child node.
Return a child node with a particular value.
Add a node as a child node.
Create a dictionary of items with occurrences above the threshold.
Build the FP tree and return the root node.
Recursively grow FP tree.
If there is a single path in the tree,         return True, else return False.
Mine the constructed FP tree for frequent patterns.
Append suffix to patterns in dictionary if         we are in a conditional FP tree.
Generate a list of patterns with support counts.
Generate subtrees and mine them for patterns.
判断 f 是否是视频文件
筛选出 path 目录下所有的视频文件
初始化 requests.Session
调用 SubSearcher 搜索并下载字幕
SubFinder 入口，开始函数
generate filename of subtitles         :TODO: fix the conflict of subname
compute videofile's hash         reference: https://docs.google.com/document/d/1w5MCBO61rKQ6hI5m9laJLWse__yTYdRugpVyz4RzrmM/preview
delete all subtitles in path recursively
move videos in sub-directory of path to path.
重命名字幕文件     @param source: 要重命名的字幕文件     @param template: 模板，例如：'Friends.S{season:02d}.E{episode:02d}.1080p.5.1Ch.BluRay.ReEnc-DeeJayAhmed.{}.{ext}'     支持的变量有：season, episode, language, ext
parse download count         text format maybe:         - pure number: 1000         - number + unit: 1万
parse search result html, return subgroups         subgroups: [{ 'title': title, 'link': link}]
return subinfo_list of videoname
解析搜索结果页面，返回字幕信息列表
解析字幕详情页面
解析下载页面，返回下载链接
根据关键词搜索，返回字幕信息列表
访问字幕详情页面，解析出下载页面的地址
获取关键词
register a subsearcher, the `name` is a key used for searching subsearchers.     if the subsearcher named `name` already exists, then it's will overrite the old subsearcher.
parse the `videofile` and return it's basename
parse videoname and return video info dict         video info contains:         - title, the name of video         - sub_title, the sub_title of video         - resolution,         - source,         -         - season, defaults to 0         - episode, defaults to 0
解压字幕文件，如果无法解压，则直接返回 compressed_file。         exts 参数用于过滤掉非字幕文件，只有文件的扩展名在 exts 中，才解压该文件。
filter subinfo list base on:         - season         - episode         - languages         - exts         -         return a best matched subinfo
下载字幕         videofile: 视频文件路径         sub_title: 字幕标题（文件名）         download_link: 下载链接         referer: referer
Create and return a connection proxy to the sqlite database.
Execute the given function on the shared connection's thread.
Execute the given query.
Execute the given multiquery.
Execute a user script.
Fetch a single row.
Fetch up to `cursor.arraysize` number of rows.
Fetch all remaining rows.
Execute function calls on a separate thread.
Queue a function with the given arguments for execution.
Connect to the actual sqlite database.
Create an aiosqlite cursor wrapping a sqlite3 cursor object.
Complete queued queries/cursors and close the connection.
Helper to insert and get the last_insert_rowid.
Helper to execute a query and return all the data.
Helper to create a cursor and execute the given multiquery.
Helper to create a cursor and execute a user script.
DocTests: 	>>> human_time(0) == '' 	True 	>>> human_time(122.1) == '2m2s' 	True 	>>> human_time(133) == '2m13s' 	True 	>>> human_time(12345678) == '20W2D21h21m18s' 	True
DocTests: 	>>> limit_unit('1m2s', 1) == '1m' 	True 	>>> limit_unit('1m2s') == '1m2s' 	True 	>>> limit_unit('1m2s', 4) == '1m2s' 	True 	>>> limit_unit('1d2h3m2s') == '1d2h' 	True 	>>> limit_unit('1d2h3m2s', 1) == '1d' 	True
>>> interpret_size(10) 	10 	>>> interpret_size('10') 	10 	>>> interpret_size('10b') 	10 	>>> interpret_size('10k') 	10240 	>>> interpret_size('10K') 	10240 	>>> interpret_size('10kb') 	10240 	>>> interpret_size('10kB') 	10240 	>>> interpret_size('a10') 	Traceback (most recent call last): 	ValueError 	>>> interpret_size('10a') 	Traceback (most recent call last): 	KeyError: 'A'
Usage: unzip <remotepath> [<subpath> [<start> [<limit>]]]
Usage: extract <remotepath> <subpath> [<saveaspath>]
Usage: revert <remotepath> revisionid [dir]
Main Entry
Usage: help <command> - provide some information for the command
Usage: list/ls [remotepath] [format] [sort] [order] - list the 'remotepath' directory at Baidu PCS     remotepath - the remote path at Baidu PCS. default: root directory '/' 	format - specifies how the list are displayed 	  $t - Type: Directory ('D') or File ('F') 	  $f - File name 	  $c - Creation time 	  $m - Modification time 	  $d - MD5 hash 	  $s - Size 	  $$ - The '$' sign 	  So '$t - $f - $s - $$' will display "Type - File - Size - $' 	  Default format: '$t $f $s $m $d'     sort - sorting by [name, time, size]. default: 'name'     order - sorting order [asc, desc]. default: 'asc'
Usage: meta <remotepath> [format] - \ get information of the given path (dir / file) at Baidu Yun.   remotepath - the remote path   format - specifies how the list are displayed     it supports all the format variables in the 'list' command, and additionally the followings: 	$i - fs_id 	$b - MD5 block_list 	$u - Has sub directory or not
Usage: upload [localpath] [remotepath] [ondup] - \ upload a file or directory (recursively)     localpath - local path, is the current directory '.' if not specified     remotepath - remote path at Baidu Yun (after app root directory at Baidu Yun)     ondup - what to do upon duplication ('overwrite' or 'newcopy'), default: 'overwrite'
Usage: combine <remotefile> [localfile] [md5s] - \ try to create a file at PCS by combining slices, having MD5s specified   remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)   localfile - local file to verify against, passing in a star '*' or '/dev/null' means no verification   md5s - MD5 digests of the slices, can be:     - list of MD5 hex strings separated by spaces     - a string in the form of 'l<path>' where <path> points to a text file containing MD5 hex strings separated by spaces or line-by-line
Usage: downfile <remotefile> [localpath] - \ download a remote file.   remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)   localpath - local path.     if it ends with '/' or '\\', it specifies the local directory     if it specifies an existing directory, it is the local directory     if not specified, the local directory is the current directory '.'     otherwise, it specifies the local file name To stream a file using downfile, you can use the 'mkfifo' trick with omxplayer etc.:   mkfifo /tmp/omx   bypy.py downfile <remotepath> /tmp/omx &   omxplayer /tmp/omx
Usage: stream <remotefile> <localpipe> [format] [chunk] - \ stream a video / audio file converted to M3U format at cloud side, to a pipe.   remotefile - remote file at Baidu Yun (after app root directory at Baidu Yun)   localpipe - the local pipe file to write to   format - output video format (M3U8_320_240 | M3U8_480_224 | \ M3U8_480_360 | M3U8_640_480 | M3U8_854_480). Default: M3U8_480_360   chunk - chunk (initial buffering) size for streaming (default: 4M) To stream a file, you can use the 'mkfifo' trick with omxplayer etc.:   mkfifo /tmp/omx   bypy.py downfile <remotepath> /tmp/omx &   omxplayer /tmp/omx   *** NOT WORKING YET ****
Usage: downdir [remotedir] [localdir] - \ download a remote directory (recursively)   remotedir - remote directory at Baidu Yun (after app root directory), if not specified, it is set to the root directory at Baidu Yun   localdir - local directory. if not specified, it is set to the current directory
Usage: download [remotepath] [localpath] - \ download a remote directory (recursively) / file   remotepath - remote path at Baidu Yun (after app root directory), if not specified, it is set to the root directory at Baidu Yun   localpath - local path. if not specified, it is set to the current directory
Usage: move/mv/rename/ren <from> <to> - \ move a file / dir remotely at Baidu Yun   from - source path (file / dir)   to - destination path (file / dir)
Usage: copy/cp <from> <to> - \ copy a file / dir remotely at Baidu Yun   from - source path (file / dir)   to - destination path (file / dir)
Usage: search <keyword> [remotepath] [recursive] - \ search for a file using keyword at Baidu Yun   keyword - the keyword to search   remotepath - remote path at Baidu Yun, if not specified, it's app's root directory   resursive - search recursively or not. default is true
Usage: listrecycle [start] [limit] - \ list the recycle contents   start - starting point, default: 0   limit - maximum number of items to display. default: 1000
Usage: restore <remotepath> - \ restore a file from the recycle bin   remotepath - the remote path to restore
Usage: compare [remotedir] [localdir] - \ compare the remote directory with the local directory   remotedir - the remote directory at Baidu Yun (after app's directory). \ if not specified, it defaults to the root directory.   localdir - the local directory, if not specified, it defaults to the current directory.   skip_remote_only_dirs - skip remote-only sub-directories (faster if the remote \ directory is much larger than the local one). it defaults to False.
Usage: syncdown [remotedir] [localdir] [deletelocal] - \ sync down from the remote directory to the local directory   remotedir - the remote directory at Baidu Yun (after app's directory) to sync from. \ if not specified, it defaults to the root directory   localdir - the local directory to sync to if not specified, it defaults to the current directory.   deletelocal - delete local files that are not inside Baidu Yun directory, default is False
Usage: syncup [localdir] [remotedir] [deleteremote] - \ sync up from the local directory to the remote directory   localdir - the local directory to sync from if not specified, it defaults to the current directory.   remotedir - the remote directory at Baidu Yun (after app's directory) to sync to. \ if not specified, it defaults to the root directory   deleteremote - delete remote files that are not inside the local directory, default is False
Usage: dumpcache - display file hash cache
Usage: cleancache - remove invalid entries from hash cache file
Usage: cdl_add <source_url> [save_path] [timeout] - add an offline (cloud) download task   source_url - the URL to download file from.   save_path - path on PCS to save file to. default is to save to root directory '/'.   timeout - timeout in seconds. default is 3600 seconds.
Usage: cdl_addmon <source_url> [save_path] [timeout] - add an offline (cloud) download task and monitor the download progress   source_url - the URL to download file from.   save_path - path on PCS to save file to. default is to save to root directory '/'.   timeout - timeout in seconds. default is 3600 seconds.
:type method: str
Compute the log probability under a multivariate Gaussian distribution.     Parameters     ----------     X : array_like, shape (n_samples, n_features)         List of n_features-dimensional data points. Each row corresponds to a         single data point.     means : array_like, shape (n_components, n_features)         List of n_features-dimensional mean vectors for n_components Gaussians.         Each row corresponds to a single mean vector.     covars : array_like         List of n_components covariance parameters for each Gaussian. The shape         depends on `covariance_type`:             (n_components, n_features)      if 'spherical',             (n_features, n_features)    if 'tied',             (n_components, n_features)    if 'diag',             (n_components, n_features, n_features) if 'full'     covariance_type : string         Type of the covariance parameters.  Must be one of         'spherical', 'tied', 'diag', 'full'.  Defaults to 'diag'.     Returns     -------     lpr : array_like, shape (n_samples, n_components)         Array containing the log probabilities of each data point in         X under each of the n_components multivariate Gaussian distributions.
Compute Gaussian log-density at X for a diagonal model.
Compute Gaussian log-density at X for a spherical model.
Compute Gaussian log-density at X for a tied model.
Log probability for full covariance matrices.
Reports convergence to :data:`sys.stderr`.          The output consists of three columns: iteration number, log         probability of the data at the current iteration and convergence         rate.  At the first iteration convergence rate is unknown and         is thus denoted by NaN.          Parameters         ----------         logprob : float             The log probability of the data as computed by EM algorithm             in the current iteration.
``True`` if the EM algorithm converged and ``False`` otherwise.
Compute the stationary distribution of states.
Compute the log probability under the model and compute posteriors.          Parameters         ----------         X : array-like, shape (n_samples, n_features)             Feature matrix of individual samples.          lengths : array-like of integers, shape (n_sequences, ), optional             Lengths of the individual sequences in ``X``. The sum of             these should be ``n_samples``.          Returns         -------         logprob : float             Log likelihood of ``X``.          posteriors : array, shape (n_samples, n_components)             State-membership probabilities for each sample in ``X``.          See Also         --------         score : Compute the log probability under the model.         decode : Find most likely state sequence corresponding to ``X``.
Compute the log probability under the model.          Parameters         ----------         X : array-like, shape (n_samples, n_features)             Feature matrix of individual samples.          lengths : array-like of integers, shape (n_sequences, ), optional             Lengths of the individual sequences in ``X``. The sum of             these should be ``n_samples``.          Returns         -------         logprob : float             Log likelihood of ``X``.          See Also         --------         score_samples : Compute the log probability under the model and             posteriors.         decode : Find most likely state sequence corresponding to ``X``.
Find most likely state sequence corresponding to ``X``.          Parameters         ----------         X : array-like, shape (n_samples, n_features)             Feature matrix of individual samples.          lengths : array-like of integers, shape (n_sequences, ), optional             Lengths of the individual sequences in ``X``. The sum of             these should be ``n_samples``.          algorithm : string             Decoder algorithm. Must be one of "viterbi" or "map".             If not given, :attr:`decoder` is used.          Returns         -------         logprob : float             Log probability of the produced state sequence.          state_sequence : array, shape (n_samples, )             Labels for each sample from ``X`` obtained via a given             decoder ``algorithm``.          See Also         --------         score_samples : Compute the log probability under the model and             posteriors.         score : Compute the log probability under the model.
Find most likely state sequence corresponding to ``X``.          Parameters         ----------         X : array-like, shape (n_samples, n_features)             Feature matrix of individual samples.          lengths : array-like of integers, shape (n_sequences, ), optional             Lengths of the individual sequences in ``X``. The sum of             these should be ``n_samples``.          Returns         -------         state_sequence : array, shape (n_samples, )             Labels for each sample from ``X``.
Compute the posterior probability for each state in the model.          X : array-like, shape (n_samples, n_features)             Feature matrix of individual samples.          lengths : array-like of integers, shape (n_sequences, ), optional             Lengths of the individual sequences in ``X``. The sum of             these should be ``n_samples``.          Returns         -------         posteriors : array, shape (n_samples, n_components)             State-membership probabilities for each sample from ``X``.
Generate random samples from the model.          Parameters         ----------         n_samples : int             Number of samples to generate.          random_state : RandomState or an int seed             A random number generator instance. If ``None``, the object's             ``random_state`` is used.          Returns         -------         X : array, shape (n_samples, n_features)             Feature matrix.          state_sequence : array, shape (n_samples, )             State sequence produced by the model.
Estimate model parameters.          An initialization step is performed before entering the         EM algorithm. If you want to avoid this step for a subset of         the parameters, pass proper ``init_params`` keyword argument         to estimator's constructor.          Parameters         ----------         X : array-like, shape (n_samples, n_features)             Feature matrix of individual samples.          lengths : array-like of integers, shape (n_sequences, )             Lengths of the individual sequences in ``X``. The sum of             these should be ``n_samples``.          Returns         -------         self : object             Returns self.
Initializes model parameters prior to fitting.          Parameters         ----------         X : array-like, shape (n_samples, n_features)             Feature matrix of individual samples.          lengths : array-like of integers, shape (n_sequences, )             Lengths of the individual sequences in ``X``. The sum of             these should be ``n_samples``.
Validates model parameters prior to fitting.          Raises         ------          ValueError             If any of the parameters are invalid, e.g. if :attr:`startprob_`             don't sum to 1.
Initializes sufficient statistics required for M-step.          The method is *pure*, meaning that it doesn't change the state of         the instance.  For extensibility computed statistics are stored         in a dictionary.          Returns         -------         nobs : int             Number of samples in the data.          start : array, shape (n_components, )             An array where the i-th element corresponds to the posterior             probability of the first sample being generated by the i-th             state.          trans : array, shape (n_components, n_components)             An array where the (i, j)-th element corresponds to the             posterior probability of transitioning between the i-th to j-th             states.
Updates sufficient statistics from a given sample.          Parameters         ----------         stats : dict             Sufficient statistics as returned by             :meth:`~base._BaseHMM._initialize_sufficient_statistics`.          X : array, shape (n_samples, n_features)             Sample sequence.          framelogprob : array, shape (n_samples, n_components)             Log-probabilities of each sample under each of the model states.          posteriors : array, shape (n_samples, n_components)             Posterior probabilities of each sample being generated by each             of the model states.          fwdlattice, bwdlattice : array, shape (n_samples, n_components)             Log-forward and log-backward probabilities.
Performs the M-step of EM algorithm.          Parameters         ----------         stats : dict             Sufficient statistics updated from all available samples.
Do basic checks on matrix covariance sizes and values.
Create all the covariance matrices from a given template.
Normalizes the input array so that it sums to 1.      Parameters     ----------     a : array         Non-normalized input data.      axis : int         Dimension along which normalization is performed.      Notes     -----     Modifies the input **inplace**.
Normalizes the input array so that the exponent of the sum is 1.      Parameters     ----------     a : array         Non-normalized input data.      axis : int         Dimension along which normalization is performed.      Notes     -----     Modifies the input **inplace**.
Computes the log of input probabilities masking divide by zero in log.      Notes     -----     During the M-step of EM-algorithm, very small intermediate start     or transition probabilities could be normalized to zero, causing a     *RuntimeWarning: divide by zero encountered in log*.      This function masks this unharmful warning.
Return covars as a full matrix.
Check if ``X`` is a sample from a Multinomial distribution.          That is ``X`` should be an array of non-negative integers from         range ``[min(X), max(X)]``, such that each integer from the range         occurs in ``X`` at least once.          For example ``[0, 0, 2, 1, 3, 1, 1]`` is a valid sample from a         Multinomial distribution, while ``[0, 0, 3, 5, 10]`` is not.
:return: new object instance of a type appropriate to represent the given             binary sha1         :param sha1: 20 byte binary sha1
Retrieve object information
Writes our data directly to the given output stream         :param ostream: File object compatible stream object.         :return: self
:return: String describing the mime type of this file (based on the filename)         :note: Defaults to 'text/plain' in case the actual file type is unknown.
Iterate remote references, and if given, constrain them to the given remote
Delete the given remote references          :note:             kwargs are given for comparability with the base class method as we             should not narrow the signature.
Initialize external projects by putting them into the path
Convenience method for setting the git executable path.
Find the git dir that's appropriate for the path
Returns an iterator yielding pairs of sha1/path pairs (as bytes) for the corresponding refs.         :note: The packed refs file will be kept open as long as we iterate
:return: hexsha stored in the reference at the given ref_path, recursively dereferencing all             intermediate references as required         :param repo: the repository containing the reference at ref_path
Return: (str(sha), str(target_ref_path)) if available, the sha the file at         rela_path points to, or None. target_ref_path is the reference we         point to, or None
:return:             The object our ref currently refers to. Refs can be cached, they will             always point to the actual object as it gets re-created on each query
:return:             Commit object we point to, works for detached and non-detached             SymbolicReferences. The symbolic reference will be dereferenced recursively.
As set_object, but restricts the type of object to be a Commit          :raise ValueError: If commit is not a Commit object or doesn't point to             a commit         :return: self
Set the object we point to, possibly dereference our symbolic reference first.         If the reference does not exist, it will be created          :param object: a refspec, a SymbolicReference or an Object instance. SymbolicReferences             will be dereferenced beforehand to obtain the object they point to         :param logmsg: If not None, the message will be used in the reflog entry to be             written. Otherwise the reflog is not altered         :note: plain SymbolicReferences may not actually point to objects by convention         :return: self
:return: Reference Object we point to         :raise TypeError: If this symbolic reference is detached, hence it doesn't point             to a reference, but to a commit
Set ourselves to the given ref. It will stay a symbol if the ref is a Reference.         Otherwise an Object, given as Object instance or refspec, is assumed and if valid,         will be set which effectively detaches the refererence if it was a purely         symbolic one.          :param ref: SymbolicReference instance, Object instance or refspec string             Only if the ref is a SymbolicRef instance, we will point to it. Everything             else is dereferenced to obtain the actual object.         :param logmsg: If set to a string, the message will be used in the reflog.             Otherwise, a reflog entry is not written for the changed reference.             The previous commit of the entry will be the commit we point to now.              See also: log_append()          :return: self         :note: This symbolic reference will not be dereferenced. For that, see             ``set_object(...)``
Append a logentry to the logfile of this ref          :param oldbinsha: binary sha this ref used to point to         :param message: A message describing the change         :param newbinsha: The sha the ref points to now. If None, our current commit sha             will be used         :return: added RefLogEntry instance
:return: string with a full repository-relative path which can be used to initialize             a Reference instance, for instance by using ``Reference.from_path``
Delete the reference at the given path          :param repo:             Repository to delete the reference from          :param path:             Short or full path pointing to the reference, i.e. refs/myreference             or just "myreference", hence 'refs/' is implied.             Alternatively the symbolic reference to be deleted
internal method used to create a new symbolic reference.         If resolve is False, the reference will be taken as is, creating         a proper symbolic reference. Otherwise it will be resolved to the         corresponding object and a detached symbolic reference will be created         instead
Create a new symbolic reference, hence a reference pointing to another reference.          :param repo:             Repository to create the reference in          :param path:             full path at which the new symbolic reference is supposed to be             created at, i.e. "NEW_HEAD" or "symrefs/my_new_symref"          :param reference:             The reference to which the new symbolic reference should point to.             If it is a commit'ish, the symbolic ref will be detached.          :param force:             if True, force creation even if a symbolic reference with that name already exists.             Raise OSError otherwise          :param logmsg:             If not None, the message to append to the reflog. Otherwise no reflog             entry is written.          :return: Newly created symbolic Reference          :raise OSError:             If a (Symbolic)Reference with the same name but different contents             already exists.          :note: This does not alter the current HEAD, index or Working Tree
Rename self to a new path          :param new_path:             Either a simple name or a full path, i.e. new_name or features/new_name.             The prefix refs/ is implied for references and will be set as needed.             In case this is a symbolic ref, there is no implied prefix          :param force:             If True, the rename will succeed even if a head with the target name             already exists. It will be overwritten in that case          :return: self         :raise OSError: In case a file at path but a different contents already exists
Find all refs in the repository          :param repo: is the Repo          :param common_path:             Optional keyword argument to the path which is to be shared by all             returned Ref objects.             Defaults to class specific portion if None assuring that only             refs suitable for the actual class are returned.          :return:             git.SymbolicReference[], each of them is guaranteed to be a symbolic             ref which is not detached and pointing to a valid ref              List is lexicographically sorted             The returned objects represent actual subclasses, such as Head or TagReference
:param path: full .git-directory-relative path name to the Reference to instantiate         :note: use to_full_path() if you only have a partial path of a known Reference Type         :return:             Instance of type Reference, Head, or Tag             depending on the given path
Reset our HEAD to the given commit optionally synchronizing         the index and working tree. The reference we refer to will be set to         commit as well.          :param commit:             Commit object, Reference Object or string identifying a revision we             should reset HEAD to.          :param index:             If True, the index will be set to match the given commit. Otherwise             it will not be touched.          :param working_tree:             If True, the working tree will be forcefully adjusted to match the given             commit, possibly overwriting uncommitted changes without warning.             If working_tree is True, index must be true as well          :param paths:             Single path or list of paths relative to the git root directory             that are to be reset. This allows to partially reset individual files.          :param kwargs:             Additional arguments passed to git-reset.          :return: self
Delete the given heads          :param force:             If True, the heads will be deleted even if they are not yet merged into             the main development stream.             Default False
Configure this branch to track the given remote reference. This will alter             this branch's configuration accordingly.          :param remote_reference: The remote reference to track or None to untrack             any references         :return: self
:return: The remote_reference we are tracking, or None if we are             not a tracking branch
Rename self to a new path          :param new_path:             Either a simple name or a path, i.e. new_name or features/new_name.             The prefix refs/heads is implied          :param force:             If True, the rename will succeed even if a head with the target name             already exists.          :return: self         :note: respects the ref log as git commands are used
Checkout this head by setting the HEAD to this reference, by updating the index         to reflect the tree we point to and by updating the working tree to reflect         the latest index.          The command will fail if changed working tree files would be overwritten.          :param force:             If True, changes to the index and the working tree will be discarded.             If False, GitCommandError will be raised in that situation.          :param kwargs:             Additional keyword arguments to be passed to git checkout, i.e.             b='new_branch' to create a new branch at the given spot.          :return:             The active branch after the checkout operation, usually self unless             a new branch has been created.             If there is no active branch, as the HEAD is now detached, the HEAD             reference will be returned instead.          :note:             By default it is only allowed to checkout heads - everything else             will leave the HEAD detached which is allowed and possible, but remains             a special state that some tools might not be able to handle.
Initialize this instance with index values read from the given stream
:return: list of entries, in a sorted fashion, first by path, then by stage
Write the current state to our file path or to the given one          :param file_path:             If None, we will write to our stored file path from which we have             been initialized. Otherwise we write to the given file path.             Please note that this will change the file_path of this index to             the one you gave.          :param ignore_extension_data:             If True, the TREE type extension data read in the index will not             be written to disk. NOTE that no extension data is actually written.             Use this if you have altered the index and             would like to use git-write-tree afterwards to create a tree             representing your written changes.             If this data is present in the written index, git-write-tree             will instead write the stored/cached tree.             Alternatively, use IndexFile.write_tree() to handle this case             automatically          :return: self
Merge the given rhs treeish into the current index, possibly taking         a common base treeish into account.          As opposed to the from_tree_ method, this allows you to use an already         existing tree as the left side of the merge          :param rhs:             treeish reference pointing to the 'other' side of the merge.          :param base:             optional treeish reference pointing to the common base of 'rhs' and             this index which equals lhs          :return:             self ( containing the merge and possibly unmerged entries in case of             conflicts )          :raise GitCommandError:             If there is a merge conflict. The error will             be raised at the first conflicting path. If you want to have proper             merge resolution to be done by yourself, you have to commit the changed             index ( or make a valid tree from it ) and retry with a three-way             index.from_tree call.
Merge the given treeish revisions into a new index which is returned.         This method behaves like git-read-tree --aggressive when doing the merge.          :param repo: The repository treeish are located in.          :param tree_sha:             20 byte or 40 byte tree sha or tree objects          :return:             New IndexFile instance. Its path will be undefined.             If you intend to write such a merged Index, supply an alternate file_path             to its 'write' method.
Merge the given treeish revisions into a new index which is returned.         The original index will remain unaltered          :param repo:             The repository treeish are located in.          :param treeish:             One, two or three Tree Objects, Commits or 40 byte hexshas. The result             changes according to the amount of trees.             If 1 Tree is given, it will just be read into a new index             If 2 Trees are given, they will be merged into a new index using a             two way merge algorithm. Tree 1 is the 'current' tree, tree 2 is the 'other'             one. It behaves like a fast-forward.             If 3 Trees are given, a 3-way merge will be performed with the first tree             being the common ancestor of tree 2 and tree 3. Tree 2 is the 'current' tree,             tree 3 is the 'other' one          :param kwargs:             Additional arguments passed to git-read-tree          :return:             New IndexFile instance. It will point to a temporary index location which             does not exist anymore. If you intend to write such a merged Index, supply             an alternate file_path to its 'write' method.          :note:             In the three-way merge case, --aggressive will be specified to automatically             resolve more cases in a commonly correct manner. Specify trivial=True as kwarg             to override that.              As the underlying git-read-tree command takes into account the current index,             it will be temporarily moved out of the way to assure there are no unsuspected             interferences.
Expand the directories in list of paths to the corresponding paths accordingly,          Note: git will add items multiple times even if a glob overlapped         with manually specified paths or if paths where specified multiple         times - we respect that and do not prune
Write path to proc.stdin and make sure it processes the item, including progress.          :return: stdout string         :param read_from_stdout: if True, proc.stdout will be read after the item             was sent to stdin. In that case, it will return None         :note: There is a bug in git-update-index that prevents it from sending             reports just in time. This is why we have a version that tries to             read stdout and one which doesn't. In fact, the stdout is not             important as the piped-in files are processed anyway and just in time         :note: Newlines are essential here, gits behaviour is somewhat inconsistent             on this depending on the version, hence we try our best to deal with             newlines carefully. Usually the last newline will not be sent, instead             we will close stdin to break the pipe.
:return: Iterator yielding tuples of Blob objects and stages, tuple(stage, Blob)          :param predicate:             Function(t) returning True if tuple(stage, Blob) should be yielded by the             iterator. A default filter, the BlobFilter, allows you to yield blobs             only if they match a given list of paths.
:return:             Iterator yielding dict(path : list( tuple( stage, Blob, ...))), being             a dictionary associating a path in the index with a list containing             sorted stage/blob pairs          :note:             Blobs that have been removed in one side simply do not exist in the             given stage. I.e. a file removed on the 'other' branch whose entries             are at stage 3 will not have a stage 3 entry.
Resolve the blobs given in blob iterator. This will effectively remove the         index entries of the respective path at all non-null stages and add the given         blob as new stage null blob.          For each path there may only be one blob, otherwise a ValueError will be raised         claiming the path is already at stage 0.          :raise ValueError: if one of the blobs already existed at stage 0         :return: self          :note:             You will have to write the index manually once you are done, i.e.             index.resolve_blobs(blobs).write()
Writes this index to a corresponding Tree object into the repository's         object database and return it.          :return: Tree object representing this index         :note: The tree will be written even if one or more objects the tree refers to             does not yet exist in the object database. This could happen if you added             Entries to the index directly.         :raise ValueError: if there are no entries in the cache         :raise UnmergedEntriesError:
:return: Version of path relative to our git directory or raise ValueError         if it is not within our git direcotory
Split the items into two lists of path strings and BaseEntries.
Store file at filepath in the database and return the base index entry         Needs the git_working_dir decorator active ! This must be assured in the calling code
Add files from the working tree, specific blobs or BaseIndexEntries         to the index.          :param items:             Multiple types of items are supported, types can be mixed within one call.             Different types imply a different handling. File paths may generally be             relative or absolute.              - path string                 strings denote a relative or absolute path into the repository pointing to                 an existing file, i.e. CHANGES, lib/myfile.ext, '/home/gitrepo/lib/myfile.ext'.                  Absolute paths must start with working tree directory of this index's repository                 to be considered valid. For example, if it was initialized with a non-normalized path, like                 `/root/repo/../repo`, absolute paths to be added must start with `/root/repo/../repo`.                  Paths provided like this must exist. When added, they will be written                 into the object database.                  PathStrings may contain globs, such as 'lib/__init__*' or can be directories                 like 'lib', the latter ones will add all the files within the dirctory and                 subdirectories.                  This equals a straight git-add.                  They are added at stage 0              - Blob or Submodule object                 Blobs are added as they are assuming a valid mode is set.                 The file they refer to may or may not exist in the file system, but                 must be a path relative to our repository.                  If their sha is null ( 40*0 ), their path must exist in the file system                 relative to the git repository as an object will be created from                 the data at the path.                 The handling now very much equals the way string paths are processed, except that                 the mode you have set will be kept. This allows you to create symlinks                 by settings the mode respectively and writing the target of the symlink                 directly into the file. This equals a default Linux-Symlink which                 is not dereferenced automatically, except that it can be created on                 filesystems not supporting it as well.                  Please note that globs or directories are not allowed in Blob objects.                  They are added at stage 0              - BaseIndexEntry or type                 Handling equals the one of Blob objects, but the stage may be                 explicitly set. Please note that Index Entries require binary sha's.          :param force:             **CURRENTLY INEFFECTIVE**             If True, otherwise ignored or excluded files will be             added anyway.             As opposed to the git-add command, we enable this flag by default             as the API user usually wants the item to be added even though             they might be excluded.          :param fprogress:             Function with signature f(path, done=False, item=item) called for each             path to be added, one time once it is about to be added where done==False             and once after it was added where done=True.             item is set to the actual item we handle, either a Path or a BaseIndexEntry             Please note that the processed path is not guaranteed to be present             in the index already as the index is currently being processed.          :param path_rewriter:             Function with signature (string) func(BaseIndexEntry) function returning a path             for each passed entry which is the path to be actually recorded for the             object created from entry.path. This allows you to write an index which             is not identical to the layout of the actual files on your hard-disk.             If not None and ``items`` contain plain paths, these paths will be             converted to Entries beforehand and passed to the path_rewriter.             Please note that entry.path is relative to the git repository.          :param write:             If True, the index will be written once it was altered. Otherwise             the changes only exist in memory and are not available to git commands.          :param write_extension_data:             If True, extension data will be written back to the index. This can lead to issues in case             it is containing the 'TREE' extension, which will cause the `git commit` command to write an             old tree, instead of a new one representing the now changed index.             This doesn't matter if you use `IndexFile.commit()`, which ignores the `TREE` extension altogether.             You should set it to True if you intend to use `IndexFile.commit()` exclusively while maintaining             support for third-party extensions. Besides that, you can usually safely ignore the built-in             extensions when using GitPython on repositories that are not handled manually at all.             All current built-in extensions are listed here:             http://opensource.apple.com/source/Git/Git-26/src/git-htmldocs/technical/index-format.txt          :return:             List(BaseIndexEntries) representing the entries just actually added.          :raise OSError:             if a supplied Path did not exist. Please note that BaseIndexEntry             Objects that do not have a null sha will be added even if their paths             do not exist.
Returns a list of repo-relative paths from the given items which         may be absolute or relative paths, entries or blobs
Remove the given items from the index and optionally from         the working tree as well.          :param items:             Multiple types of items are supported which may be be freely mixed.              - path string                 Remove the given path at all stages. If it is a directory, you must                 specify the r=True keyword argument to remove all file entries                 below it. If absolute paths are given, they will be converted                 to a path relative to the git repository directory containing                 the working tree                  The path string may include globs, such as *.c.              - Blob Object                 Only the path portion is used in this case.              - BaseIndexEntry or compatible type                 The only relevant information here Yis the path. The stage is ignored.          :param working_tree:             If True, the entry will also be removed from the working tree, physically             removing the respective file. This may fail if there are uncommitted changes             in it.          :param kwargs:             Additional keyword arguments to be passed to git-rm, such             as 'r' to allow recursive removal of          :return:             List(path_string, ...) list of repository relative paths that have             been removed effectively.             This is interesting to know in case you have provided a directory or             globs. Paths are relative to the repository.
Rename/move the items, whereas the last item is considered the destination of         the move operation. If the destination is a file, the first item ( of two )         must be a file as well. If the destination is a directory, it may be preceded         by one or more directories or files.          The working tree will be affected in non-bare repositories.          :parma items:             Multiple types of items are supported, please see the 'remove' method             for reference.         :param skip_errors:             If True, errors such as ones resulting from missing source files will             be skipped.         :param kwargs:             Additional arguments you would like to pass to git-mv, such as dry_run             or force.          :return:List(tuple(source_path_string, destination_path_string), ...)             A list of pairs, containing the source file moved as well as its             actual destination. Relative to the repository root.          :raise ValueError: If only one item was given             GitCommandError: If git could not handle your request
Commit the current default index file, creating a commit object.         For more information on the arguments, see tree.commit.          :note: If you have manually altered the .entries member of this instance,                don't forget to write() your changes to disk beforehand.                Passing skip_hooks=True is the equivalent of using `-n`                or `--no-verify` on the command line.         :return: Commit object representing the new commit
Checkout the given paths or all files from the version known to the index into         the working tree.          :note: Be sure you have written pending changes using the ``write`` method             in case you have altered the enties dictionary directly          :param paths:             If None, all paths in the index will be checked out. Otherwise an iterable             of relative or absolute paths or a single path pointing to files or directories             in the index is expected.          :param force:             If True, existing files will be overwritten even if they contain local modifications.             If False, these will trigger a CheckoutError.          :param fprogress:             see Index.add_ for signature and explanation.             The provided progress information will contain None as path and item if no             explicit paths are given. Otherwise progress information will be send             prior and after a file has been checked out          :param kwargs:             Additional arguments to be passed to git-checkout-index          :return:             iterable yielding paths to files which have been checked out and are             guaranteed to match the version stored in the index          :raise CheckoutError:             If at least one file failed to be checked out. This is a summary,             hence it will checkout as many files as it can anyway.             If one of files or directories do not exist in the index             ( as opposed to the  original git command who ignores them ).             Raise GitCommandError if error lines could not be parsed - this truly is             an exceptional state          .. note:: The checkout is limited to checking out the files in the             index. Files which are not in the index anymore and exist in             the working tree will not be deleted. This behaviour is fundamentally             different to *head.checkout*, i.e. if you want git-checkout like behaviour,             use head.checkout instead of index.checkout.
Reset the index to reflect the tree at the given commit. This will not         adjust our HEAD reference as opposed to HEAD.reset by default.          :param commit:             Revision, Reference or Commit specifying the commit we should represent.             If you want to specify a tree only, use IndexFile.from_tree and overwrite             the default index.          :param working_tree:             If True, the files in the working tree will reflect the changed index.             If False, the working tree will not be touched             Please note that changes to the working copy will be discarded without             warning !          :param head:             If True, the head will be set to the given commit. This is False by default,             but if True, this method behaves like HEAD.reset.          :param paths: if given as an iterable of absolute or repository-relative paths,             only these will be reset to their state at the given commit'ish.             The paths need to exist at the commit, otherwise an exception will be             raised.          :param kwargs:             Additional keyword arguments passed to git-reset          .. note:: IndexFile.reset, as opposed to HEAD.reset, will not delete anyfiles             in order to maintain a consistent working tree. Instead, it will just             checkout the files according to their state in the index.             If you want git-reset like behaviour, use *HEAD.reset* instead.          :return: self
Diff this index against the working copy or a Tree or Commit object          For a documentation of the parameters and return values, see         Diffable.diff          :note:             Will only work with indices that represent the default git index as             they have not been initialized with a stream.
Update the submodules of this repository to the current HEAD commit.         This method behaves smartly by determining changes of the path of a submodules         repository, next to changes to the to-be-checked-out commit or the branch to be         checked out. This works if the submodules ID does not change.         Additionally it will detect addition and removal of submodules, which will be handled         gracefully.          :param previous_commit: If set to a commit'ish, the commit we should use             as the previous commit the HEAD pointed to before it was set to the commit it points to now.             If None, it defaults to HEAD@{1} otherwise         :param recursive: if True, the children of submodules will be updated as well             using the same technique         :param force_remove: If submodules have been deleted, they will be forcibly removed.             Otherwise the update may fail if a submodule's repository cannot be deleted as             changes have been made to it (see Submodule.update() for more information)         :param init: If we encounter a new module which would need to be initialized, then do it.         :param to_latest_revision: If True, instead of checking out the revision pointed to             by this submodule's sha, the checked out tracking branch will be merged with the             latest remote branch fetched from the repository's origin.             Unless force_reset is specified, a local tracking branch will never be reset into its past, therefore             the remote branch must be in the future for this to have an effect.         :param force_reset: if True, submodules may checkout or reset their branch even if the repository has             pending changes that would be overwritten, or if the local tracking branch is in the future of the             remote tracking branch and would be reset into its past.         :param progress: RootUpdateProgress instance or None if no progress should be sent         :param dry_run: if True, operations will not actually be performed. Progress messages             will change accordingly to indicate the WOULD DO state of the operation.         :param keep_going: if True, we will ignore but log all errors, and keep going recursively.             Unless dry_run is set as well, keep_going could cause subsequent/inherited errors you wouldn't see             otherwise.             In conjunction with dry_run, it can be useful to anticipate all errors when updating submodules         :return: self
:return: Commit object the tag ref points to                  :raise ValueError: if the tag points to a tree or blob
Create a new tag reference.          :param path:             The name of the tag, i.e. 1.0 or releases/1.0.             The prefix refs/tags is implied          :param ref:             A reference to the object you want to tag. It can be a commit, tree or             blob.          :param message:             If not None, the message will be used in your tag object. This will also             create an additional tag object that allows to obtain that information, i.e.::                  tagref.tag.message          :param force:             If True, to force creation of a tag even though that tag already exists.          :param kwargs:             Additional keyword arguments to be passed to git-tag          :return: A new TagReference
Write the give list of entries into a stream using its write method     :param entries: **sorted** list of tuples with (binsha, mode, name)     :param write: write method which takes a data string
Reads the binary representation of a tree and returns tuples of Tree items     :param data: data block with tree data (as bytes)     :return: list(tuple(binsha, mode, tree_relative_path), ...)
return data entry matching the given name and tree mode     or None.     Before the item is returned, the respective data item is set     None in the tree_data list to mark it done
:return: list with entries according to the given binary tree-shas.         The result is encoded in a list         of n tuple|None per blob/commit, (n == len(tree_shas)), where         * [0] == 20 byte sha         * [1] == mode as int         * [2] == path relative to working tree root         The entry tuple is None if the respective blob/commit did not         exist in the given tree.     :param tree_shas: iterable of shas pointing to trees. All trees must         be on the same level. A tree-sha may be None in which case None     :param path_prefix: a prefix to be added to the returned paths on this level,         set it '' for the first iteration     :note: The ordering of the returned items will be partially lost
:return: list of entries of the tree pointed to by the binary tree_sha. An entry         has the following format:         * [0] 20 byte sha         * [1] mode as int         * [2] path relative to the repository     :param path_prefix: prefix to prepend to the front of all returned paths
Returns method assuring we read values (on demand) before we try to access them
Return method that checks whether given non constant function may be called.     If so, the instance will be set dirty.     Additionally, we flush the changes right to disk
Call the configuration at the given method which must take a section name         as first argument
Flush changes and release the configuration write lock. This instance must not be used anymore afterwards.         In Python 3, it's required to explicitly release locks and flush changes, as __del__ is not called         deterministically anymore.
A direct copy of the py2.4 version of the super class's _read method         to assure it uses ordered dicts. Had to change one line to make it work.          Future versions have this fixed, but in fact its quite embarrassing for the         guys not to have done it right in the first place !          Removed big comments to make it more compact.          Made sure it ignores initial whitespace as git uses tabs
Reads the data stored in the files we have been initialized with. It will         ignore files that cannot be read, possibly leaving an empty configuration          :return: Nothing         :raise IOError: if a file cannot be handled
Write an .ini-format representation of the configuration state in         git compatible format
:return: list((option, value), ...) pairs of all items in the given section
Write changes to our file, if there are changes at all          :raise IOError: if this is a read-only writer instance or if we could not obtain             a file lock
:param default:             If not None, the given default value will be returned in case             the option did not exist         :return: a properly typed value, either int, float or string          :raise TypeError: in case the value could not be understood             Otherwise the exceptions known to the ConfigParser will be raised.
Sets the given option in section to the given value.         It will create the section if required, and will not throw as opposed to the default         ConfigParser 'set' method.          :param section: Name of the section in which the option resides or should reside         :param option: Name of the options whose value to set          :param value: Value to set the option to. It must be a string or convertible             to a string         :return: this instance
rename the given section to new_name         :raise ValueError: if section doesn't exit         :raise ValueError: if a section with new_name does already exist         :return: this instance
Run the commit hook of the given name. Silently ignores hooks that do not exist.     :param name: name of hook, like 'pre-commit'     :param index: IndexFile instance     :param args: arguments passed to hook file     :raises HookExecutionError:
Convert the given mode from a stat call to the corresponding index mode     and return it
Write the cache represented by entries to a stream      :param entries: **sorted** list of entries     :param stream: stream to wrap into the AdapterStreamCls - it is used for         final output.      :param ShaStreamCls: Type to use when writing to the stream. It produces a sha         while writing to it, before the data is passed on to the wrapped stream      :param extension_data: any kind of data to write as a trailer, it must begin         a 4 byte identifier, followed by its size ( 4 bytes )
Return tuple(version_long, num_entries) from the given stream
:return: Key suitable to be used for the index.entries dictionary     :param entry: One instance of type BaseIndexEntry or the path and the stage
Read a cache file from the given stream     :return: tuple(version, entries_dict, extension_data, content_sha)     * version is the integer version number     * entries dict is a dictionary which maps IndexEntry instances to a path at a stage     * extension_data is '' or 4 bytes of type + 4 bytes of size + size bytes     * content_sha is a 20 byte sha on all cache file contents
Create a tree from the given sorted list of entries and put the respective     trees into the given object database      :param entries: **sorted** list of IndexEntries     :param odb: object database to store the trees in     :param si: start index at which we should start creating subtrees     :param sl: slice indicating the range we should process on the entries list     :return: tuple(binsha, list(tree_entry, ...)) a tuple of a sha and a list of         tree entries being a tuple of hexsha, mode, name
:return: list of BaseIndexEntries representing the aggressive merge of the given         trees. All valid entries are on stage 0, whereas the conflicting ones are left         on stage 1, 2 or 3, whereas stage 1 corresponds to the common ancestor tree,         2 to our tree and 3 to 'their' tree.     :param tree_shas: 1, 2 or 3 trees as identified by their binary 20 byte shas         If 1 or two, the entries will effectively correspond to the last given tree         If 3 are given, a 3 way merge is performed
:return: Fully equipped BaseIndexEntry at the given stage
:return: Blob using the information of this index entry
:return:             Minimal entry as created from the given BaseIndexEntry instance.             Missing values will be set to null-like values          :param base: Instance of type BaseIndexEntry
:return: Minimal entry resembling the given blob object
:return: index of an item with name, or -1 if not found
Add the given item to the tree. If an item with the given name already         exists, nothing will be done, but a ValueError will be raised if the         sha and mode of the existing item do not match the one you add, unless         force is True          :param sha: The 20 or 40 byte sha of the item to add         :param mode: int representing the stat compatible mode of the item         :param force: If True, an item with your name and information will overwrite             any existing item with the same name, no matter which information it has         :return: self
Add the given item to the tree, its correctness is assumed, which         puts the caller into responsibility to assure the input is correct.         For more information on the parameters, see ``add``         :param binsha: 20 byte binary sha
Iterable yields tuples of (binsha, mode, name), which will be converted         to the respective object representation
Find the named object in this tree's contents         :return: ``git.Blob`` or ``git.Tree`` or ``git.Submodule``          :raise KeyError: if given file or tree does not exist in tree
For documentation, see util.Traversable.traverse         Trees are set to visit_once = False to gain more performance in the traversal
For now, all lookup is done by git itself
:return: Full binary 20 byte sha from the given partial hexsha         :raise AmbiguousObjectName:         :raise BadObject:         :note: currently we only raise BadObject as git does not communicate             AmbiguousObjects separately
:param modestr: string like 755 or 644 or 100644 - only the last 6 chars will be used     :return:         String identifying a mode compatible to the mode methods ids of the         stat module regarding the rwx permissions for user, group and other,         special flags and file system flags, i.e. whether it is a symlink         for example.
:return: type suitable to handle the given object type name.         Use the type to create new instances.      :param object_type_name: Member of TYPES      :raise ValueError: In case object_type_name is unknown
As above, but inverses the operation, returning a string that can be used     in commit objects
:raise ValueError: if offset is incorrect     :return: offset
Converts a timestamp + tz_offset into an aware datetime instance.
Parse the given date as one of the following          * Git internal format: timestamp offset         * RFC 2822: Thu, 07 Apr 2005 22:13:13 +0200.         * ISO 8601 2005-04-07T22:13:13             The T can be a space as well      :return: Tuple(int(timestamp_UTC), int(offset)), both in seconds since epoch     :raise ValueError: If the format could not be understood     :note: Date can also be YYYY.MM.DD, MM/DD/YYYY and DD.MM.YYYY.
Parse out the actor (author or committer) info from a line like::          author Tom Preston-Werner <tom@mojombo.com> 1191999972 -0700      :return: [Actor, int_seconds_since_epoch, int_timezone_offset]
:return: IterableList with the results of the traversal as produced by             traverse()
:return: iterator yielding of items found when traversing self          :param predicate: f(i,d) returns False if item i at depth d should not be included in the result          :param prune:             f(i,d) return True if the search should stop at item i at depth d.             Item i will not be returned.          :param depth:             define at which level the iteration should not go deeper             if -1, there is no limit             if 0, you would effectively only get self, the root of the iteration             i.e. if 1, you would only get the first level of predecessors/successors          :param branch_first:             if True, items will be returned branch first, otherwise depth first          :param visit_once:             if True, items will only be returned once, although they might be encountered             several times. Loops are prevented that way.          :param ignore_self:             if True, self will be ignored and automatically pruned from             the result. Otherwise it will be the first item to be returned.             If as_edge is True, the source of the first edge is None          :param as_edge:             if True, return a pair of items, first being the source, second the             destination, i.e. tuple(src, dest) with the edge spanning from             source to destination
Registers for notifications to lean that process output is ready to read, and dispatches lines to     the respective line handlers.     This function returns once the finalizer returns      :return: result of finalizer     :param process: subprocess.Popen instance     :param stdout_handler: f(stdout_line_string), or None     :param stderr_handler: f(stderr_line_string), or None     :param finalizer: f(proc) - wait for proc to finish     :param decode_streams:         Assume stdout/stderr streams are binary and decode them before pushing \         their contents to handlers.         Set it to False if `universal_newline == True` (then streams are in text-mode)         or if decoding must happen later (i.e. for Diffs).
This gets called by the refresh function (see the top level         __init__).
Specify command line options to the git executable         for subsequent subcommand calls          :param kwargs:             is a dict of keyword arguments.             these arguments are passed as in _call_process             but will be passed to the git command rather than             the subcommand.
Handles executing the command on the shell and consumes and returns         the returned information (stdout)          :param command:             The command argument list to execute.             It should be a string, or a sequence of program arguments. The             program to execute is the first item in the args sequence or string.          :param istream:             Standard input filehandle passed to subprocess.Popen.          :param with_extended_output:             Whether to return a (status, stdout, stderr) tuple.          :param with_exceptions:             Whether to raise an exception when git returns a non-zero status.          :param as_process:             Whether to return the created process instance directly from which             streams can be read on demand. This will render with_extended_output and             with_exceptions ineffective - the caller will have             to deal with the details himself.             It is important to note that the process will be placed into an AutoInterrupt             wrapper that will interrupt the process once it goes out of scope. If you             use the command in iterators, you should pass the whole process instance             instead of a single stream.          :param output_stream:             If set to a file-like object, data produced by the git command will be             output to the given stream directly.             This feature only has any effect if as_process is False. Processes will             always be created with a pipe due to issues with subprocess.             This merely is a workaround as data will be copied from the             output pipe to the given output stream directly.             Judging from the implementation, you shouldn't use this flag !          :param stdout_as_string:             if False, the commands standard output will be bytes. Otherwise, it will be             decoded into a string using the default encoding (usually utf-8).             The latter can fail, if the output contains binary data.          :param env:             A dictionary of environment variables to be passed to `subprocess.Popen`.                      :param max_chunk_size:             Maximum number of bytes in one chunk of data passed to the output_stream in             one invocation of write() method. If the given number is not positive then             the default value is used.          :param subprocess_kwargs:             Keyword arguments to be passed to subprocess.Popen. Please note that             some of the valid kwargs are already set by this method, the ones you             specify may not be the same ones.          :param with_stdout: If True, default True, we open stdout on the created process         :param universal_newlines:             if True, pipes will be opened as text, and lines are split at             all known line endings.         :param shell:             Whether to invoke commands through a shell (see `Popen(..., shell=True)`).             It overrides :attr:`USE_SHELL` if it is not `None`.         :param kill_after_timeout:             To specify a timeout in seconds for the git command, after which the process             should be killed. This will have no effect if as_process is set to True. It is             set to None by default and will let the process run until the timeout is             explicitly specified. This feature is not supported on Windows. It's also worth             noting that kill_after_timeout uses SIGKILL, which can have negative side             effects on a repository. For example, stale locks in case of git gc could             render the repository incapable of accepting changes until the lock is manually             removed.          :return:             * str(output) if extended_output = False (Default)             * tuple(int(status), str(stdout), str(stderr)) if extended_output = True              if output_stream is True, the stdout value will be your output stream:             * output_stream if extended_output = False             * tuple(int(status), output_stream, str(stderr)) if extended_output = True              Note git is executed with LC_MESSAGES="C" to ensure consistent             output regardless of system language.          :raise GitCommandError:          :note:            If you add additional keyword arguments to the signature of this method,            you must update the execute_kwargs tuple housed in this module.
Set environment variables for future git invocations. Return all changed         values in a format that can be passed back into this function to revert         the changes:          ``Examples``::              old_env = self.update_environment(PWD='/tmp')             self.update_environment(**old_env)          :param kwargs: environment variables to use for git processes         :return: dict that maps environment variables to their old values
A context manager around the above ``update_environment`` method to restore the         environment back to its previous state after operation.          ``Examples``::              with self.custom_environment(GIT_SSH='/bin/ssh_wrapper'):                 repo.remotes.origin.fetch()          :param kwargs: see update_environment
Transforms Python style kwargs into git command line options.
Run the given git command with the specified arguments and return         the result as a String          :param method:             is the command. Contained "_" characters will be converted to dashes,             such as in 'ls_files' to call 'ls-files'.          :param args:             is the list of arguments. If None is included, it will be pruned.             This allows your commands to call git more conveniently as None             is realized as non-existent          :param kwargs:             It contains key-values for the following:             - the :meth:`execute()` kwds, as listed in :var:`execute_kwargs`;             - "command options" to be converted by :meth:`transform_kwargs()`;             - the `'insert_kwargs_after'` key which its value must match one of ``*args``,               and any cmd-options will be appended after the matched arg.          Examples::              git.rev_list('master', max_count=10, header=True)          turns into::             git rev-list max-count 10 --header master          :return: Same as ``execute``
:param header_line:             <hex_sha> type_string size_as_int          :return: (hex_sha, type_string, size_as_int)          :raise ValueError: if the header contains indication for an error due to             incorrect input sha
Use this method to quickly examine the type and size of the object behind         the given ref.          :note: The method will only suffer from the costs of command invocation             once and reuses the command in subsequent calls.          :return: (hexsha, type_string, size_as_int)
As get_object_header, but returns object data as well         :return: (hexsha, type_string, size_as_int,data_string)         :note: not threadsafe
Clear all kinds of internal caches to release resources.          Currently persistent commands will be interrupted.          :return: self
:return: Remote with the specified name         :raise ValueError:  if no remote with such a name exists
Create a new head within the repository.         For more documentation, please see the Head.create method.          :return: newly created Head Reference
Create a new tag reference.         For more documentation, please see the TagReference.create method.          :return: TagReference object
Create a new remote.          For more information, please see the documentation of the Remote.create         methods          :return: Remote reference
:return:             GitConfigParser allowing to read the full git configuration, but not to write it              The configuration will include values from the system, user and repository             configuration files.          :param config_level:             For possible values, see config_writer method             If None, all applicable levels will be used. Specify a level in case             you know which file you wish to read to prevent reading multiple files.         :note: On windows, system configuration cannot currently be read as the path is             unknown, instead the global path will be used.
The Commit object for the specified revision         :param rev: revision specifier, see git-rev-parse for viable options.         :return: ``git.Commit``
:return: Iterator yielding Tree objects         :note: Takes all arguments known to iter_commits method
A list of Commit objects representing the history of a given ref/commit          :param rev:             revision specifier, see git-rev-parse for viable options.             If None, the active branch will be used.          :param paths:             is an optional path or a list of paths to limit the returned commits to             Commits that do not contain that path or the paths will not be returned.          :param kwargs:             Arguments to be passed to git-rev-list - common ones are             max_count and skip          :note: to receive only commits between two named revisions, use the             "revA...revB" revision specifier          :return: ``git.Commit[]``
Find the closest common ancestor for the given revision (e.g. Commits, Tags, References, etc)          :param rev: At least two revs to find the common ancestor for.         :param kwargs: Additional arguments to be passed to the repo.git.merge_base() command which does all the work.         :return: A list of Commit objects. If --all was not specified as kwarg, the list will have at max one Commit,             or is empty if no common merge base exists.         :raises ValueError: If not at least two revs are provided
Check if a commit  is an ancestor of another          :param ancestor_rev: Rev which should be an ancestor         :param rev: Rev to test against ancestor_rev         :return: ``True``, ancestor_rev is an accestor to rev.
The list of alternates for this repo from which objects can be retrieved          :return: list of strings being pathnames of alternates
Sets the alternates          :param alts:             is the array of string paths representing the alternates at which             git should look for objects, i.e. /home/user/repo/.git/objects          :raise NoSuchPathError:         :note:             The method does not check for the existence of the paths in alts             as the caller is responsible.
:return:             ``True``, the repository is considered dirty. By default it will react             like a git-status without untracked files, hence it is dirty if the             index or the working copy have changes.
Iterator for blame information for the given file at the given revision.          Unlike .blame(), this does not return the actual file's contents, only         a stream of BlameEntry tuples.          :param rev: revision specifier, see git-rev-parse for viable options.         :return: lazy iterator of BlameEntry tuples, where the commit                  indicates the commit to blame for the line, and range                  indicates a span of line numbers in the resulting file.          If you combine all line number ranges outputted by this command, you         should get a continuous range spanning all line numbers in the file.
The blame information for the given file at the given revision.          :param rev: revision specifier, see git-rev-parse for viable options.         :return:             list: [git.Commit, list: [<line>]]             A list of tuples associating a Commit object with a list of lines that             changed within the given commit. The Commit objects will be given in order             of appearance.
Initialize a git repository at the given path if specified          :param path:             is the full path to the repo (traditionally ends with /<name>.git)             or None in which case the repository will be created in the current             working directory          :param mkdir:             if specified will create the repository directory if it doesn't             already exists. Creates the directory with a mode=0755.             Only effective if a path is explicitly given          :param odbt:             Object DataBase type - a type which is constructed by providing             the directory containing the database objects, i.e. .git/objects.             It will be used to access all object data          :param expand_vars:             if specified, environment variables will not be escaped. This             can lead to information disclosure, allowing attackers to             access the contents of environment variables          :param kwargs:             keyword arguments serving as additional options to the git-init command          :return: ``git.Repo`` (the newly created repo)
Create a clone from this repository.          :param path: is the full path of the new repo (traditionally ends with ./<name>.git).         :param progress: See 'git.remote.Remote.push'.         :param kwargs:             * odbt = ObjectDatabase Type, allowing to determine the object database               implementation used by the returned Repo instance             * All remaining keyword arguments are given to the git-clone command          :return: ``git.Repo`` (the newly cloned repo)
Create a clone from the given URL          :param url: valid git url, see http://www.kernel.org/pub/software/scm/git/docs/git-clone.html#URLS         :param to_path: Path to which the repository should be cloned to         :param progress: See 'git.remote.Remote.push'.         :param env: Optional dictionary containing the desired environment variables.         :param kwargs: see the ``clone`` method         :return: Repo instance pointing to the cloned directory
Archive the tree at the given revision.          :param ostream: file compatible stream object to which the archive will be written as bytes         :param treeish: is the treeish name/id, defaults to active branch         :param prefix: is the optional prefix to prepend to each filename in the archive         :param kwargs: Additional arguments passed to git-archive              * Use the 'format' argument to define the kind of format. Use               specialized ostreams to write any format supported by python.             * You may specify the special **path** keyword, which may either be a repository-relative               path to a directory or file to place into the archive, or a list or tuple of multiple paths.          :raise GitCommandError: in case something went wrong         :return: self
:return: True if our git_dir is not at the root of our working_tree_dir, but a .git file with a             platform agnositic symbolic link. Our git_dir will be wherever the .git file points to         :note: bare repositories will always return False here
Safely decodes a binary string to unicode
Safely decodes a binary string to unicode
Encode unicodes for process arguments on Windows.
copied from https://github.com/Byron/bcore/blob/master/src/python/butility/future.py#L15
Pure Python implementation of the PEP 383: the "surrogateescape" error     handler of Python 3. Undecodable bytes will be replaced by a Unicode     character U+DCxx on decoding, and these are translated into the     original bytes on encoding.
Returns a (unicode) string, not the more logical bytes, because the codecs     register_error functionality expects this.
Returns a (unicode) string
Count the number of commits reachable from this commit          :param paths:             is an optional path or a list of paths restricting the return value             to commits actually containing the paths          :param kwargs:             Additional options to be passed to git-rev-list. They must not alter             the output style of the command, or parsing will yield incorrect results         :return: int defining the number of reachable commits
Find all commits matching the given criteria.          :param repo: is the Repo         :param rev: revision specifier, see git-rev-parse for viable options         :param paths:             is an optional path or list of paths, if set only Commits that include the path             or paths will be considered         :param kwargs:             optional keyword arguments to git rev-list where             ``max_count`` is the maximum number of commits to fetch             ``skip`` is the number of commits to skip             ``since`` all commits since i.e. '1970-01-01'         :return: iterator yielding Commit items
Iterate _all_ parents of this commit.          :param paths:             Optional path or list of paths limiting the Commits to those that             contain at least one of the paths         :param kwargs: All arguments allowed by git-rev-list         :return: Iterator yielding Commit objects which are parents of self
Create a git stat from changes between this commit and its first parent         or from all changes done if this is the very first commit.          :return: git.Stats
Parse out commit information into a list of Commit objects         We expect one-line per commit, and parse the actual commit information directly         from our lighting fast object database          :param proc: git-rev-list process instance - one sha per line         :return: iterator returning Commit objects
Commit the given tree, creating a commit object.          :param repo: Repo object the commit should be part of         :param tree: Tree object or hex or bin sha             the tree of the new commit         :param message: Commit message. It may be an empty string if no message is provided.             It will be converted to a string in any case.         :param parent_commits:             Optional Commit objects to use as parents for the new commit.             If empty list, the commit will have no parents at all and become             a root commit.             If None , the current head commit will be the parent of the             new commit object         :param head:             If True, the HEAD will be advanced to the new commit automatically.             Else the HEAD will remain pointing on the previous commit. This could             lead to undesired results when diffing files.         :param author: The name of the author, optional. If unset, the repository             configuration is used to obtain this value.         :param committer: The name of the committer, optional. If unset, the             repository configuration is used to obtain this value.         :param author_date: The timestamp for the author field         :param commit_date: The timestamp for the committer field          :return: Commit object representing the new commit          :note:             Additional information about the committer and Author are taken from the             environment or from the git configuration, see git-commit-tree for             more information
:param from_rev_list: if true, the stream format is coming from the rev-list command         Otherwise it is assumed to be a plain data stream from our object
This is taken from the git setup.c:is_git_directory     function.      @throws WorkTreeRepositoryUnsupported if it sees a worktree directory. It's quite hacky to do that here,             but at least clearly indicates that we don't support it.             There is the unlikely danger to throw if we see directories which just look like a worktree dir,             but are none.
Search for a gitdir for this worktree.
Search for a submodule repo.
:return: object specified by the given name, hexshas ( short and long )         as well as references are supported     :param return_ref: if name specifies a reference, we will return the reference         instead of the object. Otherwise it will raise BadObject or BadName
Convert the given object to a commit if possible and return it
:return: Object at the given revision, either Commit, Tag, Tree or Blob     :param rev: git-rev-parse compatible revision specification as string, please see         http://www.kernel.org/pub/software/scm/git/docs/git-rev-parse.html         for details     :raise BadObject: if the given revision could not be found     :raise ValueError: If rev couldn't be parsed     :raise IndexError: If invalid reflog index is specified
Add the --progress flag to the given kwargs dict if supported by the     git command. If the actual progress in the given progress instance is not     given, we do not request any progress     :return: possibly altered kwargs
:return:             Remote Reference or TagReference in the local repository corresponding             to the remote_ref_string kept in this instance.
Create a new PushInfo instance as parsed from line which is expected to be like             refs/heads/master:refs/heads/master 05d2687..1d0568e as bytes
This gets called by the refresh function (see the top level         __init__).
Parse information from the given line as returned by git-fetch -v         and return a new FetchInfo object representing this information.          We can handle a line as follows         "%c %-*s %-*s -> %s%s"          Where c is either ' ', !, +, -, *, or =         ! means error         + means success forcing update         - means a tag was updated         * means birth of new branch or tag         = means the head was up to date ( and not moved )         ' ' means a fast-forward          fetch line is the corresponding line from FETCH_HEAD, like         acb0fa8b94ef421ad60c8507b634759a472cd56c    not-for-merge   branch '0.1.7RC' of /tmp/tmpya0vairemote_repo
:return: True if this is a valid, existing remote.             Valid remotes have an entry in the repository's configuration
:return: Iterator yielding Remote objects of the given repository
Configure URLs on current remote (cf command git remote set_url)          This command manages URLs on the remote.          :param new_url: string being the URL to add as an extra remote URL         :param old_url: when set, replaces this URL with new_url for the remote         :return: self
:return: Iterator yielding all configured URL targets on a remote as strings
:return:             IterableList of RemoteReference objects. It is prefixed, allowing             you to omit the remote path portion, i.e.::             remote.refs.master # yields RemoteReference('/refs/remotes/origin/master')
:return:             IterableList RemoteReference objects that do not have a corresponding             head in the remote reference anymore as they have been deleted on the             remote side, but are still available locally.              The IterableList is prefixed, hence the 'origin' must be omitted. See             'refs' property for an example.              To make things more complicated, it can be possible for the list to include             other kinds of references, for example, tag references, if these are stale             as well. This is a fix for the issue described here:             https://github.com/gitpython-developers/GitPython/issues/260
Create a new remote to the given repository         :param repo: Repository instance that is to receive the new remote         :param name: Desired name of the remote         :param url: URL which corresponds to the remote's name         :param kwargs: Additional arguments to be passed to the git-remote add command         :return: New Remote instance         :raise GitCommandError: in case an origin with that name already exists
Remove the remote with the given name         :return: the passed remote name to remove
Rename self to the given new_name         :return: self
Fetch all changes for this remote, including new branches which will         be forced in ( in case your local remote branch is not part the new remote branches         ancestry anymore ).          :param kwargs:             Additional arguments passed to git-remote update          :return: self
Turns out we can't deal with remotes if the refspec is missing
Fetch the latest changes for this remote          :param refspec:             A "refspec" is used by fetch and push to describe the mapping             between remote ref and local ref. They are combined with a colon in             the format <src>:<dst>, preceded by an optional plus sign, +.             For example: git fetch $URL refs/heads/master:refs/heads/origin means             "grab the master branch head from the $URL and store it as my origin             branch head". And git push $URL refs/heads/master:refs/heads/to-upstream             means "publish my master branch head as to-upstream branch at $URL".             See also git-push(1).              Taken from the git manual              Fetch supports multiple refspecs (as the             underlying git-fetch does) - supplying a list rather than a string             for 'refspec' will make use of this facility.         :param progress: See 'push' method         :param kwargs: Additional arguments to be passed to git-fetch         :return:             IterableList(FetchInfo, ...) list of FetchInfo instances providing detailed             information about the fetch results          :note:             As fetch does not provide progress information to non-ttys, we cannot make             it available here unfortunately as in the 'push' method.
Push changes from source branch in refspec to target branch in refspec.          :param refspec: see 'fetch' method         :param progress:             Can take one of many value types:              * None to discard progress information             * A function (callable) that is called with the progress information.                Signature: ``progress(op_code, cur_count, max_count=None, message='')``.               `Click here <http://goo.gl/NPa7st>`_ for a description of all arguments               given to the function.             * An instance of a class derived from ``git.RemoteProgress`` that               overrides the ``update()`` function.          :note: No further progress information is returned after push returns.         :param kwargs: Additional arguments to be passed to git-push         :return:             IterableList(PushInfo, ...) iterable list of PushInfo instances, each             one informing about an individual head which had been updated on the remote             side.             If the push contains rejected heads, these will have the PushInfo.ERROR bit set             in their flags.             If the operation fails completely, the length of the returned IterableList will             be null.
:return: GitConfigParser compatible object able to write options for this remote.         :note:             You can only own one writer at a time - delete it to release the             configuration file and make it usable by others.              To assure consistent results, you should only query options through the             writer. Once you are done writing, you are free to use the config reader             once again.
:return: Config Parser constrained to our submodule in read or write mode         :raise IOError: If the .gitmodules file cannot be found, either locally or in the repository             at the given parent commit. Otherwise the exception would be delayed until the first             access of the config parser
:return: Configuration file as BytesIO - we only access it through the respective blob's data
:return: Config Parser constrained to our submodule in read or write mode
:return: Repo instance of newly cloned repository         :param repo: our parent repository         :param url: url to clone from         :param path: repository-relative path to the submodule checkout location         :param name: canonical of the submodule         :param kwrags: additinoal arguments given to git.clone
:return: a path guaranteed  to be relative to the given parent-repository         :raise ValueError: if path is not contained in the parent repository's working tree
Writes a .git file containing a (preferably) relative path to the actual git module repository.         It is an error if the module_abspath cannot be made into a relative path, relative to the working_tree_dir         :note: will overwrite existing files !         :note: as we rewrite both the git file as well as the module configuration, we might fail on the configuration             and will not roll back changes done to the git file. This should be a non-issue, but may easily be fixed             if it becomes one         :param working_tree_dir: directory to write the .git file into         :param module_abspath: absolute path to the bare repository
Add a new submodule to the given repository. This will alter the index         as well as the .gitmodules file, but will not create a new commit.         If the submodule already exists, no matter if the configuration differs         from the one provided, the existing submodule will be returned.          :param repo: Repository instance which should receive the submodule         :param name: The name/identifier for the submodule         :param path: repository-relative or absolute path at which the submodule             should be located             It will be created as required during the repository initialization.         :param url: git-clone compatible URL, see git-clone reference for more information             If None, the repository is assumed to exist, and the url of the first             remote is taken instead. This is useful if you want to make an existing             repository a submodule of anotherone.         :param branch: name of branch at which the submodule should (later) be checked out.             The given branch must exist in the remote repository, and will be checked             out locally as a tracking branch.             It will only be written into the configuration if it not None, which is             when the checked out branch will be the one the remote HEAD pointed to.             The result you get in these situation is somewhat fuzzy, and it is recommended             to specify at least 'master' here.             Examples are 'master' or 'feature/new'         :param no_checkout: if True, and if the repository has to be cloned manually,             no checkout will be performed         :return: The newly created submodule instance         :note: works atomically, such that no change will be done if the repository             update fails for instance
Update the repository of this submodule to point to the checkout         we point at with the binsha of this instance.          :param recursive: if True, we will operate recursively and update child-             modules as well.         :param init: if True, the module repository will be cloned into place if necessary         :param to_latest_revision: if True, the submodule's sha will be ignored during checkout.             Instead, the remote will be fetched, and the local tracking branch updated.             This only works if we have a local tracking branch, which is the case             if the remote repository had a master branch, or of the 'branch' option             was specified for this submodule and the branch existed remotely         :param progress: UpdateProgress instance or None if no progress should be shown         :param dry_run: if True, the operation will only be simulated, but not performed.             All performed operations are read-only         :param force:             If True, we may reset heads even if the repository in question is dirty. Additinoally we will be allowed             to set a tracking branch which is ahead of its remote branch back into the past or the location of the             remote branch. This will essentially 'forget' commits.             If False, local tracking branches that are in the future of their respective remote branches will simply             not be moved.         :param keep_going: if True, we will ignore but log all errors, and keep going recursively.             Unless dry_run is set as well, keep_going could cause subsequent/inherited errors you wouldn't see             otherwise.             In conjunction with dry_run, it can be useful to anticipate all errors when updating submodules         :note: does nothing in bare repositories         :note: method is definitely not atomic if recurisve is True         :return: self
Move the submodule to a another module path. This involves physically moving         the repository at our current path, changing the configuration, as well as         adjusting our index entry accordingly.          :param module_path: the path to which to move our module in the parent repostory's working tree,             given as repository-relative or absolute path. Intermediate directories will be created             accordingly. If the path already exists, it must be empty.             Trailing (back)slashes are removed automatically         :param configuration: if True, the configuration will be adjusted to let             the submodule point to the given path.         :param module: if True, the repository managed by this submodule             will be moved as well. If False, we don't move the submodule's checkout, which may leave             the parent repository in an inconsistent state.         :return: self         :raise ValueError: if the module path existed and was not empty, or was a file         :note: Currently the method is not atomic, and it could leave the repository             in an inconsistent state if a sub-step fails for some reason
Remove this submodule from the repository. This will remove our entry         from the .gitmodules file and the entry in the .git/config file.          :param module: If True, the module checkout we point to will be deleted             as well. If the module is currently on a commit which is not part             of any branch in the remote, if the currently checked out branch             working tree, or untracked files,             is ahead of its tracking branch,  if you have modifications in the             In case the removal of the repository fails for these reasons, the             submodule status will not have been altered.             If this submodule has child-modules on its own, these will be deleted             prior to touching the own module.         :param force: Enforces the deletion of the module even though it contains             modifications. This basically enforces a brute-force file system based             deletion.         :param configuration: if True, the submodule is deleted from the configuration,             otherwise it isn't. Although this should be enabled most of the times,             this flag enables you to safely delete the repository of your submodule.         :param dry_run: if True, we will not actually do anything, but throw the errors             we would usually throw         :return: self         :note: doesn't work in bare repositories         :note: doesn't work atomically, as failure to remove any part of the submodule will leave             an inconsistent state         :raise InvalidGitRepositoryError: thrown if the repository cannot be deleted         :raise OSError: if directories or files could not be removed
Set this instance to use the given commit whose tree is supposed to         contain the .gitmodules blob.          :param commit:             Commit'ish reference pointing at the root_tree, or None to always point to the             most recent commit         :param check:             if True, relatively expensive checks will be performed to verify             validity of the submodule.         :raise ValueError: if the commit's tree didn't contain the .gitmodules blob.         :raise ValueError:             if the parent commit didn't store this submodule under the current path         :return: self
:return: a config writer instance allowing you to read and write the data         belonging to this submodule into the .gitmodules file.          :param index: if not None, an IndexFile instance which should be written.             defaults to the index of the Submodule's parent repository.         :param write: if True, the index will be written each time a configuration             value changes.         :note: the parameters allow for a more efficient writing of the index,             as you can pass in a modified index on your own, prevent automatic writing,             and write yourself once the whole operation is complete         :raise ValueError: if trying to get a writer on a parent_commit which does not             match the current head commit         :raise IOError: If the .gitmodules file/blob could not be read
Rename this submodule         :note: This method takes care of renaming the submodule in various places, such as              * $parent_git_dir/config             * $working_tree_dir/.gitmodules             * (git >=v1.8.0: move submodule repository to new name)          As .gitmodules will be changed, you would need to make a commit afterwards. The changed .gitmodules file         will already be added to the index          :return: this submodule instance
:return: Repo instance initialized from the repository at our submodule path         :raise InvalidGitRepositoryError: if a repository was not available. This could             also mean that it was not yet initialized
:return: True if the submodule exists, False otherwise. Please note that             a submodule may exist (in the .gitmodules file) even though its module             doesn't exist on disk
:return: iterator yielding Submodule instances available in the given repository
Creates diffs between two items being trees, trees and index or an         index and the working tree. It will detect renames automatically.          :param other:             Is the item to compare us with.             If None, we will be compared to the working tree.             If Treeish, it will be compared against the respective tree             If Index ( type ), it will be compared against the index.             If git.NULL_TREE, it will compare against the empty tree.             It defaults to Index to assure the method will not by-default fail             on bare repositories.          :param paths:             is a list of paths or a single path to limit the diff to.             It will only include at least one of the given path or paths.          :param create_patch:             If True, the returned Diff contains a detailed patch that if applied             makes the self to other. Patches are somewhat costly as blobs have to be read             and diffed.          :param kwargs:             Additional arguments passed to git-diff, such as             R=True to swap both sides of the diff.          :return: git.DiffIndex          :note:             On a bare repository, 'other' needs to be provided as Index or as             as Tree/Commit, or a git command error will occur
:return:             iterator yielding Diff instances that match the given change_type          :param change_type:             Member of DiffIndex.change_type, namely:              * 'A' for added paths             * 'D' for deleted paths             * 'R' for renamed paths             * 'M' for paths with modified data             * 'T' for changed in the type paths
Create a new DiffIndex from the given text which must be in patch format         :param repo: is the repository we are operating on - it is required         :param stream: result of 'git diff' as a stream (supporting file protocol)         :return: git.DiffIndex
Create a new DiffIndex from the given stream which must be in raw format.         :return: git.DiffIndex
Cache all our attributes at once
A decorator raising a TypeError if we are not a valid remote, based on the path
Special version which checks if the head-log needs an update as well         :return: self
:return: (shortest) Name of this reference - it may contain path components
Decorator for functions that alter the index using the git command. This would     invalidate our possibly existing entries dictionary which is why it must be     deleted to allow it to be lazily reread later.      :note:         This decorator will not be required once all functions are implemented         natively which in fact is possible, but probably not feasible performance wise.
Decorator assuring the wrapped method may only run if we are the default     repository index. This is as we rely on git commands that operate     on that index only.
Decorator which changes the current working dir to the one of the git     repository in order to assure relative paths are handled correctly
:return: New branch/head instance
Find the remote branch matching the name of the given branch or raise InvalidGitRepositoryError
Flush changes in our configuration file to the index
:return: a string suitable to be placed in a reflog file
:return: New instance of a RefLogEntry
:return: New RefLogEntry instance from the given revlog line.         :param line: line bytes without trailing newline         :raise ValueError: If line could not be parsed
:return: string to absolute path at which the reflog of the given ref             instance would be found. The path is not guaranteed to point to a valid             file though.         :param ref: SymbolicReference instance
:return: Iterator yielding RefLogEntry instances, one for each line read             sfrom the given stream.         :param stream: file-like object containing the revlog in its native format             or basestring instance pointing to a file to read
:return: RefLogEntry at the given index         :param filepath: full path to the index file from which to read the entry         :param index: python list compatible index, i.e. it may be negative to             specify an entry counted from the end of the list          :raise IndexError: If the entry didn't exist          .. note:: This method is faster as it only parses the entry at index, skipping             all other lines. Nonetheless, the whole file has to be read if             the index is negative
Write the contents of the reflog instance to a file at the given filepath.         :param filepath: path to file, parent directories are assumed to exist
Append a new log entry to the revlog at filepath.          :param config_reader: configuration reader of the repository - used to obtain             user information. May also be an Actor instance identifying the committer directly.             May also be None         :param filepath: full path to the log file         :param oldbinsha: binary sha of the previous commit         :param newbinsha: binary sha of the current commit         :param message: message describing the change to the reference         :param write: If True, the changes will be written right away. Otherwise             the change will not be written         :return: RefLogEntry objects which was appended to the log         :note: As we are append-only, concurrent access is not a problem as we             do not interfere with readers.
Write this instance's data to the file we are originating from         :return: self
Methods with this decorator raise InvalidGitRepositoryError if they     encounter a bare repository
Remove the given recursively.      :note: we use shutil rmtree but adjust its behaviour to see whether files that         couldn't be deleted are read-only. Windows will not remove them in that case
Ensure file deleted also on *Windows* where read-only files need special treatment.
Copy all data from the source stream into the destination stream in chunks     of size chunk_size      :return: amount of bytes written
Join path tokens together similar to osp.join, but always use     '/' instead of possibly '\' on windows.
Assure that the directory pointed to by path exists.      :param is_file: If True, path is assumed to be a file and handled correctly.         Otherwise it must be a directory     :return: True if the directory was created, False if it already existed
Use :meth:`git.cmd.Git.polish_url()` instead, that works on any environment.
Parse progress information from the given line as retrieved by git-push         or git-fetch.          - Lines that do not contain progress info are stored in :attr:`other_lines`.         - Lines that seem to contain an error (i.e. start with error: or fatal:) are stored         in :attr:`error_lines`.          :return: list(line, ...) list of lines that could not be processed
Create an Actor from a string.         :param string: is the string, which is expected to be in regular git format                  John Doe <jdoe@example.com>          :return: Actor
:return: Actor instance corresponding to the configured committer. It behaves             similar to the git implementation, such that the environment will override             configuration values of config_reader. If no value is set at all, it will be             generated         :param config_reader: ConfigReader to use to retrieve the values from in case             they are not set in the environment
Same as committer(), but defines the main author. It may be specified in the environment,         but defaults to the committer
Create a Stat object from output retrieved by git-diff.          :return: git.Stat
Release our lock if we have one
This method blocks until it obtained the lock, or raises IOError if         it ran out of time or if the parent directory was not available anymore.         If this method returns, you are guaranteed to own the lock
Find all items of this type - subclasses can specify args and kwargs differently.         If no args are given, subclasses are obliged to return all items if no additional         arguments arg given.          :note: Favor the iter_items method as it will          :return:list(Item,...) list of item instances
Accepts an input string containing an LA equation, e.g.,     "M3_mymatrix = M3_anothermatrix^-1" returns C code function     calls that implement the expression.
Scans an input file for LA equations between double square brackets,    e.g. [[ M3_mymatrix = M3_anothermatrix^-1 ]], and replaces the expression    with a comment containing the equation followed by nested function calls    that implement the equation as C code. A trailing semi-colon is appended.    The equation within [[ ]] should NOT end with a semicolon as that will raise    a ParseException. However, it is ok to have a semicolon after the right brackets.     Other text in the file is unaltered.     The arguments are file objects (NOT file names) opened for reading and    writing, respectively.
testfiles can be None, in which case the command line arguments are used as filenames.     testfiles can be a string, in which case that file is parsed.     testfiles can be a list.     In all cases, the filenames will be globbed.     If more than one file is parsed successfully, a dictionary of ParseResults is returned.     Otherwise, a simple ParseResults is returned.
generator to extract operators and operands in pairs
Parse action to convert statemachine to corresponding Python classes and methods
Parse action to convert statemachine with named transitions to corresponding Python     classes and methods
Used for debugging.
If can't continue.
This should only be used for testing. The primary mode of operation is     as an imported library.
Reads (and optionally parses) a single line.
Returns a list of all lines (optionally parsed) in the file.
create rooms, using multiline string showing map layout     string contains symbols for the following:      A-Z, a-z indicate rooms, and rooms will be stored in a dictionary by                reference letter      -, | symbols indicate connection between rooms      <, >, ^, . symbols indicate one-way connection between rooms
This function returns a parser.         The grammar should be like most full text search engines (Google, Tsearch, Lucene).          Grammar:         - a query consists of alphanumeric words, with an optional '*' wildcard           at the end of a word         - a sequence of words between quotes is a literal string         - words can be used together by using operators ('and' or 'or')         - words with operators can be grouped with parenthesis         - a word or group of words can be preceded by a 'not' operator         - the 'and' operator precedes an 'or' operator         - if an operator is missing, use an 'and' operator
Evaluate quoted strings          First is does an 'and' on the indidual search terms, then it asks the         function GetQuoted to only return the subset of ID's that contain the         literal string.
Internal helper to construct opening and closing tag expressions, given a tag name
Helper function for setting curently parsed text and position
Sets attribute's name and value
Displays the symbol table content
Inserts new symbol at the end of the symbol table.            Returns symbol index            sname - symbol name            skind - symbol kind            stype - symbol type
Clears all symbols begining with the index to the end of table
Searches for symbol, from the end to the begining.            Returns symbol index or None            sname - symbol name            skind - symbol kind (one kind, list of kinds, or None) deafult: any kind            stype - symbol type (or None) default: any type
Inserts a new identifier at the end of the symbol table, if possible.            Returns symbol index, or raises an exception if the symbol alredy exists            sname   - symbol name            skind   - symbol kind            skinds  - symbol kinds to check for            stype   - symbol type
Inserts a new global variable
Inserts a new local variable
Inserts a new parameter
Inserts a new function
Inserts a constant (or returns index if the constant already exists)            Additionally, checks for range.
Returns True if both symbol table elements are of the same type
Returns True if index and function's argument are of the same type            index - index in symbol table            function_index - function's index in symbol table            argument_number - # of function's argument
Reserves one working register and sets its type
Reserves register for function return value and sets its type
Releases working register
If index is a working register, free it, otherwise just return (helper function)
Generates label name (helper function)            name - label name            internal - boolean value, adds "@" prefix to label            definition - boolean value, adds ":" suffix to label
Generates symbol name from index
Pushes all used working registers before function call
Pops all used working registers after function call
Inserts a newline and text, optionally with indentation (helper function)
Inserts a newline and a label (helper function)            name - label name            internal - boolean value, adds "@" prefix to label            definition - boolean value, adds ":" suffix to label
Inserts a new static (global) variable definition
Generates an arithmetic instruction mnemonic
Generates an arithmetic instruction            operation - one of supporetd operations            operandX - index in symbol table or text representation of operand            First two operands are input, third one is output
Returns code for relational operator            relop - relational operator            operands_type - int or unsigned
Generates a jump instruction            relcode  - relational operator code            opposite - generate normal or opposite jump            label    - jump label
Generates a move instruction            If the output operand (opernad2) is a working register, sets it's type            operandX - index in symbol table or text representation of operand
Generates a compare instruction            operandX - index in symbol table
Inserts function name label and function frame initialization
Inserts a local variable initialization and body label
Inserts an exit label and function return instructions
Generates code for a function call            function - function index in symbol table            arguments - list of arguments (indexes in symbol table)
Displays warning message. Uses exshared for current location of parsing
Code executed after recognising a global variable
Code executed after recognising a local variable
Code executed after recognising a parameter
Code executed after recognising a constant
Code executed after recognising a function definition (type and function name)
Code executed after recognising the beginning of function's body
Code executed at the end of function definition
Code executed after recognising a return statement
Code executed after recognising an identificator in expression
Code executed after recognising an assignment statement
Code executed after recognising a mulexp expression (something *|/ something)
Code executed after recognising a numexp expression (something +|- something)
Code executed after recognising a function call (type and function name)
Code executed after recognising each of function's arguments
Code executed after recognising the whole function call
Code executed after recognising a relexp expression (something relop something)
Code executed after recognising a andexp expression (something and something)
Code executed after recognising an if statement (if keyword)
Code executed after recognising if statement's body
Code executed after recognising if statement's else body
Code executed after recognising a whole if statement
Checks if there is a 'main' function and the type of 'main' function
Parse string (helper function)
Parse file (helper function)
expop   :: '^'     multop  :: '*' | '/'     addop   :: '+' | '-'     integer :: ['+' | '-'] '0'..'9'+     atom    :: PI | E | real | fn '(' expr ')' | '(' expr ')'     factor  :: atom [ expop factor ]*     term    :: factor [ multop factor ]*     expr    :: term [ addop term ]*
This will encode a ``unicode`` value into a cookie, and sign that cookie     with the app's secret key.      :param payload: The value to encode, as `unicode`.     :type payload: unicode      :param key: The key to use when creating the cookie digest. If not                 specified, the SECRET_KEY value from app config will be used.     :type key: str
This decodes a cookie given by `encode_cookie`. If verification of the     cookie fails, ``None`` will be implicitly returned.      :param cookie: An encoded cookie.     :type cookie: str      :param key: The key to use when creating the cookie digest. If not                 specified, the SECRET_KEY value from app config will be used.     :type key: str
Reduces the scheme and host from a given URL so it can be passed to     the given `login` URL more efficiently.      :param login_url: The login URL being redirected to.     :type login_url: str     :param current_url: The URL to reduce.     :type current_url: str
Creates a URL for redirecting to a login page. If only `login_view` is     provided, this will just return the URL for it. If `next_url` is provided,     however, this will append a ``next=URL`` parameter to the query string     so that the login view can redirect back to that URL. Flask-Login's default     unauthorized handler uses this function when redirecting to your login url.     To force the host name used, set `FORCE_HOST_FOR_REDIRECTS` to a host. This     prevents from redirecting to external sites if request headers Host or     X-Forwarded-For are present.      :param login_view: The name of the login view. (Alternately, the actual                        URL to the login view.)     :type login_view: str     :param next_url: The URL to give the login view for redirection.     :type next_url: str     :param next_field: What field to store the next URL in. (It defaults to                        ``next``.)     :type next_field: str
Logs a user in. You should pass the actual user object to this. If the     user's `is_active` property is ``False``, they will not be logged in     unless `force` is ``True``.      This will return ``True`` if the log in attempt succeeds, and ``False`` if     it fails (i.e. because the user is inactive).      :param user: The user object to log in.     :type user: object     :param remember: Whether to remember the user after their session expires.         Defaults to ``False``.     :type remember: bool     :param duration: The amount of time before the remember cookie expires. If         ``None`` the value set in the settings is used. Defaults to ``None``.     :type duration: :class:`datetime.timedelta`     :param force: If the user is inactive, setting this to ``True`` will log         them in regardless. Defaults to ``False``.     :type force: bool     :param fresh: setting this to ``False`` will log in the user with a session         marked as not "fresh". Defaults to ``True``.     :type fresh: bool
Logs a user out. (You do not need to pass the actual user.) This will     also clean up the remember me cookie if it exists.
This sets the current session as fresh. Sessions become stale when they     are reloaded from a cookie.
If you decorate a view with this, it will ensure that the current user is     logged in and authenticated before calling the actual view. (If they are     not, it calls the :attr:`LoginManager.unauthorized` callback.) For     example::          @app.route('/post')         @login_required         def post():             pass      If there are only certain times you need to require that your user is     logged in, you can do so with::          if not current_user.is_authenticated:             return current_app.login_manager.unauthorized()      ...which is essentially the code that this function adds to your views.      It can be convenient to globally turn off authentication when unit testing.     To enable this, if the application configuration variable `LOGIN_DISABLED`     is set to `True`, this decorator will be ignored.      .. Note ::          Per `W3 guidelines for CORS preflight requests         <http://www.w3.org/TR/cors/#cross-origin-request-with-preflight-0>`_,         HTTP ``OPTIONS`` requests are exempt from login checks.      :param func: The view function to decorate.     :type func: function
If you decorate a view with this, it will ensure that the current user's     login is fresh - i.e. their session was not restored from a 'remember me'     cookie. Sensitive operations, like changing a password or e-mail, should     be protected with this, to impede the efforts of cookie thieves.      If the user is not authenticated, :meth:`LoginManager.unauthorized` is     called as normal. If they are authenticated, but their session is not     fresh, it will call :meth:`LoginManager.needs_refresh` instead. (In that     case, you will need to provide a :attr:`LoginManager.refresh_view`.)      Behaves identically to the :func:`login_required` decorator with respect     to configutation variables.      .. Note ::          Per `W3 guidelines for CORS preflight requests         <http://www.w3.org/TR/cors/#cross-origin-request-with-preflight-0>`_,         HTTP ``OPTIONS`` requests are exempt from login checks.      :param func: The view function to decorate.     :type func: function
Sets the login view for the app or blueprint. If a blueprint is passed,     the login view is set for this blueprint on ``blueprint_login_views``.      :param login_view: The user object to log in.     :type login_view: str     :param blueprint: The blueprint which this login view should be set on.         Defaults to ``None``.     :type blueprint: object
This method has been deprecated. Please use         :meth:`LoginManager.init_app` instead.
Configures an application. This registers an `after_request` call, and         attaches this `LoginManager` to it as `app.login_manager`.          :param app: The :class:`flask.Flask` object to configure.         :type app: :class:`flask.Flask`         :param add_context_processor: Whether to add a context processor to             the app that adds a `current_user` variable to the template.             Defaults to ``True``.         :type add_context_processor: bool
This is called when the user is required to log in. If you register a         callback with :meth:`LoginManager.unauthorized_handler`, then it will         be called. Otherwise, it will take the following actions:              - Flash :attr:`LoginManager.login_message` to the user.              - If the app is using blueprints find the login view for               the current blueprint using `blueprint_login_views`. If the app               is not using blueprints or the login view for the current               blueprint is not specified use the value of `login_view`.              - Redirect the user to the login view. (The page they were               attempting to access will be passed in the ``next`` query               string variable, so you can redirect there if present instead               of the homepage. Alternatively, it will be added to the session               as ``next`` if USE_SESSION_FOR_NEXT is set.)          If :attr:`LoginManager.login_view` is not defined, then it will simply         raise a HTTP 401 (Unauthorized) error instead.          This should be returned from a view or before/after_request function,         otherwise the redirect will have no effect.
This is called when the user is logged in, but they need to be         reauthenticated because their session is stale. If you register a         callback with `needs_refresh_handler`, then it will be called.         Otherwise, it will take the following actions:              - Flash :attr:`LoginManager.needs_refresh_message` to the user.              - Redirect the user to :attr:`LoginManager.refresh_view`. (The page               they were attempting to access will be passed in the ``next``               query string variable, so you can redirect there if present               instead of the homepage.)          If :attr:`LoginManager.refresh_view` is not defined, then it will         simply raise a HTTP 401 (Unauthorized) error instead.          This should be returned from a view or before/after_request function,         otherwise the redirect will have no effect.
Store the given user as ctx.user.
Loads user from session or remember_me cookie as applicable
Converts a pretrained tree and cluster size into a     set of labels and probabilities.
Perform check_array(X) after removing infinite values (numpy.inf) from the given distance matrix.
Perform HDBSCAN clustering from a vector array or distance matrix.      Parameters     ----------     X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \             array of shape (n_samples, n_samples)         A feature array, or array of distances between samples if         ``metric='precomputed'``.      min_cluster_size : int, optional (default=5)         The minimum number of samples in a group for that group to be         considered a cluster; groupings smaller than this size will be left         as noise.      min_samples : int, optional (default=None)         The number of samples in a neighborhood for a point         to be considered as a core point. This includes the point itself.         defaults to the min_cluster_size.      alpha : float, optional (default=1.0)         A distance scaling parameter as used in robust single linkage.         See [2]_ for more information.      metric : string or callable, optional (default='minkowski')         The metric to use when calculating distance between instances in a         feature array. If metric is a string or callable, it must be one of         the options allowed by metrics.pairwise.pairwise_distances for its         metric parameter.         If metric is "precomputed", X is assumed to be a distance matrix and         must be square.      p : int, optional (default=2)         p value to use if using the minkowski metric.      leaf_size : int, optional (default=40)         Leaf size for trees responsible for fast nearest         neighbour queries.      algorithm : string, optional (default='best')         Exactly which algorithm to use; hdbscan has variants specialised         for different characteristics of the data. By default this is set         to ``best`` which chooses the "best" algorithm given the nature of         the data. You can force other options if you believe you know         better. Options are:             * ``best``             * ``generic``             * ``prims_kdtree``             * ``prims_balltree``             * ``boruvka_kdtree``             * ``boruvka_balltree``      memory : instance of joblib.Memory or string, optional         Used to cache the output of the computation of the tree.         By default, no caching is done. If a string is given, it is the         path to the caching directory.      approx_min_span_tree : bool, optional (default=True)         Whether to accept an only approximate minimum spanning tree.         For some algorithms this can provide a significant speedup, but         the resulting clustering may be of marginally lower quality.         If you are willing to sacrifice speed for correctness you may want         to explore this; in general this should be left at the default True.      gen_min_span_tree : bool, optional (default=False)         Whether to generate the minimum spanning tree for later analysis.      core_dist_n_jobs : int, optional (default=4)         Number of parallel jobs to run in core distance computations (if         supported by the specific algorithm). For ``core_dist_n_jobs``         below -1, (n_cpus + 1 + core_dist_n_jobs) are used.      cluster_selection_method : string, optional (default='eom')         The method used to select clusters from the condensed tree. The         standard approach for HDBSCAN* is to use an Excess of Mass algorithm         to find the most persistent clusters. Alternatively you can instead         select the clusters at the leaves of the tree -- this provides the         most fine grained and homogeneous clusters. Options are:             * ``eom``             * ``leaf``      allow_single_cluster : bool, optional (default=False)         By default HDBSCAN* will not produce a single cluster, setting this         to t=True will override this and allow single cluster results in         the case that you feel this is a valid result for your dataset.         (default False)      match_reference_implementation : bool, optional (default=False)         There exist some interpretational differences between this         HDBSCAN* implementation and the original authors reference         implementation in Java. This can result in very minor differences         in clustering results. Setting this flag to True will, at a some         performance cost, ensure that the clustering results match the         reference implementation.      **kwargs : optional         Arguments passed to the distance metric      Returns     -------     labels : ndarray, shape (n_samples, )         Cluster labels for each point.  Noisy samples are given the label -1.      probabilities : ndarray, shape (n_samples, )         Cluster membership strengths for each point. Noisy samples are assigned         0.      cluster_persistence : array, shape  (n_clusters, )         A score of how persistent each cluster is. A score of 1.0 represents         a perfectly stable cluster that persists over all distance scales,         while a score of 0.0 represents a perfectly ephemeral cluster. These         scores can be guage the relative coherence of the clusters output         by the algorithm.      condensed_tree : record array         The condensed cluster hierarchy used to generate clusters.      single_linkage_tree : ndarray, shape (n_samples - 1, 4)         The single linkage tree produced during clustering in scipy         hierarchical clustering format         (see http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).      min_spanning_tree : ndarray, shape (n_samples - 1, 3)         The minimum spanning as an edgelist. If gen_min_span_tree was False         this will be None.      References     ----------      .. [1] Campello, R. J., Moulavi, D., & Sander, J. (2013, April).        Density-based clustering based on hierarchical density estimates.        In Pacific-Asia Conference on Knowledge Discovery and Data Mining        (pp. 160-172). Springer Berlin Heidelberg.      .. [2] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the        cluster tree. In Advances in Neural Information Processing Systems        (pp. 343-351).
Perform HDBSCAN clustering from features or distance matrix.          Parameters         ----------         X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \                 array of shape (n_samples, n_samples)             A feature array, or array of distances between samples if             ``metric='precomputed'``.          Returns         -------         self : object             Returns self
Create data that caches intermediate results used for predicting         the label of new/unseen points. This data is only useful if         you are intending to use functions from ``hdbscan.prediction``.
Perform a breadth first search on a tree in condensed tree format
Generates data for use in plotting the 'icicle plot' or dendrogram         plot of the condensed tree generated by HDBSCAN.          Parameters         ----------         leaf_separation : float, optional                           How far apart to space the final leaves of the                           dendrogram. (default 1)          log_size : boolean, optional                    Use log scale for the 'size' of clusters (i.e. number of                    points in the cluster at a given lambda value).                    (default False)          max_rectangles_per_icicle : int, optional             To simplify the plot this method will only emit             ``max_rectangles_per_icicle`` bars per branch of the dendrogram.             This ensures that we don't suffer from massive overplotting in             cases with a lot of data points.          Returns         -------         plot_data : dict                     Data associated to bars in a bar plot:                         `bar_centers` x coordinate centers for bars                         `bar_tops` heights of bars in lambda scale                         `bar_bottoms` y coordinate of bottoms of bars                         `bar_widths` widths of the bars (in x coord scale)                         `bar_bounds` a 4-tuple of [left, right, bottom, top]                                      giving the bounds on a full set of                                      cluster bars                     Data associates with cluster splits:                         `line_xs` x coordinates for horizontal dendrogram lines                         `line_ys` y coordinates for horizontal dendrogram lines
Use matplotlib to plot an 'icicle plot' dendrogram of the condensed tree.          Effectively this is a dendrogram where the width of each cluster bar is         equal to the number of points (or log of the number of points) in the cluster         at the given lambda value. Thus bars narrow as points progressively drop         out of clusters. The make the effect more apparent the bars are also colored         according the the number of points (or log of the number of points).          Parameters         ----------         leaf_separation : float, optional (default 1)                           How far apart to space the final leaves of the                           dendrogram.          cmap : string or matplotlib colormap, optional (default viridis)                The matplotlib colormap to use to color the cluster bars.           select_clusters : boolean, optional (default False)                           Whether to draw ovals highlighting which cluster                           bar represent the clusters that were selected by                           HDBSCAN as the final clusters.          label_clusters : boolean, optional (default False)                          If select_clusters is True then this determines                          whether to draw text labels on the clusters.          selection_palette : list of colors, optional (default None)                             If not None, and at least as long as                             the number of clusters, draw ovals                             in colors iterating through this palette.                             This can aid in cluster identification                             when plotting.          axis : matplotlib axis or None, optional (default None)                The matplotlib axis to render to. If None then a new axis                will be generated. The rendered axis will be returned.           colorbar : boolean, optional (default True)                    Whether to draw a matplotlib colorbar displaying the range                    of cluster sizes as per the colormap.          log_size : boolean, optional (default False)                    Use log scale for the 'size' of clusters (i.e. number of                    points in the cluster at a given lambda value).           max_rectangles_per_icicle : int, optional (default 20)             To simplify the plot this method will only emit             ``max_rectangles_per_icicle`` bars per branch of the dendrogram.             This ensures that we don't suffer from massive overplotting in             cases with a lot of data points.           Returns         -------         axis : matplotlib axis                The axis on which the 'icicle plot' has been rendered.
Return a pandas dataframe representation of the condensed tree.          Each row of the dataframe corresponds to an edge in the tree.         The columns of the dataframe are `parent`, `child`, `lambda_val`         and `child_size`.          The `parent` and `child` are the ids of the         parent and child nodes in the tree. Node ids less than the number         of points in the original dataset represent individual points, while         ids greater than the number of points are clusters.          The `lambda_val` value is the value (1/distance) at which the `child`         node leaves the cluster.          The `child_size` is the number of points in the `child` node.
Return a NetworkX DiGraph object representing the condensed tree.          Edge weights in the graph are the lamba values at which child nodes         'leave' the parent cluster.          Nodes have a `size` attribute attached giving the number of points         that are in the cluster (or 1 if it is a singleton point) at the         point of cluster creation (fewer points may be in the cluster at         larger lambda values).
Plot a dendrogram of the single linkage tree.          Parameters         ----------         truncate_mode : str, optional                         The dendrogram can be hard to read when the original                         observation matrix from which the linkage is derived                         is large. Truncation is used to condense the dendrogram.                         There are several modes:          ``None/'none'``                 No truncation is performed (Default).          ``'lastp'``                 The last p non-singleton formed in the linkage are the only                 non-leaf nodes in the linkage; they correspond to rows                 Z[n-p-2:end] in Z. All other non-singleton clusters are                 contracted into leaf nodes.          ``'level'/'mtica'``                 No more than p levels of the dendrogram tree are displayed.                 This corresponds to Mathematica(TM) behavior.          p : int, optional             The ``p`` parameter for ``truncate_mode``.          vary_line_width : boolean, optional             Draw downward branches of the dendrogram with line thickness that             varies depending on the size of the cluster.          cmap : string or matplotlib colormap, optional                The matplotlib colormap to use to color the cluster bars.                A value of 'none' will result in black bars.                (default 'viridis')          colorbar : boolean, optional                    Whether to draw a matplotlib colorbar displaying the range                    of cluster sizes as per the colormap. (default True)          Returns         -------         axis : matplotlib axis                The axis on which the dendrogram plot has been rendered.
Return a pandas dataframe representation of the single linkage tree.          Each row of the dataframe corresponds to an edge in the tree.         The columns of the dataframe are `parent`, `left_child`,         `right_child`, `distance` and `size`.          The `parent`, `left_child` and `right_child` are the ids of the         parent and child nodes in the tree. Node ids less than the number         of points in the original dataset represent individual points, while         ids greater than the number of points are clusters.          The `distance` value is the at which the child nodes merge to form         the parent node.          The `size` is the number of points in the `parent` node.
Return a NetworkX DiGraph object representing the single linkage tree.          Edge weights in the graph are the distance values at which child nodes         merge to form the parent cluster.          Nodes have a `size` attribute attached giving the number of points         that are in the cluster.
Plot the minimum spanning tree (as projected into 2D by t-SNE if required).          Parameters         ----------          axis : matplotlib axis, optional                The axis to render the plot to          node_size : int, optional                 The size of nodes in the plot (default 40).          node_color : matplotlib color spec, optional                 The color to render nodes (default black).          node_alpha : float, optional                 The alpha value (between 0 and 1) to render nodes with                 (default 0.8).          edge_cmap : matplotlib colormap, optional                 The colormap to color edges by (varying color by edge                     weight/distance). Can be a cmap object or a string                     recognised by matplotlib. (default `viridis_r`)          edge_alpha : float, optional                 The alpha value (between 0 and 1) to render edges with                 (default 0.5).          edge_linewidth : float, optional                 The linewidth to use for rendering edges (default 2).          vary_line_width : bool, optional                 Edge width is proportional to (log of) the inverse of the                 mutual reachability distance. (default True)          colorbar : bool, optional                 Whether to draw a colorbar. (default True)          Returns         -------          axis : matplotlib axis                 The axis used the render the plot.
Return a Pandas dataframe of the minimum spanning tree.          Each row is an edge in the tree; the columns are `from`,         `to`, and `distance` giving the two vertices of the edge         which are indices into the dataset, and the distance         between those datapoints.
Return a NetworkX Graph object representing the minimum spanning tree.          Edge weights in the graph are the distance between the nodes they connect.          Nodes have a `data` attribute attached giving the data vector of the         associated point.
Compute the all-points-core-distance for all the points of a cluster.      Parameters     ----------     distance_matrix : array (cluster_size, cluster_size)         The pairwise distance matrix between points in the cluster.      d : integer         The dimension of the data set, which is used in the computation         of the all-point-core-distance as per the paper.      Returns     -------     core_distances : array (cluster_size,)         The all-points-core-distance of each point in the cluster      References     ----------     Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,     2014. Density-Based Clustering Validation. In SDM (pp. 839-847).
Compute the all-points-mutual-reachability distances for all the points of     a cluster.      If metric is 'precomputed' then assume X is a distance matrix for the full     dataset. Note that in this case you must pass in 'd' the dimension of the     dataset.      Parameters     ----------     X : array (n_samples, n_features) or (n_samples, n_samples)         The input data of the clustering. This can be the data, or, if         metric is set to `precomputed` the pairwise distance matrix used         for the clustering.      labels : array (n_samples)         The label array output by the clustering, providing an integral         cluster label to each data point, with -1 for noise points.      cluster_id : integer         The cluster label for which to compute the all-points         mutual-reachability (which should be done on a cluster         by cluster basis).      metric : string         The metric used to compute distances for the clustering (and         to be re-used in computing distances for mr distance). If         set to `precomputed` then X is assumed to be the precomputed         distance matrix between samples.      d : integer (or None)         The number of features (dimension) of the dataset. This need only         be set in the case of metric being set to `precomputed`, where         the ambient dimension of the data is unknown to the function.      **kwd_args :         Extra arguments to pass to the distance computation for other         metrics, such as minkowski, Mahanalobis etc.      Returns     -------      mutual_reachaibility : array (n_samples, n_samples)         The pairwise mutual reachability distances between all points in `X`         with `label` equal to `cluster_id`.      core_distances : array (n_samples,)         The all-points-core_distance of all points in `X` with `label` equal         to `cluster_id`.      References     ----------     Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,     2014. Density-Based Clustering Validation. In SDM (pp. 839-847).
Compute the 'internal' minimum spanning tree given a matrix of mutual     reachability distances. Given a minimum spanning tree the 'internal'     graph is the subgraph induced by vertices of degree greater than one.      Parameters     ----------     mr_distances : array (cluster_size, cluster_size)         The pairwise mutual reachability distances, inferred to be the edge         weights of a complete graph. Since MSTs are computed per cluster         this is the all-points-mutual-reacability for points within a single         cluster.      Returns     -------     internal_nodes : array         An array listing the indices of the internal nodes of the MST      internal_edges : array (?, 3)         An array of internal edges in weighted edge list format; that is         an edge is an array of length three listing the two vertices         forming the edge and weight of the edge.      References     ----------     Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,     2014. Density-Based Clustering Validation. In SDM (pp. 839-847).
Compute the density separation between two clusters. This is the minimum     all-points mutual reachability distance between pairs of points, one from     internal nodes of MSTs of each cluster.      Parameters     ----------     X : array (n_samples, n_features) or (n_samples, n_samples)         The input data of the clustering. This can be the data, or, if         metric is set to `precomputed` the pairwise distance matrix used         for the clustering.      labels : array (n_samples)         The label array output by the clustering, providing an integral         cluster label to each data point, with -1 for noise points.      cluster_id1 : integer         The first cluster label to compute separation between.      cluster_id2 : integer         The second cluster label to compute separation between.      internal_nodes1 : array         The vertices of the MST for `cluster_id1` that were internal vertices.      internal_nodes2 : array         The vertices of the MST for `cluster_id2` that were internal vertices.      core_distances1 : array (size of cluster_id1,)         The all-points-core_distances of all points in the cluster         specified by cluster_id1.      core_distances2 : array (size of cluster_id2,)         The all-points-core_distances of all points in the cluster         specified by cluster_id2.      metric : string         The metric used to compute distances for the clustering (and         to be re-used in computing distances for mr distance). If         set to `precomputed` then X is assumed to be the precomputed         distance matrix between samples.      **kwd_args :         Extra arguments to pass to the distance computation for other         metrics, such as minkowski, Mahanalobis etc.      Returns     -------     The 'density separation' between the clusters specified by     `cluster_id1` and `cluster_id2`.      References     ----------     Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,     2014. Density-Based Clustering Validation. In SDM (pp. 839-847).
Compute the density based cluster validity index for the     clustering specified by `labels` and for each cluster in `labels`.      Parameters     ----------     X : array (n_samples, n_features) or (n_samples, n_samples)         The input data of the clustering. This can be the data, or, if         metric is set to `precomputed` the pairwise distance matrix used         for the clustering.      labels : array (n_samples)         The label array output by the clustering, providing an integral         cluster label to each data point, with -1 for noise points.      metric : optional, string (default 'euclidean')         The metric used to compute distances for the clustering (and         to be re-used in computing distances for mr distance). If         set to `precomputed` then X is assumed to be the precomputed         distance matrix between samples.      d : optional, integer (or None) (default None)         The number of features (dimension) of the dataset. This need only         be set in the case of metric being set to `precomputed`, where         the ambient dimension of the data is unknown to the function.      per_cluster_scores : optional, boolean (default False)         Whether to return the validity index for individual clusters.         Defaults to False with the function returning a single float         value for the whole clustering.      **kwd_args :         Extra arguments to pass to the distance computation for other         metrics, such as minkowski, Mahanalobis etc.      Returns     -------     validity_index : float         The density based cluster validity index for the clustering. This         is a numeric value between -1 and 1, with higher values indicating         a 'better' clustering.      per_cluster_validity_index : array (n_clusters,)         The cluster validity index of each individual cluster as an array.         The overall validity index is the weighted average of these values.         Only returned if per_cluster_scores is set to True.      References     ----------     Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and Sander, J.,     2014. Density-Based Clustering Validation. In SDM (pp. 839-847).
Perform robust single linkage clustering from a vector array     or distance matrix.      Parameters     ----------     X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \             array of shape (n_samples, n_samples)         A feature array, or array of distances between samples if         ``metric='precomputed'``.      cut : float         The reachability distance value to cut the cluster heirarchy at         to derive a flat cluster labelling.      k : int, optional (default=5)         Reachability distances will be computed with regard to the `k`         nearest neighbors.      alpha : float, optional (default=np.sqrt(2))         Distance scaling for reachability distance computation. Reachability         distance is computed as         $max \{ core_k(a), core_k(b), 1/\alpha d(a,b) \}$.      gamma : int, optional (default=5)         Ignore any clusters in the flat clustering with size less than gamma,         and declare points in such clusters as noise points.      metric : string, or callable, optional (default='euclidean')         The metric to use when calculating distance between instances in a         feature array. If metric is a string or callable, it must be one of         the options allowed by metrics.pairwise.pairwise_distances for its         metric parameter.         If metric is "precomputed", X is assumed to be a distance matrix and         must be square.      algorithm : string, optional (default='best')         Exactly which algorithm to use; hdbscan has variants specialised         for different characteristics of the data. By default this is set         to ``best`` which chooses the "best" algorithm given the nature of         the data. You can force other options if you believe you know         better. Options are:             * ``generic``             * ``best``             * ``prims_kdtree``             * ``prims_balltree``             * ``boruvka_kdtree``             * ``boruvka_balltree``      memory : Instance of joblib.Memory or string (optional)         Used to cache the output of the computation of the tree.         By default, no caching is done. If a string is given, it is the         path to the caching directory.      leaf_size : int, optional (default=40)         Leaf size for trees responsible for fast nearest         neighbour queries.      core_dist_n_jobs : int, optional         Number of parallel jobs to run in core distance computations (if         supported by the specific algorithm). For ``core_dist_n_jobs``         below -1, (n_cpus + 1 + core_dist_n_jobs) are used.         (default 4)      Returns     -------     labels : ndarray, shape (n_samples, )         Cluster labels for each point.  Noisy samples are given the label -1.      single_linkage_tree : ndarray, shape (n_samples - 1, 4)         The single linkage tree produced during clustering in scipy         hierarchical clustering format         (see http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).      References     ----------     .. [1] Chaudhuri, K., & Dasgupta, S. (2010). Rates of convergence for the        cluster tree. In Advances in Neural Information Processing Systems        (pp. 343-351).
Perform robust single linkage clustering from features or         distance matrix.          Parameters         ----------         X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \                 array of shape (n_samples, n_samples)             A feature array, or array of distances between samples if             ``metric='precomputed'``.          Returns         -------         self : object             Returns self
Find the nearest mutual reachability neighbor of a point, and  compute     the associated lambda value for the point, given the mutual reachability     distance to a nearest neighbor.      Parameters     ----------     neighbor_indices : array (2 * min_samples, )         An array of raw distance based nearest neighbor indices.      neighbor_distances : array (2 * min_samples, )         An array of raw distances to the nearest neighbors.      core_distances : array (n_samples, )         An array of core distances for all points      min_samples : int         The min_samples value used to generate core distances.      Returns     -------     neighbor : int         The index into the full raw data set of the nearest mutual reachability         distance neighbor of the point.      lambda_ : float         The lambda value at which this point joins/merges with `neighbor`.
Create a new condensed tree with an additional point added, allowing for     computations as if this point had been part of the original tree. Note     that this makes as little change to the tree as possible, with no     re-optimizing/re-condensing so that the selected clusters remain     effectively unchanged.      Parameters     ----------     tree : structured array         The raw format condensed tree to update.      neighbor_indices : array (2 * min_samples, )         An array of raw distance based nearest neighbor indices.      neighbor_distances : array (2 * min_samples, )         An array of raw distances to the nearest neighbors.      core_distances : array (n_samples, )         An array of core distances for all points      min_samples : int         The min_samples value used to generate core distances.      Returns     -------     new_tree : structured array         The original tree with an extra row providing the parent cluster         and lambda information for a new point given index -1.
Return the cluster label (of the original clustering) and membership     probability of a new data point.      Parameters     ----------     tree : CondensedTree         The condensed tree associated with the clustering.      cluster_tree : structured_array         The raw form of the condensed tree with only cluster information (no         data on individual points). This is significantly more compact.      neighbor_indices : array (2 * min_samples, )         An array of raw distance based nearest neighbor indices.      neighbor_distances : array (2 * min_samples, )         An array of raw distances to the nearest neighbors.      core_distances : array (n_samples, )         An array of core distances for all points      cluster_map : dict         A dictionary mapping cluster numbers in the condensed tree to labels         in the final selected clustering.      max_lambdas : dict         A dictionary mapping cluster numbers in the condensed tree to the         maximum lambda value seen in that cluster.      min_samples : int         The min_samples value used to generate core distances.
Predict the cluster label of new points. The returned labels     will be those of the original clustering found by ``clusterer``,     and therefore are not (necessarily) the cluster labels that would     be found by clustering the original data combined with     ``points_to_predict``, hence the 'approximate' label.      If you simply wish to assign new points to an existing clustering     in the 'best' way possible, this is the function to use. If you     want to predict how ``points_to_predict`` would cluster with     the original data under HDBSCAN the most efficient existing approach     is to simply recluster with the new point(s) added to the original dataset.      Parameters     ----------     clusterer : HDBSCAN         A clustering object that has been fit to the data and         either had ``prediction_data=True`` set, or called the         ``generate_prediction_data`` method after the fact.      points_to_predict : array, or array-like (n_samples, n_features)         The new data points to predict cluster labels for. They should         have the same dimensionality as the original dataset over which         clusterer was fit.      Returns     -------     labels : array (n_samples,)         The predicted labels of the ``points_to_predict``      probabilities : array (n_samples,)         The soft cluster scores for each of the ``points_to_predict``      See Also     --------     :py:func:`hdbscan.predict.membership_vector`     :py:func:`hdbscan.predict.all_points_membership_vectors`
Predict soft cluster membership. The result produces a vector     for each point in ``points_to_predict`` that gives a probability that     the given point is a member of a cluster for each of the selected clusters     of the ``clusterer``.      Parameters     ----------     clusterer : HDBSCAN         A clustering object that has been fit to the data and         either had ``prediction_data=True`` set, or called the         ``generate_prediction_data`` method after the fact.      points_to_predict : array, or array-like (n_samples, n_features)         The new data points to predict cluster labels for. They should         have the same dimensionality as the original dataset over which         clusterer was fit.      Returns     -------     membership_vectors : array (n_samples, n_clusters)         The probability that point ``i`` is a member of cluster ``j`` is         in ``membership_vectors[i, j]``.      See Also     --------     :py:func:`hdbscan.predict.predict`     :py:func:`hdbscan.predict.all_points_membership_vectors`
Predict soft cluster membership vectors for all points in the     original dataset the clusterer was trained on. This function is more     efficient by making use of the fact that all points are already in the     condensed tree, and processing in bulk.      Parameters     ----------     clusterer : HDBSCAN          A clustering object that has been fit to the data and         either had ``prediction_data=True`` set, or called the         ``generate_prediction_data`` method after the fact.         This method does not work if the clusterer was trained         with ``metric='precomputed'``.      Returns     -------     membership_vectors : array (n_samples, n_clusters)         The probability that point ``i`` of the original dataset is a member of         cluster ``j`` is in ``membership_vectors[i, j]``.      See Also     --------     :py:func:`hdbscan.predict.predict`     :py:func:`hdbscan.predict.all_points_membership_vectors`
very very hardcoded/dirty re/split stuff, but no dependencies
very very hardcoded/dirty re/split stuff, but no dependencies
Filter cell outliers based on counts and numbers of genes expressed.      For instance, only keep cells with at least `min_counts` counts or     `min_genes` genes expressed. This is to filter measurement outliers,     i.e. “unreliable” observations.      Only provide one of the optional parameters ``min_counts``, ``min_genes``,     ``max_counts``, ``max_genes`` per call.      Parameters     ----------     data         The (annotated) data matrix of shape ``n_obs`` × ``n_vars``.         Rows correspond to cells and columns to genes.     min_counts         Minimum number of counts required for a cell to pass filtering.     min_genes         Minimum number of genes expressed required for a cell to pass filtering.     max_counts         Maximum number of counts required for a cell to pass filtering.     max_genes         Maximum number of genes expressed required for a cell to pass filtering.     inplace         Perform computation inplace or return result.      Returns     -------     Depending on ``inplace``, returns the following arrays or directly subsets     and annotates the data matrix:      cells_subset : numpy.ndarray         Boolean index mask that does filtering. ``True`` means that the         cell is kept. ``False`` means the cell is removed.     number_per_cell : numpy.ndarray         Depending on what was tresholded (``counts`` or ``genes``), the array stores         ``n_counts`` or ``n_cells`` per gene.      Examples     --------     >>> adata = sc.datasets.krumsiek11()     >>> adata.n_obs     640     >>> adata.var_names     ['Gata2' 'Gata1' 'Fog1' 'EKLF' 'Fli1' 'SCL' 'Cebpa'      'Pu.1' 'cJun' 'EgrNab' 'Gfi1']     >>> # add some true zeros     >>> adata.X[adata.X < 0.3] = 0     >>> # simply compute the number of genes per cell     >>> sc.pp.filter_cells(adata, min_genes=0)     >>> adata.n_obs     640     >>> adata.obs['n_genes'].min()     1     >>> # filter manually     >>> adata_copy = adata[adata.obs['n_genes'] >= 3]     >>> adata_copy.obs['n_genes'].min()     >>> adata.n_obs     554     >>> adata.obs['n_genes'].min()     3     >>> # actually do some filtering     >>> sc.pp.filter_cells(adata, min_genes=3)     >>> adata.n_obs     554     >>> adata.obs['n_genes'].min()     3
Filter genes based on number of cells or counts.      Keep genes that have at least ``min_counts`` counts or are expressed in at     least ``min_cells`` cells or have at most ``max_counts`` counts or are expressed     in at most ``max_cells`` cells.      Only provide one of the optional parameters ``min_counts``, ``min_cells``,     ``max_counts``, ``max_cells`` per call.      Parameters     ----------     data         An annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond         to cells and columns to genes.     min_counts         Minimum number of counts required for a gene to pass filtering.     min_cells         Minimum number of cells expressed required for a gene to pass filtering.     max_counts         Maximum number of counts required for a gene to pass filtering.     max_cells         Maximum number of cells expressed required for a gene to pass filtering.     inplace         Perform computation inplace or return result.      Returns     -------     Depending on `inplace`, returns the following arrays or directly subsets     and annotates the data matrix      gene_subset : numpy.ndarray         Boolean index mask that does filtering. `True` means that the         gene is kept. `False` means the gene is removed.     number_per_gene : numpy.ndarray         Depending on what was tresholded (`counts` or `cells`), the array stores         `n_counts` or `n_cells` per gene.
Logarithmize the data matrix.      Computes :math:`X = \\log(X + 1)`, where :math:`log` denotes the natural logarithm.      Parameters     ----------     data         The (annotated) data matrix of shape ``n_obs`` × ``n_vars``.         Rows correspond to cells and columns to genes.     copy         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned.     chunked         Process the data matrix in chunks, which will save memory.         Applies only to :class:`~anndata.AnnData`.     chunk_size         ``n_obs`` of the chunks to process the data in.      Returns     -------     Returns or updates ``data``, depending on ``copy``.
Square root the data matrix.      Computes :math:`X = \\sqrt(X)`.      Parameters     ----------     data         The (annotated) data matrix of shape ``n_obs`` × ``n_vars``.         Rows correspond to cells and columns to genes.     copy         If an :class:`~scanpy.api.AnnData` is passed,         determines whether a copy is returned.     chunked         Process the data matrix in chunks, which will save memory.         Applies only to :class:`~anndata.AnnData`.     chunk_size         ``n_obs`` of the chunks to process the data in.      Returns     -------     Returns or updates `data`, depending on `copy`.
Principal component analysis [Pedregosa11]_.      Computes PCA coordinates, loadings and variance decomposition. Uses the     implementation of *scikit-learn* [Pedregosa11]_.      Parameters     ----------     data         The (annotated) data matrix of shape ``n_obs`` × ``n_vars``.         Rows correspond to cells and columns to genes.     n_comps         Number of principal components to compute.     zero_center         If `True`, compute standard PCA from covariance matrix.         If ``False``, omit zero-centering variables         (uses :class:`~sklearn.decomposition.TruncatedSVD`),         which allows to handle sparse input efficiently.         Passing ``None`` decides automatically based on sparseness of the data.     svd_solver         SVD solver to use:          ``'arpack'``           for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`)          ``'randomized'``           for the randomized algorithm due to Halko (2009).          ``'auto'`` (the default)           chooses automatically depending on the size of the problem.      random_state         Change to use different initial states for the optimization.     return_info         Only relevant when not passing an :class:`~anndata.AnnData`:         see “**Returns**”.     use_highly_variable         Whether to use highly variable genes only, stored in         ``.var['highly_variable']``.         By default uses them if they have been determined beforehand.     dtype         Numpy data type string to which to convert the result.     copy         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned. Is ignored otherwise.     chunked         If ``True``, perform an incremental PCA on segments of ``chunk_size``.         The incremental PCA automatically zero centers and ignores settings of         ``random_seed`` and ``svd_solver``. If ``False``, perform a full PCA.     chunk_size         Number of observations to include in each chunk.         Required if ``chunked=True`` was passed.      Returns     -------     X_pca : :class:`scipy.sparse.spmatrix` or :class:`numpy.ndarray`         If `data` is array-like and ``return_info=False`` was passed,         this function only returns `X_pca`…     adata : anndata.AnnData         …otherwise if ``copy=True`` it returns or else adds fields to ``adata``:          ``.obsm['X_pca']``              PCA representation of data.          ``.varm['PCs']``              The principal components containing the loadings.          ``.uns['pca']['variance_ratio']``)              Ratio of explained variance.          ``.uns['pca']['variance']``              Explained variance, equivalent to the eigenvalues of the covariance matrix.
Normalize total counts per cell.      .. warning::         .. deprecated:: 1.3.7             Use :func:`~scanpy.api.pp.normalize_total` instead.             The new function is equivalent to the present             function, except that              * the new function doesn't filter cells based on `min_counts`,               use :func:`~scanpy.api.pp.filter_cells` if filtering is needed.             * some arguments were renamed             * `copy` is replaced by `inplace`      Normalize each cell by total counts over all genes, so that every cell has     the same total count after normalization.      Similar functions are used, for example, by Seurat [Satija15]_, Cell Ranger     [Zheng17]_ or SPRING [Weinreb17]_.      Parameters     ----------     data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`         The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond         to cells and columns to genes.     counts_per_cell_after : `float` or `None`, optional (default: `None`)         If `None`, after normalization, each cell has a total count equal         to the median of the *counts_per_cell* before normalization.     counts_per_cell : `np.array`, optional (default: `None`)         Precomputed counts per cell.     key_n_counts : `str`, optional (default: `'n_counts'`)         Name of the field in `adata.obs` where the total counts per cell are         stored.     copy : `bool`, optional (default: `False`)         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned.     min_counts : `int`, optional (default: 1)         Cells with counts less than `min_counts` are filtered out during         normalization.      Returns     -------     Returns or updates `adata` with normalized version of the original     `adata.X`, depending on `copy`.      Examples     --------     >>> adata = AnnData(     >>>     data=np.array([[1, 0], [3, 0], [5, 6]]))     >>> print(adata.X.sum(axis=1))     [  1.   3.  11.]     >>> sc.pp.normalize_per_cell(adata)     >>> print(adata.obs)     >>> print(adata.X.sum(axis=1))        n_counts     0       1.0     1       3.0     2      11.0     [ 3.  3.  3.]     >>> sc.pp.normalize_per_cell(adata, counts_per_cell_after=1,     >>>                          key_n_counts='n_counts2')     >>> print(adata.obs)     >>> print(adata.X.sum(axis=1))        n_counts  n_counts2     0       1.0        3.0     1       3.0        3.0     2      11.0        3.0     [ 1.  1.  1.]
Normalize each cell [Weinreb17]_.      This is a deprecated version. See `normalize_per_cell` instead.      Normalize each cell by UMI count, so that every cell has the same total     count.      Parameters     ----------     X : np.ndarray         Expression matrix. Rows correspond to cells and columns to genes.     max_fraction : float, optional         Only use genes that make up more than max_fraction of the total         reads in every cell.     mult_with_mean: bool, optional         Multiply the result with the mean of total counts.      Returns     -------     Normalized version of the original expression matrix.
Regress out unwanted sources of variation.      Uses simple linear regression. This is inspired by Seurat's `regressOut`     function in R [Satija15].      Parameters     ----------     adata : :class:`~anndata.AnnData`         The annotated data matrix.     keys : `str` or list of `str`         Keys for observation annotation on which to regress on.     n_jobs : `int` or `None`, optional. If None is given, then the n_jobs seting is used (default: `None`)         Number of jobs for parallel computation.     copy : `bool`, optional (default: `False`)         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned.      Returns     -------     Depending on `copy` returns or updates `adata` with the corrected data matrix.
Scale data to unit variance and zero mean.      .. note::         Variables (genes) that do not display any variation (are constant across         all observations) are retained and set to 0 during this operation. In         the future, they might be set to NaNs.      Parameters     ----------     data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`         The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond         to cells and columns to genes.     zero_center : `bool`, optional (default: `True`)         If `False`, omit zero-centering variables, which allows to handle sparse         input efficiently.     max_value : `float` or `None`, optional (default: `None`)         Clip (truncate) to this value after scaling. If `None`, do not clip.     copy : `bool`, optional (default: `False`)         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned.      Returns     -------     Depending on `copy` returns or updates `adata` with a scaled `adata.X`.
Subsample to a fraction of the number of observations.      Parameters     ----------     data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`         The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond         to cells and columns to genes.     fraction : `float` in [0, 1] or `None`, optional (default: `None`)         Subsample to this `fraction` of the number of observations.     n_obs : `int` or `None`, optional (default: `None`)         Subsample to this number of observations.     random_state : `int` or `None`, optional (default: 0)         Random seed to change subsampling.     copy : `bool`, optional (default: `False`)         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned.      Returns     -------     Returns `X[obs_indices], obs_indices` if data is array-like, otherwise     subsamples the passed :class:`~anndata.AnnData` (`copy == False`) or     returns a subsampled copy of it (`copy == True`).
Downsample counts from count matrix.      If `counts_per_cell` is specified, each cell will downsampled. If     `total_counts` is specified, expression matrix will be downsampled to     contain at most `total_counts`.      Parameters     ----------     adata         Annotated data matrix.     counts_per_cell         Target total counts per cell. If a cell has more than 'counts_per_cell',         it will be downsampled to this number. Resulting counts can be specified         on a per cell basis by passing an array.Should be an integer or integer         ndarray with same length as number of obs.     total_counts         Target total counts. If the count matrix has more than `total_counts`         it will be downsampled to have this number.     random_state         Random seed for subsampling.     replace         Whether to sample the counts with replacement.     copy         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned.      Returns     -------     Depending on `copy` returns or updates an `adata` with downsampled `.X`.
Evenly reduce counts in cell to target amount.      This is an internal function and has some restrictions:      * `dtype` of col must be an integer (i.e. satisfy issubclass(col.dtype.type, np.integer))     * total counts in cell must be less than target
Z-score standardize each variable/gene in X.      Use `scale` instead.      Reference: Weinreb et al. (2017).      Parameters     ----------     X         Data matrix. Rows correspond to cells and columns to genes.      Returns     -------     Z-score standardized version of the data matrix.
Format time in seconds.      Parameters     ----------     t : int         Time in seconds.
Embed the neighborhood graph using UMAP [McInnes18]_.      UMAP (Uniform Manifold Approximation and Projection) is a manifold learning     technique suitable for visualizing high-dimensional data. Besides tending to     be faster than tSNE, it optimizes the embedding such that it best reflects     the topology of the data, which we represent throughout Scanpy using a     neighborhood graph. tSNE, by contrast, optimizes the distribution of     nearest-neighbor distances in the embedding such that these best match the     distribution of distances in the high-dimensional space.  We use the     implementation of `umap-learn <https://github.com/lmcinnes/umap>`__     [McInnes18]_. For a few comparisons of UMAP with tSNE, see this `preprint     <https://doi.org/10.1101/298430>`__.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     min_dist : `float`, optional (default: 0.5)         The effective minimum distance between embedded points. Smaller values         will result in a more clustered/clumped embedding where nearby points on         the manifold are drawn closer together, while larger values will result         on a more even dispersal of points. The value should be set relative to         the ``spread`` value, which determines the scale at which embedded         points will be spread out. The default of in the `umap-learn` package is         0.1.     spread : `float` (optional, default 1.0)         The effective scale of embedded points. In combination with `min_dist`         this determines how clustered/clumped the embedded points are.     n_components : `int`, optional (default: 2)         The number of dimensions of the embedding.     maxiter : `int`, optional (default: `None`)         The number of iterations (epochs) of the optimization. Called `n_epochs`         in the original UMAP.     alpha : `float`, optional (default: 1.0)         The initial learning rate for the embedding optimization.     gamma : `float` (optional, default 1.0)         Weighting applied to negative samples in low dimensional embedding         optimization. Values higher than one will result in greater weight         being given to negative samples.     negative_sample_rate : `int` (optional, default 5)         The number of negative edge/1-simplex samples to use per positive         edge/1-simplex sample in optimizing the low dimensional embedding.     init_pos : `string` or `np.array`, optional (default: 'spectral')         How to initialize the low dimensional embedding. Called `init` in the         original UMAP.         Options are:          * Any key for `adata.obsm`.         * 'paga': positions from :func:`~scanpy.api.pl.paga`.         * 'spectral': use a spectral embedding of the graph.         * 'random': assign initial embedding positions at random.         * A numpy array of initial embedding positions.     random_state : `int`, `RandomState` or `None`, optional (default: 0)         If `int`, `random_state` is the seed used by the random number generator;         If `RandomState`, `random_state` is the random number generator;         If `None`, the random number generator is the `RandomState` instance used         by `np.random`.     a : `float` (optional, default `None`)         More specific parameters controlling the embedding. If `None` these         values are set automatically as determined by `min_dist` and         `spread`.     b : `float` (optional, default `None`)         More specific parameters controlling the embedding. If `None` these         values are set automatically as determined by `min_dist` and         `spread`.     copy : `bool` (default: `False`)         Return a copy instead of writing to adata.      Returns     -------     Depending on `copy`, returns or updates `adata` with the following fields.      **X_umap** : `adata.obsm` field         UMAP coordinates of data.
Generate pairs of genes [Scialdone15]_ [Fechtner18]_.      Calculates the pairs of genes serving as marker pairs for each phase,     based on a matrix of gene counts and an annotation of known phases.      This reproduces the approach of [Scialdone15]_ in the implementation of     [Fechtner18]_.      More information and bug reports `here     <https://github.com/rfechtner/pypairs>`__.      Parameters     ----------     adata : :class:`~anndata.AnnData`         The annotated data matrix.     categories : `dict`         Dictionary of lists, i.e. {phase: [sample, ...]},         containing annotation of samples to their phase     gene_names: `list`         List of genes.     sample_names: `list`         List of samples.     fraction : `float`, optional (default: 0.5)         Fraction to be used as threshold.     filter_genes : `list` or `None`, optional (default: `None`)         Genes for sampling the reference set. Default is all genes.     filter_samples : `list` or `None`, optional (default: `None`)         Cells for sampling the reference set. Default is all samples.      Returns     -------     `dict` of `list` of `tuple`, i.e.     {phase: [(Gene1, Gene2), ...]},     containing marker pairs per phase
Assigns scores and predicted class to observations [Scialdone15]_ [Fechtner18]_.      Calculates scores for each observation and each phase and assigns prediction     based on marker pairs indentified by sandbag.      This reproduces the approach of [Scialdone15]_ in the implementation of     [Fechtner18]_.      Parameters     ----------     adata : :class:`~anndata.AnnData`         The annotated data matrix.     marker_pairs : `dict`         Dictionary of marker pairs. See :func:`~scanpy.api.sandbag` output.     gene_names: `list`         List of genes.     sample_names: `list`         List of samples.     iterations : `int`, optional (default: 1000)         An integer scalar specifying the number of         iterations for random sampling to obtain a cycle score.     min_iter : `int`, optional (default: 100)         An integer scalar specifying the minimum number of iterations         for score estimation     min_pairs : `int`, optional (default: 50)         An integer scalar specifying the minimum number of iterations         for score estimation      Returns     -------     A :class:`~pandas.DataFrame` with samples as index and categories as columns with scores for each category for each     sample and a additional column with the name of the max scoring category for each sample.      If marker pairs contain only the cell cycle categories G1, S and G2M an additional column     ``pypairs_cc_prediction`` will be added. Where category S is assigned to samples where G1 and G2M score are     below 0.5.
Deep count autoencoder [Eraslan18]_.      Fits a count autoencoder to the raw count data given in the anndata object     in order to denoise the data and to capture hidden representation of     cells in low dimensions. Type of the autoencoder and return values are     determined by the parameters.          .. note::         More information and bug reports `here <https://github.com/theislab/dca>`__.      Parameters     ----------     adata : :class:`~anndata.AnnData`         An anndata file with `.raw` attribute representing raw counts.     mode : `str`, optional. `denoise`(default), or `latent`.         `denoise` overwrites `adata.X` with denoised expression values.         In `latent` mode DCA adds `adata.obsm['X_dca']` to given adata         object. This matrix represent latent representation of cells via DCA.     ae_type : `str`, optional. `zinb-conddisp`(default), `zinb`, `nb-conddisp` or `nb`.         Type of the autoencoder. Return values and the architecture is         determined by the type e.g. `nb` does not provide dropout         probabilities. Types that end with "-conddisp", assumes that dispersion is mean dependant.     normalize_per_cell : `bool`, optional. Default: `True`.         If true, library size normalization is performed using         the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata         object. Mean layer is re-introduces library size differences by         scaling the mean value of each cell in the output layer. See the         manuscript for more details.     scale : `bool`, optional. Default: `True`.         If true, the input of the autoencoder is centered using         `sc.pp.scale` function of Scanpy. Note that the output is kept as raw         counts as loss functions are designed for the count data.     log1p : `bool`, optional. Default: `True`.         If true, the input of the autoencoder is log transformed with a         pseudocount of one using `sc.pp.log1p` function of Scanpy.     hidden_size : `tuple` or `list`, optional. Default: (64, 32, 64).         Width of hidden layers.     hidden_dropout : `float`, `tuple` or `list`, optional. Default: 0.0.         Probability of weight dropout in the autoencoder (per layer if list         or tuple).     batchnorm : `bool`, optional. Default: `True`.         If true, batch normalization is performed.     activation : `str`, optional. Default: `relu`.         Activation function of hidden layers.     init : `str`, optional. Default: `glorot_uniform`.         Initialization method used to initialize weights.     network_kwds : `dict`, optional.         Additional keyword arguments for the autoencoder.     epochs : `int`, optional. Default: 300.         Number of total epochs in training.     reduce_lr : `int`, optional. Default: 10.         Reduces learning rate if validation loss does not improve in given number of epochs.     early_stop : `int`, optional. Default: 15.         Stops training if validation loss does not improve in given number of epochs.     batch_size : `int`, optional. Default: 32.         Number of samples in the batch used for SGD.     optimizer : `str`, optional. Default: "rmsprop".         Type of optimization method used for training.     random_state : `int`, optional. Default: 0.         Seed for python, numpy and tensorflow.     threads : `int` or None, optional. Default: None         Number of threads to use in training. All cores are used by default.     verbose : `bool`, optional. Default: `False`.         If true, prints additional information about training and architecture.     training_kwds : `dict`, optional.         Additional keyword arguments for the training process.     return_model : `bool`, optional. Default: `False`.         If true, trained autoencoder object is returned. See "Returns".     return_info : `bool`, optional. Default: `False`.         If true, all additional parameters of DCA are stored in `adata.obsm` such as dropout         probabilities (obsm['X_dca_dropout']) and estimated dispersion values         (obsm['X_dca_dispersion']), in case that autoencoder is of type         zinb or zinb-conddisp.     copy : `bool`, optional. Default: `False`.         If true, a copy of anndata is returned.      Returns     -------     If `copy` is true and `return_model` is false, AnnData object is returned.      In "denoise" mode, `adata.X` is overwritten with the denoised values. In "latent" mode, latent\     low dimensional representation of cells are stored in `adata.obsm['X_dca']` and `adata.X`\     is not modified. Note that these values are not corrected for library size effects.      If `return_info` is true, all estimated distribution parameters are stored in AnnData such as:      - `.obsm["X_dca_dropout"]` which is the mixture coefficient (pi) of the zero component\     in ZINB, i.e. dropout probability (only if `ae_type` is `zinb` or `zinb-conddisp`).      - `.obsm["X_dca_dispersion"]` which is the dispersion parameter of NB.      - `.uns["dca_loss_history"]` which stores the loss history of the training. See `.history`\     attribute of Keras History class for mode details.      Finally, the raw counts are stored in `.raw` attribute of AnnData object.      If `return_model` is given, trained model is returned. When both `copy` and `return_model`\     are true, a tuple of anndata and model is returned in that order.
Score a set of genes [Satija15]_.      The score is the average expression of a set of genes subtracted with the     average expression of a reference set of genes. The reference set is     randomly sampled from the `gene_pool` for each binned expression value.      This reproduces the approach in Seurat [Satija15]_ and has been implemented     for Scanpy by Davide Cittaro.      Parameters     ----------     adata : :class:`~anndata.AnnData`         The annotated data matrix.     gene_list : iterable         The list of gene names used for score calculation.     ctrl_size : `int`, optional (default: 50)         Number of reference genes to be sampled. If `len(gene_list)` is not too         low, you can set `ctrl_size=len(gene_list)`.     gene_pool : `list` or `None`, optional (default: `None`)         Genes for sampling the reference set. Default is all genes.     n_bins : `int`, optional (default: 25)         Number of expression level bins for sampling.     score_name : `str`, optional (default: `'score'`)         Name of the field to be added in `.obs`.     random_state : `int`, optional (default: 0)         The random seed for sampling.     copy : `bool`, optional (default: `False`)         Copy `adata` or modify it inplace.     use_raw : `bool`, optional (default: `False`)         Use `raw` attribute of `adata` if present.     Returns     -------     Depending on `copy`, returns or updates `adata` with an additional field     `score_name`.      Examples     --------     See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.
Score cell cycle genes [Satija15]_.      Given two lists of genes associated to S phase and G2M phase, calculates     scores and assigns a cell cycle phase (G1, S or G2M). See     :func:`~scanpy.api.score_genes` for more explanation.      Parameters     ----------     adata : :class:`~anndata.AnnData`         The annotated data matrix.     s_genes : `list`         List of genes associated with S phase.     g2m_genes : `list`         List of genes associated with G2M phase.     copy : `bool`, optional (default: `False`)         Copy `adata` or modify it inplace.     **kwargs : optional keyword arguments         Are passed to :func:`~scanpy.api.score_genes`. `ctrl_size` is not         possible, as it's set as `min(len(s_genes), len(g2m_genes))`.      Returns     -------     Depending on `copy`, returns or updates `adata` with the following fields.      **S_score** : `adata.obs`, dtype `object`         The score for S phase for each cell.     **G2M_score** : `adata.obs`, dtype `object`         The score for G2M phase for each cell.     **phase** : `adata.obs`, dtype `object`         The cell cycle phase (`S`, `G2M` or `G1`) for each cell.      See also     --------     score_genes      Examples     --------     See this `notebook <https://github.com/theislab/scanpy_usage/tree/master/180209_cell_cycle>`__.
Markov Affinity-based Graph Imputation of Cells (MAGIC) API [vanDijk18]_.      MAGIC is an algorithm for denoising and transcript recover of single cells     applied to single-cell sequencing data. MAGIC builds a graph from the data     and uses diffusion to smooth out noise and recover the data manifold.      More information and bug reports     `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit     <https://krishnaswamylab.org/get-help>.      Parameters     ----------     adata : :class:`~scanpy.api.AnnData`         An anndata file with `.raw` attribute representing raw counts.     name_list : `list`, `'all_genes'`, or `'pca_only'`, optional (default: `'all_genes'`)         Denoised genes to return. Default is all genes, but this         may require a large amount of memory if the input data is sparse.     k : int, optional, default: 10         number of nearest neighbors on which to build kernel     a : int, optional, default: 15         sets decay rate of kernel tails.         If None, alpha decaying kernel is not used     t : int, optional, default: 'auto'         power to which the diffusion operator is powered.         This sets the level of diffusion. If 'auto', t is selected         according to the Procrustes disparity of the diffused data     n_pca : int, optional, default: 100         Number of principal components to use for calculating         neighborhoods. For extremely large datasets, using         n_pca < 20 allows neighborhoods to be calculated in         roughly log(n_samples) time.     knn_dist : string, optional, default: 'euclidean'         recommended values: 'euclidean', 'cosine', 'precomputed'         Any metric from `scipy.spatial.distance` can be used         distance metric for building kNN graph. If 'precomputed',         `data` should be an n_samples x n_samples distance or         affinity matrix     random_state : `int`, `numpy.RandomState` or `None`, optional (default: `None`)         Random seed. Defaults to the global `numpy` random number generator     n_jobs : `int` or None, optional. Default: None         Number of threads to use in training. All cores are used by default.     verbose : `bool`, `int` or `None`, optional (default: `sc.settings.verbosity`)         If `True` or an integer `>= 2`, print status messages.         If `None`, `sc.settings.verbosity` is used.     copy : `bool` or `None`, optional. Default: `None`.         If true, a copy of anndata is returned. If `None`, `copy` is True if         `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False         if `genes` is `'all_genes'` or `'pca_only'`, as the resultant data         will otherwise have different column names from the input data.     kwargs : additional arguments to `magic.MAGIC`      Returns     -------     If `copy` is True, AnnData object is returned.      If `subset_genes` is not `all_genes`, PCA on MAGIC values of cells are stored in     `adata.obsm['X_magic']` and `adata.X` is not modified.      The raw counts are stored in `.raw` attribute of AnnData object.      Examples     --------     >>> import scanpy.api as sc     >>> import magic     >>> adata = sc.datasets.paul15()     >>> sc.pp.normalize_per_cell(adata)     >>> sc.pp.sqrt(adata)  # or sc.pp.log1p(adata)     >>> adata_magic = sc.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], k=5)     >>> adata_magic.shape     (2730, 3)     >>> sc.pp.magic(adata, name_list='pca_only', k=5)     >>> adata.obsm['X_magic'].shape     (2730, 100)     >>> sc.pp.magic(adata, name_list='all_genes', k=5)     >>> adata.X.shape     (2730, 3451)
Mapping out the coarse-grained connectivity structures of complex manifolds [Wolf19]_.      By quantifying the connectivity of partitions (groups, clusters) of the     single-cell graph, partition-based graph abstraction (PAGA) generates a much     simpler abstracted graph (*PAGA graph*) of partitions, in which edge weights     represent confidence in the presence of connections. By tresholding this     confidence in :func:`~scanpy.pl.paga`, a much simpler representation of the     manifold data is obtained, which is nonetheless faithful to the topology of     the manifold.      The confidence should be interpreted as the ratio of the actual versus the     expected value of connetions under the null model of randomly connecting     partitions. We do not provide a p-value as this null model does not     precisely capture what one would consider "connected" in real data, hence it     strongly overestimates the expected value. See an extensive discussion of     this in [Wolf19]_.      .. note::         Note that you can use the result of :func:`~scanpy.pl.paga` in         :func:`~scanpy.tl.umap` and :func:`~scanpy.tl.draw_graph` via         `init_pos='paga'` to get single-cell embeddings that are typically more         faithful to the global topology.      Parameters     ----------     adata : :class:`~anndata.AnnData`         An annotated data matrix.     groups : key for categorical in `adata.obs`, optional (default: 'louvain')         You can pass your predefined groups by choosing any categorical         annotation of observations (`adata.obs`).     use_rna_velocity : `bool` (default: `False`)         Use RNA velocity to orient edges in the abstracted graph and estimate         transitions. Requires that `adata.uns` contains a directed single-cell         graph with key `['velocity_graph']`. This feature might be subject         to change in the future.     model : {'v1.2', 'v1.0'}, optional (default: 'v1.2')         The PAGA connectivity model.     copy : `bool`, optional (default: `False`)         Copy `adata` before computation and return a copy. Otherwise, perform         computation inplace and return `None`.      Returns     -------     **connectivities** : :class:`numpy.ndarray` (adata.uns['connectivities'])         The full adjacency matrix of the abstracted graph, weights correspond to         confidence in the connectivities of partitions.     **connectivities_tree** : :class:`scipy.sparse.csr_matrix` (adata.uns['connectivities_tree'])         The adjacency matrix of the tree-like subgraph that best explains         the topology.      Notes     -----     Together with a random walk-based distance measure     (e.g. :func:`scanpy.tl.dpt`) this generates a partial coordinatization of     data useful for exploring and explaining its variation.      See Also     --------     pl.paga     pl.paga_path     pl.paga_compare
Compute the degree of each node in the abstracted graph.      Parameters     ----------     adata : AnnData         Annotated data matrix.      Returns     -------     List of degrees for each node.
Compute the median expression entropy for each node-group.      Parameters     ----------     adata : AnnData         Annotated data matrix.      Returns     -------     Entropies of median expressions for each node.
Compare paths in abstracted graphs in two datasets.      Compute the fraction of consistent paths between leafs, a measure for the     topological similarity between graphs.      By increasing the verbosity to level 4 and 5, the paths that do not agree     and the paths that agree are written to the output, respectively.      The PAGA "groups key" needs to be the same in both objects.      Parameters     ----------     adata1, adata2 : AnnData         Annotated data matrices to compare.     adjacency_key : str         Key for indexing the adjacency matrices in `.uns['paga']` to be used in         adata1 and adata2.     adjacency_key2 : str, None         If provided, used for adata2.       Returns     -------     OrderedTuple with attributes ``n_steps`` (total number of steps in paths)     and ``frac_steps`` (fraction of consistent steps), ``n_paths`` and     ``frac_paths``.
Function to calculate the density of cells in an embedding.
Calculate the density of cells in an embedding (per condition)      Gaussian kernel density estimation is used to calculate the density of     cells in an embedded space. This can be performed per category over a     categorical cell annotation. The cell density can be plotted using the      `sc.pl.embedding_density()` function.      Note that density values are scaled to be between 0 and 1. Thus, the     density value at each cell is only comparable to other densities in      the same condition category.      This function was written by Sophie Tritschler and implemented into     Scanpy by Malte Luecken.          Parameters     ----------     adata         The annotated data matrix.     basis         The embedding over which the density will be calculated. This embedded         representation should be found in `adata.obsm['X_[basis]']``.     groupby         Keys for categorical observation/cell annotation for which densities         are calculated per category. Columns with up to ten categories are         accepted.     key_added         Name of the `.obs` covariate that will be added with the density         estimates.     components         The embedding dimensions over which the density should be calculated.         This is limited to two components.      Returns     -------     Updates `adata.obs` with an additional field specified by the `key_added`     parameter. This parameter defaults to `[basis]_density_[groupby]`, where     where `[basis]` is one of `umap`, `diffmap`, `pca`, `tsne`, or `draw_graph_fa`     and `[groupby]` denotes the parameter input.     Updates `adata.uns` with an additional field `[key_added]_params`.      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.tl.umap(adata)     >>> sc.tl.embedding_density(adata, basis='umap', groupby='phase')     >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase',      ...                         group='G1')     >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase',      ...                         group='S')
Correct batch effects by matching mutual nearest neighbors [Haghverdi18]_ [Kang18]_.      This uses the implementation of `mnnpy     <https://github.com/chriscainx/mnnpy>`__ [Kang18]_.      Depending on `do_concatenate`, returns matrices or `AnnData` objects in the     original order containing corrected expression values or a concatenated     matrix or AnnData object.      Be reminded that it is not advised to use the corrected data matrices for     differential expression testing.      More information and bug reports `here <https://github.com/chriscainx/mnnpy>`__.      Parameters     ----------     datas : `numpy.ndarray` or :class:`~anndata.AnnData`         Expression matrices or AnnData objects. Matrices should be shaped like         n_obs * n_vars (n_cell * n_gene) and have consistent number of         columns. AnnData objects should have same number of variables.     var_index : `list` or `None`, optional (default: None)         The index (list of str) of vars (genes). Necessary when using only a         subset of vars to perform MNN correction, and should be supplied with         var_subset. When datas are AnnData objects, var_index is ignored.     var_subset : `list` or `None`, optional (default: None)         The subset of vars (list of str) to be used when performing MNN         correction. Typically, a list of highly variable genes (HVGs). When set         to None, uses all vars.     batch_key : `str`, optional (default: 'batch')         The batch_key for AnnData.concatenate. Only valid when do_concatenate         and supplying AnnData objects.     index_unique : `str`, optional (default: '-')         The index_unique for AnnData.concatenate. Only valid when do_concatenate         and supplying AnnData objects.     batch_categories : `list` or `None`, optional (default: None)         The batch_categories for AnnData.concatenate. Only valid when         do_concatenate and supplying AnnData objects.     k : `int`, optional (default: 20)         Number of mutual nearest neighbors.     sigma : `float`, optional (default: 1)         The bandwidth of the Gaussian smoothing kernel used to compute the         correction vectors. Default is 1.     cos_norm_in : `bool`, optional (default: True)         Whether cosine normalization should be performed on the input data prior         to calculating distances between cells.     cos_norm_out : `bool`, optional (default: True)         Whether cosine normalization should be performed prior to computing corrected expression values.     svd_dim : `int` or `None`, optional (default: None)         The number of dimensions to use for summarizing biological substructure         within each batch. If None, biological components will not be removed         from the correction vectors.     var_adj : `bool`, optional (default: True)         Whether to adjust variance of the correction vectors. Note this step         takes most computing time.     compute_angle : `bool`, optional (default: False)         Whether to compute the angle between each cell’s correction vector and         the biological subspace of the reference batch.     mnn_order : `list` or `None`, optional (default: None)         The order in which batches are to be corrected. When set to None, datas         are corrected sequentially.     svd_mode : `str`, optional (default: 'rsvd')         One of 'svd', 'rsvd', and 'irlb'. 'svd' computes SVD using a         non-randomized SVD-via-ID algorithm, while 'rsvd' uses a randomized         version. 'irlb' performes truncated SVD by implicitly restarted Lanczos         bidiagonalization (forked from https://github.com/airysen/irlbpy).     do_concatenate : `bool`, optional (default: True)         Whether to concatenate the corrected matrices or AnnData objects. Default is True.     save_raw : `bool`, optional (default: False)         Whether to save the original expression data in the .raw attribute of AnnData objects.     n_jobs : `int` or `None`, optional (default: None)         The number of jobs. When set to None, automatically uses the number of cores.     kwargs : `dict` or `None`, optional (default: None)         optional keyword arguments for irlb.      Returns     -------     **datas** : :class:`~numpy.ndarray` or :class:`~anndata.AnnData`         Corrected matrix/matrices or AnnData object/objects, depending on the         input type and `do_concatenate`.     **mnn_list** : ``List[pandas.DataFrame]``         A list containing MNN pairing information as DataFrames in each iteration step.     **angle_list** : ``List[Tuple[Optional[float], int]]`` or ``None``         A list containing angles of each batch.
Read file and return :class:`~anndata.AnnData` object.      To speed up reading, consider passing `cache=True`, which creates an hdf5     cache file.      Parameters     ----------     filename : `str`         If the filename has no file extension, it is interpreted as a key for         generating a filename via `sc.settings.writedir + filename +         sc.settings.file_format_data`.  This is the same behavior as in         `sc.read(filename, ...)`.     backed : {`False`, `True`, 'r', 'r+'}, optional (default: `False`)         Load :class:`~anndata.AnnData` in `backed` mode instead of fully         loading it into memory (`memory` mode). Only applies to `.h5ad` files.         `True` and 'r' are equivalent. If you want to modify backed attributes         of the AnnData object, you need to choose 'r+'.     sheet : `str`, optional (default: `None`)         Name of sheet/table in hdf5 or Excel file.     cache : `bool`, optional (default: `False`)         If `False`, read from source, if `True`, read from fast 'h5ad' cache.     ext : `str`, optional (default: `None`)         Extension that indicates the file type. If `None`, uses extension of         filename.     delimiter : `str`, optional (default: `None`)         Delimiter that separates data within text file. If `None`, will split at         arbitrary number of white spaces, which is different from enforcing         splitting at any single white space ' '.     first_column_names : `bool`, optional (default: `False`)         Assume the first column stores row names. This is only necessary if         these are not strings: strings in the first column are automatically         assumed to be row names.     backup_url : `str`, optional (default: `None`)         Retrieve the file from an URL if not present on disk.      Returns     -------     An :class:`~anndata.AnnData` object
Read 10x-Genomics-formatted hdf5 file.      Parameters     ----------     filename : `str` | :class:`~pathlib.Path`         Filename.     genome : `str`, optional (default: ``None``)         Filter expression to this genes within this genome. For legacy 10x h5         files, this must be provided if the data contains more than one genome.     gex_only : `bool`, optional (default: `True`)         Only keep 'Gene Expression' data and ignore other feature types,         e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'      Returns     -------     Annotated data matrix, where obsevations/cells are named by their     barcode and variables/genes by gene name. The data matrix is stored in     `adata.X`, cell names in `adata.obs_names` and gene names in     `adata.var_names`. The gene IDs are stored in `adata.var['gene_ids']`.     The feature types are stored in `adata.var['feature_types']`
Read hdf5 file from Cell Ranger v2 or earlier versions.
Read hdf5 file from Cell Ranger v3 or later versions.
Read 10x-Genomics-formatted mtx directory.      Parameters     ----------     path : `str`         Path to directory for `.mtx` and `.tsv` files,         e.g. './filtered_gene_bc_matrices/hg19/'.     var_names : {'gene_symbols', 'gene_ids'}, optional (default: 'gene_symbols')         The variables index.     make_unique : `bool`, optional (default: `True`)         Whether to make the variables index unique by appending '-1',         '-2' etc. or not.     cache : `bool`, optional (default: `False`)         If `False`, read from source, if `True`, read from fast 'h5ad' cache.     gex_only : `bool`, optional (default: `True`)         Only keep 'Gene Expression' data and ignore other feature types,         e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'      Returns     -------     An :class:`~anndata.AnnData` object
Read mex from output from Cell Ranger v2 or earlier versions
Write :class:`~anndata.AnnData` objects to file.      Parameters     ----------     filename : `str`         If the filename has no file extension, it is interpreted as a key for         generating a filename via `sc.settings.writedir + filename +         sc.settings.file_format_data`.  This is the same behavior as in         :func:`~scanpy.api.read`.     adata : :class:`~anndata.AnnData`         Annotated data matrix.     ext : {`None`, `'h5'`, `'csv'`, `'txt'`, `'npz'`} (default: `None`)         File extension from wich to infer file format. If `None`, defaults to         `sc.settings.file_format_data`.     compression : {`None`, 'gzip', 'lzf'}, optional (default: `'gzip'`)         See http://docs.h5py.org/en/latest/high/dataset.html.     compression_opts : `int`, optional (default: `None`)         See http://docs.h5py.org/en/latest/high/dataset.html.
Read parameter dictionary from text file.      Assumes that parameters are specified in the format:         par1 = value1         par2 = value2      Comments that start with '#' are allowed.      Parameters     ----------     filename : str, Path         Filename of data file.     asheader : bool, optional         Read the dictionary from the header (comment section) of a file.      Returns     -------     Dictionary that stores parameters.
Write parameters to file, so that it's readable by read_params.      Uses INI file format.
Transform params list to dictionary.
Make a path into a filename.
Read a SOFT format data file.      The SOFT format is documented here     http://www.ncbi.nlm.nih.gov/geo/info/soft2.html.      Notes     -----     The function is based on a script by Kerby Shedden.     http://dept.stat.lsa.umich.edu/~kshedden/Python-Workshop/gene_expression_comparison.html
Check whether string is boolean.
Convert string to int, float or bool.
Get files used by processes with name scanpy.
Check whether the file is present, otherwise download.
Check whether the argument is a filename.
Plot correlation matrix.              Plot a correlation matrix for genes strored in sample annotation using rank_genes_groups.py              Parameters             ----------             adata : :class:`~anndata.AnnData`                 Annotated data matrix.             groupby : `str`, optional (default: None)                 If specified, searches data_annotation for correlation_matrix+groupby+str(group)             group : int                 Identifier of the group (necessary if and only if groupby is also specified)             corr_matrix : DataFrame, optional (default: None)                 Correlation matrix as a DataFrame (annotated axis) that can be transferred manually if wanted             annotation_key: `str`, optional (default: None)                 If specified, looks in data annotation for this key.
For one group, output a detailed chart analyzing highly ranked genes detailly.                  This is a visualization tools that helps to find significant markers and get a better understanding of                 the underlying data.                  Parameters                 ----------                 adata : :class:`~anndata.AnnData`                     Annotated data matrix.                 groupby : `str`                     The key of the sample grouping to consider.                 groupid: int                     The group for which detailed analysis should be displayed.                 x : 'str'                     x-axis labelling for plots                 y : 'str'                     y-axis labelling for plots                 groups : `str`, `list`, optional (default: `'all'`)                     Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall                     be restricted. If not passed, a ranking will be generated for all                     groups.                 n : `int`, optional (default: 100)                     Number of datapoints in the scatterplot. If less are available, use all that are available                 special_markers: 'dict', optional (default: None)                     If provided, this should be a dict containing a list of gene names for each group in groupby.                     Special marked genes are highlighted in the visualization                 coloring : {'scores', 'absolute'}, optional (default: 'scores')                     Rank either according to Scores, or to absolute test-statistic value.                     In either case, results are scaled so as to guarantee sufficient contrast.                 annotate : bool, optional (default: False)                     If True, annotate each datapoint in (each?) scatterplot. Helps identify specific genes. Only                     recommended for small n.
For one group, output a detailed chart analyzing highly ranked genes detailly.                  This is a visualization tools that helps to find significant markers and get a better understanding of                 the underlying data.                  Parameters                 ----------                 adata : :class:`~anndata.AnnData`                     Annotated data matrix.                 groupby : `str`                     The key of the sample grouping to consider.                 groupid: int                     The group for which detailed analysis should be displayed.                 x : 'str'                     x-axis labelling for plots                 y : 'str'                     y-axis labelling for plots                 n : `int`, optional (default: 100)                     Number of datapoints in the scatterplot. If less are available, use all that are available                 special_markers: 'dict', optional (default: None)                     If provided, this should be a dict containing a list of gene names for each group in groupby.                     Special marked genes are highlighted in the visualization                 coloring : {'scores', 'absolute'}, optional (default: 'scores')                     Rank either according to Scores, or to absolute test-statistic value.                     In either case, results are scaled so as to guarantee sufficient contrast.                 size: int, optional (default: 12)                     Determines scatter plot size. Large scatter-plots make it easier to identify specific genes using                     annotate=True                 annotate : bool, optional (default: False)                     If True, annotate each datapoint in (each?) scatterplot. Helps identify specific genes. Only                     recommended for small n.
Calculate correlation matrix.          Calculate a correlation matrix for genes strored in sample annotation using rank_genes_groups.py          Parameters         ----------         adata : :class:`~anndata.AnnData`             Annotated data matrix.         groupby : `str`             The key of the sample grouping to consider.         group : `str`, int, optional (default: None)             Group name or index for which the correlation matrix for top_ranked genes should be calculated.             If no parameter is passed, ROC/AUC is calculated for all groups         n_genes : `int`, optional (default: 100)             For how many genes to calculate ROC and AUC. If no parameter is passed, calculation is done for             all stored top ranked genes.         special_markers: 'dict', optional (default: None)             If provided, this should be a dict containing a list of gene names for each group in groupby.             Special marked genes are highlighted in the visualization         coloring : {'scores', 'absolute'}, optional (default: 'scores')             Rank either according to Scores, or to absolute test-statistic value.             In either case, results are scaled so as to guarantee sufficient contrast.         size: int, optional (default: 12)             Determines scatter plot size. Large scatter-plots make it easier to identify specific genes using             annotate=True         annotate : bool, optional (default: False)             If True, annotate each datapoint in (each?) scatterplot. Helps identify specific genes. Only             recommended for small n.
Wraps tqdm instance.      Don't forget to close() or __exit__()     the tqdm instance once you're done with it (easiest using `with` syntax).     Example     -------     >>> with tqdm(...) as t:     ...     reporthook = my_hook(t)     ...     urllib.urlretrieve(..., reporthook=reporthook)
Load a dataset from the `EBI Single Cell Expression Atlas <https://www.ebi.ac.uk/gxa/sc/experiments>`__.      Downloaded datasets are saved in directory specified by `sc.settings.datasetdir`.      Params     ------     accession         Dataset accession. Like ``E-GEOD-98816`` or ``E-MTAB-4888``. This can         be found in the url on the datasets page. For example:         ``https://www.ebi.ac.uk/gxa/sc/experiments/E-GEOD-98816/results/tsne``     filter_boring         Whether boring labels in `.obs` should be automatically removed.      Example     -------     >>> adata = sc.datasets.ebi_expression_atlas("E-MTAB-4888")
Estimate RNA velocity [LaManno17]_      This requires generating a loom file with Velocyto, which will store the     counts of spliced, unspliced and ambiguous RNA for every cell and every     gene.      In contrast to Velocyto, here, we neither use RNA velocities for     extrapolation nor for constructing a Markov process. Instead, we directly     orient and weight edges in the nearest neighbor graph by computing     ``cosine_similarity((x_i - x_j), v_i)``, where `i` labels a cell, `j` a     neighbor of the cell, `x` a gene expression vector and `v` a velocity     vector.
PhenoGraph clustering [Levine15]_.           :param adata:    Numpy ndarray of data to cluster,                          or sparse matrix of k-nearest neighbor graph                          If ndarray, n-by-d array of n cells in d dimensions                          If sparse matrix, n-by-n adjacency matrix         :param k:        Number of nearest neighbors to use in first                          step of graph construction         :param directed: Whether to use a symmetric (default) or                          asymmetric ("directed") graph                          The graph construction process produces a directed graph,                          which is symmetrized by one of two methods (see below)         :param prune:    Whether to symmetrize by taking the average (prune=False) or                          product (prune=True) between the graph and its transpose         :param min_cluster_size:                          Cells that end up in a cluster smaller than min_cluster_size                          are considered outliers and are assigned to -1 in the cluster labels         :param jaccard:  If True, use Jaccard metric between k-neighborhoods to build graph                          If False, use a Gaussian kernel         :param primary_metric:                          Distance metric to define nearest neighbors                          Options include: {'euclidean','manhattan','correlation','cosine'}.                          Note that performance will be slower for correlation and cosine         :param n_jobs:   Nearest Neighbors and Jaccard coefficients will be computed                          in parallel using n_jobs. If n_jobs=-1, the number of jobs is                          determined automatically         :param q_tol:    Tolerance (i.e., precision) for monitoring modularity optimization         :param louvain_time_limit:                          Maximum number of seconds to run modularity optimization.                          If exceeded the best result so far is returned         :param nn_method:                          Whether to use brute force or kdtree for nearest neighbor                          search. For very large high-dimensional data sets, brute force                          (with parallel computation) performs faster than kdtree          :return communities:                         numpy integer array of community assignments for each                         row in data         :return graph:  numpy sparse array of the graph that was used for clustering         :return Q:      the modularity score for communities on graph           Example         -------                  >>> import scanpy.external as sce         >>> import scanpy.api as sc         >>> import numpy as np         >>> import pandas as pd                  Assume adata is your annotated data which has the normalized data.          Then do PCA:                  >>> sc.tl.pca(adata, n_comps = 100)                  Compute phenograph clusters:                  >>> result = sce.tl.phenograph(adata.obsm['X_pca'], k = 30)                  Embed the phenograph result into adata as a *categorical* variable (this helps in plotting):                  >>> adata.obs['pheno'] = pd.Categorical(result[0])                  Check by typing "adata" and you should see under obs a key called 'pheno'.                  Now to show phenograph on tSNE (for example):                  Compute tSNE:                  >>> sc.tl.tsne(adata, random_state = 7)                  Plot phenograph clusters on tSNE:                  >>> sc.pl.tsne(adata, color = ['pheno'], s = 100, palette = sc.pl.palettes.vega_20_scanpy, legend_fontsize = 10)                  Cluster and cluster centroids for input Numpy ndarray                  >>> df = np.random.rand(1000,40)         >>> df.shape         (1000, 40)         >>> result = sce.tl.phenograph(df, k=50)         Finding 50 nearest neighbors using minkowski metric and 'auto' algorithm         Neighbors computed in 0.16141605377197266 seconds         Jaccard graph constructed in 0.7866239547729492 seconds         Wrote graph to binary file in 0.42542195320129395 seconds         Running Louvain modularity optimization         After 1 runs, maximum modularity is Q = 0.223536         After 2 runs, maximum modularity is Q = 0.235874         Louvain completed 22 runs in 1.5609488487243652 seconds         PhenoGraph complete in 2.9466471672058105 seconds                  New results can be pushed into adata object:                  >>> dframe = pd.DataFrame(data=df, columns=range(df.shape[1]),index=range(df.shape[0]) )         >>> adata = sc.AnnData( X=dframe, obs=dframe, var=dframe)         >>> adata.obs['pheno'] = pd.Categorical(result[0])
Plot a matrix.
Plot X. See timeseries_subplot.
Plot X.      Parameters     ----------     X : np.ndarray         Call this with:         X with one column, color categorical.         X with one column, color continuous.         X with n columns, color is of length n.
Plot timeseries as heatmap.      Parameters     ----------     X : np.ndarray         Data array.     var_names : array_like         Array of strings naming variables stored in columns of X.
Save current figure to file.      The `filename` is generated as follows:          filename = settings.figdir + writekey + settings.plot_suffix + '.' + settings.file_format_figs
Scatter of group using representation of data Y.
Grid of axes for plotting, legends and colorbars.
Plot scatter plot of data.      Parameters     ----------     Y : np.ndarray         Data array.     projection : {'2d', '3d'}      Returns     -------     Depending on whether supplying a single array or a list of arrays,     return a single axis or a list of axes.
Plot scatter plot of data.      Parameters     ----------     ax : matplotlib.axis         Axis to plot on.     Y : np.array         Data array, data to be plotted needs to be in the first two columns.
Plot arrows of transitions in data matrix.      Parameters     ----------     ax : matplotlib.axis         Axis object from matplotlib.     X : np.array         Data array, any representation wished (X, psi, phi, etc).     indices : array_like         Indices storing the transitions.
Take some 1d data and scale it so that min matches 0 and max 1.
Tree layout for networkx graph.         See https://stackoverflow.com/questions/29586520/can-one-get-hierarchical-graphs-from-networkx-with-python-3        answer by burubum.         If there is a cycle that is reachable from root, then this will see        infinite recursion.         Parameters        ----------        G: the graph        root: the root node        levels: a dictionary                key: level number (starting from 0)                value: number of nodes in this level        width: horizontal space allocated for drawing        height: vertical space allocated for drawing
Zoom into axis.      Parameters     ----------
Get axis size      Parameters     ----------     ax : matplotlib.axis         Axis object from matplotlib.     fig : matplotlib.Figure         Figure.
For a width in axis coordinates, return the corresponding in data     coordinates.      Parameters     ----------     ax : matplotlib.axis         Axis object from matplotlib.     width : float         Width in xaxis coordinates.
Map points in axis coordinates to data coordinates.      Uses matplotlib.transform.      Parameters     ----------     ax : matplotlib.axis         Axis object from matplotlib.     points_axis : np.array         Points in axis coordinates.
\     {norm_descr}      {params_bulk}      {norm_return}      {examples}
Set matplotlib.rcParams to Scanpy defaults.      Call this through `settings.set_figure_params`.
Parameters     ----------     n_pcs : `int` or `None`, optional (default: `None`)         If `n_pcs=0`, do not preprocess with PCA.         If `None` and there is a PCA version of the data, use this.         If an integer, compute the PCA.
Run a builtin scanpy command or a scanpy-* subcommand.      Uses :func:`subcommand.run` for the latter: ``~run(['scanpy', *argv], **runargs)``
This serves as CLI entry point and will not show a Python traceback if a called command fails
Cluster cells into subgroups [Blondel08]_ [Levine15]_ [Traag17]_.      Cluster cells using the Louvain algorithm [Blondel08]_ in the implementation     of [Traag17]_. The Louvain algorithm has been proposed for single-cell     analysis by [Levine15]_.      This requires having ran :func:`~scanpy.pp.neighbors` or :func:`~scanpy.external.pp.bbknn` first,     or explicitly passing a ``adjacency`` matrix.      Parameters     ----------     adata         The annotated data matrix.     resolution         For the default flavor (``'vtraag'``), you can provide a resolution         (higher resolution means finding more and smaller clusters),         which defaults to 1.0. See “Time as a resolution parameter” in [Lambiotte09]_.     random_state         Change the initialization of the optimization.     restrict_to         Restrict the clustering to the categories within the key for sample         annotation, tuple needs to contain ``(obs_key, list_of_categories)``.     key_added         Key under which to add the cluster labels. (default: ``'louvain'``)     adjacency         Sparse adjacency matrix of the graph, defaults to         ``adata.uns['neighbors']['connectivities']``.     flavor : {``'vtraag'``, ``'igraph'``}         Choose between to packages for computing the clustering.         ``'vtraag'`` is much more powerful, and the default.     directed         Interpret the ``adjacency`` matrix as directed graph?     use_weights         Use weights from knn graph.     partition_type         Type of partition to use.         Only a valid argument if ``flavor`` is ``'vtraag'``.     partition_kwargs         Key word arguments to pass to partitioning,         if ``vtraag`` method is being used.     copy         Copy adata or modify it inplace.      Returns     -------     :obj:`None`         By default (``copy=False``), updates ``adata`` with the following fields:          ``adata.obs['louvain']`` (:class:`pandas.Series`, dtype ``category``)             Array of dim (number of samples) that stores the subgroup id             (``'0'``, ``'1'``, ...) for each cell.      :class:`~anndata.AnnData`         When ``copy=True`` is set, a copy of ``adata`` with those fields is returned.
Rank genes for characterizing groups.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     groupby : `str`         The key of the observations grouping to consider.     use_raw : `bool`, optional (default: `True`)         Use `raw` attribute of `adata` if present.     groups         Subset of groups, e.g. `['g1', 'g2', 'g3']`, to which comparison shall         be restricted, or `'all'` (default), for all groups.     reference : `str`, optional (default: `'rest'`)         If `'rest'`, compare each group to the union of the rest of the group.  If         a group identifier, compare with respect to this group.     n_genes : `int`, optional (default: 100)         The number of genes that appear in the returned tables.     method : `{'logreg', 't-test', 'wilcoxon', 't-test_overestim_var'}`, optional (default: 't-test_overestim_var')         If 't-test', uses t-test, if 'wilcoxon', uses Wilcoxon-Rank-Sum. If         't-test_overestim_var', overestimates variance of each group. If         'logreg' uses logistic regression, see [Ntranos18]_, `here         <https://github.com/theislab/scanpy/issues/95>`__ and `here         <http://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__, for         why this is meaningful.     corr_method : `{'benjamini-hochberg', 'bonferroni'}`, optional (default: 'benjamini-hochberg')         p-value correction method. Used only for 't-test', 't-test_overestim_var',         and 'wilcoxon' methods.     rankby_abs : `bool`, optional (default: `False`)         Rank genes by the absolute value of the score, not by the         score. The returned scores are never the absolute values.     **kwds : keyword parameters         Are passed to test methods. Currently this affects only parameters that         are passed to `sklearn.linear_model.LogisticRegression         <http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html>`__.         For instance, you can pass `penalty='l1'` to try to come up with a         minimal set of genes that are good predictors (sparse solution meaning         few non-zero fitted coefficients).      Returns     -------     **names** : structured `np.ndarray` (`.uns['rank_genes_groups']`)         Structured array to be indexed by group id storing the gene         names. Ordered according to scores.     **scores** : structured `np.ndarray` (`.uns['rank_genes_groups']`)         Structured array to be indexed by group id storing the z-score         underlying the computation of a p-value for each gene for each         group. Ordered according to scores.     **logfoldchanges** : structured `np.ndarray` (`.uns['rank_genes_groups']`)         Structured array to be indexed by group id storing the log2         fold change for each gene for each group. Ordered according to         scores. Only provided if method is 't-test' like.         Note: this is an approximation calculated from mean-log values.     **pvals** : structured `np.ndarray` (`.uns['rank_genes_groups']`)         p-values.     **pvals_adj** : structured `np.ndarray` (`.uns['rank_genes_groups']`)         Corrected p-values.      Notes     -----     There are slight inconsistencies depending on whether sparse     or dense data are passed. See `here <https://github.com/theislab/scanpy/blob/master/scanpy/tests/test_rank_genes_groups.py>`__.      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')      # to visualize the results     >>> sc.pl.rank_genes_groups(adata)
Filters out genes based on fold change and fraction of genes expressing the gene within and outside the `groupby` categories.      See :func:`~scanpy.tl.rank_genes_groups`.      Results are stored in `adata.uns[key_added]` (default: 'rank_genes_groups_filtered').      To preserve the original structure of adata.uns['rank_genes_groups'], filtered genes     are set to `NaN`.      Parameters     ----------     adata: :class:`~anndata.AnnData`     key     groupby     use_raw     log : if true, it means that the values to work with are in log scale     key_added     min_in_group_fraction     min_fold_change     max_out_group_fraction      Returns     -------     Same output as :ref:`scanpy.tl.rank_genes_groups` but with filtered genes names set to     `nan`      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')     >>> sc.tl.filter_rank_genes_groups(adata, min_fold_change=3)     >>> # visualize results     >>> sc.pl.rank_genes_groups(adata, key='rank_genes_groups_filtered')     >>> # visualize results using dotplot     >>> sc.pl.rank_genes_groups_dotplot(adata, key='rank_genes_groups_filtered')
Gaussian Blobs.      Parameters     ----------     n_variables : `int`, optional (default: 11)         Dimension of feature space.     n_centers : `int`, optional (default: 5)         Number of cluster centers.     cluster_std : `float`, optional (default: 1.0)         Standard deviation of clusters.     n_observations : `int`, optional (default: 640)         Number of observations. By default, this is the same observation number as in         ``sc.datasets.krumsiek11()``.      Returns     -------     Annotated data matrix containing a observation annotation 'blobs' that     indicates cluster identity.
Bulk data with conditions ulcerative colitis (UC) and Crohn's disease (CD).      The study assesses transcriptional profiles in peripheral blood mononuclear     cells from 42 healthy individuals, 59 CD patients, and 26 UC patients by     hybridization to microarrays interrogating more than 22,000 sequences.      Reference     ---------     Burczynski et al., "Molecular classification of Crohn's disease and     ulcerative colitis patients using transcriptional profiles in peripheral     blood mononuclear cells"     J Mol Diagn 8, 51 (2006). PMID:16436634.
Simulated myeloid progenitors [Krumsiek11]_.      The literature-curated boolean network from [Krumsiek11]_ was used to     simulate the data. It describes development to four cell fates: 'monocyte',     'erythrocyte', 'megakaryocyte' and 'neutrophil'.      See also the discussion of this data in [Wolf19]_.      Simulate via :func:`~scanpy.api.sim`.      Returns     -------     Annotated data matrix.
Hematopoiesis in early mouse embryos [Moignard15]_.      Returns     -------     Annotated data matrix.
Development of Myeloid Progenitors [Paul15]_.      Non-logarithmized raw data.      The data has been sent out by Email from the Amit Lab. An R version for     loading the data can be found here     https://github.com/theislab/scAnalysisTutorial      Returns     -------     Annotated data matrix.
Simulated toggleswitch.      Data obtained simulating a simple toggleswitch `Gardner *et al.*, Nature     (2000) <https://doi.org/10.1038/35002131>`__.      Simulate via :func:`~scanpy.api.sim`.      Returns     -------     Annotated data matrix.
Subsampled and processed 68k PBMCs.      10x PBMC 68k dataset from     https://support.10xgenomics.com/single-cell-gene-expression/datasets      The original PBMC 68k dataset was preprocessed using scanpy and was saved     keeping only 724 cells and 221 highly variable genes.      The saved file contains the annotation of cell types (key: 'bulk_labels'), UMAP coordinates,     louvain clustering and gene rankings based on the bulk_labels.      Returns     -------     Annotated data matrix.
\     Compute a neighborhood graph of observations [McInnes18]_.      The neighbor search efficiency of this heavily relies on UMAP [McInnes18]_,     which also provides a method for estimating connectivities of data points -     the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,     connectivities are computed according to [Coifman05]_, in the adaption of     [Haghverdi16]_.      Parameters     ----------     adata         Annotated data matrix.     n_neighbors         The size of local neighborhood (in terms of number of neighboring data         points) used for manifold approximation. Larger values result in more         global views of the manifold, while smaller values result in more local         data being preserved. In general values should be in the range 2 to 100.         If `knn` is `True`, number of nearest neighbors to be searched. If `knn`         is `False`, a Gaussian kernel width is set to the distance of the         `n_neighbors` neighbor.     {n_pcs}     {use_rep}     knn         If `True`, use a hard threshold to restrict the number of neighbors to         `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian         Kernel to assign low weights to neighbors more distant than the         `n_neighbors` nearest neighbor.     random_state         A numpy random seed.     method : {{'umap', 'gauss', `None`}}  (default: `'umap'`)         Use 'umap' [McInnes18]_ or 'gauss' (Gauss kernel following [Coifman05]_         with adaptive width [Haghverdi16]_) for computing connectivities.     metric         A known metric’s name or a callable that returns a distance.     metric_kwds         Options for the metric.     copy         Return a copy instead of writing to adata.      Returns     -------     Depending on `copy`, updates or returns `adata` with the following:      **connectivities** : sparse matrix (`.uns['neighbors']`, dtype `float32`)         Weighted adjacency matrix of the neighborhood graph of data         points. Weights should be interpreted as connectivities.     **distances** : sparse matrix (`.uns['neighbors']`, dtype `float32`)         Instead of decaying weights, this stores distances for each pair of         neighbors.
This is from umap.fuzzy_simplicial_set [McInnes18]_.      Given a set of data X, a neighborhood size, and a measure of distance     compute the fuzzy simplicial set (here represented as a fuzzy graph in     the form of a sparse matrix) associated to the data. This is done by     locally approximating geodesic distance at each point, creating a fuzzy     simplicial set for each such point, and then combining all the local     fuzzy simplicial sets into a global one via a fuzzy union.      Parameters     ----------     X: array of shape (n_samples, n_features)         The data to be modelled as a fuzzy simplicial set.     n_neighbors: int         The number of neighbors to use to approximate geodesic distance.         Larger numbers induce more global estimates of the manifold that can         miss finer detail, while smaller values will focus on fine manifold         structure to the detriment of the larger picture.     random_state: numpy RandomState or equivalent         A state capable being used as a numpy random state.     metric: string or function (optional, default 'euclidean')         The metric to use to compute distances in high dimensional space.         If a string is passed it must match a valid predefined metric. If         a general metric is required a function that takes two 1d arrays and         returns a float can be provided. For performance purposes it is         required that this be a numba jit'd function. Valid string metrics         include:             * euclidean             * manhattan             * chebyshev             * minkowski             * canberra             * braycurtis             * mahalanobis             * wminkowski             * seuclidean             * cosine             * correlation             * haversine             * hamming             * jaccard             * dice             * russelrao             * kulsinski             * rogerstanimoto             * sokalmichener             * sokalsneath             * yule         Metrics that take arguments (such as minkowski, mahalanobis etc.)         can have arguments passed via the metric_kwds dictionary. At this         time care must be taken and dictionary elements must be ordered         appropriately; this will hopefully be fixed in the future.     metric_kwds: dict (optional, default {})         Arguments to pass on to the metric, such as the ``p`` value for         Minkowski distance.     angular: bool (optional, default False)         Whether to use angular/cosine distance for the random projection         forest for seeding NN-descent to determine approximate nearest         neighbors.     verbose: bool (optional, default False)         Whether to report information on the current progress of the algorithm.      Returns     -------     **knn_indices**, **knn_dists** : np.arrays of shape (n_observations, n_neighbors)
This is from umap.fuzzy_simplicial_set [McInnes18]_.      Given a set of data X, a neighborhood size, and a measure of distance     compute the fuzzy simplicial set (here represented as a fuzzy graph in     the form of a sparse matrix) associated to the data. This is done by     locally approximating geodesic distance at each point, creating a fuzzy     simplicial set for each such point, and then combining all the local     fuzzy simplicial sets into a global one via a fuzzy union.
Generate a view restricted to a subset of indices.
Transition matrix (sparse matrix).          Is conjugate to the symmetrized transition matrix via::              self.transitions = self.Z *  self.transitions_sym / self.Z          where ``self.Z`` is the diagonal matrix storing the normalization of the         underlying kernel matrix.          Notes         -----         This has not been tested, in contrast to `transitions_sym`.
\         Compute distances and connectivities of neighbors.          Parameters         ----------         n_neighbors              Use this number of nearest neighbors.         knn              Restrict result to `n_neighbors` nearest neighbors.         {n_pcs}         {use_rep}          Returns         -------         Writes sparse graph attributes `.distances` and `.connectivities`.         Also writes `.knn_indices` and `.knn_distances` if         `write_knn_indices==True`.
Compute transition matrix.          Parameters         ----------         density_normalize : `bool`             The density rescaling of Coifman and Lafon (2006): Then only the             geometry of the data matters, not the sampled density.          Returns         -------         Makes attributes `.transitions_sym` and `.transitions` available.
Compute eigen decomposition of transition matrix.          Parameters         ----------         n_comps : `int`             Number of eigenvalues/vectors to be computed, set `n_comps = 0` if             you need all eigenvectors.         sym : `bool`             Instead of computing the eigendecomposition of the assymetric             transition matrix, computed the eigendecomposition of the symmetric             Ktilde matrix.         matrix : sparse matrix, np.ndarray, optional (default: `.connectivities`)             Matrix to diagonalize. Merely for testing and comparison purposes.          Returns         -------         Writes the following attributes.          eigen_values : numpy.ndarray             Eigenvalues of transition matrix.         eigen_basis : numpy.ndarray              Matrix of eigenvectors (stored in columns).  `.eigen_basis` is              projection of data matrix on right eigenvectors, that is, the              projection on the diffusion components.  these are simply the              components of the right eigenvectors and can directly be used for              plotting.
See Fouss et al. (2006) and von Luxburg et al. (2007).          See Proposition 6 in von Luxburg (2007) and the inline equations         right in the text above.
See Fouss et al. (2006) and von Luxburg et al. (2007).          This is the commute-time matrix. It's a squared-euclidian distance         matrix in :math:`\\mathbb{R}^n`.
See Fouss et al. (2006).          This is the mean-first passage time matrix. It's not a distance.          Mfp[i, k] := m(k|i) in the notation of Fouss et al. (2006). This         corresponds to the standard notation for transition matrices (left index         initial state, right index final state, i.e. a right-stochastic         matrix, with each row summing to one).
Return pseudotime with respect to root point.
Determine the index of the root cell.          Given an expression vector, find the observation index that is closest         to this vector.          Parameters         ----------         xroot : np.ndarray             Vector that marks the root cell, the vector storing the initial             condition, only relevant for computing pseudotime.
Mitochondrial gene symbols for specific organism through BioMart.      Parameters     ----------     host : {{'www.ensembl.org', ...}}         A valid BioMart host URL.     org : {{'hsapiens', 'mmusculus', 'drerio'}}         Organism to query. Currently available are human ('hsapiens'), mouse         ('mmusculus') and zebrafish ('drerio').      Returns     -------     A :class:`pandas.Index` containing mitochondrial gene symbols.
Retrieve gene coordinates for specific organism through BioMart.     Parameters     ----------     host : {{'www.ensembl.org', ...}}         A valid BioMart host URL. Can be used to control genome build.     org : {{'hsapiens', 'mmusculus', 'drerio'}}         Organism to query. Currently available are human ('hsapiens'), mouse         ('mmusculus') and zebrafish ('drerio').     gene :         The gene symbol (e.g. 'hgnc_symbol' for human) for which to retrieve         coordinates.     chr_exclude :         A list of chromosomes to exclude from query.     Returns     -------     A `pd.DataFrame` containing gene coordinates for the specified gene symbol.
\     Fraction of counts assigned to each gene over all cells.      Computes, for each gene, the fraction of counts assigned to that gene within     a cell. The `n_top` genes with the highest mean fraction over all cells are     plotted as boxplots.      This plot is similar to the `scater` package function `plotHighestExprs(type     = "highest-expression")`, see `here     <https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html>`__. Quoting     from there:          *We expect to see the “usual suspects”, i.e., mitochondrial genes, actin,         ribosomal protein, MALAT1. A few spike-in transcripts may also be         present here, though if all of the spike-ins are in the top 50, it         suggests that too much spike-in RNA was added. A large number of         pseudo-genes or predicted genes may indicate problems with alignment.*         -- Davis McCarthy and Aaron Lun      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     n_top : `int`, optional (default:30)         Number of top     {show_save_ax}     gene_symbols : `str`, optional (default:None)         Key for field in .var that stores gene symbols if you do not want to use .var_names.     **kwds : keyword arguments         Are passed to `seaborn.boxplot`.      Returns     -------     If `show==False` a :class:`~matplotlib.axes.Axes`.
Extract highly variable genes [Satija15]_ [Zheng17]_.      .. warning::         .. deprecated:: 1.3.6             Use :func:`~scanpy.api.pp.highly_variable_genes`             instead. The new function is equivalent to the present             function, except that              * the new function always expects logarithmized data             * `subset=False` in the new function, it suffices to               merely annotate the genes, tools like `pp.pca` will               detect the annotation             * you can now call: `sc.pl.highly_variable_genes(adata)`             * `copy` is replaced by `inplace`      If trying out parameters, pass the data matrix instead of AnnData.      Depending on `flavor`, this reproduces the R-implementations of Seurat     [Satija15]_ and Cell Ranger [Zheng17]_.      The normalized dispersion is obtained by scaling with the mean and standard     deviation of the dispersions for genes falling into a given bin for mean     expression of genes. This means that for each bin of mean expression, highly     variable genes are selected.      Use `flavor='cell_ranger'` with care and in the same way as in     :func:`~scanpy.api.pp.recipe_zheng17`.      Parameters     ----------     data : :class:`~anndata.AnnData`, `np.ndarray`, `sp.sparse`         The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond         to cells and columns to genes.     flavor : {'seurat', 'cell_ranger'}, optional (default: 'seurat')         Choose the flavor for computing normalized dispersion. If choosing         'seurat', this expects non-logarithmized data - the logarithm of mean         and dispersion is taken internally when `log` is at its default value         `True`. For 'cell_ranger', this is usually called for logarithmized data         - in this case you should set `log` to `False`. In their default         workflows, Seurat passes the cutoffs whereas Cell Ranger passes         `n_top_genes`.     min_mean=0.0125, max_mean=3, min_disp=0.5, max_disp=`None` : `float`, optional         If `n_top_genes` unequals `None`, these cutoffs for the means and the         normalized dispersions are ignored.     n_bins : `int` (default: 20)         Number of bins for binning the mean gene expression. Normalization is         done with respect to each bin. If just a single gene falls into a bin,         the normalized dispersion is artificially set to 1. You'll be informed         about this if you set `settings.verbosity = 4`.     n_top_genes : `int` or `None` (default: `None`)         Number of highly-variable genes to keep.     log : `bool`, optional (default: `True`)         Use the logarithm of the mean to variance ratio.     subset : `bool`, optional (default: `True`)         Keep highly-variable genes only (if True) else write a bool array for h         ighly-variable genes while keeping all genes     copy : `bool`, optional (default: `False`)         If an :class:`~anndata.AnnData` is passed, determines whether a copy         is returned.      Returns     -------     If an AnnData `adata` is passed, returns or updates `adata` depending on     `copy`. It filters the `adata` and adds the annotations      **means** : adata.var         Means per gene. Logarithmized when `log` is `True`.     **dispersions** : adata.var         Dispersions per gene. Logarithmized when `log` is `True`.     **dispersions_norm** : adata.var         Normalized dispersions per gene. Logarithmized when `log` is `True`.      If a data matrix `X` is passed, the annotation is returned as `np.recarray`     with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.
Filter genes by coefficient of variance and mean.      See `filter_genes_dispersion`.      Reference: Weinreb et al. (2017).
Filter genes by fano factor and mean.      See `filter_genes_dispersion`.      Reference: Weinreb et al. (2017).
Convert distributed arrays to ndarrays.
Merge AnnData objects and correct batch effects using the MNN method.      Batch effect correction by matching mutual nearest neighbors [Haghverdi18]_     has been implemented as a function 'mnnCorrect' in the R package     `scran <https://bioconductor.org/packages/release/bioc/html/scran.html>`__     This function provides a wrapper to use the mnnCorrect function when     concatenating Anndata objects by using the Python-R interface `rpy2     <https://pypi.org/project/rpy2/>`__.      Parameters     ----------     adatas : :class:`~anndata.AnnData`         AnnData matrices to concatenate with. Each dataset should generally be         log-transformed, e.g., log-counts. Datasets should have the same number         of genes, or at lease have all the genes in geneset.     geneset : `list`, optional (default: `None`)         A list specifying the genes with which distances between cells are         calculated in mnnCorrect, typically the highly variable genes.         All genes are used if no geneset provided. See the `scran manual         <https://bioconductor.org/packages/release/bioc/html/scran.html>`__ for         details.     k : `int`, ptional (default: 20)         See the `scran manual <https://bioconductor.org/packages/release/bioc/html/scran.html>`__         for details.     sigma : `int`, ptional (default: 20)         See the `scran manual <https://bioconductor.org/packages/release/bioc/html/scran.html>`__         for details.     n_jobs : `int` or `None` (default: `sc.settings.n_jobs`)         Number of jobs.     kwds :         Keyword arguments passed to Anndata.concatenate      Returns     -------     An :class:`~anndata.AnnData` object with MNN corrected data matrix X.      Example     -------     >>> adata1     AnnData object with n_obs × n_vars = 223 × 33694         obs: 'n_genes', 'percent_mito', 'n_counts', 'Sample', 'Donor', 'Tissue'         var: 'gene_ids', 'n_cells'     >>> adata2     AnnData object with n_obs × n_vars = 1457 × 33694         obs: 'n_genes', 'percent_mito', 'n_counts', 'Sample', 'Donor', 'Tissue'         var: 'gene_ids', 'n_cells'     >>> adata3 = sc.pp.mnnconcatenate(adata2, adata1, geneset = hvgs)
Cluster cells into subgroups [Traag18]_.      Cluster cells using the Leiden algorithm [Traag18]_, an improved version of the     Louvain algorithm [Blondel08]_. The Louvain algorithm has been proposed for single-cell     analysis by [Levine15]_.      This requires having ran :func:`~scanpy.pp.neighbors` or :func:`~scanpy.external.pp.bbknn` first.      Parameters     ----------     adata         The annotated data matrix.     resolution         A parameter value controlling the coarseness of the clustering.         Higher values lead to more clusters. Set to `None` if overriding `partition_type`         to one that doesn’t accept a `resolution_parameter`.     random_state         Change the initialization of the optimization.     restrict_to         Restrict the clustering to the categories within the key for sample         annotation, tuple needs to contain `(obs_key, list_of_categories)`.     key_added         `adata.obs` key under which to add the cluster labels. (default: `'leiden'`)     adjacency         Sparse adjacency matrix of the graph, defaults to         `adata.uns['neighbors']['connectivities']`.     directed         Whether to treat the graph as directed or undirected.     use_weights         If `True`, edge weights from the graph are used in the computation         (placing more emphasis on stronger edges).     n_iterations         How many iterations of the Leiden clustering algorithm to perform.         Positive values above 2 define the total number of iterations to perform,         -1 has the algorithm run until it reaches its optimal clustering.     partition_type         Type of partition to use. Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.         For the available options, consult the documentation for :func:`~leidenalg.find_partition`.     copy         Whether to copy `adata` or modify it inplace.     **partition_kwargs         Any further arguments to pass to `~leidenalg.find_partition`         (which in turn passes arguments to the `partition_type`).      Returns     -------     `adata.obs[key_added]`         Array of dim (number of samples) that stores the subgroup id (`'0'`, `'1'`, ...) for each cell.     `adata.uns['leiden']['params']`         A dict with the values for the parameters `resolution`, `random_state`, and `n_iterations`.
Computes a simple design matrix.      Parameters     --------     model         Contains the batch annotation     batch_key         Name of the batch column     batch_levels         Levels of the batch annotation      Returns     --------     The design matrix for the regression problem
Standardizes the data per gene.      The aim here is to make mean and variance be comparable across batches.      Parameters     --------     model         Contains the batch annotation     data         Contains the Data     batch_key         Name of the batch column in the model matrix      Returns     --------     s_data : pandas.DataFrame         Standardized Data     design : pandas.DataFrame         Batch assignment as one-hot encodings     var_pooled : numpy.ndarray         Pooled variance per gene     stand_mean : numpy.ndarray         Gene-wise mean
ComBat function for batch effect correction [Johnson07]_ [Leek12]_ [Pedersen12]_.      Corrects for batch effects by fitting linear models, gains statistical power     via an EB framework where information is borrowed across genes. This uses the     implementation of `ComBat <https://github.com/brentp/combat.py>`__ [Pedersen12]_.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix     key: `str`, optional (default: `"batch"`)         Key to a categorical annotation from adata.obs that will be used for batch effect removal     covariates         Additional covariates such as adjustment variables or biological condition. Note that         not including covariates may introduce bias or lead to the removal of biological signal          in unbalanced designs.     inplace: bool, optional (default: `True`)         Wether to replace adata.X or to return the corrected data      Returns     -------     Depending on the value of inplace, either returns an updated AnnData object     or modifies the passed one.
Iteratively compute the conditional posterior means for gamma and delta.      gamma is an estimator for the additive batch effect, deltat is an estimator     for the multiplicative batch effect. We use an EB framework to estimate these     two. Analytical expressions exist for both parameters, which however depend on each other.     We therefore iteratively evalutate these two expressions until convergence is reached.      Parameters     --------     s_data : pd.DataFrame         Contains the standardized Data     g_hat : float         Initial guess for gamma     d_hat : float         Initial guess for delta     g_bar, t_2, a, b : float         Hyperparameters     conv: float, optional (default: `0.0001`)         convergence criterium      Returns:     --------     gamma : float         estimated value for gamma     delta : float         estimated value for delta
\     t-SNE [Maaten08]_ [Amir13]_ [Pedregosa11]_.      t-distributed stochastic neighborhood embedding (tSNE) [Maaten08]_ has been     proposed for visualizating single-cell data by [Amir13]_. Here, by default,     we use the implementation of *scikit-learn* [Pedregosa11]_. You can achieve     a huge speedup and better convergence if you install `Multicore-tSNE     <https://github.com/DmitryUlyanov/Multicore-TSNE>`__ by [Ulyanov16]_, which     will be automatically detected by Scanpy.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     {doc_n_pcs}     {use_rep}     perplexity : `float`, optional (default: 30)         The perplexity is related to the number of nearest neighbors that         is used in other manifold learning algorithms. Larger datasets         usually require a larger perplexity. Consider selecting a value         between 5 and 50. The choice is not extremely critical since t-SNE         is quite insensitive to this parameter.     early_exaggeration : `float`, optional (default: 12.0)         Controls how tight natural clusters in the original space are in the         embedded space and how much space will be between them. For larger         values, the space between natural clusters will be larger in the         embedded space. Again, the choice of this parameter is not very         critical. If the cost function increases during initial optimization,         the early exaggeration factor or the learning rate might be too high.     learning_rate : `float`, optional (default: 1000)         Note that the R-package "Rtsne" uses a default of 200.         The learning rate can be a critical parameter. It should be         between 100 and 1000. If the cost function increases during initial         optimization, the early exaggeration factor or the learning rate         might be too high. If the cost function gets stuck in a bad local         minimum increasing the learning rate helps sometimes.     random_state : `int` or `None`, optional (default: 0)         Change this to use different intial states for the optimization. If `None`,         the initial state is not reproducible.     use_fast_tsne : `bool`, optional (default: `True`)         Use the MulticoreTSNE package by D. Ulyanov if it is installed.     n_jobs : `int` or `None` (default: `sc.settings.n_jobs`)         Number of jobs.     copy : `bool` (default: `False`)         Return a copy instead of writing to adata.      Returns     -------     Depending on `copy`, returns or updates `adata` with the following fields.      **X_tsne** : `np.ndarray` (`adata.obs`, dtype `float`)         tSNE coordinates of data.
Calculate quality control metrics.      Calculates a number of qc metrics for an AnnData object, see section     `Returns` for specifics. Largely based on `calculateQCMetrics` from scater     [McCarthy17]_. Currently is most efficient on a sparse CSR or dense matrix.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     expr_type : `str`, optional (default: `"counts"`)         Name of kind of values in X.     var_type : `str`, optional (default: `"genes"`)         The kind of thing the variables are.     qc_vars : `Container`, optional (default: `()`)         Keys for boolean columns of `.var` which identify variables you could          want to control for (e.g. "ERCC" or "mito").     percent_top : `Container[int]`, optional (default: `(50, 100, 200, 500)`)         Which proportions of top genes to cover. If empty or `None` don't         calculate. Values are considered 1-indexed, `percent_top=[50]` finds         cumulative proportion to the 50th most expressed gene.     inplace : bool, optional (default: `False`)         Whether to place calculated metrics in `.obs` and `.var`      Returns     -------     Depending on `inplace` returns calculated metrics (`pd.DataFrame`) or     updates `adata`'s `obs` and `var`.      Observation level metrics include:      `total_{var_type}_by_{expr_type}`         E.g. "total_genes_by_counts". Number of genes with positive counts in a cell.     `total_{expr_type}`         E.g. "total_counts". Total number of counts for a cell.     `pct_{expr_type}_in_top_{n}_{var_type}` - for `n` in `percent_top`         E.g. "pct_counts_in_top_50_genes". Cumulative percentage of counts         for 50 most expressed genes in a cell.     `total_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`         E.g. "total_counts_mito". Total number of counts for variabes in         `qc_vars`.     `pct_{expr_type}_{qc_var}` - for `qc_var` in `qc_vars`         E.g. "pct_counts_mito". Proportion of total counts for a cell which         are mitochondrial.      Variable level metrics include:      `total_{expr_type}`         E.g. "total_counts". Sum of counts for a gene.     `mean_{expr_type}`         E.g. "mean counts". Mean expression over all cells.     `n_cells_by_{expr_type}`         E.g. "n_cells_by_counts". Number of cells this expression is         measured in.     `pct_dropout_by_{expr_type}`         E.g. "pct_dropout_by_counts". Percentage of cells this feature does         not appear in.      Example     -------     Calculate qc metrics for visualization.      >>> adata = sc.datasets.pbmc3k()     >>> sc.pp.calculate_qc_metrics(adata, inplace=True)     >>> sns.jointplot("log1p_total_counts", "log1p_n_genes_by_counts", data=adata.obs, kind="hex")
Calculates cumulative proportions of top expressed genes      Parameters     ----------     mtx : `Union[np.array, sparse.spmatrix]`         Matrix, where each row is a sample, each column a feature.     n : `int`         Rank to calculate proportions up to. Value is treated as 1-indexed,         `n=50` will calculate cumulative proportions up to the 50th most         expressed gene.
Calculates total percentage of counts in top ns genes.      Parameters     ----------     mtx : `Union[np.array, sparse.spmatrix]`         Matrix, where each row is a sample, each column a feature.     ns : `Container[Int]`         Positions to calculate cumulative proportion at. Values are considered         1-indexed, e.g. `ns=[50]` will calculate cumulative proportion up to         the 50th most expressed gene.
Simulate dynamic gene expression data [Wittmann09]_ [Wolf18]_.      Sample from a stochastic differential equation model built from     literature-curated boolean gene regulatory networks, as suggested by     [Wittmann09]_. The Scanpy implementation is due to [Wolf18]_.      Parameters     ----------     model : {'krumsiek11', 'toggleswitch'}         Model file in 'sim_models' directory.     params_file : `bool`, (default: `True`)         Read default params from file.     tmax : `int`, optional (default: `None`)         Number of time steps per realization of time series.     branching : `bool`, optional (default: `None`)         Only write realizations that contain new branches.     nrRealizations : int, optional (default: `None`)         Number of realizations.     noiseObs : float, optional (default: `None`)         Observatory/Measurement noise.     noiseDyn : float, optional (default: `None`)         Dynamic noise.     step : int, optional (default: `None`)         Interval for saving state of system.     seed : int, optional (default: `None`)         Seed for generation of random numbers.     writedir: str, optional (default: `None`)         Path to directory for writing output files.      Returns     -------     Annotated data matrix.      Examples     --------     See this `use case <https://github.com/theislab/scanpy_usage/tree/master/170430_krumsiek11>`__
Update parser with tool specific arguments.      This overwrites was is done in utils.uns_args.
Helper function.
Write simulated data.          Accounts for saving at the same time an ID         and a model file.
\     Check whether time series branches.      Parameters     ----------     X (np.array): current time series data.     Xsamples (np.array): list of previous branching samples.     restart (int): counts number of restart trials.     threshold (float, optional): sets threshold for attractor         identification.      Returns     -------     check : bool         true if branching realization     Xsamples         updated list
\     Checks that there are no cycles in graph described by adjacancy matrix.      Parameters     ----------     Adj (np.array): adjancancy matrix of dimension (dim, dim)      Returns     -------     True if there is no cycle, False otherwise.
\     Sample coupling matrix.      Checks that returned graphs contain no self-cycles.      Parameters     ----------     dim : int         dimension of coupling matrix.     connectivity : float         fraction of connectivity, fully connected means 1.,         not-connected means 0, in the case of fully connected, one has         dim*(dim-1)/2 edges in the graph.      Returns     -------     Tuple (Coupl,Adj,Adj_signed) of coupling matrix, adjancancy and     signed adjacancy matrix.
Simulate the model.
Build Xdiff from coefficients of boolean network,             that is, using self.boolCoeff. The employed functions             are Hill type activation and deactivation functions.              See Wittmann et al., BMC Syst. Biol. 3, 98 (2009),             doi:10.1186/1752-0509-3-98 for more details.
Activating hill function.
Inhibiting hill function.              Is equivalent to 1-hill_a(self,x,power,threshold).
Normalized activating hill function.
Normalized inhibiting hill function.              Is equivalent to 1-nhill_a(self,x,power,threshold).
Read the model and the couplings from the model file.
Construct the coupling matrix (and adjacancy matrix) from predefined models             or via sampling.
Using the adjacency matrix, sample a coupling matrix.
In model 1, we want enforce the following signs             on the couplings. Model 2 has the same couplings             but arbitrary signs.
Toggle switch.
Variant of toggle switch.
Simulate the model backwards in time.
Determine parents based on boolean updaterule.              Returns list of parents.
Compute coefficients for tuple space.
Process a string that denotes a boolean rule.
\         Simulate data given only an adjacancy matrix and a model.          The model is a bivariate funtional dependence. The adjacancy matrix         needs to be acyclic.          Parameters         ----------         Adj             adjacancy matrix of shape (dim,dim).          Returns         -------         Data array of shape (n_samples,dim).
Simulate data to model combi regulation.
Plot dispersions versus means for genes.      Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() of Seurat.      Parameters     ----------     adata : :class:`~anndata.AnnData`, `np.recarray`         Result of :func:`~scanpy.api.pp.highly_variable_genes`.     log : `bool`         Plot on logarithmic axes.     show : bool, optional (default: `None`)          Show the plot, do not return axis.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.
Plot dispersions versus means for genes.      Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() of Seurat.      Parameters     ----------     result : `np.recarray`         Result of :func:`~scanpy.api.pp.filter_genes_dispersion`.     log : `bool`         Plot on logarithmic axes.     show : bool, optional (default: `None`)          Show the plot, do not return axis.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.
Calculate overlap count between the values of two dictionaries      Note: dict values must be sets
Calculate overlap coefficient between the values of two dictionaries      Note: dict values must be sets
Calculate jaccard index between the values of two dictionaries      Note: dict values must be sets
Calculate an overlap score between data-deriven marker genes and      provided markers      Marker gene overlap scores can be quoted as overlap counts, overlap      coefficients, or jaccard indices. The method returns a pandas dataframe     which can be used to annotate clusters based on marker gene overlaps.      This function was written by Malte Luecken.      Parameters     ----------     adata         The annotated data matrix.     reference_markers         A marker gene dictionary object. Keys should be strings with the          cell identity name and values are sets or lists of strings which match          format of `adata.var_name`.     key         The key in `adata.uns` where the rank_genes_groups output is stored.         By default this is `'rank_genes_groups'`.     method : `{'overlap_count', 'overlap_coef', 'jaccard'}`, optional          (default: `overlap_count`)         Method to calculate marker gene overlap. `'overlap_count'` uses the         intersection of the gene set, `'overlap_coef'` uses the overlap          coefficient, and `'jaccard'` uses the Jaccard index.     normalize : `{'reference', 'data', 'None'}`, optional (default: `None`)         Normalization option for the marker gene overlap output. This parameter         can only be set when `method` is set to `'overlap_count'`. `'reference'`         normalizes the data by the total number of marker genes given in the          reference annotation per group. `'data'` normalizes the data by the         total number of marker genes used for each cluster.     top_n_markers         The number of top data-derived marker genes to use. By default all          calculated marker genes are used. If `adj_pval_threshold` is set along         with `top_n_markers`, then `adj_pval_threshold` is ignored.     adj_pval_threshold         A significance threshold on the adjusted p-values to select marker          genes. This can only be used when adjusted p-values are calculated by          `sc.tl.rank_genes_groups()`. If `adj_pval_threshold` is set along with          `top_n_markers`, then `adj_pval_threshold` is ignored.     key_added         Name of the `.uns` field that will contain the marker overlap scores.     inplace         Return a marker gene dataframe or store it inplace in `adata.uns`.       Returns     -------     A pandas dataframe with the marker gene overlap scores if `inplace=False`.     For `inplace=True` `adata.uns` is updated with an additional field      specified by the `key_added` parameter (default = 'marker_gene_overlap').        Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.pp.pca(adata, svd_solver='arpack')     >>> sc.pp.neighbors(adata)     >>> sc.tl.louvain(adata)     >>> sc.tl.rank_genes_groups(adata, groupby='louvain')     >>> marker_genes = {'CD4 T cells':{'IL7R'},'CD14+ Monocytes':{'CD14',      ...                 'LYZ'}, 'B cells':{'MS4A1'}, 'CD8 T cells':{'CD8A'},      ...                 'NK cells':{'GNLY', 'NKG7'}, 'FCGR3A+ Monocytes':     ...                 {'FCGR3A', 'MS4A7'}, 'Dendritic Cells':{'FCER1A',      ...                 'CST3'}, 'Megakaryocytes':{'PPBP'}}     >>> marker_matches = sc.tl.marker_gene_overlap(adata, marker_genes)
\     Plot PCA results.      The parameters are the ones of the scatter plot. Call pca_ranking separately     if you want to change the default settings.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     color : string or list of strings, optional (default: `None`)         Keys for observation/cell annotation either as list `["ann1", "ann2"]` or         string `"ann1,ann2,..."`.     use_raw : `bool`, optional (default: `True`)         Use `raw` attribute of `adata` if present.     {scatter_bulk}     show : bool, optional (default: `None`)          Show the plot, do not return axis.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.
Rank genes according to contributions to PCs.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     components : str or list of integers, optional         For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third         principal component.     show : bool, optional (default: `None`)         Show the plot, do not return axis.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on {'.pdf', '.png', '.svg'}.
Plot the variance ratio.      Parameters     ----------     n_pcs : `int`, optional (default: `30`)          Number of PCs to show.     log : `bool`, optional (default: `False`)          Plot on logarithmic scale..     show : `bool`, optional (default: `None`)          Show the plot, do not return axis.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on {'.pdf', '.png', '.svg'}.
Heatmap of pseudotime series.      Parameters     ----------     as_heatmap : bool (default: False)         Plot the timeseries as heatmap.
Plot groups and pseudotime.
\     Plot ranking of genes.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     groups : `str` or `list` of `str`         The groups for which to show the gene ranking.     gene_symbols : `str`         Key for field in `.var` that stores gene symbols if you do not want to         use `.var_names`.     n_genes : `int`, optional (default: 20)         Number of genes to show.     fontsize : `int`, optional (default: 8)         Fontsize for gene names.     ncols : `int`, optional (default: 4)         Number of panels shown per row.     sharey: `bool`, optional (default: True)         Controls if the y-axis of each panels should be shared. But passing         `sharey=False`, each panel has its own y-axis range.     {show_save_ax}
\     Plot ranking of genes using the specified plot type      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     groups : `str` or `list` of `str`         The groups for which to show the gene ranking.     n_genes : `int`, optional (default: 10)         Number of genes to show.     groupby : `str` or `None`, optional (default: `None`)         The key of the observation grouping to consider. By default,         the groupby is chosen from the rank genes groups parameter but         other groupby options can be used.     {show_save_ax}
\     Plot ranking of genes using heatmap plot (see `scanpy.api.pl.heatmap`)      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     groups : `str` or `list` of `str`         The groups for which to show the gene ranking.     n_genes : `int`, optional (default: 10)         Number of genes to show.     groupby : `str` or `None`, optional (default: `None`)         The key of the observation grouping to consider. By default,         the groupby is chosen from the rank genes groups parameter but         other groupby options can be used.  It is expected that         groupby is a categorical. If groupby is not a categorical observation,         it would be subdivided into `num_categories` (see `scanpy.api.pl.heatmap`).     key : `str`         Key used to store the ranking results in `adata.uns`.     **kwds : keyword arguments         Are passed to `scanpy.api.pl.heatmap`.     {show_save_ax}
\     Plot ranking of genes for all tested comparisons.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     groups : list of `str`, optional (default: `None`)         List of group names.     n_genes : `int`, optional (default: 20)         Number of genes to show. Is ignored if `gene_names` is passed.     gene_names : `None` or list of `str` (default: `None`)         List of genes to plot. Is only useful if interested in a custom gene list,         which is not the result of :func:`scanpy.api.tl.rank_genes_groups`.     gene_symbols : `str`, optional (default: `None`)         Key for field in `.var` that stores gene symbols if you do not want to         use `.var_names` displayed in the plot.     use_raw : `bool`, optional (default: `None`)         Use `raw` attribute of `adata` if present. Defaults to the value that         was used in :func:`~scanpy.api.tl.rank_genes_groups`.     split : `bool`, optional (default: `True`)         Whether to split the violins or not.     scale : `str`, optional (default: 'width')         See `seaborn.violinplot`.     strip : `bool`, optional (default: `True`)         Show a strip plot on top of the violin plot.     jitter : `int`, `float`, `bool`, optional (default: `True`)         If set to 0, no points are drawn. See `seaborn.stripplot`.     size : `int`, optional (default: 1)         Size of the jitter points.     {show_save_ax}
Plot results of simulation.      Parameters     ----------     as_heatmap : bool (default: False)         Plot the timeseries as heatmap.     tmax_realization : int or None (default: False)         Number of observations in one realization of the time series. The data matrix         adata.X consists in concatenated realizations.     shuffle : bool, optional (default: False)         Shuffle the data.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on {{'.pdf', '.png', '.svg'}}.     show : bool, optional (default: `None`)         Show the plot, do not return axis.
Plot the density of cells in an embedding (per condition)      Plots the gaussian kernel density estimates (over condition) from the     `sc.tl.embedding_density()` output.      This function was written by Sophie Tritschler and implemented into     Scanpy by Malte Luecken.          Parameters     ----------     adata         The annotated data matrix.     basis         The embedding over which the density was calculated. This embedded         representation should be found in `adata.obsm['X_[basis]']``.     key         Name of the `.obs` covariate that contains the density estimates     group         The category in the categorical observation annotation to be plotted.         For example, 'G1' in the cell cycle 'phase' covariate.     color_map         Matplolib color map to use for density plotting.     bg_dotsize         Dot size for background data points not in the `group`.     fg_dotsize         Dot size for foreground data points in the `group`.     vmax         Density that corresponds to color bar maximum.     vmin         Density that corresponds to color bar minimum.     {show_save_ax}      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.tl.umap(adata)     >>> sc.tl.embedding_density(adata, basis='umap', groupby='phase')     >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase',      ...                         group='G1')     >>> sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase',      ...                         group='S')
Run Diffusion maps using the adaptive anisotropic kernel [Setty18]_.      Palantir is an algorithm to align cells along differentiation trajectories.     Palantir models differentiation as a stochastic process where stem cells     differentiate to terminally differentiated cells by a series of steps through     a low dimensional phenotypic manifold. Palantir effectively captures the     continuity in cell states and the stochasticity in cell fate determination.     Palantir has been designed to work with multidimensional single cell data     from diverse technologies such as Mass cytometry and single cell RNA-seq.      .. note::         More information and bug reports `here <https://github.com/dpeerlab/Palantir>`__.      Parameters     ----------     adata         An AnnData object, or Dataframe of cells X genes.      Returns     -------     `.uns['palantir_norm_data']`         A `data_df` copy of adata if normalized      `pca_results`         PCA projections and explained variance ratio of adata:         - `.uns['palantir_pca_results']['pca_projections']`         - `.uns['palantir_pca_results']['variance_ratio']`      `dm_res`         Diffusion components, corresponding eigen values and diffusion operator:         - `.uns['palantir_diff_maps']['EigenVectors']`         - `.uns['palantir_diff_maps']['EigenValues']`         - `.uns['palantir_diff_maps']['T']`      `.uns['palantir_ms_data']`         The `ms_data` - Multi scale data matrix      `.uns['palantir_tsne']` : `tsne`         tSNE on diffusion maps      `.uns['palantir_imp_df']` : `imp_df`         Imputed data matrix (MAGIC imputation)      Example     -------      >>> import scanpy.external as sce     >>> import scanpy as sc      A sample data is available `here <https://github.com/dpeerlab/Palantir/tree/master/data>`_.      To view the plots, it is recommended to run Jupyter notebook      *Load sample data*      >>> adata = sc.read_csv(filename="Palantir/data/marrow_sample_scseq_counts.csv.gz")      **Pre-processing**      The provided adata will be used as input to the embedded `palantir` methods:      >>> d = sce.tl.palantir( adata=adata )      At this point, a new class object, `d`, will be instantiated. If the data     needs pre-processing - filtering low genes/cells counts, or normalization,     or log transformation, set the `filter_low`, `normalize`, or `log_transform`     to `True`:      >>> d.filter_low = True     >>> d.normalize = True     >>> d.log_transform = True      The created object `d.palantir` can be used to override the default     parameters used for pre-processing.      Follow the next step to pass the data to palantir methods, to generate the     return objects listed above.      **Run Palantir**      >>> d.process()      By calling this method `palantir` will run and generate the various outputs.     The generated objects will be pushed to `adata` and stored for further use.     Once instantiated, *Principal component analysis*, *Diffusion maps*,     *tSNE on Diffusion maps*, and *MAGIC imputation* data objects will be created     using the `palantir` default parameters.      If running `palantir` using default parameters is not satisfactory,     `d.palantir` methods can be used to override and substitute the individual     outputs already embedded into `adata`.      **Plotting**      *tSNE visualization*      >>> fig, ax = d.palantir.plot.plot_tsne(d.tsne)     >>> fig, ax = d.palantir.plot.plot_tsne_by_cell_sizes(d.data_df, d.tsne)      *Gene expression can be visualized on tSNE maps*      >>> d.palantir.plot.plot_gene_expression(d.imp_df, d.tsne, ['CD34', 'MPO', 'GATA1', 'IRF8'])      *Diffusion maps*      >>> d.palantir.plot.plot_diffusion_components(d.tsne, d.dm_res)      **Visualizing Palantir results**      Palantir can be run by specifying an approximate early cell. While Palantir     automatically determines the terminal states, they can also be specified using the     `termine_states` parameter.      >>> start_cell = 'Run5_164698952452459'     >>> pr_res = d.palantir.core.run_palantir(d.ms_data, start_cell, num_waypoints=500)     >>> palantir.plot.plot_palantir_results(pr_res, d.tsne)      - note that a `start_cell` must be defined for every data set. The start cell for     this dataset was chosen based on high expression of CD34.      For further demonstration of palantir visualizations please follow this notebook     `Palantir_sample_notebook.ipynb <https://github.com/dpeerlab/Palantir/blob/master/notebooks/Palantir_sample_notebook.ipynb>`_.     It provides a comprehensive guide to draw *gene expression trends*, amongst other things.
Exports to a SPRING project directory [Weinreb17]_.      Visualize annotation present in `adata`. By default, export all gene expression data     from `adata.raw` and categorical and continuous annotations present in `adata.obs`.      See `SPRING <https://github.com/AllonKleinLab/SPRING>`__ or [Weinreb17]_ for details.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix: `adata.uns['neighbors']` needs to         be present.     project_dir : `str`         Path to directory for exported SPRING files.     embedding_method: `str`         Name of a 2-D embedding in `adata.obsm`     subplot_name: `str`, optional (default: `None`)         Name of subplot folder to be created at `project_dir+"/"+subplot_name`     cell_groupings : `str`, `list` of `str`, optional (default: `None`)         Instead of importing all categorical annotations when `None`,         pass a list of keys for `adata.obs`.     custom_color_tracks : `str`, `list` of `str`, optional (default: `None`)         Specify specific `adata.obs` keys for continuous coloring.     total_counts_key: `str`, optional (default: "n_counts")         Name of key for total transcript counts in `adata.obs`.     overwrite: `boolean`, optional (default: `False`)         When `True`, existing counts matrices in `project_dir` are overwritten.      Examples     --------     See this `tutorial <https://github.com/theislab/scanpy_usage/tree/master/171111_SPRING_export>`__.
SPRING standard: filename = main_spring_dir + "counts_norm_sparse_genes.hdf5"
SPRING standard: filename = main_spring_dir + "counts_norm_sparse_cells.hdf5"
SPRING standard: filename = main_spring_dir + "/counts_norm.npz"
Export adata to a UCSC Cell Browser project directory. If `html_dir` is     set, subsequently build the html files from the project directory into     `html_dir`.  If `port` is set, start an HTTP server in the background and     serve `html_dir` on `port`.      By default, export all gene expression data from `adata.raw`, the     annotations `louvain`, `percent_mito`, `n_genes` and `n_counts` and the top     `nb_marker` cluster markers. All existing files in data_dir are     overwritten, except cellbrowser.conf.      See `UCSC Cellbrowser <https://github.com/maximilianh/cellBrowser>`__ for     details.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix     data_dir : `str`         Path to directory for exported Cell Browser files.         Usually these are the files `exprMatrix.tsv.gz`, `meta.tsv`,         coordinate files like `tsne.coords.tsv`,         and cluster marker gene lists like `markers.tsv`.         A file `cellbrowser.conf` is also created with pointers to these files.         As a result, each adata object should have its own project_dir.     data_name : `str`         Name of dataset in Cell Browser, a string without special characters.         This is written to `data_dir`/cellbrowser.conf.         Ideally this is a short unique name for the dataset,         like "pbmc3k" or "tabulamuris".     embedding_keys: `list` of `str` or `dict` of `key (str)`->`display label (str)`         2-D embeddings in `adata.obsm` to export.         The prefix "`X_`" or "`X_draw_graph_`" is not necessary.         Coordinates missing from `adata` are skipped.         By default, these keys are tried: ["tsne", "umap", "pagaFa", "pagaFr",         "pagaUmap", "phate", "fa", "fr", "kk", "drl", "rt"].         For these, default display labels are automatically used.         For other values, you can specify a dictionary instead of a list,         the values of the dictionary are then the display labels for the         coordinates, e.g. `{'tsne' : "t-SNE by Scanpy"}`     annot_keys: `list` of `str` or `dict` of `key (str)`->`display label (str)`         Annotations in `adata.obsm` to export.         Can be a dictionary with key -> display label.     skip_matrix: `boolean`         Do not export the matrix.         If you had previously exported this adata into the same `data_dir`,         then there is no need to export the whole matrix again.         This option will make the export a lot faster,         e.g. when only coordinates or meta data were changed.     html_dir: `str`         If this variable is set, the export will build html         files from `data_dir` to `html_dir`, creating html/js/json files.         Usually there is one global html output directory for all datasets.         Often, `html_dir` is located under a webserver's (like Apache)         htdocs directory or is copied to one.         A directory `html_dir`/`project_name` will be created and         an index.html will be created under `html_dir` for all subdirectories.         Existing files will be overwritten.         If do not to use html_dir,         you can use the command line tool `cbBuild` to build the html directory.     port: `int`         If this variable and `html_dir` are set,         Python's built-in web server will be spawned as a daemon in the         background and serve the files under `html_dir`.         To kill the process, call `cellbrowser.cellbrowser.stop()`.     do_debug: `boolean`         Activate debugging output      Examples     --------     See this     `tutorial <https://github.com/theislab/scanpy_usage/tree/master/181126_Cellbrowser_exports>`__.
Force-directed graph drawing [Islam11]_ [Jacomy14]_ [Chippada18]_.      An alternative to tSNE that often preserves the topology of the data     better. This requires to run :func:`~scanpy.api.pp.neighbors`, first.      The default layout ('fa', `ForceAtlas2`) [Jacomy14]_ uses the package `fa2     <https://github.com/bhargavchippada/forceatlas2>`__ [Chippada18]_, which can     be installed via `pip install fa2`.      `Force-directed graph drawing     <https://en.wikipedia.org/wiki/Force-directed_graph_drawing>`__ describes a     class of long-established algorithms for visualizing graphs. It has been     suggested for visualizing single-cell data by [Islam11]_. Many other layouts     as implemented in igraph [Csardi06]_ are available. Similar approaches have     been used by [Zunder15]_ or [Weinreb17]_.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     layout : `str`, optional (default: 'fa')         'fa' (`ForceAtlas2`) or any valid `igraph layout         <http://igraph.org/c/doc/igraph-Layout.html>`__. Of particular interest         are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid Fruchterman Reingold,         faster than 'fr'), 'kk' (Kamadi Kawai', slower than 'fr'), 'lgl' (Large         Graph, very fast), 'drl' (Distributed Recursive Layout, pretty fast) and         'rt' (Reingold Tilford tree layout).     root : `int` or `None`, optional (default: `None`)         Root for tree layouts.     random_state : `int` or `None`, optional (default: 0)         For layouts with random initialization like 'fr', change this to use         different intial states for the optimization. If `None`, no seed is set.     adjacency : sparse matrix or `None`, optional (default: `None`)         Sparse adjacency matrix of the graph, defaults to         `adata.uns['neighbors']['connectivities']`.     key_ext : `str`, optional (default: `None`)         By default, append `layout`.     proceed : `bool`, optional (default: `None`)         Continue computation, starting off with 'X_draw_graph_`layout`'.     init_pos : {'paga', any valid 2d-`.obsm` key, `False`}, optional (default: `False`)         Use precomputed coordinates for initialization.     copy : `bool` (default: `False`)         Return a copy instead of writing to adata.     **kwds : further parameters         Parameters of chosen igraph layout. See, e.g.,         `fruchterman_reingold <http://igraph.org/python/doc/igraph.Graph-class.html#layout_fruchterman_reingold>`__. One of the most important ones is `maxiter`.      Returns     -------     Depending on `copy`, returns or updates `adata` with the following field.      **X_draw_graph_layout** : `adata.obsm`         Coordinates of graph layout. E.g. for layout='fa' (the default), the field is called 'X_draw_graph_fa'
Set resolution/size, styling and format of figures.          Parameters         ----------         scanpy : `bool`, optional (default: `True`)             Init default values for ``matplotlib.rcParams`` suited for Scanpy.         dpi : `int`, optional (default: `80`)             Resolution of rendered figures - this influences the size of figures in notebooks.         dpi_save : `int`, optional (default: `150`)             Resolution of saved figures. This should typically be higher to achieve             publication quality.         frameon : `bool`, optional (default: `True`)             Add frames and axes labels to scatter plots.         vector_friendly : `bool`, optional (default: `True`)             Plot scatter plots using `png` backend even when exporting as `pdf` or `svg`.         fontsize : `int`, optional (default: 14)             Set the fontsize for several `rcParams` entries. Ignored if `scanpy=False`.         color_map : `str`, optional (default: `None`)             Convenience method for setting the default color map. Ignored if `scanpy=False`.         format : {'png', 'pdf', 'svg', etc.}, optional (default: 'pdf')             This sets the default format for saving figures: `file_format_figs`.         transparent : `bool`, optional (default: `True`)             Save figures with transparent back ground. Sets             `rcParams['savefig.transparent']`.         ipython_format : list of `str`, optional (default: 'png2x')             Only concerns the notebook/IPython environment; see             `IPython.core.display.set_matplotlib_formats` for more details.
\     Scatter plot in UMAP basis.      Parameters     ----------     {adata_color_etc}     {edges_arrows}     {scatter_bulk}     {show_save_ax}      Returns     -------     If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
\     Scatter plot in tSNE basis.      Parameters     ----------     {adata_color_etc}     {edges_arrows}     {scatter_bulk}     {show_save_ax}      Returns     -------     If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
\     Scatter plot in Diffusion Map basis.      Parameters     ----------     {adata_color_etc}     {scatter_bulk}     {show_save_ax}      Returns     -------     If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
\     Scatter plot in graph-drawing basis.      Parameters     ----------     {adata_color_etc}     layout : {{'fa', 'fr', 'drl', ...}}, optional (default: last computed)         One of the `draw_graph` layouts, see         :func:`~scanpy.api.tl.draw_graph`. By default, the last computed layout         is used.     {edges_arrows}     {scatter_bulk}     {show_save_ax}      Returns     -------     If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
\     Scatter plot in PCA coordinates.      Parameters     ----------     {adata_color_etc}     {scatter_bulk}     {show_save_ax}      Returns     -------     If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
Returns the data points corresponding to the selected basis, projection and/or components.      Because multiple components are given (eg components=['1,2', '2,3'] the     returned data are lists, containing each of the components. When only one component is plotted     the list length is 1.      Returns     -------     data_points : list         Each entry is a numpy array containing the data points     components : list         The cleaned list of components. Eg. [(0,1)] or [(0,1), (1,2)]         for components = [1,2] and components=['1,2', '2,3'] respectively
Adds a color bar or a legend to the given ax. A legend is added when the     data is categorical and a color bar is added when a continuous value was used.
Sets the adata.uns[value_to_plot + '_colors'] according to the given palette      Parameters     ----------     adata         annData object     value_to_plot         name of a valid categorical observation     palette         Palette should be either a valid :func:`~matplotlib.pyplot.colormaps` string,         a list of colors (in a format that can be understood by matplotlib,         eg. RGB, RGBS, hex, or a cycler object with key='color'      Returns     -------     None
Sets the adata.uns[value_to_plot + '_colors'] using default color palettes      Parameters     ----------     adata : annData object     value_to_plot : name of a valid categorical observation      Returns     -------     None
Returns the value or color associated to each data point.     For categorical data, the return value is list of colors taken     from the category palette or from the given `palette` value.      For non-categorical data, the values are returned
converts the 'basis' into the proper name.
Calculate correlation matrix.          Calculate a correlation matrix for genes strored in sample annotation using :func:`~scanpy.api.tl.rank_genes_groups`.          Parameters         ----------         adata : :class:`~anndata.AnnData`             Annotated data matrix.         name_list : list, optional (default: None)             Takes a list of genes for which to calculate the correlation matrix         groupby : `str`, optional (default: None)             If no name list is passed, genes are selected from the             results of rank_gene_groups. Then this is the key of the sample grouping to consider.             Note that in this case also a group index has to be specified.         group : `int`, optional (default: None)             Group index for which the correlation matrix for top_ranked genes should be calculated.             Currently only int is supported, will change very soon         n_genes : `int`, optional (default: 20)             For how many genes to calculate correlation matrix? If specified, cuts the name list             (in whatever order it is passed).         data : {'Complete', 'Group', 'Rest'}, optional (default: 'Complete')             At the moment, this is only relevant for the case that name_list is drawn from rank_gene_groups results.             If specified, collects mask for the called group and then takes only those cells specified.             If 'Complete', calculate correlation using full data             If 'Group', calculate correlation within the selected group.             If 'Rest', calculate corrlation for everything except the group         method : {‘pearson’, ‘kendall’, ‘spearman’} optional (default: 'pearson')             Which kind of correlation coefficient to use             pearson : standard correlation coefficient             kendall : Kendall Tau correlation coefficient             spearman : Spearman rank correlation         annotation_key: String, optional (default: None)             Allows to define the name of the anndata entry where results are stored.
Calculate correlation matrix.              Calculate a correlation matrix for genes strored in sample annotation using rank_genes_groups.py              Parameters             ----------             adata : :class:`~anndata.AnnData`                 Annotated data matrix.             groupby : `str`                 The key of the sample grouping to consider.             group : `str`, int, optional (default: None)                 Group name or index for which the correlation matrix for top_ranked genes should be calculated.                 If no parameter is passed, ROC/AUC is calculated for all groups             n_genes : `int`, optional (default: 100)                 For how many genes to calculate ROC and AUC. If no parameter is passed, calculation is done for                 all stored top ranked genes.
Normalization and filtering as of [Weinreb17]_.      Expects non-logarithmized data. If using logarithmized data, pass `log=False`.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     copy : bool (default: False)         Return a copy if true.
Normalization and filtering as of Seurat [Satija15]_.      This uses a particular preprocessing.      Expects non-logarithmized data. If using logarithmized data, pass `log=False`.
Normalization and filtering as of [Zheng17]_.      Reproduces the preprocessing of [Zheng17]_ - the Cell Ranger R Kit of 10x     Genomics.      Expects non-logarithmized data. If using logarithmized data, pass `log=False`.      The recipe runs the following steps      .. code:: python          sc.pp.filter_genes(adata, min_counts=1)  # only consider genes with more than 1 count         sc.pp.normalize_per_cell(                # normalize with total UMI count per cell              adata, key_n_counts='n_counts_all')         filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes             adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False)         adata = adata[:, filter_result.gene_subset]     # subset the genes         sc.pp.normalize_per_cell(adata)          # renormalize after filtering         if log: sc.pp.log1p(adata)               # log transform: adata.X = log(adata.X + 1)         sc.pp.scale(adata)                       # scale to unit variance and shift to zero mean       Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     n_top_genes : `int`, optional (default: 1000)         Number of genes to keep.     log : `bool`, optional (default: `True`)         Take logarithm.     plot : `bool`, optional (default: `True`)         Show a plot of the gene dispersion vs. mean relation.     copy : `bool`, optional (default: `False`)         Return a copy of `adata` instead of updating it.      Returns     -------     Returns or updates `adata` depending on `copy`.
\     Computes a hierarchical clustering for the given `groupby` categories.      By default, the PCA representation is used unless `.X` has less than 50 variables.      Alternatively, a list of `var_names` (e.g. genes) can be given.      Average values of either `var_names` or components are used to compute a correlation matrix.      The hierarchical clustering can be visualized using `sc.pl.dendrogram` or multiple other     visualizations that can include a dendrogram: `matrixplot`, `heatmap`, `dotplot` and `stacked_violin`      .. note::         The computation of the hierarchical clustering is based on predefined groups and not         per cell. The correlation matrix is computed using by default pearson but other methods         are available.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix     {n_pcs}     {use_rep}     var_names : `list of str` (default: None)         List of var_names to use for computing the hierarchical clustering. If `var_names` is given,         then `use_rep` and `n_pcs` is ignored.     use_raw : `bool`, optional (default: None)         Only when `var_names` is not None. Use `raw` attribute of `adata` if present.     cor_method : `str`, optional (default: `"pearson"`)         correlation method to use. Options are 'pearson', 'kendall', and 'spearman'     linkage_method : `str`, optional (default: `"complete"`)         linkage method to use. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html         for more information.     key_added : : `str`, optional (default: `None`)         By default, the dendrogram information is added to `.uns['dendrogram_' + groupby]`. Notice         that the `groupby` information is added to the dendrogram.      Returns     -------     adata.uns['dendrogram'] (or instead of 'dendrogram' the value selected for `key_added`) is updated     with the dendrogram information      Examples     --------      >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.tl.dendrogram(adata, groupby='bulk_labels')     >>> sc.pl.dendrogram(adata)     >>> sc.pl.dotplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],     ...               groupby='bulk_labels', dendrogram=True)
PHATE [Moon17]_.      Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE)     embeds high dimensional single-cell data into two or three dimensions for     visualization of biological progressions.      For more information and access to the object-oriented interface, read the     `PHATE documentation <https://phate.readthedocs.io/>`__.  For     tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE     GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help     using PHATE, go `here <https://krishnaswamylab.org/get-help>`__.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     n_components : `int`, optional (default: 2)         number of dimensions in which the data will be embedded     k : `int`, optional (default: 5)         number of nearest neighbors on which to build kernel     a : `int`, optional (default: 15)         sets decay rate of kernel tails.         If None, alpha decaying kernel is not used     n_landmark : `int`, optional (default: 2000)         number of landmarks to use in fast PHATE     t : `int` or 'auto', optional (default: 'auto')         power to which the diffusion operator is powered         sets the level of diffusion. If 'auto', t is selected         according to the knee point in the Von Neumann Entropy of         the diffusion operator     gamma : float, optional, default: 1         Informational distance constant between -1 and 1.         `gamma=1` gives the PHATE log potential, `gamma=0` gives         a square root potential.     n_pca : `int`, optional (default: 100)         Number of principal components to use for calculating         neighborhoods. For extremely large datasets, using         n_pca < 20 allows neighborhoods to be calculated in         log(n_samples) time.     knn_dist : string, optional (default: 'euclidean')         recommended values: 'euclidean' and 'cosine'         Any metric from `scipy.spatial.distance` can be used         distance metric for building kNN graph     mds_dist : string, optional (default: 'euclidean')         recommended values: 'euclidean' and 'cosine'         Any metric from `scipy.spatial.distance` can be used         distance metric for MDS     mds : {'classic', 'metric', 'nonmetric'}, optional (default: 'metric')         Selects which MDS algorithm is used for dimensionality reduction     n_jobs : `int` or `None`, optional (default: `sc.settings.n_jobs`)         The number of jobs to use for the computation.         If `None`, `sc.settings.n_jobs` is used.         If -1 all CPUs are used. If 1 is given, no parallel computing code is         used at all, which is useful for debugging.         For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for         n_jobs = -2, all CPUs but one are used     random_state : `int`, `numpy.RandomState` or `None`, optional (default: `None`)         Random seed. Defaults to the global `numpy` random number generator     verbose : `bool`, `int` or `None`, optional (default: `sc.settings.verbosity`)         If `True` or an integer `>= 2`, print status messages.         If `None`, `sc.settings.verbosity` is used.     copy : `bool` (default: `False`)         Return a copy instead of writing to `adata`.     kwargs : additional arguments to `phate.PHATE`      Returns     -------     Depending on `copy`, returns or updates `adata` with the following fields.      **X_phate** : `np.ndarray`, (`adata.obs`, shape=[n_samples, n_components], dtype `float`)         PHATE coordinates of data.      Examples     --------     >>> import scanpy.api as sc     >>> import phate     >>> tree_data, tree_clusters = phate.tree.gen_dla(n_dim=100,                                                       n_branch=20,                                                       branch_length=100)     >>> tree_data.shape     (2000, 100)     >>> adata = sc.AnnData(tree_data)     >>> sc.tl.phate(adata, k=5, a=20, t=150)     >>> adata.obsm['X_phate'].shape     (2000, 2)     >>> sc.pl.phate(adata)
Scatter and PAGA graph side-by-side.      Consists in a scatter plot and the abstracted graph. See     :func:`~scanpy.api.pl.paga` for all related parameters.      See :func:`~scanpy.api.pl.paga_path` for visualizing gene changes along paths     through the abstracted graph.      Additional parameters are as follows.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     kwds_scatter : `dict`         Keywords for :func:`~scanpy.api.pl.scatter`.     kwds_paga : `dict`         Keywords for :func:`~scanpy.api.pl.paga`.      Returns     -------     A list of `matplotlib.axes.Axes` if `show` is `False`.
Plot the PAGA graph through thresholding low-connectivity edges.      Compute a coarse-grained layout of the data. Reuse this by passing     `init_pos='paga'` to :func:`~scanpy.api.tl.umap` or     :func:`~scanpy.api.tl.draw_graph` and obtain embeddings with more meaningful     global topology [Wolf19]_.      This uses ForceAtlas2 or igraph's layout algorithms for most layouts [Csardi06]_.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     threshold : `float` or `None`, optional (default: 0.01)         Do not draw edges for weights below this threshold. Set to 0 if you want         all edges. Discarding low-connectivity edges helps in getting a much         clearer picture of the graph.     color : gene name or obs. annotation, optional (default: `None`)         The node colors. Also plots the degree of the abstracted graph when         passing {'degree_dashed', 'degree_solid'}.     labels : `None`, `str`, `list`, `dict`, optional (default: `None`)         The node labels. If `None`, this defaults to the group labels stored in         the categorical for which :func:`~scanpy.api.tl.paga` has been computed.     pos : `np.ndarray`, filename of `.gdf` file,  optional (default: `None`)         Two-column array-like storing the x and y coordinates for drawing.         Otherwise, path to a `.gdf` file that has been exported from Gephi or         a similar graph visualization software.     layout : {'fa', 'fr', 'rt', 'rt_circular', 'eq_tree', ...}, optional (default: 'fr')         Plotting layout that computes positions. 'fa' stands for ForceAtlas2, 'fr' stands for         Fruchterman-Reingold, 'rt' stands for Reingold Tilford. 'eq_tree' stands         for 'eqally spaced tree'. All but 'fa' and 'eq_tree' are igraph         layouts. All other igraph layouts are also permitted. See also parameter         `pos` and :func:`~scanpy.api.tl.draw_graph`.     init_pos : `np.ndarray`, optional (default: `None`)         Two-column array storing the x and y coordinates for initializing the         layout.     random_state : `int` or `None`, optional (default: 0)         For layouts with random initialization like 'fr', change this to use         different intial states for the optimization. If `None`, the initial         state is not reproducible.     root : `int`, `str` or list of `int`, optional (default: 0)         If choosing a tree layout, this is the index of the root node or a list         of root node indices. If this is a non-empty vector then the supplied         node IDs are used as the roots of the trees (or a single tree if the         graph is connected). If this is `None` or an empty list, the root         vertices are automatically calculated based on topological sorting.     transitions : `str` or `None`, optional (default: `None`)         Key for `.uns['paga']` that specifies the matrix that - for instance         `'transistions_confidence'` - that specifies the matrix that stores the         arrows.     solid_edges : `str`, optional (default: 'paga_connectivities')         Key for `.uns['paga']` that specifies the matrix that stores the edges         to be drawn solid black.     dashed_edges : `str` or `None`, optional (default: `None`)         Key for `.uns['paga']` that specifies the matrix that stores the edges         to be drawn dashed grey. If `None`, no dashed edges are drawn.     single_component : `bool`, optional (default: `False`)         Restrict to largest connected component.     fontsize : `int` (default: `None`)         Font size for node labels.     text_kwds : keywords for `matplotlib.text`         See `here         <https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.text.html#matplotlib.axes.Axes.text>`_.     node_size_scale : `float` (default: 1.0)         Increase or decrease the size of the nodes.     node_size_power : `float` (default: 0.5)         The power with which groups sizes influence the radius of the nodes.     edge_width_scale : `float`, optional (default: 5)         Edge with scale in units of `rcParams['lines.linewidth']`.     min_edge_width : `float`, optional (default: `None`)         Min width of solid edges.     max_edge_width : `float`, optional (default: `None`)         Max width of solid and dashed edges.     arrowsize : `int`, optional (default: 30)        For directed graphs, choose the size of the arrow head head's length and        width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute        `mutation_scale` for more info.     export_to_gexf : `bool`, optional (default: `None`)         Export to gexf format to be read by graph visualization programs such as         Gephi.     normalize_to_color : `bool`, optional (default: `False`)         Whether to normalize categorical plots to `color` or the underlying         grouping.     cmap : color map         The color map.     cax : :class:`~matplotlib.axes.Axes`         A matplotlib axes object for a potential colorbar.     cb_kwds : colorbar keywords         See `here         <https://matplotlib.org/api/colorbar_api.html#matplotlib.colorbar.ColorbarBase>`__,         for instance, `ticks`.     add_pos : `bool`, optional (default: `True`)         Add the positions to `adata.uns['paga']`.     title : `str`, optional (default: `None`)         Provide a title.     frameon : `bool`, optional (default: `None`)         Draw a frame around the PAGA graph.     hide : `bool`, optional (default: `False`)         Do not create a plot.     plot : `bool`, optional (default: `True`)         If `False`, do not create the figure, simply compute the layout.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.     ax : :class:`~matplotlib.axes.Axes`         A matplotlib axes object.      Returns     -------     If `show==False`, one or more :class:`~matplotlib.axes.Axes` objects.     Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`.      Notes     -----     When initializing the positions, note that - for some reason - igraph     mirrors coordinates along the x axis... that is, you should increase the     `maxiter` parameter by 1 if the layout is flipped.      See also     --------     tl.paga     pl.paga_compare     pl.paga_path
Gene expression and annotation changes along paths in the abstracted graph.      Parameters     ----------     adata : :class:`~anndata.AnnData`         An annotated data matrix.     nodes : list of group names or their category indices         A path through nodes of the abstracted graph, that is, names or indices         (within `.categories`) of groups that have been used to run PAGA.     keys : list of str         Either variables in `adata.var_names` or annotations in         `adata.obs`. They are plotted using `color_map`.     use_raw : `bool`, optional (default: `True`)         Use `adata.raw` for retrieving gene expressions if it has been set.     annotations : list of annotations, optional (default: ['dpt_pseudotime'])         Plot these keys with `color_maps_annotations`. Need to be keys for         `adata.obs`.     color_map : color map for plotting keys or `None`, optional (default: `None`)         Matplotlib colormap.     color_maps_annotations : dict storing color maps or `None`, optional (default: {'dpt_pseudotime': 'Greys'})         Color maps for plotting the annotations. Keys of the dictionary must         appear in `annotations`.     palette_groups : list of colors or `None`, optional (default: `None`)         Ususally, use the same `sc.pl.palettes...` as used for coloring the         abstracted graph.     n_avg : `int`, optional (default: 1)         Number of data points to include in computation of running average.     groups_key : `str`, optional (default: `None`)         Key of the grouping used to run PAGA. If `None`, defaults to         `adata.uns['paga']['groups']`.     as_heatmap : `bool`, optional (default: `True`)         Plot the timeseries as heatmap. If not plotting as heatmap,         `annotations` have no effect.     show_node_names : `bool`, optional (default: `True`)         Plot the node names on the nodes bar.     show_colorbar : `bool`, optional (default: `True`)         Show the colorbar.     show_yticks : `bool`, optional (default: `True`)         Show the y ticks.     normalize_to_zero_one : `bool`, optional (default: `True`)         Shift and scale the running average to [0, 1] per gene.     return_data : `bool`, optional (default: `False`)         Return the timeseries data in addition to the axes if `True`.     show : `bool`, optional (default: `None`)          Show the plot, do not return axis.     save : `bool` or `str`, optional (default: `None`)         If `True` or a `str`, save the figure. A string is appended to the         default filename. Infer the filetype if ending on \\{'.pdf', '.png', '.svg'\\}.     ax : :class:`~matplotlib.axes.Axes`          A matplotlib axes object.      Returns     -------     A :class:`~matplotlib.axes.Axes` object, if `ax` is `None`, else `None`.     If `return_data`, return the timeseries data in addition to an axes.
Connectivity of paga groups.
\     Scatter plot along observations or variables axes.      Color the plot using annotations of observations (`.obs`), variables     (`.var`) or expression of genes (`.var_names`).      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     x : `str` or `None`         x coordinate.     y : `str` or `None`         y coordinate.     color : string or list of strings, optional (default: `None`)         Keys for annotations of observations/cells or variables/genes, e.g.,         `'ann1'` or `['ann1', 'ann2']`.     use_raw : `bool`, optional (default: `None`)         Use `raw` attribute of `adata` if present.     layers : `str` or tuple of strings, optional (default: `X`)         Use the `layers` attribute of `adata` if present: specify the layer for         `x`, `y` and `color`. If `layers` is a string, then it is expanded to         `(layers, layers, layers)`.     basis : {{'pca', 'tsne', 'umap', 'diffmap', 'draw_graph_fr', etc.}}         String that denotes a plotting tool that computed coordinates.     {scatter_temp}     {show_save_ax}      Returns     -------     If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
See docstring of scatter.
Plot rankings.      See, for example, how this is used in pl.pca_ranking.      Parameters     ----------     adata : AnnData         The data.     attr : {'var', 'obs', 'uns', 'varm', 'obsm'}         The attribute of AnnData that contains the score.     keys : str or list of str         The scores to look up an array from the attribute of adata.      Returns     -------     Returns matplotlib gridspec with access to the axes.
\     Violin plot.      Wraps `seaborn.violinplot` for :class:`~anndata.AnnData`.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     keys : `str` or list of `str`         Keys for accessing variables of `.var_names` or fields of `.obs`.     groupby : `str` or `None`, optional (default: `None`)         The key of the observation grouping to consider.     log : `bool`, optional (default: `False`)         Plot on logarithmic axis.     use_raw : `bool`, optional (default: `None`)         Use `raw` attribute of `adata` if present.     multi_panel : `bool`, optional (default: `False`)         Display keys in multiple panels also when `groupby is not None`.     stripplot : `bool` optional (default: `True`)         Add a stripplot on top of the violin plot.         See `seaborn.stripplot`.     jitter : `float` or `bool`, optional (default: `True`)         Add jitter to the stripplot (only when stripplot is True)         See `seaborn.stripplot`.     size : int, optional (default: 1)         Size of the jitter points.     order : list of str, optional (default: `True`)         Order in which to show the categories.     scale : {{'area', 'count', 'width'}}, optional (default: 'width')         The method used to scale the width of each violin. If 'area', each         violin will have the same area. If 'count', the width of the violins         will be scaled by the number of observations in that bin. If 'width',         each violin will have the same width.     xlabel : `str`, optional (default: `''`)         Label of the x axis. Defaults to `groupby` if `rotation` is `None`,         otherwise, no label is shown.     rotation : `float`, optional (default: `None`)         Rotation of xtick labels.     {show_save_ax}     **kwds : keyword arguments         Are passed to `seaborn.violinplot`.      Returns     -------     A :class:`~matplotlib.axes.Axes` object if `ax` is `None` else `None`.
\     Hierarchically-clustered heatmap.      Wraps `seaborn.clustermap     <https://seaborn.pydata.org/generated/seaborn.clustermap.html>`__ for     :class:`~anndata.AnnData`.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     obs_keys : `str`         Categorical annotation to plot with a different color map.         Currently, only a single key is supported.     use_raw : `bool`, optional (default: `None`)         Use `raw` attribute of `adata` if present.     {show_save_ax}     **kwds : keyword arguments         Keyword arguments passed to `seaborn.clustermap         <https://seaborn.pydata.org/generated/seaborn.clustermap.html>`__.      Returns     -------     If `show == False`, a `seaborn.ClusterGrid` object.      Notes     -----     The returned object has a savefig() method that should be used if you want     to save the figure object without clipping the dendrograms.      To access the reordered row indices, use:     clustergrid.dendrogram_row.reordered_ind      Column indices, use: clustergrid.dendrogram_col.reordered_ind      Examples     --------     Soon to come with figures. In the meanwile, see     https://seaborn.pydata.org/generated/seaborn.clustermap.html.      >>> import scanpy.api as sc     >>> adata = sc.datasets.krumsiek11()     >>> sc.pl.clustermap(adata, obs_keys='cell_type')
\     Stacked violin plots.      Makes a compact image composed of individual violin plots (from `seaborn.violinplot`)     stacked on top of each other. Useful to visualize gene expression per cluster.      Wraps `seaborn.violinplot` for :class:`~anndata.AnnData`.      Parameters     ----------     {common_plot_args}     stripplot : `bool` optional (default: `True`)         Add a stripplot on top of the violin plot.         See `seaborn.stripplot`.     jitter : `float` or `bool`, optional (default: `True`)         Add jitter to the stripplot (only when stripplot is True)         See `seaborn.stripplot`.     size : int, optional (default: 1)         Size of the jitter points.     order : list of str, optional (default: `True`)         Order in which to show the categories.     scale : {{'area', 'count', 'width'}}, optional (default: 'width')         The method used to scale the width of each violin. If 'area', each         violin will have the same area. If 'count', the width of the violins         will be scaled by the number of observations in that bin. If 'width',         each violin will have the same width.     row_palette: `str` (default: `muted`)         The row palette determines the colors to use in each of the stacked violin plots. The value         should be a valid seaborn palette name or a valic matplotlib colormap         (see https://seaborn.pydata.org/generated/seaborn.color_palette.html). Alternatively,         a single color name or hex value can be passed. E.g. 'red' or '#cc33ff'     standard_scale : {{'var', 'obs'}}, optional (default: None)         Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,         subtract the minimum and divide each by its maximum.     swap_axes: `bool`, optional (default: `False`)          By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby` categories.          By setting `swap_axes` then x are the `groupby` categories and y the `var_names`. When swapping          axes var_group_positions are no longer used     {show_save_ax}     **kwds : keyword arguments         Are passed to `seaborn.violinplot`.      Returns     -------     List of :class:`~matplotlib.axes.Axes`      Examples     -------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.pl.stacked_violin(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],     ...                      groupby='bulk_labels', dendrogram=True)
\     Heatmap of the expression values of genes.      If `groupby` is given, the heatmap is ordered by the respective group. For     example, a list of marker genes can be plotted, ordered by clustering. If     the `groupby` observation annotation is not categorical the observation     annotation is turned into a categorical by binning the data into the number     specified in `num_categories`.      Parameters     ----------     {common_plot_args}     standard_scale : {{'var', 'obs'}}, optional (default: None)         Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,         subtract the minimum and divide each by its maximum.     swap_axes: `bool`, optional (default: `False`)          By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`          categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.     show_gene_labels: `bool`, optional (default: `None`).          By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.     {show_save_ax}     **kwds : keyword arguments         Are passed to `matplotlib.imshow`.      Returns     -------     List of :class:`~matplotlib.axes.Axes`      Examples     -------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.pl.heatmap(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],     ...               groupby='bulk_labels', dendrogram=True, swap_axes=True)
\     Makes a *dot plot* of the expression values of `var_names`.      For each var_name and each `groupby` category a dot is plotted. Each dot     represents two values: mean expression within each category (visualized by     color) and fraction of cells expressing the var_name in the     category (visualized by the size of the dot).  If groupby is not given, the     dotplot assumes that all data belongs to a single category.      **Note**: A gene is considered expressed if the expression value in the adata     (or adata.raw) is above the specified threshold which is zero by default.      An example of dotplot usage is to visualize, for multiple marker genes,     the mean value and the percentage of cells expressing the gene accross multiple clusters.      Parameters     ----------     {common_plot_args}     expression_cutoff : `float` (default: `0.`)         Expression cutoff that is used for binarizing the gene expression and determining the fraction         of cells expressing given genes. A gene is expressed only if the expression value is greater than         this threshold.     mean_only_expressed : `bool` (default: `False`)         If True, gene expression is averaged only over the cells expressing the given genes.     color_map : `str`, optional (default: `Reds`)         String denoting matplotlib color map.     dot_max : `float` optional (default: `None`)         If none, the maximum dot size is set to the maximum fraction value found (e.g. 0.6). If given,         the value should be a number between 0 and 1. All fractions larger than dot_max are clipped to         this value.     dot_min : `float` optional (default: `None`)         If none, the minimum dot size is set to 0. If given,         the value should be a number between 0 and 1. All fractions smaller than dot_min are clipped to         this value.     standard_scale : {{'var', 'group'}}, optional (default: None)         Whether or not to standardize that dimension between 0 and 1, meaning for each variable or group,         subtract the minimum and divide each by its maximum.     smallest_dot : `float` optional (default: 0.)         If none, the smallest dot has size 0. All expression levels with `dot_min` are potted with         `smallest_dot` dot size.      {show_save_ax}     **kwds : keyword arguments         Are passed to `matplotlib.pyplot.scatter`.      Returns     -------     List of :class:`~matplotlib.axes.Axes`      Examples     -------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.pl.dotplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],     ...               groupby='bulk_labels', dendrogram=True)
\     Creates a heatmap of the mean expression values per cluster of each var_names     If groupby is not given, the matrixplot assumes that all data belongs to a single     category.      Parameters     ----------     {common_plot_args}     standard_scale : {{'var', 'group'}}, optional (default: None)         Whether or not to standardize that dimension between 0 and 1, meaning for each variable or group,         subtract the minimum and divide each by its maximum.     {show_save_ax}     **kwds : keyword arguments         Are passed to `matplotlib.pyplot.pcolor`.      Returns     -------     List of :class:`~matplotlib.axes.Axes`      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.pl.matrixplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],     ... groupby='bulk_labels', dendrogram=True)
\     In this type of plot each var_name is plotted as a filled line plot where the     y values correspond to the var_name values and x is each of the cells. Best results     are obtained when using raw counts that are not log.      `groupby` is required to sort and order the values using the respective group     and should be a categorical value.      Parameters     ----------     {common_plot_args}     {show_save_ax}     **kwds : keyword arguments         Are passed to `seaborn.heatmap`.      Returns     -------     A list of :class:`~matplotlib.axes.Axes`.      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.pl.tracksplot(adata, ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ'],     ...                  'bulk_labels', dendrogram=True)
Plots a dendrogram of the categories defined in `groupby`.      See :func:`~scanpy.tl.dendrogram`.      Parameters     ----------     adata : :class:`~anndata.AnnData`     groupby : `str`         Categorical data column used to create the dendrogram     dendrogram_key : `str`, optional(default: `None`)         Key under with the dendrogram information was stored.         By default the dendrogram information is stored under .uns['dendrogram_' + groupby].     orientation : `str`, optional(default: `top`)         Options are `top`, `bottom`, `left`, and `right`. Only when `show_correlation` is False     remove_labels : `bool`, optional(default: `False`)     {show_save_ax}      Returns     -------      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.tl.dendrogram(adata, 'bulk_labels')     >>> sc.pl.dendrogram(adata, 'bulk_labels')
Plots the correlation matrix computed as part of `sc.tl.dendrogram`.      Parameters     ----------     adata : :class:`~anndata.AnnData`     groupby : `str`         Categorical data column used to create the dendrogram     show_correlation_numbers : `bool`, optional(default: `False`)         If `show_correlation` is True, plot the correlation number on top of each cell.     dendrogram: `bool` or `str`, optional (default: `False`)         If True or a valid dendrogram key, a dendrogram based on the hierarchical clustering         between the `groupby` categories is added. The dendrogram information is computed         using :ref:`scanpy.tl.dendrogram`. If `tl.dendrogram` has not been called previously         the function is called with default parameters.     figsize : (`float`, `float`), optional (default: `None`)         By default a figure size that aims to produce a squared correlation matrix plot is used.         Format is (width, height)     {show_save_ax}     **kwds : keyword arguments         Only if `show_correlation` is True: Are passed to `matplotlib.pyplot.pcolormesh` when plotting         the correlation heatmap. Useful values to pas are `vmax`, `vmin` and `cmap`.      Returns     -------      Examples     --------     >>> adata = sc.datasets.pbmc68k_reduced()     >>> sc.tl.dendrogram(adata, 'bulk_labels')     >>> sc.pl.correlation(adata, 'bulk_labels')
Given the anndata object, prepares a data frame in which the row index are the categories     defined by group by and the columns correspond to var_names.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     var_names : `str` or list of `str`         `var_names` should be a valid subset of  `adata.var_names`.     groupby : `str` or `None`, optional (default: `None`)         The key of the observation grouping to consider. It is expected that         groupby is a categorical. If groupby is not a categorical observation,         it would be subdivided into `num_categories`.     log : `bool`, optional (default: `False`)         Use the log of the values     use_raw : `bool`, optional (default: `None`)         Use `raw` attribute of `adata` if present.     num_categories : `int`, optional (default: `7`)         Only used if groupby observation is not categorical. This value         determines the number of groups into which the groupby observation         should be subdivided.     gene_symbols : string, optional (default: `None`)         Key for field in .var that stores gene symbols.      Returns     -------     Tuple of `pandas.DataFrame` and list of categories.
Draws brackets that represent groups of genes on the give axis.     For best results, this axis is located on top of an image whose     x axis contains gene names.      The gene_groups_ax should share the x axis with the main ax.      Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)      This function is used by dotplot, heatmap etc.      Parameters     ----------     gene_groups_ax : matplotlib axis         In this axis the gene marks are drawn     group_positions : list of `tuples`         Each item in the list, should contain the start and end position that the         bracket should cover.         Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes)         in positions 0-4 and other for positions 5-8     group_labels :  list         List of group labels     left_adjustment : `float`         adjustment to plot the bracket start slightly before or after the first gene position.         If the value is negative the start is moved before.     right_adjustment : `float`         adjustment to plot the bracket end slightly before or after the last gene position         If the value is negative the start is moved before.     rotation : `float` (default None)         rotation degrees for the labels. If not given, small labels (<4 characters) are not         rotated, otherwise, they are rotated 90 degrees     orientation : `str` (default `top`)         location of the brackets. Either `top` or `right`     Returns     -------     None
Function used by plotting functions that need to reorder the the groupby observations     based on the dendrogram results.      The function checks if a dendrogram has already been precomputed. If not, sc.tl.dendrogram     is run with default parameters.      The results found in .uns[dendrogram_key] are used to reorder var_group_labels     and var_group_positions.       Returns     -------     dictionary with keys: 'categories_idx_ordered','var_group_names_idx_ordered',                       'var_group_labels', and 'var_group_positions'
Plots a dendrogram on the given ax using the precomputed dendrogram information     stored in .uns[dendrogram_key]
Uses interpolation to reduce the number of observations (cells).     This is useful for plotting functions that otherwise will ignore     most of the cells' values.      The reduction and smoothing is only done per column      Parameters     ----------     obs_tidy : Pandas DataFrame. rows = obs (eg. cells), cols = vars (eg. genes)     goal_size : number of cells to keep      Returns     -------
Plots categories as colored blocks. If orientation is 'left', the categories are plotted vertically, otherwise     they are plotted horizontally.      Parameters     ----------     groupby_ax : matplotlib ax     obs_tidy     colors : list of valid color names optional (default: `None`)         Color to use for each category.     orientation : `str`, optional (default: `left`)     cmap_name : `str`         Name of colormap to use, in case colors is None      Returns     -------     ticks position, labels, colormap
Plots a vertical color bar based on mappable.     The height of the colorbar is min(figure-height, max_cmap_height)      Parameters     ----------     mappable : The image to which the colorbar applies.     fig ; The figure object     subplot_spec : the gridspec subplot. Eg. axs[1,2]     max_cbar_height : `float`         The maximum colorbar height      Returns     -------     color bar ax
Infer progression of cells through geodesic distance along the graph [Haghverdi16]_ [Wolf19]_.      Reconstruct the progression of a biological process from snapshot     data. `Diffusion Pseudotime` has been introduced by [Haghverdi16]_ and     implemented within Scanpy [Wolf18]_. Here, we use a further developed     version, which is able to deal with disconnected graphs [Wolf19]_ and can     be run in a `hierarchical` mode by setting the parameter     `n_branchings>1`. We recommend, however, to only use     :func:`~scanpy.api.tl.dpt` for computing pseudotime (`n_branchings=0`) and     to detect branchings via :func:`~scanpy.api.paga`. For pseudotime, you need     to annotate your data with a root cell. For instance::          adata.uns['iroot'] = np.flatnonzero(adata.obs['cell_types'] == 'Stem')[0]      This requires to run :func:`~scanpy.api.pp.neighbors`, first. In order to     reproduce the original implementation of DPT, use `method=='gauss'` in     this. Using the default `method=='umap'` only leads to minor quantitative     differences, though.      .. versionadded:: 1.1      :func:`~scanpy.api.tl.dpt` also requires to run     :func:`~scanpy.api.tl.diffmap` first. As previously,     :func:`~scanpy.api.tl.dpt` came with a default parameter of ``n_dcs=10`` but     :func:`~scanpy.api.tl.diffmap` has a default parameter of ``n_comps=15``,     you need to pass ``n_comps=10`` in :func:`~scanpy.api.tl.diffmap` in order     to exactly reproduce previous :func:`~scanpy.api.tl.dpt` results.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     n_dcs : `int`, optional (default: 10)         The number of diffusion components to use.     n_branchings : `int`, optional (default: 0)         Number of branchings to detect.     min_group_size : [0, 1] or `float`, optional (default: 0.01)         During recursive splitting of branches ('dpt groups') for `n_branchings`         > 1, do not consider groups that contain less than `min_group_size` data         points. If a float, `min_group_size` refers to a fraction of the total         number of data points.     allow_kendall_tau_shift : `bool`, optional (default: `True`)         If a very small branch is detected upon splitting, shift away from         maximum correlation in Kendall tau criterion of [Haghverdi16]_ to         stabilize the splitting.     copy : `bool`, optional (default: `False`)         Copy instance before computation and return a copy. Otherwise, perform         computation inplace and return None.      Returns     -------     Depending on `copy`, returns or updates `adata` with the following fields.      If `n_branchings==0`, no field `dpt_groups` will be written.      **dpt_pseudotime** : :class:`pandas.Series` (`adata.obs`, dtype `float`)         Array of dim (number of samples) that stores the pseudotime of each         cell, that is, the DPT distance with respect to the root cell.     **dpt_groups** : :class:`pandas.Series` (`adata.obs`, dtype `category`)         Array of dim (number of samples) that stores the subgroup id ('0',         '1', ...) for each cell. The groups  typically correspond to         'progenitor cells', 'undecided cells' or 'branches' of a process.      Notes     -----     The tool is similar to the R package `destiny` of [Angerer16]_.
Detect branchings and partition the data into corresponding segments.          Detect all branchings up to `n_branchings`.          Writes         ------         segs : np.ndarray             Array of dimension (number of segments) × (number of data             points). Each row stores a mask array that defines a segment.         segs_tips : np.ndarray             Array of dimension (number of segments) × 2. Each row stores the             indices of the two tip points of each segment.         segs_names : np.ndarray             Array of dimension (number of data points). Stores an integer label             for each segment.
Detect all branchings up to `n_branchings`.          Writes Attributes         -----------------         segs : np.ndarray             List of integer index arrays.         segs_tips : np.ndarray             List of indices of the tips of segments.
Out of a list of line segments, choose segment that has the most         distant second data point.          Assume the distance matrix Ddiff is sorted according to seg_idcs.         Compute all the distances.          Returns         -------         iseg : int             Index identifying the position within the list of line segments.         tips3 : int             Positions of tips within chosen segment.
Convert the format of the segment class members.
Return a single array that stores integer segment labels.
Define indices that reflect segment and pseudotime order.          Writes         ------         indices : np.ndarray             Index array of shape n, which stores an ordering of the data points             with respect to increasing segment index and increasing pseudotime.         changepoints : np.ndarray             Index array of shape len(ssegs)-1, which stores the indices of             points where the segment index changes, with respect to the ordering             of indices.
Detect branching on given segment.          Updates all list parameters inplace.          Call function _detect_branching and perform bookkeeping on segs and         segs_tips.          Parameters         ----------         segs : list of np.ndarray             Dchosen distance matrix restricted to segment.         segs_tips : list of np.ndarray             Stores all tip points for the segments in segs.         iseg : int             Position of segment under study in segs.         tips3 : np.ndarray             The three tip points. They form a 'triangle' that contains the data.
Detect branching on given segment.          Call function __detect_branching three times for all three orderings of         tips. Points that do not belong to the same segment in all three         orderings are assigned to a fourth segment. The latter is, by Haghverdi         et al. (2016) referred to as 'undecided cells'.          Parameters         ----------         Dseg             Dchosen distance matrix restricted to segment.         tips             The three tip points. They form a 'triangle' that contains the data.          Returns         -------         ssegs : list of np.ndarray             List of segments obtained from splitting the single segment defined             via the first two tip cells.         ssegs_tips : list of np.ndarray             List of tips of segments in ssegs.
Detect branching on given segment.
Detect branching on given segment.          Compute point that maximizes kendall tau correlation of the sequences of         distances to the second and the third tip, respectively, when 'moving         away' from the first tip: tips[0]. 'Moving away' means moving in the         direction of increasing distance from the first tip.          Parameters         ----------         Dseg : np.ndarray             Dchosen distance matrix restricted to segment.         tips : np.ndarray             The three tip points. They form a 'triangle' that contains the data.          Returns         -------         ssegs : list of np.ndarray             List of segments obtained from "splitting away the first tip cell".
Return splitting index that maximizes correlation in the sequences.          Compute difference in Kendall tau for all splitted sequences.          For each splitting index i, compute the difference of the two         correlation measures kendalltau(a[:i], b[:i]) and         kendalltau(a[i:], b[i:]).          Returns the splitting index that maximizes             kendalltau(a[:i], b[:i]) - kendalltau(a[i:], b[i:])          Parameters         ----------         a, b : np.ndarray             One dimensional sequences.          Returns         -------         Splitting index according to above description.
Compute Kendall tau delta.          The new sequence has length len_old + 1.          Parameters         ----------         len_old : int             The length of the old sequence, used to compute tau_old.         diff_pos : int             Difference between concordant and non-concordant pairs.         tau_old : float             Kendall rank correlation of the old sequence.
Compute Kendall tau delta.          The new sequence has length len_old - 1.          Parameters         ----------         len_old : int             The length of the old sequence, used to compute tau_old.         diff_neg : int             Difference between concordant and non-concordant pairs.         tau_old : float             Kendall rank correlation of the old sequence.
Compute difference in concordance of pairs in split sequences.          Consider splitting a and b at index i.          Parameters         ----------         a             ?         b             ?          Returns         -------         diff_pos             Difference between concordant pairs for both subsequences.         diff_neg             Difference between non-concordant pairs for both subsequences.
Decorator which marks a functions keyword arguments as deprecated. It will     result in a warning being emitted when the deprecated keyword argument is     used, and the function being called with the new argument.      Parameters     ----------     arg_mapping : dict[str, str]         Mapping from deprecated argument name to current argument name.
\     Docstrings should start with "\" in the first line for proper formatting.
Parameters     ----------     map_colors : `dict`         Dict with color specification for new groups that have no corresponding         old group.
Compare neighborhood graph of representation based on cross entropy.      `n_points` denotes the number of points to add as highlight annotation.      Returns     -------     The cross entropy and the geodesic-distance-weighted cross entropy as     ``entropy, geo_entropy_d, geo_entropy_o``.      Adds the most overlapping or disconnected points as annotation to `adata`.
Get graph_tool graph from adjacency matrix.
Get igraph graph from adjacency matrix.
Compute overlaps between groups.      See ``identify_groups`` for identifying the groups.      Parameters     ----------     adata : AnnData     prediction : str         Field name of adata.obs.     reference : str         Field name of adata.obs.     normalization : {'prediction', 'reference'}         Whether to normalize with respect to the predicted groups or the         reference groups.     threshold : float, optional (default: 0.01)         Do not consider associations whose overlap is below this fraction.     max_n_names : int or None, optional (default: 2)         Control how many reference names you want to be associated with per         predicted name. Set to `None`, if you want all.      Returns     -------     Tuple of     asso_names : list of associated reference names (`max_n_names` for each         predicted name)     asso_matrix : matrix where rows correspond to the predicted labels and         columns to the reference labels, entries are proportional to degree of         association
How well do the pred_labels explain the ref_labels?      A predicted cluster explains a reference cluster if it is contained within the reference     cluster with at least 50% (threshold_overlap_pred) of its points and these correspond     to at least 50% (threshold_overlap_ref) of the reference cluster.
Which predicted label explains which reference label?      A predicted label explains the reference label which maximizes the minimum     of ``relative_overlaps_pred`` and ``relative_overlaps_ref``.      Compare this with ``compute_association_matrix_of_groups``.      Returns     -------     A dictionary of length ``len(np.unique(ref_labels))`` that stores for each     reference label the predicted label that best explains it.      If ``return_overlaps`` is ``True``, this will in addition return the overlap     of the reference group with the predicted group; normalized with respect to     the reference group size and the predicted group size, respectively.
Pass array-like categories, return sorted cleaned unique categories.
Update the 'examples dictionary' _examples.example_parameters.      If a datakey (key in 'datafile dictionary') is not present in the 'examples     dictionary' it is used to initialize an entry with that key.      If not specified otherwise, any 'exkey' (key in 'examples dictionary') is     used as 'datakey'.
Moving average over one-dimensional array.      Parameters     ----------     a : np.ndarray         One-dimensional array.     n : int         Number of entries to average over. n=2 means averaging over the currrent         the previous entry.      Returns     -------     An array view storing the moving average.
Update old_params with new_params.      If check==False, this merely adds and overwrites the content of old_params.      If check==True, this only allows updating of parameters that are already     present in old_params.      Parameters     ----------     old_params : dict     new_params : dict     check : bool, optional (default: False)      Returns     -------     updated_params : dict
Read args for single tool.
Create default parser for single tools.
Get subset of groups in adata.obs[key].
Pretty output of nested dictionaries.
Markdown output that can be pasted in the examples/README.md.
Given any number of dicts, shallow copy and merge into a new dict,     precedence goes to key value pairs in latter dicts.      Notes     -----     http://stackoverflow.com/questions/38987/how-to-merge-two-python-dictionaries-in-a-single-expression
Make an array in which rows store 1d mask arrays from list of index lists.      Parameters     ----------     n : int         Maximal index / number of samples.
Get full tracebacks when warning is raised by setting      warnings.showwarning = warn_with_traceback      See also     --------     http://stackoverflow.com/questions/22373927/get-traceback-of-warnings
Subsample a fraction of 1/subsample samples from the rows of X.      Parameters     ----------     X : np.ndarray         Data array.     subsample : int         1/subsample is the fraction of data sampled, n = X.shape[0]/subsample.     seed : int         Seed for sampling.      Returns     -------     Xsampled : np.ndarray         Subsampled X.     rows : np.ndarray         Indices of rows that are stored in Xsampled.
Subsample n samples from rows of array.      Parameters     ----------     X : np.ndarray         Data array.     seed : int         Seed for sampling.      Returns     -------     Xsampled : np.ndarray         Subsampled X.     rows : np.ndarray         Indices of rows that are stored in Xsampled.
Check if file is present otherwise download.
Cluster matrix using hierarchical clustering.      Parameters     ----------     M : np.ndarray         Matrix, for example, distance matrix.      Returns     -------     Mclus : np.ndarray         Clustered matrix.     indices : np.ndarray         Indices used to cluster the matrix.
\     Batch balanced kNN [Park18]_.      Batch balanced kNN alters the kNN procedure to identify each     cell's top neighbours in each batch separately instead of the     entire cell pool with no accounting for batch. Aligns batches in a     quick and lightweight manner.      For use in the scanpy workflow as an alternative to :func:`scanpi.pp.neighbors`.      .. note::          This is just a wrapper of :func:`bbknn.bbknn`: more information         and bug reports `here <https://github.com/Teichlab/bbknn>`__.          Params     ------     adata : ``AnnData``         Needs the PCA computed and stored in ``adata.obsm["X_pca"]``.     batch_key : ``str``, optional (default: "batch")         ``adata.obs`` column name discriminating between your batches.     neighbors_within_batch : ``int``, optional (default: 3)         How many top neighbours to report for each batch; total number of neighbours         will be this number times the number of batches.     n_pcs : ``int``, optional (default: 50)         How many principal components to use in the analysis.     trim : ``int`` or ``None``, optional (default: ``None``)         If not ``None``, trim the neighbours of each cell to these         many top connectivities.  May help with population         independence and improve the tidiness of clustering.     approx : ``bool``, optional (default: ``True``)         If ``True``, use annoy's approximate neighbour finding. This         results in a quicker run time for large datasets while also         potentially increasing the degree of batch correction.     n_trees : ``int``, optional (default: 10)         Only used when ``approx=True``. The number of trees to         construct in the annoy forest.  More trees give higher         precision when querying, at the cost of increased run time and         resource intensity.     use_faiss : ``bool``, optional (default: ``True``)         If ``approx=False`` and the metric is "euclidean", use the         faiss package to compute nearest neighbours if installed. This         improves performance at a minor cost to numerical precision as         faiss operates on float32.     metric : ``str`` or ``sklearn.neighbors.DistanceMetric``, optional (default: "angular")         What distance metric to use. If using ``approx=True``, the         options are "angular", "euclidean", "manhattan" and         "hamming". Otherwise, the options are "euclidean", a member of         the ``sklearn.neighbors.KDTree.valid_metrics`` list, or         parameterised ``sklearn.neighbors.DistanceMetric`` `objects         <https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html>`_::              >>> from sklearn import neighbors             >>> neighbors.KDTree.valid_metrics             ['p', 'chebyshev', 'cityblock', 'minkowski', 'infinity',              'l2', 'euclidean', 'manhattan', 'l1']             >>> pass_as_metric = neighbors.DistanceMetric.get_metric('minkowski', p=3)      bandwidth : ``float``, optional (default: 1)         ``scanpy.neighbors.compute_connectivities_umap`` parameter,         higher values result in a gentler slope of the connectivities         exponentials (i.e. larger connectivity values being returned)     local_connectivity : ``int``, optional (default: 1)         ``scanpy.neighbors.compute_connectivities_umap`` parameter,         how many nearest neighbors of each cell are assumed to be         fully connected (and given a connectivity value of 1)     save_knn : ``bool``, optional (default: ``False``)         If ``True``, save the indices of the nearest neighbours for         each cell in ``adata.uns['bbknn']``.     copy : ``bool``, optional (default: ``False``)         If ``True``, return a copy instead of writing to the supplied adata.      Returns     -------     The `adata` with the batch-corrected graph.
Diffusion Maps [Coifman05]_ [Haghverdi15]_ [Wolf18]_.      Diffusion maps [Coifman05]_ has been proposed for visualizing single-cell     data by [Haghverdi15]_. The tool uses the adapted Gaussian kernel suggested     by [Haghverdi16]_ in the implementation of [Wolf18]_.      The width ("sigma") of the connectivity kernel is implicitly determined by     the number of neighbors used to compute the single-cell graph in     :func:`~scanpy.api.neighbors`. To reproduce the original implementation     using a Gaussian kernel, use `method=='gauss'` in     :func:`~scanpy.api.neighbors`. To use an exponential kernel, use the default     `method=='umap'`. Differences between these options shouldn't usually be     dramatic.      Parameters     ----------     adata : :class:`~anndata.AnnData`         Annotated data matrix.     n_comps : `int`, optional (default: 15)         The number of dimensions of the representation.     copy : `bool` (default: `False`)         Return a copy instead of writing to adata.      Returns     -------     Depending on `copy`, returns or updates `adata` with the following fields.      **X_diffmap** : :class:`numpy.ndarray` (`adata.obsm`)         Diffusion map representation of data, which is the right eigen basis of         the transition matrix with eigenvectors as columns.     **diffmap_evals** : :class:`numpy.ndarray` (`adata.uns`)         Array of size (number of eigen vectors). Eigenvalues of transition matrix.
Note: Must be holding the _lazyLock
Note: Must be holding the _lazyLock
Note: Must be holding the _lazyLock, or in main init path
Get version namespace from version
Get all the versions for the service with specified namespace (partially) ordered    by compatibility (i.e. any version in the list that is compatible with some version    v in the list will preceed v)
Set a WSDL method with wsdl namespace and wsdl name    Returns added method / existing method if (ns, wsdlName) already in the map     Note: Must be holding the _lazyLock
Get wsdl method from ns, wsdlName
Get type from vmodl name
Note: Must be holding the _lazyLock
Determine if the object should be exploded.
Simple command-line program for powering on virtual machines on a system.
Supports the command-line arguments listed below.
Print information for a particular virtual machine or recurse into a folder    or vApp with depth protection
Simple command-line program for listing the virtual machines on a system.
Connections to 'localhost' do not need SSL verification as a certificate     will never match. The OS provides security by only allowing root to bind     to low-numbered ports.
Connect to the specified server, login and return the service    instance object.     Throws any exception back to caller. The service instance object is    also saved in the library for easy access.     Clients should modify the service parameter only when connecting to    a VMOMI server other than hostd/vpxd. For both of the latter, the    default value is fine.     @param host: Which host to connect to.    @type  host: string    @param port: Port    @type  port: int    @param user: User    @type  user: string    @param pwd: Password    @type  pwd: string    @param service: Service    @type  service: string    @param adapter: Adapter    @type  adapter: string    @param namespace: Namespace *** Deprecated: Use version instead ***    @type  namespace: string    @param path: Path    @type  path: string    @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never                                  closing the connections    @type  connectionPoolTimeout: int    @param version: Version    @type  version: string    @param keyFile: ssl key file path    @type  keyFile: string    @param certFile: ssl cert file path    @type  certFile: string    @param thumbprint: host cert thumbprint    @type  thumbprint: string    @param sslContext: SSL Context describing the various SSL options. It is only                       supported in Python 2.7.9 or higher.    @type  sslContext: SSL.Context    @param b64token: base64 encoded token    @type  b64token: string    @param mechanism: authentication mechanism: userpass or sspi    @type  mechanism: string
Provides a standard method for connecting to a specified server without SSL    verification. Useful when connecting to servers with self-signed certificates    or when you wish to ignore SSL altogether. Will attempt to create an unverified    SSL context and then connect via the Connect method.
Private method that performs the actual Connect and returns a    connected service instance object.     @param host: Which host to connect to.    @type  host: string    @param port: Port    @type  port: int    @param user: User    @type  user: string    @param pwd: Password    @type  pwd: string    @param service: Service    @type  service: string    @param adapter: Adapter    @type  adapter: string    @param version: Version    @type  version: string    @param path: Path    @type  path: string    @param keyFile: ssl key file path    @type  keyFile: string    @param certFile: ssl cert file path    @type  certFile: string    @param thumbprint: host cert thumbprint    @type  thumbprint: string    @param sslContext: SSL Context describing the various SSL options. It is only                       supported in Python 2.7.9 or higher.    @type  sslContext: SSL.Context    @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never                                  closing the connections    @type  connectionPoolTimeout: int
Private method that performs the actual Connect and returns a    connected service instance object.     @param host: Which host to connect to.    @type  host: string    @param port: Port    @type  port: int    @param service: Service    @type  service: string    @param adapter: Adapter    @type  adapter: string    @param version: Version    @type  version: string    @param path: Path    @type  path: string    @param keyFile: ssl key file path    @type  keyFile: string    @param certFile: ssl cert file path    @type  certFile: string    @param thumbprint: host cert thumbprint    @type  thumbprint: string    @param sslContext: SSL Context describing the various SSL options. It is only                       supported in Python 2.7.9 or higher.    @type  sslContext: SSL.Context    @param b64token: base64 encoded token    @type  b64token: string    @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never                                  closing the connections    @type  connectionPoolTimeout: int
Disconnect (logout) service instance    @param si: Service instance (returned from Connect)
Retrieve service instance for connection.    @param host: Which host to connect to.    @type  host: string    @param port: Port    @type  port: int    @param adapter: Adapter    @type  adapter: string    @param version: Version    @type  version: string    @param path: Path    @type  path: string    @param keyFile: ssl key file path    @type  keyFile: string    @param certFile: ssl cert file path    @type  certFile: string    @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never                                  closing the connections    @type  connectionPoolTimeout: int
Private method that returns a root from ElementTree for a remote XML document.     @param protocol: What protocol to use for the connection (e.g. https or http).    @type  protocol: string    @param server: Which server to connect to.    @type  server: string    @param port: Port    @type  port: int    @param path: Path    @type  path: string    @param sslContext: SSL Context describing the various SSL options. It is only                       supported in Python 2.7.9 or higher.    @type  sslContext: SSL.Context
Private method that returns a root from an ElementTree describing the API versions    supported by the specified server.  The result will be vimServiceVersions.xml    if it exists, otherwise vimService.wsdl if it exists, otherwise None.     @param protocol: What protocol to use for the connection (e.g. https or http).    @type  protocol: string    @param server: Which server to connect to.    @type  server: string    @param port: Port    @type  port: int    @param path: Path    @type  path: string    @param sslContext: SSL Context describing the various SSL options. It is only                       supported in Python 2.7.9 or higher.    @type  sslContext: SSL.Context
Private method that returns true if the service version description document    indicates that the desired version is supported     @param desiredVersion: The version we want to see if the server supports                           (eg. vim.version.version2.    @type  desiredVersion: string    @param serviceVersionDescription: A root ElementTree for vimServiceVersions.xml                                      or vimService.wsdl.    @type  serviceVersionDescription: root ElementTree
Private method that returns the most preferred API version supported by the    specified server,     @param protocol: What protocol to use for the connection (e.g. https or http).    @type  protocol: string    @param server: Which server to connect to.    @type  server: string    @param port: Port    @type  port: int    @param path: Path    @type  path: string    @param preferredApiVersions: Acceptable API version(s) (e.g. vim.version.version3)                                 If a list of versions is specified the versions should                                 be ordered from most to least preferred.    @type  preferredApiVersions: string or string list    @param sslContext: SSL Context describing the various SSL options. It is only                       supported in Python 2.7.9 or higher.    @type  sslContext: SSL.Context
Determine the most preferred API version supported by the specified server,    then create a soap stub adapter using that version     The parameters are the same as for pyVmomi.SoapStubAdapter except for    version which is renamed to prefferedApiVersions     @param preferredApiVersions: Acceptable API version(s) (e.g. vim.version.version3)                                 If a list of versions is specified the versions should                                 be ordered from most to least preferred.  If None is                                 specified, the list of versions support by pyVmomi will                                 be used.    @type  preferredApiVersions: string or string list
Determine the most preferred API version supported by the specified server,    then connect to the specified server using that API version, login and return    the service instance object.     Throws any exception back to caller. The service instance object is    also saved in the library for easy access.     Clients should modify the service parameter only when connecting to    a VMOMI server other than hostd/vpxd. For both of the latter, the    default value is fine.     @param protocol: What protocol to use for the connection (e.g. https or http).    @type  protocol: string    @param host: Which host to connect to.    @type  host: string    @param port: Port    @type  port: int    @param user: User    @type  user: string    @param pwd: Password    @type  pwd: string    @param service: Service    @type  service: string    @param path: Path    @type  path: string    @param connectionPoolTimeout: Timeout in secs for idle connections to close, specify negative numbers for never                                  closing the connections    @type  connectionPoolTimeout: int    @param preferredApiVersions: Acceptable API version(s) (e.g. vim.version.version3)                                 If a list of versions is specified the versions should                                 be ordered from most to least preferred.  If None is                                 specified, the list of versions support by pyVmomi will                                 be used.    @type  preferredApiVersions: string or string list    @param keyFile: ssl key file path    @type  keyFile: string    @param certFile: ssl cert file path    @type  certFile: string    @param thumbprint: host cert thumbprint    @type  thumbprint: string    @param sslContext: SSL Context describing the various SSL options. It is only                       supported in Python 2.7.9 or higher.    @type  sslContext: SSL.Context
Provides a standard method for connecting to a specified server without SSL    verification. Useful when connecting to servers with self-signed certificates    or when you wish to ignore SSL altogether. Will attempt to create an unverified    SSL context and then connect via the SmartConnect method.
Open the specified URL, using HTTP basic authentication to provide    the specified credentials to the server as part of the request.    Returns the response as a file-like object.
Open the specified path using HTTP, using the host/port/protocol    associated with the specified stub.  If the stub has a session cookie,    it is included with the HTTP request.  Returns the response as a    file-like object.
Return a function that will call the vim.SessionManager.Login() method       with the given parameters.  The result of this function can be passed as       the "loginMethod" to a SessionOrientedStub constructor.
Return a function that will call the vim.SessionManager.Login() method       with the given parameters.  The result of this function can be passed as       the "loginMethod" to a SessionOrientedStub constructor.
Return a function that will call the vim.SessionManager.LoginByToken()       after obtaining a HoK SAML token from the STS. The result of this function       can be passed as the "loginMethod" to a SessionOrientedStub constructor.        @param stsUrl: URL of the SAML Token issuing service. (i.e. SSO server).       @param stsCert: public key of the STS service.
Return a function that will call the vim.SessionManager.LoginByToken()       after obtaining a Bearer token from the STS. The result of this function       can be passed as the "loginMethod" to a SessionOrientedStub constructor.        @param username: username of the user/service registered with STS.       @param password: password of the user/service registered with STS.       @param stsUrl: URL of the SAML Token issueing service. (i.e. SSO server).       @param stsCert: public key of the STS service.
Simple command-line program for dumping the contents of any managed object.
Serialize an object
Serialize an object
Get xml ns prefix. self.nsMap must be set
Get fully qualified wsdl name (prefix:name)
Serialize an object
Map prefix:name tag into ns, name
Split tag into ns, name
Lookup wsdl type. Handle special case for some vmodl version
See if the passed in type is a Primitive Type
Diff any two objects. Objects can either be primitive type       or DataObjects
Diff any two Objects
Diff two DataObject arrays
Diff two arrays which contain Any objects
Diff two primitive arrays
Method which deligates the diffing of arrays based on the type
Diff Data Objects
Function cache decorator
Get dynamic type manager
Build dynamic types
Create pyVmomi types from vmodl.reflect.DynamicTypeManager.AllTypeInfo
Convert all dynamic types to pyVmomi type definitions
Create pyVmomi types from pyVmomi type definitions
Convert annotations to pyVmomi flags
Convert vmodl.reflect.DynamicTypeManager.ParamTypeInfo to pyVmomi param       definition
Convert vmodl.reflect.DynamicTypeManager.MethodTypeInfo to pyVmomi method       definition
Convert vmodl.reflect.DynamicTypeManager.PropertyTypeInfo to pyVmomi       managed property definition
Convert vmodl.reflect.DynamicTypeManager.ManagedTypeInfo to pyVmomi       managed type definition
Convert vmodl.reflect.DynamicTypeManager.PropertyTypeInfo to pyVmomi       data property definition
Convert vmodl.reflect.DynamicTypeManager.DataTypeInfo to pyVmomi data       type definition
Convert vmodl.reflect.DynamicTypeManager.EnumTypeInfo to pyVmomi enum       type definition
Parse ISO 8601 date time from string.    Returns datetime if ok, None otherwise    Note: Allows YYYY / YYYY-MM, but truncate YYYY -> YYYY-01-01,                                              YYYY-MM -> YYYY-MM-01          Truncate microsecond to most significant 6 digits
Python datetime isoformat() has the following problems:    - leave trailing 0 at the end of microseconds (violates XMLSchema rule)    - tz print +00:00 instead of Z    - Missing timezone offset for datetime without tzinfo
Get / Add timezone info
Wait for task to complete.      @type  raiseOnError      : bool     @param raiseOnError      : Any exception thrown is thrown up to the caller                                if raiseOnError is set to true.     @type  si                : ManagedObjectReference to a ServiceInstance.     @param si                : ServiceInstance to use. If None, use the                                information from the task.     @type  pc                : ManagedObjectReference to a PropertyCollector.     @param pc                : Property collector to use. If None, get it from                                the ServiceInstance.     @type  onProgressUpdate  : callable     @param onProgressUpdate  : Callable to call with task progress updates.          For example::              def OnTaskProgressUpdate(task, percentDone):                 print 'Task %s is %d%% complete.' % (task, percentDone)
Wait for mulitiple tasks to complete. Much faster than calling WaitForTask     N times
Create property collector filter for tasks
Check to see if VM needs to ask a question, throw exception
adb command, add -s serial by default. return the subprocess.Popen object.
get a dict of attached devices. key is the device serial, value is device name.
sdk version of connected device.
Stop the rpc server.
click at arbitrary coordinates.
long click at arbitrary coordinates.
dump device window and pull to local file.
take screenshot.
setter of orientation property.
Open notification or quick settings.         Usage:         d.open.notification()         d.open.quick_settings()
press key via name or key code. Supported key name includes:         home, back, left, right, up, down, center, menu, search, enter,         delete(or del), recent(recent apps), volume_up, volume_down,         volume_mute, camera, power.         Usage:         d.press.back()  # press back key         d.press.menu()  # press home key         d.press(89)     # press keycode
Turn on/off screen.         Usage:         d.screen.on()         d.screen.off()          d.screen == 'on'  # Check if the screen is on, same as 'd.screenOn'         d.screen == 'off'  # Check if the screen is off, same as 'not d.screenOn'
Waits for the current application to idle or window update event occurs.         Usage:         d.wait.idle(timeout=1000)         d.wait.update(timeout=1000, package_name="com.android.settings")
set the text field.
click on the ui object.         Usage:         d(text="Clock").click()  # click on the center of the ui object         d(text="OK").click.wait(timeout=3000) # click and wait for the new window update         d(text="John").click.topleft() # click on the topleft of the ui object         d(text="John").click.bottomright() # click on the bottomright of the ui object
Perform a long click action on the object.         Usage:         d(text="Image").long_click()  # long click on the center of the ui object         d(text="Image").long_click.topleft()  # long click on the topleft of the ui object         d(text="Image").long_click.bottomright()  # long click on the topleft of the ui object
Drag the ui object to other point or ui object.         Usage:         d(text="Clock").drag.to(x=100, y=100)  # drag to point (x,y)         d(text="Clock").drag.to(text="Remove") # drag to another object
perform two point gesture.         Usage:         d().gesture(startPoint1, startPoint2).to(endPoint1, endPoint2, steps)         d().gesture(startPoint1, startPoint2, endPoint1, endPoint2, steps)
Perform two point gesture from edge to center(in) or center to edge(out).         Usages:         d().pinch.In(percent=100, steps=10)         d().pinch.Out(percent=100, steps=100)
Perform swipe action. if device platform greater than API 18, percent can be used and value between 0 and 1         Usages:         d().swipe.right()         d().swipe.left(steps=10)         d().swipe.up(steps=10)         d().swipe.down()         d().swipe("right", steps=20)         d().swipe("right", steps=20, percent=0.5)
Wait until the ui object gone or exist.         Usage:         d(text="Clock").wait.gone()  # wait until it's gone.         d(text="Settings").wait.exists() # wait until it appears.
set childSelector.
set fromParent selector.
Perform fling action.         Usage:         d().fling()  # default vertically, forward         d().fling.horiz.forward()         d().fling.vert.backward()         d().fling.toBeginning(max_swipes=100) # vertically         d().fling.horiz.toEnd()
Perfrom scroll action.         Usage:         d().scroll(steps=50) # default vertically and forward         d().scroll.horiz.forward(steps=100)         d().scroll.vert.backward(steps=100)         d().scroll.horiz.toBeginning(steps=100, max_swipes=100)         d().scroll.vert.toEnd(steps=100)         d().scroll.horiz.to(text="Clock")
Minimize a keras model for given data and implicit hyperparameters.      Parameters     ----------     model: A function defining a keras model with hyperas templates, which returns a         valid hyperopt results dictionary, e.g.         return {'loss': -acc, 'status': STATUS_OK}     data: A parameter-less function that defines and return all data needed in the above         model definition.     algo: A hyperopt algorithm, like tpe.suggest or rand.suggest     max_evals: Maximum number of optimization runs     trials: A hyperopt trials object, used to store intermediate results for all         optimization runs     rseed: Integer random seed for experiments     notebook_name: If running from an ipython notebook, provide filename (not path)     verbose: Print verbose output     eval_space: Evaluate the best run in the search space such that 'choice's contain actually meaningful values instead of mere indices     return_space: Return the hyperopt search space object (e.g. for further processing) as last return value     keep_temp: Keep temp_model.py file on the filesystem      Returns     -------     If `return_space` is False: A pair consisting of the results dictionary of the best run and the corresponding     keras model.     If `return_space` is True: The pair of best result and corresponding keras model, and the hyperopt search space
Adds line numbers to each line of a source code fragment      Parameters     ----------     code : string        any multiline text, such as as (fragments) of source code      Returns     -------     str : string        The input with added <n>: for each line      Example     -------     code = "def do_stuff(x):\n\tprint(x)\n"     with_line_numbers(code)      1: def do_stuff(x):     2:     print(x)     3:
Figure out the character(s) used for indents in a given source code fragement.      Parameters     ----------     str : string       source code starting at an indent of 0 and containing at least one indented block.      Returns     -------     string       The character(s) used for indenting.      Example     -------     code = "def do_stuff(x)\n   print(x)\n"     indent = determine_indent(str)     print("The code '", code, "' is indented with \n'", indent, "' (size: ", len(indent), ")")
Unpack values from a hyperopt return dictionary where values are wrapped in a list.     :param vals: dict     :return: dict         copy of the dictionary with unpacked values
Find the index of the colon in the function signature.     :param model_string: string         source code of the model     :return: int         the index of the colon
Create your model...
Model providing function:      Create Keras model with double curly brackets dropped-in as needed.     Return value has to be a valid python dictionary with two customary keys:         - loss: Specify a numeric evaluation metric to be minimized         - status: Just use STATUS_OK and see hyperopt documentation if not feasible     The last one is optional, though recommended, namely:         - model: specify the model just created so that we can later use it again.
Compress data with zlib level 0.
Parse BSON codec options.
Representation of the arguments used to create this object.
Make a copy of this CodecOptions, overriding some options::              >>> from bson.codec_options import DEFAULT_CODEC_OPTIONS             >>> DEFAULT_CODEC_OPTIONS.tz_aware             False             >>> options = DEFAULT_CODEC_OPTIONS.with_options(tz_aware=True)             >>> options.tz_aware             True          .. versionadded:: 3.5
Merge a write command result into the full bulk result.
Raise a BulkWriteError from the full bulk api result.
Create an update document and add it to the list of ops.
Execute using write commands.
Execute insert, returning no results.
Execute write commands with OP_MSG and w=0 writeConcern, unordered.
Execute write commands with OP_MSG and w=0 WriteConcern, ordered.
Execute all operations, returning no results (w=0).
Execute operations.
Execute all provided operations.          :Parameters:           - write_concern (optional): the write concern for this bulk             execution.
Run a _Query or _GetMore operation and return a Response object.          This method is used only to run _Query/_GetMore operations from         cursors.         Can raise ConnectionFailure, OperationFailure, etc.          :Parameters:           - `operation`: A _Query or _GetMore object.           - `set_slave_okay`: Pass to operation.get_message.           - `all_credentials`: dict, maps auth source to MongoCredential.           - `listeners`: Instance of _EventListeners or None.           - `exhaust`: If True, then this is an exhaust cursor operation.           - `unpack_res`: A callable that decodes the wire protocol response.
Checks if this server supports retryable writes.
Takes message data, compresses it, and adds an OP_COMPRESSED header.
Takes message data and adds a message header based on the operation.      Returns the resultant message string.
Get an OP_INSERT message
Internal compressed unacknowledged insert message helper.
Internal insert message helper.
Get an **insert** message.
Get an OP_UPDATE message.
Internal compressed unacknowledged update message helper.
Internal update message helper.
Get an **update** message.
Get a OP_MSG message.      Note: this method handles multiple documents in a type one payload but     it does not perform batch splitting and the total message size is     only checked *after* generating the entire message.
Internal OP_MSG message helper.
Internal compressed OP_MSG message helper.
Get a OP_MSG message.
Get an OP_QUERY message.
Internal compressed query message helper.
Internal query message helper.
Get a **query** message.
Get an OP_GET_MORE message.
Internal compressed getMore message helper.
Get a **getMore** message.
Get an OP_DELETE message.
Internal compressed unacknowledged delete message helper.
Internal delete message helper.
Get a **delete** message.      `opts` is a CodecOptions. `flags` is a bit vector that may contain     the SingleRemove flag or not:      http://docs.mongodb.org/meta-driver/latest/legacy/mongodb-wire-protocol/#op-delete
Get a **killCursors** message.
Internal helper for raising DocumentTooLarge.
Insert `docs` using multiple batches.
Create a batched OP_MSG write.
Encode the next batched insert, update, or delete operation     as OP_MSG.
Create the next batched insert, update, or delete operation     with OP_MSG, compressed.
OP_MSG implementation entry point.
Create the next batched insert, update, or delete operation     using OP_MSG.
Create the next batched insert, update, or delete command, compressed.
Encode the next batched insert, update, or delete command.
Create the next batched insert, update, or delete command.
Batched write commands entry point.
Bulk write commands entry point.
Create a batched OP_QUERY write command.
Simple query helper for retrieving a first (and possibly only) batch.
Return a find command document for this query.
Get a query message, possibly setting the slaveOk bit.
Return a getMore command document for this query.
Get a getmore message.
Check the response header from the database, without decoding BSON.          Check the response for errors and unpack.          Can raise CursorNotFound, NotMasterError, ExecutionTimeout, or         OperationFailure.          :Parameters:           - `cursor_id` (optional): cursor_id we sent to get this response -             used for raising an informative exception when we get cursor id not             valid at server response.
Unpack a response from the database and decode the BSON document(s).          Check the response for errors and unpack, returning a dictionary         containing the response data.          Can raise CursorNotFound, NotMasterError, ExecutionTimeout, or         OperationFailure.          :Parameters:           - `cursor_id` (optional): cursor_id we sent to get this response -             used for raising an informative exception when we get cursor id not             valid at server response           - `codec_options` (optional): an instance of             :class:`~bson.codec_options.CodecOptions`
Construct an _OpReply from raw bytes.
Unpack a OP_MSG command response.          :Parameters:           - `cursor_id` (optional): Ignored, for compatibility with _OpReply.           - `codec_options` (optional): an instance of             :class:`~bson.codec_options.CodecOptions`
Construct an _OpMsg from raw bytes.
Whether there are any Servers of types besides Unknown.
Add an index to the index cache for ensure_index operations.
Watch changes on this cluster.          Performs an aggregation with an implicit initial ``$changeStream``         stage and returns a         :class:`~pymongo.change_stream.ClusterChangeStream` cursor which         iterates over changes on all databases on this cluster.          Introduced in MongoDB 4.0.          .. code-block:: python             with client.watch() as stream:                for change in stream:                    print(change)          The :class:`~pymongo.change_stream.ClusterChangeStream` iterable         blocks until the next change document is returned or an error is         raised. If the         :meth:`~pymongo.change_stream.ClusterChangeStream.next` method         encounters a network error when retrieving a batch from the server,         it will automatically attempt to recreate the cursor such that no         change events are missed. Any error encountered during the resume         attempt indicates there may be an outage and will be raised.          .. code-block:: python              try:                 with client.watch(                         [{'$match': {'operationType': 'insert'}}]) as stream:                     for insert_change in stream:                         print(insert_change)             except pymongo.errors.PyMongoError:                 # The ChangeStream encountered an unrecoverable error or the                 # resume attempt failed to recreate the cursor.                 logging.error('...')          For a precise description of the resume process see the         `change streams specification`_.          :Parameters:           - `pipeline` (optional): A list of aggregation pipeline stages to             append to an initial ``$changeStream`` stage. Not all             pipeline stages are valid after a ``$changeStream`` stage, see the             MongoDB documentation on change streams for the supported stages.           - `full_document` (optional): The fullDocument to pass as an option             to the ``$changeStream`` stage. Allowed values: 'default',             'updateLookup'.  Defaults to 'default'.             When set to 'updateLookup', the change notification for partial             updates will include both a delta describing the changes to the             document, as well as a copy of the entire document that was             changed from some time after the change occurred.           - `resume_after` (optional): The logical starting point for this             change stream.           - `max_await_time_ms` (optional): The maximum time in milliseconds             for the server to wait for changes before responding to a getMore             operation.           - `batch_size` (optional): The maximum number of documents to return             per batch.           - `collation` (optional): The :class:`~pymongo.collation.Collation`             to use for the aggregation.           - `start_at_operation_time` (optional): If provided, the resulting             change stream will only return changes that occurred at or after             the specified :class:`~bson.timestamp.Timestamp`. Requires             MongoDB >= 4.0.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           A :class:`~pymongo.change_stream.ClusterChangeStream` cursor.          .. versionadded:: 3.7          .. mongodoc:: changeStreams          .. _change streams specification:             https://github.com/mongodb/specifications/blob/master/source/change-streams/change-streams.rst
Send endSessions command(s) with the given session ids.
Cleanup client resources and disconnect from MongoDB.          On MongoDB >= 3.6, end all server sessions created by this client by         sending one or more endSessions commands.          Close all sockets in the connection pools and stop the monitor threads.         If this instance is used again it will be automatically re-opened and         the threads restarted.          .. versionchanged:: 3.6            End all server sessions created by this client.
Get the internal :class:`~pymongo.topology.Topology` object.          If this client was created with "connect=False", calling _get_topology         launches the connection process in the background.
Select a server to run an operation on this client.          :Parameters:           - `server_selector`: The server selector to use if the session is             not pinned and no address is given.           - `session`: The ClientSession for the next operation, or None. May             be pinned to a mongos server address.           - `address` (optional): Address when sending a message             to a specific server, used for getMore.
Run a _Query/_GetMore operation and return a Response.          :Parameters:           - `operation`: a _Query or _GetMore object.           - `unpack_res`: A callable that decodes the wire protocol response.           - `exhaust` (optional): If True, the socket used stays checked out.             It is returned along with its Pool in the Response.           - `address` (optional): Optional address when sending a message             to a specific server, used for getMore.
On "not master" or "node is recovering" errors reset the server         according to the SDAM spec.          Unpin the session on transient transaction errors.
Execute an operation with at most one consecutive retries          Returns func()'s return value on success. On error retries the same         command once.          Re-raises any exception thrown by func().
Execute an operation with at most one consecutive retries          Returns func()'s return value on success. On error retries the same         command once.          Re-raises any exception thrown by func().
Internal retryable write helper.
DEPRECATED - Send a kill cursors message soon with the given id.          Raises :class:`TypeError` if `cursor_id` is not an instance of         ``(int, long)``. What closing the cursor actually means         depends on this client's cursor manager.          This method may be called from a :class:`~pymongo.cursor.Cursor`         destructor during garbage collection, so it isn't safe to take a         lock or do network I/O. Instead, we schedule the cursor to be closed         soon on a background thread.          :Parameters:           - `cursor_id`: id of cursor to close           - `address` (optional): (host, port) pair of the cursor's server.             If it is not provided, the client attempts to close the cursor on             the primary or standalone, or a mongos server.          .. versionchanged:: 3.7            Deprecated.          .. versionchanged:: 3.0            Added ``address`` parameter.
Send a kill cursors message with the given id.          What closing the cursor actually means depends on this client's         cursor manager. If there is none, the cursor is closed asynchronously         on a background thread.
Send a kill cursors message with the given id.          What closing the cursor actually means depends on this client's         cursor manager. If there is none, the cursor is closed synchronously         on the current thread.
Send a kill cursors message with the given ids.
Process any pending kill cursors requests and         maintain connection pool parameters.
Start a logical session.          This method takes the same parameters as         :class:`~pymongo.client_session.SessionOptions`. See the         :mod:`~pymongo.client_session` module for details and examples.          Requires MongoDB 3.6. It is an error to call :meth:`start_session`         if this client has been authenticated to multiple databases using the         deprecated method :meth:`~pymongo.database.Database.authenticate`.          A :class:`~pymongo.client_session.ClientSession` may only be used with         the MongoClient that started it.          :Returns:           An instance of :class:`~pymongo.client_session.ClientSession`.          .. versionadded:: 3.6
If provided session is None, lend a temporary session.
If provided session is None, lend a temporary session.
Get information about the MongoDB server we're connected to.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Get a cursor over the databases of the connected server.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): Optional parameters of the             `listDatabases command             <https://docs.mongodb.com/manual/reference/command/listDatabases/>`_             can be passed as keyword arguments to this method. The supported             options differ by server version.          :Returns:           An instance of :class:`~pymongo.command_cursor.CommandCursor`.          .. versionadded:: 3.6
Get a list of the names of all databases on the connected server.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionadded:: 3.6
**DEPRECATED**: Get a list of the names of all databases on the         connected server.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.7            Deprecated. Use :meth:`list_database_names` instead.          .. versionchanged:: 3.6            Added ``session`` parameter.
Drop a database.          Raises :class:`TypeError` if `name_or_database` is not an instance of         :class:`basestring` (:class:`str` in python 3) or         :class:`~pymongo.database.Database`.          :Parameters:           - `name_or_database`: the name of a database to drop, or a             :class:`~pymongo.database.Database` instance representing the             database to drop           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. note:: The :attr:`~pymongo.mongo_client.MongoClient.write_concern` of            this client is automatically applied to this operation when using            MongoDB >= 3.4.          .. versionchanged:: 3.4            Apply this client's write concern automatically to this operation            when connected to MongoDB >= 3.4.
Get the database named in the MongoDB connection URI.          >>> uri = 'mongodb://host/my_database'         >>> client = MongoClient(uri)         >>> db = client.get_default_database()         >>> assert db.name == 'my_database'         >>> db = client.get_database()         >>> assert db.name == 'my_database'          Useful in scripts where you want to choose which database to use         based only on the URI in a configuration file.          :Parameters:           - `default` (optional): the database name to use if no database name             was provided in the URI.           - `codec_options` (optional): An instance of             :class:`~bson.codec_options.CodecOptions`. If ``None`` (the             default) the :attr:`codec_options` of this :class:`MongoClient` is             used.           - `read_preference` (optional): The read preference to use. If             ``None`` (the default) the :attr:`read_preference` of this             :class:`MongoClient` is used. See :mod:`~pymongo.read_preferences`             for options.           - `write_concern` (optional): An instance of             :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the             default) the :attr:`write_concern` of this :class:`MongoClient` is             used.           - `read_concern` (optional): An instance of             :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the             default) the :attr:`read_concern` of this :class:`MongoClient` is             used.          .. versionchanged:: 3.8            Undeprecated. Added the ``default``, ``codec_options``,            ``read_preference``, ``write_concern`` and ``read_concern``            parameters.          .. versionchanged:: 3.5            Deprecated, use :meth:`get_database` instead.
Get a Database instance with the default settings.
Unlock a previously locked server.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Helper method for split_options which creates the options dict.     Also handles the creation of a list for the URI tag_sets/     readpreferencetags portion and the use of the tlsInsecure option.
Issue appropriate warnings when deprecated options are present in the     options dictionary. Removes deprecated option key, value pairs if the     options dictionary is found to also have the renamed option.
Renames keys in the options dictionary to their internally-used     names.
Takes the options portion of a MongoDB URI, validates each option     and returns the options in a dictionary.      :Parameters:         - `opt`: A string representing MongoDB URI options.         - `validate`: If ``True`` (the default), validate and normalize all           options.         - `warn`: If ``False`` (the default), suppress all warnings raised           during validation of options.         - `normalize`: If ``True`` (the default), renames all options to their           internally-used names.
Parse and validate a MongoDB URI.      Returns a dict of the form::          {             'nodelist': <list of (host, port) tuples>,             'username': <username> or None,             'password': <password> or None,             'database': <database name> or None,             'collection': <collection name> or None,             'options': <dict of MongoDB URI options>         }      If the URI scheme is "mongodb+srv://" DNS SRV and TXT lookups will be done     to build nodelist and options.      :Parameters:         - `uri`: The MongoDB URI to parse.         - `default_port`: The port number to use when one wasn't specified           for a host in the URI.         - `validate`: If ``True`` (the default), validate and normalize all           options.         - `warn` (optional): When validating, if ``True`` then will warn           the user then ignore any invalid options or values. If ``False``,           validation will error when options are unsupported or values are           invalid.      .. versionchanged:: 3.6         Added support for mongodb+srv:// URIs      .. versionchanged:: 3.5         Return the original value of the ``readPreference`` MongoDB URI option         instead of the validated read preference mode.      .. versionchanged:: 3.1         ``warn`` added so invalid options can be ignored.
Create a new file in GridFS.          Returns a new :class:`~gridfs.grid_file.GridIn` instance to         which data can be written. Any keyword arguments will be         passed through to :meth:`~gridfs.grid_file.GridIn`.          If the ``"_id"`` of the file is manually specified, it must         not already exist in GridFS. Otherwise         :class:`~gridfs.errors.FileExists` is raised.          :Parameters:           - `**kwargs` (optional): keyword arguments for file creation
Get a file from GridFS by ``"_id"``.          Returns an instance of :class:`~gridfs.grid_file.GridOut`,         which provides a file-like interface for reading.          :Parameters:           - `file_id`: ``"_id"`` of the file to get           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`          .. versionchanged:: 3.6            Added ``session`` parameter.
Get a file from GridFS by ``"filename"`` or metadata fields.          Returns a version of the file in GridFS whose filename matches         `filename` and whose metadata fields match the supplied keyword         arguments, as an instance of :class:`~gridfs.grid_file.GridOut`.          Version numbering is a convenience atop the GridFS API provided         by MongoDB. If more than one file matches the query (either by         `filename` alone, by metadata fields, or by a combination of         both), then version ``-1`` will be the most recently uploaded         matching file, ``-2`` the second most recently         uploaded, etc. Version ``0`` will be the first version         uploaded, ``1`` the second version, etc. So if three versions         have been uploaded, then version ``0`` is the same as version         ``-3``, version ``1`` is the same as version ``-2``, and         version ``2`` is the same as version ``-1``.          Raises :class:`~gridfs.errors.NoFile` if no such version of         that file exists.          :Parameters:           - `filename`: ``"filename"`` of the file to get, or `None`           - `version` (optional): version of the file to get (defaults             to -1, the most recent version uploaded)           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`           - `**kwargs` (optional): find files by custom metadata.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.1            ``get_version`` no longer ensures indexes.
Get the most recent version of a file in GridFS by ``"filename"``         or metadata fields.          Equivalent to calling :meth:`get_version` with the default         `version` (``-1``).          :Parameters:           - `filename`: ``"filename"`` of the file to get, or `None`           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`           - `**kwargs` (optional): find files by custom metadata.          .. versionchanged:: 3.6            Added ``session`` parameter.
Delete a file from GridFS by ``"_id"``.          Deletes all data belonging to the file with ``"_id"``:         `file_id`.          .. warning:: Any processes/threads reading from the file while            this method is executing will likely see an invalid/corrupt            file. Care should be taken to avoid concurrent reads to a file            while it is being deleted.          .. note:: Deletes of non-existent files are considered successful            since the end result is the same: no file with that _id remains.          :Parameters:           - `file_id`: ``"_id"`` of the file to delete           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.1            ``delete`` no longer ensures indexes.
List the names of all files stored in this instance of         :class:`GridFS`.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.1            ``list`` no longer ensures indexes.
Check if a file exists in this instance of :class:`GridFS`.          The file to check for can be specified by the value of its         ``_id`` key, or by passing in a query document. A query         document can be passed in as dictionary, or by using keyword         arguments. Thus, the following three calls are equivalent:          >>> fs.exists(file_id)         >>> fs.exists({"_id": file_id})         >>> fs.exists(_id=file_id)          As are the following two calls:          >>> fs.exists({"filename": "mike.txt"})         >>> fs.exists(filename="mike.txt")          And the following two:          >>> fs.exists({"foo": {"$gt": 12}})         >>> fs.exists(foo={"$gt": 12})          Returns ``True`` if a matching file exists, ``False``         otherwise. Calls to :meth:`exists` will not automatically         create appropriate indexes; application developers should be         sure to create indexes if needed and as appropriate.          :Parameters:           - `document_or_id` (optional): query document, or _id of the             document to check for           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`           - `**kwargs` (optional): keyword arguments are used as a             query document, if they're present.          .. versionchanged:: 3.6            Added ``session`` parameter.
Opens a Stream that the application can write the contents of the         file to.          The user must specify the filename, and can choose to add any         additional information in the metadata field of the file document or         modify the chunk size.         For example::            my_db = MongoClient().test           fs = GridFSBucket(my_db)           grid_in = fs.open_upload_stream(                 "test_file", chunk_size_bytes=4,                 metadata={"contentType": "text/plain"})           grid_in.write("data I want to store!")           grid_in.close()  # uploaded on close          Returns an instance of :class:`~gridfs.grid_file.GridIn`.          Raises :exc:`~gridfs.errors.NoFile` if no such version of         that file exists.         Raises :exc:`~ValueError` if `filename` is not a string.          :Parameters:           - `filename`: The name of the file to upload.           - `chunk_size_bytes` (options): The number of bytes per chunk of this             file. Defaults to the chunk_size_bytes in :class:`GridFSBucket`.           - `metadata` (optional): User data for the 'metadata' field of the             files collection document. If not provided the metadata field will             be omitted from the files collection document.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`          .. versionchanged:: 3.6            Added ``session`` parameter.
Uploads a user file to a GridFS bucket.          Reads the contents of the user file from `source` and uploads         it to the file `filename`. Source can be a string or file-like object.         For example::            my_db = MongoClient().test           fs = GridFSBucket(my_db)           file_id = fs.upload_from_stream(               "test_file",               "data I want to store!",               chunk_size_bytes=4,               metadata={"contentType": "text/plain"})          Returns the _id of the uploaded file.          Raises :exc:`~gridfs.errors.NoFile` if no such version of         that file exists.         Raises :exc:`~ValueError` if `filename` is not a string.          :Parameters:           - `filename`: The name of the file to upload.           - `source`: The source stream of the content to be uploaded. Must be             a file-like object that implements :meth:`read` or a string.           - `chunk_size_bytes` (options): The number of bytes per chunk of this             file. Defaults to the chunk_size_bytes of :class:`GridFSBucket`.           - `metadata` (optional): User data for the 'metadata' field of the             files collection document. If not provided the metadata field will             be omitted from the files collection document.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`          .. versionchanged:: 3.6            Added ``session`` parameter.
Write the contents of `filename` (with optional `revision`) to         `destination`.          For example::            my_db = MongoClient().test           fs = GridFSBucket(my_db)           # Get file to write to           file = open('myfile','wb')           fs.download_to_stream_by_name("test_file", file)          Raises :exc:`~gridfs.errors.NoFile` if no such version of         that file exists.          Raises :exc:`~ValueError` if `filename` is not a string.          :Parameters:           - `filename`: The name of the file to read from.           - `destination`: A file-like object that implements :meth:`write`.           - `revision` (optional): Which revision (documents with the same             filename and different uploadDate) of the file to retrieve.             Defaults to -1 (the most recent revision).           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`          :Note: Revision numbers are defined as follows:            - 0 = the original stored file           - 1 = the first revision           - 2 = the second revision           - etc...           - -2 = the second most recent revision           - -1 = the most recent revision          .. versionchanged:: 3.6            Added ``session`` parameter.
Renames the stored file with the specified file_id.          For example::            my_db = MongoClient().test           fs = GridFSBucket(my_db)           # Get _id of file to rename           file_id = fs.upload_from_stream("test_file", "data I want to store!")           fs.rename(file_id, "new_test_name")          Raises :exc:`~gridfs.errors.NoFile` if no file with file_id exists.          :Parameters:           - `file_id`: The _id of the file to be renamed.           - `new_filename`: The new name of the file.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`          .. versionchanged:: 3.6            Added ``session`` parameter.
Get a clone of this database changing the specified settings.            >>> db1.read_preference           Primary()           >>> from pymongo import ReadPreference           >>> db2 = db1.with_options(read_preference=ReadPreference.SECONDARY)           >>> db1.read_preference           Primary()           >>> db2.read_preference           Secondary(tag_sets=None)          :Parameters:           - `codec_options` (optional): An instance of             :class:`~bson.codec_options.CodecOptions`. If ``None`` (the             default) the :attr:`codec_options` of this :class:`Collection`             is used.           - `read_preference` (optional): The read preference to use. If             ``None`` (the default) the :attr:`read_preference` of this             :class:`Collection` is used. See :mod:`~pymongo.read_preferences`             for options.           - `write_concern` (optional): An instance of             :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the             default) the :attr:`write_concern` of this :class:`Collection`             is used.           - `read_concern` (optional): An instance of             :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the             default) the :attr:`read_concern` of this :class:`Collection`             is used.          .. versionadded:: 3.8
Create a new :class:`~pymongo.collection.Collection` in this         database.          Normally collection creation is automatic. This method should         only be used to specify options on         creation. :class:`~pymongo.errors.CollectionInvalid` will be         raised if the collection already exists.          Options should be passed as keyword arguments to this method. Supported         options vary with MongoDB release. Some examples include:            - "size": desired initial size for the collection (in             bytes). For capped collections this size is the max             size of the collection.           - "capped": if True, this is a capped collection           - "max": maximum number of objects if capped (optional)          See the MongoDB documentation for a full list of supported options by         server version.          :Parameters:           - `name`: the name of the collection to create           - `codec_options` (optional): An instance of             :class:`~bson.codec_options.CodecOptions`. If ``None`` (the             default) the :attr:`codec_options` of this :class:`Database` is             used.           - `read_preference` (optional): The read preference to use. If             ``None`` (the default) the :attr:`read_preference` of this             :class:`Database` is used.           - `write_concern` (optional): An instance of             :class:`~pymongo.write_concern.WriteConcern`. If ``None`` (the             default) the :attr:`write_concern` of this :class:`Database` is             used.           - `read_concern` (optional): An instance of             :class:`~pymongo.read_concern.ReadConcern`. If ``None`` (the             default) the :attr:`read_concern` of this :class:`Database` is             used.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): additional keyword arguments will             be passed as options for the create collection command          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Added the collation option.          .. versionchanged:: 3.0            Added the codec_options, read_preference, and write_concern options.          .. versionchanged:: 2.2            Removed deprecated argument: options
Watch changes on this database.          Performs an aggregation with an implicit initial ``$changeStream``         stage and returns a         :class:`~pymongo.change_stream.DatabaseChangeStream` cursor which         iterates over changes on all collections in this database.          Introduced in MongoDB 4.0.          .. code-block:: python             with db.watch() as stream:                for change in stream:                    print(change)          The :class:`~pymongo.change_stream.DatabaseChangeStream` iterable         blocks until the next change document is returned or an error is         raised. If the         :meth:`~pymongo.change_stream.DatabaseChangeStream.next` method         encounters a network error when retrieving a batch from the server,         it will automatically attempt to recreate the cursor such that no         change events are missed. Any error encountered during the resume         attempt indicates there may be an outage and will be raised.          .. code-block:: python              try:                 with db.watch(                         [{'$match': {'operationType': 'insert'}}]) as stream:                     for insert_change in stream:                         print(insert_change)             except pymongo.errors.PyMongoError:                 # The ChangeStream encountered an unrecoverable error or the                 # resume attempt failed to recreate the cursor.                 logging.error('...')          For a precise description of the resume process see the         `change streams specification`_.          :Parameters:           - `pipeline` (optional): A list of aggregation pipeline stages to             append to an initial ``$changeStream`` stage. Not all             pipeline stages are valid after a ``$changeStream`` stage, see the             MongoDB documentation on change streams for the supported stages.           - `full_document` (optional): The fullDocument to pass as an option             to the ``$changeStream`` stage. Allowed values: 'default',             'updateLookup'.  Defaults to 'default'.             When set to 'updateLookup', the change notification for partial             updates will include both a delta describing the changes to the             document, as well as a copy of the entire document that was             changed from some time after the change occurred.           - `resume_after` (optional): The logical starting point for this             change stream.           - `max_await_time_ms` (optional): The maximum time in milliseconds             for the server to wait for changes before responding to a getMore             operation.           - `batch_size` (optional): The maximum number of documents to return             per batch.           - `collation` (optional): The :class:`~pymongo.collation.Collation`             to use for the aggregation.           - `start_at_operation_time` (optional): If provided, the resulting             change stream will only return changes that occurred at or after             the specified :class:`~bson.timestamp.Timestamp`. Requires             MongoDB >= 4.0.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           A :class:`~pymongo.change_stream.DatabaseChangeStream` cursor.          .. versionadded:: 3.7          .. mongodoc:: changeStreams          .. _change streams specification:             https://github.com/mongodb/specifications/blob/master/source/change-streams/change-streams.rst
Internal command helper.
Same as command but used for retryable read commands.
Internal listCollections helper.
Get a cursor over the collectons of this database.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `filter` (optional):  A query document to filter the list of             collections returned from the listCollections command.           - `**kwargs` (optional): Optional parameters of the             `listCollections command             <https://docs.mongodb.com/manual/reference/command/listCollections/>`_             can be passed as keyword arguments to this method. The supported             options differ by server version.          :Returns:           An instance of :class:`~pymongo.command_cursor.CommandCursor`.          .. versionadded:: 3.6
Get a list of all the collection names in this database.          For example, to list all non-system collections::              filter = {"name": {"$regex": r"^(?!system\.)"}}             db.list_collection_names(filter=filter)          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `filter` (optional):  A query document to filter the list of             collections returned from the listCollections command.           - `**kwargs` (optional): Optional parameters of the             `listCollections command             <https://docs.mongodb.com/manual/reference/command/listCollections/>`_             can be passed as keyword arguments to this method. The supported             options differ by server version.          .. versionchanged:: 3.8            Added the ``filter`` and ``**kwargs`` parameters.          .. versionadded:: 3.6
**DEPRECATED**: Get a list of all the collection names in this         database.          :Parameters:           - `include_system_collections` (optional): if ``False`` list             will not include system collections (e.g ``system.indexes``)           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.7            Deprecated. Use :meth:`list_collection_names` instead.          .. versionchanged:: 3.6            Added ``session`` parameter.
Drop a collection.          :Parameters:           - `name_or_collection`: the name of a collection to drop or the             collection object itself           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. note:: The :attr:`~pymongo.database.Database.write_concern` of            this database is automatically applied to this operation when using            MongoDB >= 3.4.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Apply this database's write concern automatically to this operation            when connected to MongoDB >= 3.4.
Validate a collection.          Returns a dict of validation info. Raises CollectionInvalid if         validation fails.          :Parameters:           - `name_or_collection`: A Collection object or the name of a             collection to validate.           - `scandata`: Do extra checks beyond checking the overall             structure of the collection.           - `full`: Have the server do a more thorough scan of the             collection. Use with `scandata` for a thorough scan             of the structure of the collection and the individual             documents.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Get information on operations currently running.          :Parameters:           - `include_all` (optional): if ``True`` also list currently             idle operations in the result           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Get the database's current profiling level.          Returns one of (:data:`~pymongo.OFF`,         :data:`~pymongo.SLOW_ONLY`, :data:`~pymongo.ALL`).          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. mongodoc:: profiling
Set the database's profiling level.          :Parameters:           - `level`: Specifies a profiling level, see list of possible values             below.           - `slow_ms`: Optionally modify the threshold for the profile to             consider a query or operation.  Even if the profiler is off queries             slower than the `slow_ms` level will get written to the logs.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          Possible `level` values:          +----------------------------+------------------------------------+         | Level                      | Setting                            |         +============================+====================================+         | :data:`~pymongo.OFF`       | Off. No profiling.                 |         +----------------------------+------------------------------------+         | :data:`~pymongo.SLOW_ONLY` | On. Only includes slow operations. |         +----------------------------+------------------------------------+         | :data:`~pymongo.ALL`       | On. Includes all operations.       |         +----------------------------+------------------------------------+          Raises :class:`ValueError` if level is not one of         (:data:`~pymongo.OFF`, :data:`~pymongo.SLOW_ONLY`,         :data:`~pymongo.ALL`).          .. versionchanged:: 3.6            Added ``session`` parameter.          .. mongodoc:: profiling
**DEPRECATED**: Create user `name` with password `password`.          Add a new user with permissions for this :class:`Database`.          .. note:: Will change the password if user `name` already exists.          .. note:: add_user is deprecated and will be removed in PyMongo           4.0. Starting with MongoDB 2.6 user management is handled with four           database commands, createUser_, usersInfo_, updateUser_, and           dropUser_.            To create a user::              db.command("createUser", "admin", pwd="password", roles=["root"])            To create a read-only user::              db.command("createUser", "user", pwd="password", roles=["read"])            To change a password::              db.command("updateUser", "user", pwd="newpassword")            Or change roles::              db.command("updateUser", "user", roles=["readWrite"])          .. _createUser: https://docs.mongodb.com/manual/reference/command/createUser/         .. _usersInfo: https://docs.mongodb.com/manual/reference/command/usersInfo/         .. _updateUser: https://docs.mongodb.com/manual/reference/command/updateUser/         .. _dropUser: https://docs.mongodb.com/manual/reference/command/createUser/          .. warning:: Never create or modify users over an insecure network without           the use of TLS. See :doc:`/examples/tls` for more information.          :Parameters:           - `name`: the name of the user to create           - `password` (optional): the password of the user to create. Can not             be used with the ``userSource`` argument.           - `read_only` (optional): if ``True`` the user will be read only           - `**kwargs` (optional): optional fields for the user document             (e.g. ``userSource``, ``otherDBRoles``, or ``roles``). See             `<http://docs.mongodb.org/manual/reference/privilege-documents>`_             for more information.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.7            Added support for SCRAM-SHA-256 users with MongoDB 4.0 and later.          .. versionchanged:: 3.6            Added ``session`` parameter. Deprecated add_user.          .. versionchanged:: 2.5            Added kwargs support for optional fields introduced in MongoDB 2.4          .. versionchanged:: 2.2            Added support for read only users
**DEPRECATED**: Remove user `name` from this :class:`Database`.          User `name` will no longer have permissions to access this         :class:`Database`.          .. note:: remove_user is deprecated and will be removed in PyMongo           4.0. Use the dropUser command instead::              db.command("dropUser", "user")          :Parameters:           - `name`: the name of the user to remove           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter. Deprecated remove_user.
Dereference a :class:`~bson.dbref.DBRef`, getting the         document it points to.          Raises :class:`TypeError` if `dbref` is not an instance of         :class:`~bson.dbref.DBRef`. Returns a document, or ``None`` if         the reference does not point to a valid document.  Raises         :class:`ValueError` if `dbref` has a database specified that         is different from the current database.          :Parameters:           - `dbref`: the reference           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): any additional keyword arguments             are the same as the arguments to             :meth:`~pymongo.collection.Collection.find`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Closes this cursor.
Send a getmore message and handle the response.
Refreshes the cursor with more data from the server.          Returns the length of self.__data after refresh. Will exit early if         self.__data is already non-empty. Raises OperationFailure when the         cursor cannot be refreshed due to an error on the query.
Advance the cursor.
Advance the cursor blocking for at most one getMore command.
Generate a 5-byte random number once per process.
Generate a new value for this ObjectId.
A :class:`datetime.datetime` instance representing the time of         generation for this :class:`ObjectId`.          The :class:`datetime.datetime` is timezone aware, and         represents the generation time in UTC. It is precise to the         second.
Given (host, port) and PoolOptions, return a configured socket.      Can raise socket.error, ConnectionFailure, or CertificateError.      Sets socket's SSL and timeout options.
Execute a command or raise an error.          :Parameters:           - `dbname`: name of the database on which to run the command           - `spec`: a command document as a dict, SON, or mapping object           - `slave_ok`: whether to set the SlaveOkay wire protocol bit           - `read_preference`: a read preference           - `codec_options`: a CodecOptions instance           - `check`: raise OperationFailure if there are errors           - `allowable_errors`: errors to ignore if `check` is True           - `check_keys`: if True, check `spec` for invalid keys           - `read_concern`: The read concern for this command.           - `write_concern`: The write concern for this command.           - `parse_write_concern_error`: Whether to parse the             ``writeConcernError`` field in the command response.           - `collation`: The collation for this command.           - `session`: optional ClientSession instance.           - `client`: optional MongoClient for gossipping $clusterTime.           - `retryable_write`: True if this command is a retryable write.           - `publish_events`: Should we publish events for this command?           - `user_fields` (optional): Response fields that should be decoded             using the TypeDecoders from codec_options, passed to             bson._decode_all_selective.
Send OP_INSERT, etc., optionally returning response as a dict.          Can raise ConnectionFailure or OperationFailure.          :Parameters:           - `request_id`: an int.           - `msg`: bytes, an OP_INSERT, OP_UPDATE, or OP_DELETE message,             perhaps with a getlasterror command appended.           - `max_doc_size`: size in bytes of the largest document in `msg`.           - `with_last_error`: True if a getlasterror command is appended.
Send "insert" etc. command, returning response as a dict.          Can raise ConnectionFailure or OperationFailure.          :Parameters:           - `request_id`: an int.           - `msg`: bytes, the command message.
Validate this session before use with client.          Raises error if this session is logged in as a different user or         the client is not the one that created the session.
Add cluster time for MongoDB >= 3.6.
Removes stale sockets then adds new ones if pool is too small.
Connect to Mongo and return a new SocketInfo.          Can raise ConnectionFailure or CertificateError.          Note that the pool does not keep a reference to the socket -- you         must call return_socket() when you're done with it.
Get or create a SocketInfo. Can raise ConnectionFailure.
Return the socket to the pool, or if it's closed discard it.
This side-effecty function checks if this socket has been idle for         for longer than the max idle time, or if the socket has been closed by         some external network error, and if so, attempts to create a new         socket. If this connection attempt fails we raise the         ConnectionFailure.          Checking sockets lets us avoid seeing *some*         :class:`~pymongo.errors.AutoReconnect` exceptions on server         hiccups, etc. We only check if the socket was closed by an external         error if it has been > 1 second since the socket was checked into the         pool, to keep performance reasonable - we can't avoid AutoReconnects         completely anyway.
Execute a command over the socket, or raise socket.error.      :Parameters:       - `sock`: a raw socket instance       - `dbname`: name of the database on which to run the command       - `spec`: a command document as an ordered dict type, eg SON.       - `slave_ok`: whether to set the SlaveOkay wire protocol bit       - `is_mongos`: are we connected to a mongos?       - `read_preference`: a read preference       - `codec_options`: a CodecOptions instance       - `session`: optional ClientSession instance.       - `client`: optional MongoClient instance for updating $clusterTime.       - `check`: raise OperationFailure if there are errors       - `allowable_errors`: errors to ignore if `check` is True       - `address`: the (host, port) of `sock`       - `check_keys`: if True, check `spec` for invalid keys       - `listeners`: An instance of :class:`~pymongo.monitoring.EventListeners`       - `max_bson_size`: The maximum encoded bson size for this server       - `read_concern`: The read concern for this command.       - `parse_write_concern_error`: Whether to parse the ``writeConcernError``         field in the command response.       - `collation`: The collation for this command.       - `compression_ctx`: optional compression Context.       - `use_op_msg`: True if we should use OP_MSG.       - `unacknowledged`: True if this is an unacknowledged command.       - `user_fields` (optional): Response fields that should be decoded         using the TypeDecoders from codec_options, passed to         bson._decode_all_selective.
Receive a raw BSON message or raise socket.error.
Return True if we know socket has been closed, False otherwise.
Validate and return a BSON document's size.
Decode a BSON subdocument to opts.document_class or bson.dbref.DBRef.
Decode a BSON array to python list.
Decode a single key, value pair.
Decode a BSON document into result.
Decode a BSON string to document_class.
Encode a single name, value pair.
Convert datetime to milliseconds since epoch UTC.
Decode BSON data to a single document while using user-provided     custom decoding logic.      `data` must be a string representing a valid, BSON-encoded document.      :Parameters:       - `data`: BSON data       - `codec_options`: An instance of         :class:`~bson.codec_options.CodecOptions` with user-specified type         decoders. If no decoders are found, this method is the same as         ``decode_all``.       - `fields`: Map of document namespaces where data that needs         to be custom decoded lives or None. For example, to custom decode a         list of objects in 'field1.subfield1', the specified value should be         ``{'field1': {'subfield1': 1}}``. If ``fields``  is an empty map or         None, this method is the same as ``decode_all``.      :Returns:       - `document_list`: Single-member list containing the decoded document.      .. versionadded:: 3.8
Internal clone helper.
Closes this cursor.
**DEPRECATED** - Get the size of the results set for this query.          The :meth:`count` method is deprecated and **not** supported in a         transaction. Please use         :meth:`~pymongo.collection.Collection.count_documents` instead.          Returns the number of documents in the results set for this query. Does         not take :meth:`limit` and :meth:`skip` into account by default - set         `with_limit_and_skip` to ``True`` if that is the desired behavior.         Raises :class:`~pymongo.errors.OperationFailure` on a database error.          When used with MongoDB >= 2.6, :meth:`~count` uses any :meth:`~hint`         applied to the query. In the following example the hint is passed to         the count command:            collection.find({'field': 'value'}).hint('field_1').count()          The :meth:`count` method obeys the         :attr:`~pymongo.collection.Collection.read_preference` of the         :class:`~pymongo.collection.Collection` instance on which         :meth:`~pymongo.collection.Collection.find` was called.          :Parameters:           - `with_limit_and_skip` (optional): take any :meth:`limit` or             :meth:`skip` that has been applied to this cursor into account when             getting the count          .. note:: The `with_limit_and_skip` parameter requires server            version **>= 1.1.4-**          .. versionchanged:: 3.7            Deprecated.          .. versionchanged:: 2.8            The :meth:`~count` method now supports :meth:`~hint`.
Send a query or getmore operation and handles the response.          If operation is ``None`` this is an exhaust cursor, which reads         the next result batch off the exhaust socket instead of         sending getMore messages to the server.          Can raise ConnectionFailure.
Refreshes the cursor with more data from Mongo.          Returns the length of self.__data after refresh. Will exit early if         self.__data is already non-empty. Raises OperationFailure when the         cursor cannot be refreshed due to an error on the query.
Returns an explain plan record for this cursor.          .. mongodoc:: explain
Helper function that wraps :func:`json.loads`.      Automatically passes the object_hook for BSON type conversion.      Raises ``TypeError``, ``ValueError``, ``KeyError``, or     :exc:`~bson.errors.InvalidId` on invalid MongoDB Extended JSON.      :Parameters:       - `json_options`: A :class:`JSONOptions` instance used to modify the         decoding of MongoDB Extended JSON types. Defaults to         :const:`DEFAULT_JSON_OPTIONS`.      .. versionchanged:: 3.5        Parses Relaxed and Canonical Extended JSON as well as PyMongo's legacy        format. Now raises ``TypeError`` or ``ValueError`` when parsing JSON        type wrappers with values of the wrong type or any extra keys.      .. versionchanged:: 3.4        Accepts optional parameter `json_options`. See :class:`JSONOptions`.
Validate that an explicit session is not used with an unack'ed write.      Returns the session to use for the next operation.
Return the inherited TransactionOption value.
Execute a callback in a transaction.          This method starts a transaction on this session, executes ``callback``         once, and then commits the transaction. For example::            def callback(session):               orders = session.client.db.orders               inventory = session.client.db.inventory               orders.insert_one({"sku": "abc123", "qty": 100}, session=session)               inventory.update_one({"sku": "abc123", "qty": {"$gte": 100}},                                    {"$inc": {"qty": -100}}, session=session)            with client.start_session() as session:               session.with_transaction(callback)          To pass arbitrary arguments to the ``callback``, wrap your callable         with a ``lambda`` like this::            def callback(session, custom_arg, custom_kwarg=None):               # Transaction operations...            with client.start_session() as session:               session.with_transaction(                   lambda s: callback(s, "custom_arg", custom_kwarg=1))          In the event of an exception, ``with_transaction`` may retry the commit         or the entire transaction, therefore ``callback`` may be invoked         multiple times by a single call to ``with_transaction``. Developers         should be mindful of this possiblity when writing a ``callback`` that         modifies application state or has any other side-effects.         Note that even when the ``callback`` is invoked multiple times,         ``with_transaction`` ensures that the transaction will be committed         at-most-once on the server.          The ``callback`` should not attempt to start new transactions, but         should simply run operations meant to be contained within a         transaction. The ``callback`` should also not commit the transaction;         this is handled automatically by ``with_transaction``. If the         ``callback`` does commit or abort the transaction without error,         however, ``with_transaction`` will return without taking further         action.          When ``callback`` raises an exception, ``with_transaction``         automatically aborts the current transaction. When ``callback`` or         :meth:`~ClientSession.commit_transaction` raises an exception that         includes the ``"TransientTransactionError"`` error label,         ``with_transaction`` starts a new transaction and re-executes         the ``callback``.          When :meth:`~ClientSession.commit_transaction` raises an exception with         the ``"UnknownTransactionCommitResult"`` error label,         ``with_transaction`` retries the commit until the result of the         transaction is known.          This method will cease retrying after 120 seconds has elapsed. This         timeout is not configurable and any exception raised by the         ``callback`` or by :meth:`ClientSession.commit_transaction` after the         timeout is reached will be re-raised. Applications that desire a         different timeout duration should not use this method.          :Parameters:           - `callback`: The callable ``callback`` to run inside a transaction.             The callable must accept a single argument, this session. Note,             under certain error conditions the callback may be run multiple             times.           - `read_concern` (optional): The             :class:`~pymongo.read_concern.ReadConcern` to use for this             transaction.           - `write_concern` (optional): The             :class:`~pymongo.write_concern.WriteConcern` to use for this             transaction.           - `read_preference` (optional): The read preference to use for this             transaction. If ``None`` (the default) the :attr:`read_preference`             of this :class:`Database` is used. See             :mod:`~pymongo.read_preferences` for options.          :Returns:           The return value of the ``callback``.          .. versionadded:: 3.9
Start a multi-statement transaction.          Takes the same arguments as :class:`TransactionOptions`.          .. versionadded:: 3.7
Commit a multi-statement transaction.          .. versionadded:: 3.7
Abort a multi-statement transaction.          .. versionadded:: 3.7
Run commit or abort with one retry after any retryable error.          :Parameters:           - `command_name`: Either "commitTransaction" or "abortTransaction".           - `explict_retry`: True when this is an explict commit retry attempt,             ie the application called session.commit_transaction() twice.
Internal cluster time helper.
Update the cluster time for this session.          :Parameters:           - `cluster_time`: The             :data:`~pymongo.client_session.ClientSession.cluster_time` from             another `ClientSession` instance.
Internal operation time helper.
Update the operation time for this session.          :Parameters:           - `operation_time`: The             :data:`~pymongo.client_session.ClientSession.operation_time` from             another `ClientSession` instance.
Process a response to a command that was run with this session.
Pin this session to the given mongos Server.
Parse write concern options.
Parse ssl options.
Parse connection pool options.
Helper to generate an index specifying document.      Takes a list of (key, direction) pairs.
Check the response to a command for errors.
Return getlasterror response as a dict, or raise OperationFailure.
Backward compatibility helper for write command error handling.
Backward compatibility helper for insert error handling.
Start monitoring, or restart after a fork.          No effect if called multiple times.          .. warning:: Topology is shared among multiple threads and is protected           by mutual exclusion. Using Topology from a process other than the one           that initialized it will emit a warning and may result in deadlock. To           prevent this from happening, MongoClient must be created after any           forking.
Return a list of Servers matching selector, or time out.          :Parameters:           - `selector`: function that takes a list of Servers and returns             a subset of them.           - `server_selection_timeout` (optional): maximum seconds to wait.             If not provided, the default value common.SERVER_SELECTION_TIMEOUT             is used.           - `address`: optional server address to select.          Calls self.open() if needed.          Raises exc:`ServerSelectionTimeoutError` after         `server_selection_timeout` if no matching servers are found.
select_servers() guts. Hold the lock when calling this.
Process a new ServerDescription on an opened topology.          Hold the lock when calling this.
Process a new ServerDescription after an ismaster call completes.
Start or resume a server session, or raise ConfigurationError.
Start monitors, or restart after a fork.          Hold the lock when calling this.
Call ismaster once or twice. Reset server's pool on error.          Returns a ServerDescription.
A single attempt to call ismaster.          Returns a ServerDescription, or raises an exception.
Return (IsMaster, round_trip_time).          Can raise ConnectionFailure or OperationFailure.
Clear the given database/collection object's type registry.
Remove all chunks/files that may have been uploaded and close.
Reads a chunk at a time. If the current position is within a         chunk the remainder of the chunk is returned.
Read at most `size` bytes from the file (less if there         isn't enough data).          The bytes are returned as an instance of :class:`str` (:class:`bytes`         in python 3). If `size` is negative or omitted all data is read.          :Parameters:           - `size` (optional): the number of bytes to read          .. versionchanged:: 3.8            This method now only checks for extra chunks after reading the            entire file. Previously, this method would check for extra chunks            on every call.
Read one line or up to `size` bytes from the file.          :Parameters:          - `size` (optional): the maximum number of bytes to read
Return the next chunk and retry once on CursorNotFound.          We retry on CursorNotFound to maintain backwards compatibility in         cases where two calls to read occur more than 10 minutes apart (the         server's default cursor timeout).
Get next GridOut object from cursor.
Build and return a mechanism specific credentials tuple.
Authenticate using SCRAM.
Get a password digest to use for authentication.
Get an auth key to use for authentication.
Internal command helper.          :Parameters:           - `sock_info` - A SocketInfo instance.           - `command` - The command itself, as a SON instance.           - `slave_ok`: whether to set the SlaveOkay wire protocol bit.           - `codec_options` (optional) - An instance of             :class:`~bson.codec_options.CodecOptions`.           - `check`: raise OperationFailure if there are errors           - `allowable_errors`: errors to ignore if `check` is True           - `read_concern` (optional) - An instance of             :class:`~pymongo.read_concern.ReadConcern`.           - `write_concern`: An instance of             :class:`~pymongo.write_concern.WriteConcern`. This option is only             valid for MongoDB 3.4 and above.           - `collation` (optional) - An instance of             :class:`~pymongo.collation.Collation`.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `retryable_write` (optional): True if this command is a retryable             write.           - `user_fields` (optional): Response fields that should be decoded             using the TypeDecoders from codec_options, passed to             bson._decode_all_selective.          :Returns:           The result document.
Sends a create command with the given options.
Send a batch of write operations to the server.          Requests are passed as a list of write operation instances (         :class:`~pymongo.operations.InsertOne`,         :class:`~pymongo.operations.UpdateOne`,         :class:`~pymongo.operations.UpdateMany`,         :class:`~pymongo.operations.ReplaceOne`,         :class:`~pymongo.operations.DeleteOne`, or         :class:`~pymongo.operations.DeleteMany`).            >>> for doc in db.test.find({}):           ...     print(doc)           ...           {u'x': 1, u'_id': ObjectId('54f62e60fba5226811f634ef')}           {u'x': 1, u'_id': ObjectId('54f62e60fba5226811f634f0')}           >>> # DeleteMany, UpdateOne, and UpdateMany are also available.           ...           >>> from pymongo import InsertOne, DeleteOne, ReplaceOne           >>> requests = [InsertOne({'y': 1}), DeleteOne({'x': 1}),           ...             ReplaceOne({'w': 1}, {'z': 1}, upsert=True)]           >>> result = db.test.bulk_write(requests)           >>> result.inserted_count           1           >>> result.deleted_count           1           >>> result.modified_count           0           >>> result.upserted_ids           {2: ObjectId('54f62ee28891e756a6e1abd5')}           >>> for doc in db.test.find({}):           ...     print(doc)           ...           {u'x': 1, u'_id': ObjectId('54f62e60fba5226811f634f0')}           {u'y': 1, u'_id': ObjectId('54f62ee2fba5226811f634f1')}           {u'z': 1, u'_id': ObjectId('54f62ee28891e756a6e1abd5')}          :Parameters:           - `requests`: A list of write operations (see examples above).           - `ordered` (optional): If ``True`` (the default) requests will be             performed on the server serially, in the order provided. If an error             occurs all remaining operations are aborted. If ``False`` requests             will be performed on the server in arbitrary order, possibly in             parallel, and all operations will be attempted.           - `bypass_document_validation`: (optional) If ``True``, allows the             write to opt-out of document level validation. Default is             ``False``.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           An instance of :class:`~pymongo.results.BulkWriteResult`.          .. seealso:: :ref:`writes-and-ids`          .. note:: `bypass_document_validation` requires server version           **>= 3.2**          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.2           Added bypass_document_validation support          .. versionadded:: 3.0
Internal legacy unacknowledged write helper.
Internal helper for inserting a single document.
Internal insert helper.
Insert a single document.            >>> db.test.count_documents({'x': 1})           0           >>> result = db.test.insert_one({'x': 1})           >>> result.inserted_id           ObjectId('54f112defba522406c9cc208')           >>> db.test.find_one({'x': 1})           {u'x': 1, u'_id': ObjectId('54f112defba522406c9cc208')}          :Parameters:           - `document`: The document to insert. Must be a mutable mapping             type. If the document does not have an _id field one will be             added automatically.           - `bypass_document_validation`: (optional) If ``True``, allows the             write to opt-out of document level validation. Default is             ``False``.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           - An instance of :class:`~pymongo.results.InsertOneResult`.          .. seealso:: :ref:`writes-and-ids`          .. note:: `bypass_document_validation` requires server version           **>= 3.2**          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.2           Added bypass_document_validation support          .. versionadded:: 3.0
Internal update / replace helper.
Internal update / replace helper.
Replace a single document matching the filter.            >>> for doc in db.test.find({}):           ...     print(doc)           ...           {u'x': 1, u'_id': ObjectId('54f4c5befba5220aa4d6dee7')}           >>> result = db.test.replace_one({'x': 1}, {'y': 1})           >>> result.matched_count           1           >>> result.modified_count           1           >>> for doc in db.test.find({}):           ...     print(doc)           ...           {u'y': 1, u'_id': ObjectId('54f4c5befba5220aa4d6dee7')}          The *upsert* option can be used to insert a new document if a matching         document does not exist.            >>> result = db.test.replace_one({'x': 1}, {'x': 1}, True)           >>> result.matched_count           0           >>> result.modified_count           0           >>> result.upserted_id           ObjectId('54f11e5c8891e756a6e1abd4')           >>> db.test.find_one({'x': 1})           {u'x': 1, u'_id': ObjectId('54f11e5c8891e756a6e1abd4')}          :Parameters:           - `filter`: A query that matches the document to replace.           - `replacement`: The new document.           - `upsert` (optional): If ``True``, perform an insert if no documents             match the filter.           - `bypass_document_validation`: (optional) If ``True``, allows the             write to opt-out of document level validation. Default is             ``False``.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           - An instance of :class:`~pymongo.results.UpdateResult`.          .. note:: `bypass_document_validation` requires server version           **>= 3.2**          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4           Added the `collation` option.          .. versionchanged:: 3.2           Added bypass_document_validation support          .. versionadded:: 3.0
Update a single document matching the filter.            >>> for doc in db.test.find():           ...     print(doc)           ...           {u'x': 1, u'_id': 0}           {u'x': 1, u'_id': 1}           {u'x': 1, u'_id': 2}           >>> result = db.test.update_one({'x': 1}, {'$inc': {'x': 3}})           >>> result.matched_count           1           >>> result.modified_count           1           >>> for doc in db.test.find():           ...     print(doc)           ...           {u'x': 4, u'_id': 0}           {u'x': 1, u'_id': 1}           {u'x': 1, u'_id': 2}          :Parameters:           - `filter`: A query that matches the document to update.           - `update`: The modifications to apply.           - `upsert` (optional): If ``True``, perform an insert if no documents             match the filter.           - `bypass_document_validation`: (optional) If ``True``, allows the             write to opt-out of document level validation. Default is             ``False``.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.           - `array_filters` (optional): A list of filters specifying which             array elements an update should apply. Requires MongoDB 3.6+.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           - An instance of :class:`~pymongo.results.UpdateResult`.          .. note:: `bypass_document_validation` requires server version           **>= 3.2**          .. versionchanged:: 3.6            Added the `array_filters` and ``session`` parameters.          .. versionchanged:: 3.4           Added the `collation` option.          .. versionchanged:: 3.2           Added bypass_document_validation support          .. versionadded:: 3.0
Alias for :meth:`~pymongo.database.Database.drop_collection`.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          The following two calls are equivalent:            >>> db.foo.drop()           >>> db.drop_collection("foo")          .. versionchanged:: 3.7            :meth:`drop` now respects this :class:`Collection`'s :attr:`write_concern`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Internal delete helper.
Internal delete helper.
Delete a single document matching the filter.            >>> db.test.count_documents({'x': 1})           3           >>> result = db.test.delete_one({'x': 1})           >>> result.deleted_count           1           >>> db.test.count_documents({'x': 1})           2          :Parameters:           - `filter`: A query that matches the document to delete.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           - An instance of :class:`~pymongo.results.DeleteResult`.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4           Added the `collation` option.          .. versionadded:: 3.0
Delete one or more documents matching the filter.            >>> db.test.count_documents({'x': 1})           3           >>> result = db.test.delete_many({'x': 1})           >>> result.deleted_count           3           >>> db.test.count_documents({'x': 1})           0          :Parameters:           - `filter`: A query that matches the documents to delete.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           - An instance of :class:`~pymongo.results.DeleteResult`.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4           Added the `collation` option.          .. versionadded:: 3.0
Query the database and retrieve batches of raw BSON.          Similar to the :meth:`find` method but returns a         :class:`~pymongo.cursor.RawBatchCursor`.          This example demonstrates how to work with raw batches, but in practice         raw batches should be passed to an external library that can decode         BSON into another data type, rather than used with PyMongo's         :mod:`bson` module.            >>> import bson           >>> cursor = db.test.find_raw_batches()           >>> for batch in cursor:           ...     print(bson.decode_all(batch))          .. note:: find_raw_batches does not support sessions.          .. versionadded:: 3.6
**DEPRECATED**: Scan this entire collection in parallel.          Returns a list of up to ``num_cursors`` cursors that can be iterated         concurrently. As long as the collection is not modified during         scanning, each document appears once in one of the cursors result         sets.          For example, to process each document in a collection using some         thread-safe ``process_document()`` function:            >>> def process_cursor(cursor):           ...     for document in cursor:           ...     # Some thread-safe processing function:           ...     process_document(document)           >>>           >>> # Get up to 4 cursors.           ...           >>> cursors = collection.parallel_scan(4)           >>> threads = [           ...     threading.Thread(target=process_cursor, args=(cursor,))           ...     for cursor in cursors]           >>>           >>> for thread in threads:           ...     thread.start()           >>>           >>> for thread in threads:           ...     thread.join()           >>>           >>> # All documents have now been processed.          The :meth:`parallel_scan` method obeys the :attr:`read_preference` of         this :class:`Collection`.          :Parameters:           - `num_cursors`: the number of cursors to return           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs`: additional options for the parallelCollectionScan             command can be passed as keyword arguments.          .. note:: Requires server version **>= 2.5.5**.          .. versionchanged:: 3.7            Deprecated.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Added back support for arbitrary keyword arguments. MongoDB 3.4            adds support for maxTimeMS as an option to the            parallelCollectionScan command.          .. versionchanged:: 3.0            Removed support for arbitrary keyword arguments, since            the parallelCollectionScan command has no optional arguments.
Internal count helper.
Internal helper to run an aggregate that returns a single result.
Get an estimate of the number of documents in this collection using         collection metadata.          The :meth:`estimated_document_count` method is **not** supported in a         transaction.          All optional parameters should be passed as keyword arguments         to this method. Valid options include:            - `maxTimeMS` (int): The maximum amount of time to allow this             operation to run, in milliseconds.          :Parameters:           - `**kwargs` (optional): See list of options above.          .. versionadded:: 3.7
Count the number of documents in this collection.          The :meth:`count_documents` method is supported in a transaction.          All optional parameters should be passed as keyword arguments         to this method. Valid options include:            - `skip` (int): The number of matching documents to skip before             returning results.           - `limit` (int): The maximum number of documents to count. Must be             a positive integer. If not provided, no limit is imposed.           - `maxTimeMS` (int): The maximum amount of time to allow this             operation to run, in milliseconds.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.           - `hint` (string or list of tuples): The index to use. Specify either             the index name as a string or the index specification as a list of             tuples (e.g. [('a', pymongo.ASCENDING), ('b', pymongo.ASCENDING)]).             This option is only supported on MongoDB 3.6 and above.          The :meth:`count_documents` method obeys the :attr:`read_preference` of         this :class:`Collection`.          .. note:: When migrating from :meth:`count` to :meth:`count_documents`            the following query operators must be replaced:             +-------------+-------------------------------------+            | Operator    | Replacement                         |            +=============+=====================================+            | $where      | `$expr`_                            |            +-------------+-------------------------------------+            | $near       | `$geoWithin`_ with `$center`_       |            +-------------+-------------------------------------+            | $nearSphere | `$geoWithin`_ with `$centerSphere`_ |            +-------------+-------------------------------------+             $expr requires MongoDB 3.6+          :Parameters:           - `filter` (required): A query document that selects which documents             to count in the collection. Can be an empty document to count all             documents.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): See list of options above.          .. versionadded:: 3.7          .. _$expr: https://docs.mongodb.com/manual/reference/operator/query/expr/         .. _$geoWithin: https://docs.mongodb.com/manual/reference/operator/query/geoWithin/         .. _$center: https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center         .. _$centerSphere: https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere
**DEPRECATED** - Get the number of documents in this collection.          The :meth:`count` method is deprecated and **not** supported in a         transaction. Please use :meth:`count_documents` or         :meth:`estimated_document_count` instead.          All optional count parameters should be passed as keyword arguments         to this method. Valid options include:            - `skip` (int): The number of matching documents to skip before             returning results.           - `limit` (int): The maximum number of documents to count. A limit             of 0 (the default) is equivalent to setting no limit.           - `maxTimeMS` (int): The maximum amount of time to allow the count             command to run, in milliseconds.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.           - `hint` (string or list of tuples): The index to use. Specify either             the index name as a string or the index specification as a list of             tuples (e.g. [('a', pymongo.ASCENDING), ('b', pymongo.ASCENDING)]).          The :meth:`count` method obeys the :attr:`read_preference` of         this :class:`Collection`.          .. note:: When migrating from :meth:`count` to :meth:`count_documents`            the following query operators must be replaced:             +-------------+-------------------------------------+            | Operator    | Replacement                         |            +=============+=====================================+            | $where      | `$expr`_                            |            +-------------+-------------------------------------+            | $near       | `$geoWithin`_ with `$center`_       |            +-------------+-------------------------------------+            | $nearSphere | `$geoWithin`_ with `$centerSphere`_ |            +-------------+-------------------------------------+             $expr requires MongoDB 3.6+          :Parameters:           - `filter` (optional): A query document that selects which documents             to count in the collection.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): See list of options above.          .. versionchanged:: 3.7            Deprecated.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Support the `collation` option.          .. _$expr: https://docs.mongodb.com/manual/reference/operator/query/expr/         .. _$geoWithin: https://docs.mongodb.com/manual/reference/operator/query/geoWithin/         .. _$center: https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center         .. _$centerSphere: https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere
Create one or more indexes on this collection.            >>> from pymongo import IndexModel, ASCENDING, DESCENDING           >>> index1 = IndexModel([("hello", DESCENDING),           ...                      ("world", ASCENDING)], name="hello_world")           >>> index2 = IndexModel([("goodbye", DESCENDING)])           >>> db.test.create_indexes([index1, index2])           ["hello_world", "goodbye_-1"]          :Parameters:           - `indexes`: A list of :class:`~pymongo.operations.IndexModel`             instances.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): optional arguments to the createIndexes             command (like maxTimeMS) can be passed as keyword arguments.          .. note:: `create_indexes` uses the `createIndexes`_ command            introduced in MongoDB **2.6** and cannot be used with earlier            versions.          .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of            this collection is automatically applied to this operation when using            MongoDB >= 3.4.          .. versionchanged:: 3.6            Added ``session`` parameter. Added support for arbitrary keyword            arguments.          .. versionchanged:: 3.4            Apply this collection's write concern automatically to this operation            when connected to MongoDB >= 3.4.         .. versionadded:: 3.0          .. _createIndexes: https://docs.mongodb.com/manual/reference/command/createIndexes/
Internal create index helper.          :Parameters:           - `keys`: a list of tuples [(key, type), (key, type), ...]           - `index_options`: a dict of index options.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.
Creates an index on this collection.          Takes either a single key or a list of (key, direction) pairs.         The key(s) must be an instance of :class:`basestring`         (:class:`str` in python 3), and the direction(s) must be one of         (:data:`~pymongo.ASCENDING`, :data:`~pymongo.DESCENDING`,         :data:`~pymongo.GEO2D`, :data:`~pymongo.GEOHAYSTACK`,         :data:`~pymongo.GEOSPHERE`, :data:`~pymongo.HASHED`,         :data:`~pymongo.TEXT`).          To create a single key ascending index on the key ``'mike'`` we just         use a string argument::            >>> my_collection.create_index("mike")          For a compound index on ``'mike'`` descending and ``'eliot'``         ascending we need to use a list of tuples::            >>> my_collection.create_index([("mike", pymongo.DESCENDING),           ...                             ("eliot", pymongo.ASCENDING)])          All optional index creation parameters should be passed as         keyword arguments to this method. For example::            >>> my_collection.create_index([("mike", pymongo.DESCENDING)],           ...                            background=True)          Valid options include, but are not limited to:            - `name`: custom name to use for this index - if none is             given, a name will be generated.           - `unique`: if ``True`` creates a uniqueness constraint on the index.           - `background`: if ``True`` this index should be created in the             background.           - `sparse`: if ``True``, omit from the index any documents that lack             the indexed field.           - `bucketSize`: for use with geoHaystack indexes.             Number of documents to group together within a certain proximity             to a given longitude and latitude.           - `min`: minimum value for keys in a :data:`~pymongo.GEO2D`             index.           - `max`: maximum value for keys in a :data:`~pymongo.GEO2D`             index.           - `expireAfterSeconds`: <int> Used to create an expiring (TTL)             collection. MongoDB will automatically delete documents from             this collection after <int> seconds. The indexed field must             be a UTC datetime or the data will not expire.           - `partialFilterExpression`: A document that specifies a filter for             a partial index.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.          See the MongoDB documentation for a full list of supported options by         server version.          .. warning:: `dropDups` is not supported by MongoDB 3.0 or newer. The           option is silently ignored by the server and unique index builds           using the option will fail if a duplicate value is detected.          .. note:: `partialFilterExpression` requires server version **>= 3.2**          .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of            this collection is automatically applied to this operation when using            MongoDB >= 3.4.          :Parameters:           - `keys`: a single key or a list of (key, direction)             pairs specifying the index to create           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): any additional index creation             options (see the above list) should be passed as keyword             arguments          .. versionchanged:: 3.6            Added ``session`` parameter. Added support for passing maxTimeMS            in kwargs.         .. versionchanged:: 3.4            Apply this collection's write concern automatically to this operation            when connected to MongoDB >= 3.4. Support the `collation` option.         .. versionchanged:: 3.2             Added partialFilterExpression to support partial indexes.         .. versionchanged:: 3.0             Renamed `key_or_list` to `keys`. Removed the `cache_for` option.             :meth:`create_index` no longer caches index names. Removed support             for the drop_dups and bucket_size aliases.          .. mongodoc:: indexes
Rebuilds all indexes on this collection.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): optional arguments to the reIndex             command (like maxTimeMS) can be passed as keyword arguments.          .. warning:: reindex blocks all other operations (indexes            are built in the foreground) and will be slow for large            collections.          .. versionchanged:: 3.6            Added ``session`` parameter. Added support for arbitrary keyword            arguments.          .. versionchanged:: 3.4            Apply this collection's write concern automatically to this operation            when connected to MongoDB >= 3.4.          .. versionchanged:: 3.5            We no longer apply this collection's write concern to this operation.            MongoDB 3.4 silently ignored the write concern. MongoDB 3.6+ returns            an error if we include the write concern.
Get a cursor over the index documents for this collection.            >>> for index in db.test.list_indexes():           ...     print(index)           ...           SON([(u'v', 1), (u'key', SON([(u'_id', 1)])),                (u'name', u'_id_'), (u'ns', u'test.test')])          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          :Returns:           An instance of :class:`~pymongo.command_cursor.CommandCursor`.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionadded:: 3.0
Get information on this collection's indexes.          Returns a dictionary where the keys are index names (as         returned by create_index()) and the values are dictionaries         containing information about each index. The dictionary is         guaranteed to contain at least a single key, ``"key"`` which         is a list of (key, direction) pairs specifying the index (as         passed to create_index()). It will also contain any other         metadata about the indexes, except for the ``"ns"`` and         ``"name"`` keys, which are cleaned. Example output might look         like this:          >>> db.test.create_index("x", unique=True)         u'x_1'         >>> db.test.index_information()         {u'_id_': {u'key': [(u'_id', 1)]},          u'x_1': {u'unique': True, u'key': [(u'x', 1)]}}          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Get the options set on this collection.          Returns a dictionary of options and their values - see         :meth:`~pymongo.database.Database.create_collection` for more         information on the possible options. Returns an empty         dictionary if the collection has not been created yet.          :Parameters:           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.          .. versionchanged:: 3.6            Added ``session`` parameter.
Perform an aggregation using the aggregation framework on this         collection.          All optional `aggregate command`_ parameters should be passed as         keyword arguments to this method. Valid options include, but are not         limited to:            - `allowDiskUse` (bool): Enables writing to temporary files. When set             to True, aggregation stages can write data to the _tmp subdirectory             of the --dbpath directory. The default is False.           - `maxTimeMS` (int): The maximum amount of time to allow the operation             to run in milliseconds.           - `batchSize` (int): The maximum number of documents to return per             batch. Ignored if the connected mongod or mongos does not support             returning aggregate results using a cursor, or `useCursor` is             ``False``.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.           - `useCursor` (bool): Deprecated. Will be removed in PyMongo 4.0.          The :meth:`aggregate` method obeys the :attr:`read_preference` of this         :class:`Collection`. Please note that using the ``$out`` pipeline stage         requires a read preference of         :attr:`~pymongo.read_preferences.ReadPreference.PRIMARY` (the default).         The server will raise an error if the ``$out`` pipeline stage is used         with any other read preference.          .. note:: This method does not support the 'explain' option. Please            use :meth:`~pymongo.database.Database.command` instead. An            example is included in the :ref:`aggregate-examples` documentation.          .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of            this collection is automatically applied to this operation when using            MongoDB >= 3.4.          :Parameters:           - `pipeline`: a list of aggregation pipeline stages           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): See list of options above.          :Returns:           A :class:`~pymongo.command_cursor.CommandCursor` over the result           set.          .. versionchanged:: 3.9            Apply this collection's read concern to pipelines containing the            `$out` stage when connected to MongoDB >= 4.2.         .. versionchanged:: 3.6            Added the `session` parameter. Added the `maxAwaitTimeMS` option.            Deprecated the `useCursor` option.         .. versionchanged:: 3.4            Apply this collection's write concern automatically to this operation            when connected to MongoDB >= 3.4. Support the `collation` option.         .. versionchanged:: 3.0            The :meth:`aggregate` method always returns a CommandCursor. The            pipeline argument must be a list.         .. versionchanged:: 2.7            When the cursor option is used, return            :class:`~pymongo.command_cursor.CommandCursor` instead of            :class:`~pymongo.cursor.Cursor`.         .. versionchanged:: 2.6            Added cursor support.         .. versionadded:: 2.3          .. seealso:: :doc:`/examples/aggregation`          .. _aggregate command:             https://docs.mongodb.com/manual/reference/command/aggregate
Perform an aggregation and retrieve batches of raw BSON.          Similar to the :meth:`aggregate` method but returns a         :class:`~pymongo.cursor.RawBatchCursor`.          This example demonstrates how to work with raw batches, but in practice         raw batches should be passed to an external library that can decode         BSON into another data type, rather than used with PyMongo's         :mod:`bson` module.            >>> import bson           >>> cursor = db.test.aggregate_raw_batches([           ...     {'$project': {'x': {'$multiply': [2, '$x']}}}])           >>> for batch in cursor:           ...     print(bson.decode_all(batch))          .. note:: aggregate_raw_batches does not support sessions.          .. versionadded:: 3.6
Perform a query similar to an SQL *group by* operation.          **DEPRECATED** - The group command was deprecated in MongoDB 3.4. The         :meth:`~group` method is deprecated and will be removed in PyMongo 4.0.         Use :meth:`~aggregate` with the `$group` stage or :meth:`~map_reduce`         instead.          .. versionchanged:: 3.5            Deprecated the group method.         .. versionchanged:: 3.4            Added the `collation` option.         .. versionchanged:: 2.2            Removed deprecated argument: command
Rename this collection.          If operating in auth mode, client must be authorized as an         admin to perform this operation. Raises :class:`TypeError` if         `new_name` is not an instance of :class:`basestring`         (:class:`str` in python 3). Raises :class:`~pymongo.errors.InvalidName`         if `new_name` is not a valid collection name.          :Parameters:           - `new_name`: new name for this collection           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): additional arguments to the rename command             may be passed as keyword arguments to this helper method             (i.e. ``dropTarget=True``)          .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of            this collection is automatically applied to this operation when using            MongoDB >= 3.4.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Apply this collection's write concern automatically to this operation            when connected to MongoDB >= 3.4.
Get a list of distinct values for `key` among all documents         in this collection.          Raises :class:`TypeError` if `key` is not an instance of         :class:`basestring` (:class:`str` in python 3).          All optional distinct parameters should be passed as keyword arguments         to this method. Valid options include:            - `maxTimeMS` (int): The maximum amount of time to allow the count             command to run, in milliseconds.           - `collation` (optional): An instance of             :class:`~pymongo.collation.Collation`. This option is only supported             on MongoDB 3.4 and above.          The :meth:`distinct` method obeys the :attr:`read_preference` of         this :class:`Collection`.          :Parameters:           - `key`: name of the field for which we want to get the distinct             values           - `filter` (optional): A query document that specifies the documents             from which to retrieve the distinct values.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): See list of options above.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Support the `collation` option.
Internal mapReduce helper.
Perform a map/reduce operation on this collection.          If `full_response` is ``False`` (default) returns a         :class:`~pymongo.collection.Collection` instance containing         the results of the operation. Otherwise, returns the full         response from the server to the `map reduce command`_.          :Parameters:           - `map`: map function (as a JavaScript string)           - `reduce`: reduce function (as a JavaScript string)           - `out`: output collection name or `out object` (dict). See             the `map reduce command`_ documentation for available options.             Note: `out` options are order sensitive. :class:`~bson.son.SON`             can be used to specify multiple options.             e.g. SON([('replace', <collection name>), ('db', <database name>)])           - `full_response` (optional): if ``True``, return full response to             this command - otherwise just return the result collection           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): additional arguments to the             `map reduce command`_ may be passed as keyword arguments to this             helper method, e.g.::              >>> db.test.map_reduce(map, reduce, "myresults", limit=2)          .. note:: The :meth:`map_reduce` method does **not** obey the            :attr:`read_preference` of this :class:`Collection`. To run            mapReduce on a secondary use the :meth:`inline_map_reduce` method            instead.          .. note:: The :attr:`~pymongo.collection.Collection.write_concern` of            this collection is automatically applied to this operation (if the            output is not inline) when using MongoDB >= 3.4.          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Apply this collection's write concern automatically to this operation            when connected to MongoDB >= 3.4.          .. seealso:: :doc:`/examples/aggregation`          .. versionchanged:: 3.4            Added the `collation` option.         .. versionchanged:: 2.2            Removed deprecated arguments: merge_output and reduce_output          .. _map reduce command: http://docs.mongodb.org/manual/reference/command/mapReduce/          .. mongodoc:: mapreduce
Perform an inline map/reduce operation on this collection.          Perform the map/reduce operation on the server in RAM. A result         collection is not created. The result set is returned as a list         of documents.          If `full_response` is ``False`` (default) returns the         result documents in a list. Otherwise, returns the full         response from the server to the `map reduce command`_.          The :meth:`inline_map_reduce` method obeys the :attr:`read_preference`         of this :class:`Collection`.          :Parameters:           - `map`: map function (as a JavaScript string)           - `reduce`: reduce function (as a JavaScript string)           - `full_response` (optional): if ``True``, return full response to             this command - otherwise just return the result collection           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): additional arguments to the             `map reduce command`_ may be passed as keyword arguments to this             helper method, e.g.::              >>> db.test.inline_map_reduce(map, reduce, limit=2)          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.4            Added the `collation` option.
Internal findAndModify helper.
Finds a single document and deletes it, returning the document.            >>> db.test.count_documents({'x': 1})           2           >>> db.test.find_one_and_delete({'x': 1})           {u'x': 1, u'_id': ObjectId('54f4e12bfba5220aa4d6dee8')}           >>> db.test.count_documents({'x': 1})           1          If multiple documents match *filter*, a *sort* can be applied.            >>> for doc in db.test.find({'x': 1}):           ...     print(doc)           ...           {u'x': 1, u'_id': 0}           {u'x': 1, u'_id': 1}           {u'x': 1, u'_id': 2}           >>> db.test.find_one_and_delete(           ...     {'x': 1}, sort=[('_id', pymongo.DESCENDING)])           {u'x': 1, u'_id': 2}          The *projection* option can be used to limit the fields returned.            >>> db.test.find_one_and_delete({'x': 1}, projection={'_id': False})           {u'x': 1}          :Parameters:           - `filter`: A query that matches the document to delete.           - `projection` (optional): a list of field names that should be             returned in the result document or a mapping specifying the fields             to include or exclude. If `projection` is a list "_id" will             always be returned. Use a mapping to exclude fields from             the result (e.g. projection={'_id': False}).           - `sort` (optional): a list of (key, direction) pairs             specifying the sort order for the query. If multiple documents             match the query, they are sorted and the first is deleted.           - `session` (optional): a             :class:`~pymongo.client_session.ClientSession`.           - `**kwargs` (optional): additional command arguments can be passed             as keyword arguments (for example maxTimeMS can be used with             recent server versions).          .. versionchanged:: 3.6            Added ``session`` parameter.          .. versionchanged:: 3.2            Respects write concern.          .. warning:: Starting in PyMongo 3.2, this command uses the            :class:`~pymongo.write_concern.WriteConcern` of this            :class:`~pymongo.collection.Collection` when connected to MongoDB >=            3.2. Note that using an elevated write concern with this command may            be slower compared to using the default write concern.          .. versionchanged:: 3.4            Added the `collation` option.         .. versionadded:: 3.0
Remove a document(s) from this collection.          **DEPRECATED** - Use :meth:`delete_one` or :meth:`delete_many` instead.          .. versionchanged:: 3.0            Removed the `safe` parameter. Pass ``w=0`` for unacknowledged write            operations.
Update and return an object.          **DEPRECATED** - Use :meth:`find_one_and_delete`,         :meth:`find_one_and_replace`, or :meth:`find_one_and_update` instead.
Return the full aggregation pipeline for this ChangeStream.
Run the full aggregation pipeline for this ChangeStream and return         the corresponding CommandCursor.
Reestablish this change stream after a resumable error.
Advance the cursor.          This method blocks until the next change document is returned or an         unrecoverable error is raised. This method is used when iterating over         all changes in the cursor. For example::              try:                 with db.collection.watch(                         [{'$match': {'operationType': 'insert'}}]) as stream:                     for insert_change in stream:                         print(insert_change)             except pymongo.errors.PyMongoError:                 # The ChangeStream encountered an unrecoverable error or the                 # resume attempt failed to recreate the cursor.                 logging.error('...')          Raises :exc:`StopIteration` if this ChangeStream is closed.
Advance the cursor without blocking indefinitely.          This method returns the next change document without waiting         indefinitely for the next change. For example::              with db.collection.watch() as stream:                 while stream.alive:                     change = stream.try_next()                     if change is not None:                         print(change)                     elif stream.alive:                         # We end up here when there are no recent changes.                         # Sleep for a while to avoid flooding the server with                         # getMore requests when no changes are available.                         time.sleep(10)          If no change document is cached locally then this method runs a single         getMore command. If the getMore yields any documents, the next         document is returned, otherwise, if the getMore returns no documents         (because there have been no changes) then ``None`` is returned.          :Returns:           The next change document or ``None`` when no document is available           after running a single getMore or when the cursor is closed.          .. versionadded:: 3.8
Validates that 'value' is an integer (or basestring representation).
Validates that 'value' is an integer or string.
Validate the document_class option.
Validate the type_registry option.
Validate the type of method arguments that expect a MongoDB document.
Validate the driver keyword arg.
Validates that 'value' is a callable.
Read only access to the read preference of this instance or session.
Get data files
Create a command class with the given optional wrappers.     Parameters     ----------     develop_wrapper: list(str), optional         The cmdclass names to run before running other commands     distribute_wrappers: list(str), optional         The cmdclass names to run before running other commands     data_dirs: list(str), optional.         The directories containing static data.
Echo a command before running it.  Defaults to repo as cwd
Test whether the target file/directory is stale based on the source        file/directory.
Return a Command that combines several commands.
Compare the newest/oldest mtime for all files in a directory.     Cutoff should be another mtime to be compared against. If an mtime that is     newer/older than the cutoff is found it will return True.     E.g. if newest=True, and a file in path is newer than the cutoff, it will     return True.
Gets the newest/oldest mtime for all files in a directory.
Return a Command for managing an node_modules installation.     Note: The command is skipped if the `--skip-yarn` flag is used.          Parameters     ----------     path: str, optional         The base path of the node package.  Defaults to the repo root.     build_dir: str, optional         The target build directory.  If this and source_dir are given,         the JavaScript will only be build if necessary.     source_dir: str, optional         The source code directory.     build_cmd: str, optional         The yarn command to build assets to the build_dir.
Return a Command for running gradle scripts.      Parameters     ----------     path: str, optional         The base path of the node package.  Defaults to the repo root.     cmd: str, optional         The command to run with gradlew.
Wrap a setup command     Parameters     ----------     cmds: list(str)         The names of the other commands to run prior to the command.     strict: boolean, optional         Wether to raise errors when a pre-command fails.
Read the initialization status of Vault.          Supported methods:             GET: /sys/init. Produces: 200 application/json          :return: The JSON response of the request.         :rtype: dict
Initialize a new Vault.          The Vault must not have been previously initialized. The recovery options, as well as the stored shares option,         are only available when using Vault HSM.          Supported methods:             PUT: /sys/init. Produces: 200 application/json          :param secret_shares: The number of shares to split the master key into.         :type secret_shares: int         :param secret_threshold: Specifies the number of shares required to reconstruct the master key. This must be             less than or equal secret_shares. If using Vault HSM with auto-unsealing, this value must be the same as             secret_shares.         :type secret_threshold: int         :param pgp_keys: List of PGP public keys used to encrypt the output unseal keys.             Ordering is preserved. The keys must be base64-encoded from their original binary representation.             The size of this array must be the same as secret_shares.         :type pgp_keys: list         :param root_token_pgp_key: Specifies a PGP public key used to encrypt the initial root token. The             key must be base64-encoded from its original binary representation.         :type root_token_pgp_key: str | unicode         :param stored_shares: <enterprise only> Specifies the number of shares that should be encrypted by the HSM and             stored for auto-unsealing. Currently must be the same as secret_shares.         :type stored_shares: int         :param recovery_shares: <enterprise only> Specifies the number of shares to split the recovery key into.         :type recovery_shares: int         :param recovery_threshold: <enterprise only> Specifies the number of shares required to reconstruct the recovery             key. This must be less than or equal to recovery_shares.         :type recovery_threshold: int         :param recovery_pgp_keys: <enterprise only> Specifies an array of PGP public keys used to encrypt the output             recovery keys. Ordering is preserved. The keys must be base64-encoded from their original binary             representation. The size of this array must be the same as recovery_shares.         :type recovery_pgp_keys: list         :return: The JSON response of the request.         :rtype: dict
Return a list of key names at the specified location.          Folders are suffixed with /. The input must be a folder; list on a file will not return a value. Note that no         policy-based filtering is performed on keys; do not encode sensitive information in key names. The values         themselves are not accessible via this command.          Supported methods:             LIST: /{mount_point}/{path}. Produces: 200 application/json          :param path: Specifies the path of the secrets to list.             This is specified as part of the URL.         :type path: str | unicode         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the list_secrets request.         :rtype: dict
Store a secret at the specified location.          If the value does not yet exist, the calling token must have an ACL policy granting the create capability.         If the value already exists, the calling token must have an ACL policy granting the update capability.          Supported methods:             POST: /{mount_point}/{path}. Produces: 204 (empty body)             PUT: /{mount_point}/{path}. Produces: 204 (empty body)          :param path: Specifies the path of the secrets to create/update. This is specified as part of the URL.         :type path: str | unicode         :param secret: Specifies keys, paired with associated values, to be held at the given location. Multiple             key/value pairs can be specified, and all will be returned on a read operation. A key called ttl will             trigger some special behavior. See the Vault KV secrets engine documentation for details.         :type secret: dict         :param method: Optional parameter to explicitly request a POST (create) or PUT (update) request to the selected             kv secret engine. If no argument is provided for this parameter, hvac attempts to intelligently determine             which method is appropriate.         :type method: str | unicode         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The response of the create_or_update_secret request.         :rtype: requests.Response
Delete the secret at the specified location.          Supported methods:             DELETE: /{mount_point}/{path}. Produces: 204 (empty body)           :param path: Specifies the path of the secret to delete.             This is specified as part of the URL.         :type path: str | unicode         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The response of the delete_secret request.         :rtype: requests.Response
Configure the RADIUS auth method.          Supported methods:             POST: /auth/{mount_point}/config. Produces: 204 (empty body)          :param host: The RADIUS server to connect to. Examples: radius.myorg.com, 127.0.0.1         :type host: str | unicode         :param secret: The RADIUS shared secret.         :type secret: str | unicode         :param port: The UDP port where the RADIUS server is listening on. Defaults is 1812.         :type port: int         :param unregistered_user_policies: A comma-separated list of policies to be granted to unregistered users.         :type unregistered_user_policies: list         :param dial_timeout: Number of second to wait for a backend connection before timing out. Default is 10.         :type dial_timeout: int         :param nas_port: The NAS-Port attribute of the RADIUS request. Defaults is 10.         :type nas_port: int         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the configure request.         :rtype: requests.Response
Configure the root IAM credentials to communicate with AWS.          There are multiple ways to pass root IAM credentials to the Vault server, specified below with the highest         precedence first. If credentials already exist, this will overwrite them.          The official AWS SDK is used for sourcing credentials from env vars, shared files, or IAM/ECS instances.              * Static credentials provided to the API as a payload             * Credentials in the AWS_ACCESS_KEY, AWS_SECRET_KEY, and AWS_REGION environment variables on the server             * Shared credentials files             * Assigned IAM role or ECS task role credentials          At present, this endpoint does not confirm that the provided AWS credentials are valid AWS credentials with         proper permissions.          Supported methods:             POST: /{mount_point}/config/root. Produces: 204 (empty body)          :param access_key: Specifies the AWS access key ID.         :type access_key: str | unicode         :param secret_key: Specifies the AWS secret access key.         :type secret_key: str | unicode         :param region: Specifies the AWS region. If not set it will use the AWS_REGION env var, AWS_DEFAULT_REGION env             var, or us-east-1 in that order.         :type region: str | unicode         :param iam_endpoint: Specifies a custom HTTP IAM endpoint to use.         :type iam_endpoint: str | unicode         :param sts_endpoint: Specifies a custom HTTP STS endpoint to use.         :type sts_endpoint: str | unicode         :param max_retries: Number of max retries the client should use for recoverable errors. The default (-1) falls             back to the AWS SDK's default behavior.         :type max_retries: int         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Rotate static root IAM credentials.          When you have configured Vault with static credentials, you can use this endpoint to have Vault rotate the         access key it used. Note that, due to AWS eventual consistency, after calling this endpoint, subsequent calls         from Vault to AWS may fail for a few seconds until AWS becomes consistent again.          In order to call this endpoint, Vault's AWS access key MUST be the only access key on the IAM user; otherwise,         generation of a new access key will fail. Once this method is called, Vault will now be the only entity that         knows the AWS secret key is used to access AWS.          Supported methods:             POST: /{mount_point}/config/rotate-root. Produces: 200 application/json          :return: The JSON response of the request.         :rtype: dict
Configure lease settings for the AWS secrets engine.          It is optional, as there are default values for lease and lease_max.          Supported methods:             POST: /{mount_point}/config/lease. Produces: 204 (empty body)          :param lease: Specifies the lease value provided as a string duration with time suffix. "h" (hour) is the             largest suffix.         :type lease: str | unicode         :param lease_max: Specifies the maximum lease value provided as a string duration with time suffix. "h" (hour)             is the largest suffix.         :type lease_max: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Read the current lease settings for the AWS secrets engine.          Supported methods:             GET: /{mount_point}/config/lease. Produces: 200 application/json          :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Create or update the role with the given name.          If a role with the name does not exist, it will be created. If the role exists, it will be updated with the new         attributes.          Supported methods:             POST: /{mount_point}/roles/{name}. Produces: 204 (empty body)          :param name: Specifies the name of the role to create. This is part of the request URL.         :type name: str | unicode         :param credential_type: Specifies the type of credential to be used when retrieving credentials from the role.             Must be one of iam_user, assumed_role, or federation_token.         :type credential_type: str | unicode         :param policy_document: The IAM policy document for the role. The behavior depends on the credential type. With             iam_user, the policy document will be attached to the IAM user generated and augment the permissions the IAM             user has. With assumed_role and federation_token, the policy document will act as a filter on what the             credentials can do.         :type policy_document: dict | str | unicode         :param default_sts_ttl: The default TTL for STS credentials. When a TTL is not specified when STS credentials             are requested, and a default TTL is specified on the role, then this default TTL will be used. Valid only             when credential_type is one of assumed_role or federation_token.         :type default_sts_ttl: str | unicode         :param max_sts_ttl: The max allowed TTL for STS credentials (credentials TTL are capped to max_sts_ttl). Valid             only when credential_type is one of assumed_role or federation_token.         :type max_sts_ttl: str | unicode         :param role_arns: Specifies the ARNs of the AWS roles this Vault role is allowed to assume. Required when             credential_type is assumed_role and prohibited otherwise. This is a comma-separated string or JSON array.             String types supported for Vault legacy parameters.         :type role_arns: list | str | unicode         :param policy_arns: Specifies the ARNs of the AWS managed policies to be attached to IAM users when they are             requested. Valid only when credential_type is iam_user. When credential_type is iam_user, at least one of             policy_arns or policy_document must be specified. This is a comma-separated string or JSON array.         :type policy_arns: list         :param legacy_params: Flag to send legacy (Vault versions < 0.11.0) parameters in the request. When this is set             to True, policy_document and policy_arns are the only parameters used from this method.         :type legacy_params: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Delete an existing role by the given name.          If the role does not exist, a 404 is returned.          Supported methods:             DELETE: /{mount_point}/roles/{name}. Produces: 204 (empty body)          :param name: the name of the role to delete. This             is part of the request URL.         :type name: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Generates credential based on the named role.          This role must be created before queried.          The /aws/creds and /aws/sts endpoints are almost identical. The exception is when retrieving credentials for a         role that was specified with the legacy arn or policy parameter. In this case, credentials retrieved through         /aws/sts must be of either the assumed_role or federation_token types, and credentials retrieved through         /aws/creds must be of the iam_user type.          :param name: Specifies the name of the role to generate credentials against. This is part of the request URL.         :type name: str | unicode         :param role_arn: The ARN of the role to assume if credential_type on the Vault role is assumed_role. Must match             one of the allowed role ARNs in the Vault role. Optional if the Vault role only allows a single AWS role             ARN; required otherwise.         :type role_arn: str | unicode         :param ttl: Specifies the TTL for the use of the STS token. This is specified as a string with a duration             suffix. Valid only when credential_type is assumed_role or federation_token. When not specified, the default             sts_ttl set for the role will be used. If that is also not set, then the default value of 3600s will be             used. AWS places limits on the maximum TTL allowed. See the AWS documentation on the DurationSeconds             parameter for AssumeRole (for assumed_role credential types) and GetFederationToken (for federation_token             credential types) for more details.         :type ttl: str | unicode         :param endpoint: Supported endpoints:             GET: /{mount_point}/creds/{name}. Produces: 200 application/json             GET: /{mount_point}/sts/{name}. Produces: 200 application/json         :type endpoint: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Configure the credentials required for the plugin to perform API calls to Azure.          These credentials will be used to query roles and create/delete service principals. Environment variables will         override any parameters set in the config.          Supported methods:             POST: /{mount_point}/config. Produces: 204 (empty body)           :param subscription_id: The subscription id for the Azure Active Directory         :type subscription_id: str | unicode         :param tenant_id: The tenant id for the Azure Active Directory.         :type tenant_id: str | unicode         :param client_id: The OAuth2 client id to connect to Azure.         :type client_id: str | unicode         :param client_secret: The OAuth2 client secret to connect to Azure.         :type client_secret: str | unicode         :param environment: The Azure environment. If not specified, Vault will use Azure Public Cloud.         :type environment: str | unicode         :param mount_point: The OAuth2 client secret to connect to Azure.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Delete the stored Azure configuration and credentials.          Supported methods:             DELETE: /auth/{mount_point}/config. Produces: 204 (empty body)           :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Create or update a Vault role.          The provided Azure roles must exist for this call to succeed. See the Azure secrets roles docs for more         information about roles.          Supported methods:             POST: /{mount_point}/roles/{name}. Produces: 204 (empty body)           :param name: Name of the role.         :type name: str | unicode         :param azure_roles:  List of Azure roles to be assigned to the generated service principal.         :type azure_roles: list(dict)         :param ttl: Specifies the default TTL for service principals generated using this role. Accepts time suffixed             strings ("1h") or an integer number of seconds. Defaults to the system/engine default TTL time.         :type ttl: str | unicode         :param max_ttl: Specifies the maximum TTL for service principals generated using this role. Accepts time             suffixed strings ("1h") or an integer number of seconds. Defaults to the system/engine max TTL time.         :type max_ttl: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Configure the LDAP auth method.          Supported methods:             POST: /auth/{mount_point}/config. Produces: 204 (empty body)          :param user_dn: Base DN under which to perform user search. Example: ou=Users,dc=example,dc=com         :type user_dn: str | unicode         :param group_dn: LDAP search base to use for group membership search. This can be the root containing either             groups or users. Example: ou=Groups,dc=example,dc=com         :type group_dn: str | unicode         :param url: The LDAP server to connect to. Examples: ldap://ldap.myorg.com, ldaps://ldap.myorg.com:636.             Multiple URLs can be specified with commas, e.g. ldap://ldap.myorg.com,ldap://ldap2.myorg.com; these will be             tried in-order.         :type url: str | unicode         :param case_sensitive_names: If set, user and group names assigned to policies within the backend will be case             sensitive. Otherwise, names will be normalized to lower case. Case will still be preserved when sending the             username to the LDAP server at login time; this is only for matching local user/group definitions.         :type case_sensitive_names: bool         :param starttls: If true, issues a StartTLS command after establishing an unencrypted connection.         :type starttls: bool         :param tls_min_version: Minimum TLS version to use. Accepted values are tls10, tls11 or tls12.         :type tls_min_version: str | unicode         :param tls_max_version: Maximum TLS version to use. Accepted values are tls10, tls11 or tls12.         :type tls_max_version: str | unicode         :param insecure_tls: If true, skips LDAP server SSL certificate verification - insecure, use with caution!         :type insecure_tls: bool         :param certificate: CA certificate to use when verifying LDAP server certificate, must be x509 PEM encoded.         :type certificate: str | unicode         :param bind_dn: Distinguished name of object to bind when performing user search. Example:             cn=vault,ou=Users,dc=example,dc=com         :type bind_dn: str | unicode         :param bind_pass:  Password to use along with binddn when performing user search.         :type bind_pass: str | unicode         :param user_attr: Attribute on user attribute object matching the username passed when authenticating. Examples:             sAMAccountName, cn, uid         :type user_attr: str | unicode         :param discover_dn: Use anonymous bind to discover the bind DN of a user.         :type discover_dn: bool         :param deny_null_bind: This option prevents users from bypassing authentication when providing an empty password.         :type deny_null_bind: bool         :param upn_domain: The userPrincipalDomain used to construct the UPN string for the authenticating user. The             constructed UPN will appear as [username]@UPNDomain. Example: example.com, which will cause vault to bind as             username@example.com.         :type upn_domain: str | unicode         :param group_filter: Go template used when constructing the group membership query. The template can access the             following context variables: [UserDN, Username]. The default is             `(|(memberUid={{.Username}})(member={{.UserDN}})(uniqueMember={{.UserDN}}))`, which is compatible with several             common directory schemas. To support nested group resolution for Active Directory, instead use the following             query: (&(objectClass=group)(member:1.2.840.113556.1.4.1941:={{.UserDN}})).         :type group_filter: str | unicode         :param group_attr: LDAP attribute to follow on objects returned by groupfilter in order to enumerate user group             membership. Examples: for groupfilter queries returning group objects, use: cn. For queries returning user             objects, use: memberOf. The default is cn.         :type group_attr: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the configure request.         :rtype: requests.Response
Create or update LDAP group policies.          Supported methods:             POST: /auth/{mount_point}/groups/{name}. Produces: 204 (empty body)           :param name: The name of the LDAP group         :type name: str | unicode         :param policies: List of policies associated with the group. This parameter is transformed to a comma-delimited             string before being passed to Vault.         :type policies: list         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the create_or_update_group request.         :rtype: requests.Response
Create or update LDAP users policies and group associations.          Supported methods:             POST: /auth/{mount_point}/users/{username}. Produces: 204 (empty body)           :param username: The username of the LDAP user         :type username: str | unicode         :param policies: List of policies associated with the user. This parameter is transformed to a comma-delimited             string before being passed to Vault.         :type policies: str | unicode         :param groups: List of groups associated with the user. This parameter is transformed to a comma-delimited             string before being passed to Vault.         :type groups: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the create_or_update_user request.         :rtype: requests.Response
Return the original response inside the given wrapping token.          Unlike simply reading cubbyhole/response (which is deprecated), this endpoint provides additional validation         checks on the token, returns the original value on the wire rather than a JSON string representation of it, and         ensures that the response is properly audit-logged.          Supported methods:             POST: /sys/wrapping/unwrap. Produces: 200 application/json          :param token: Specifies the wrapping token ID. This is required if the client token is not the wrapping token.             Do not use the wrapping token in both locations.         :type token: str | unicode         :return: The JSON response of the request.         :rtype: dict
Configure backend level settings that are applied to every key in the key-value store.          Supported methods:             POST: /{mount_point}/config. Produces: 204 (empty body)           :param max_versions: The number of versions to keep per key. This value applies to all keys, but a key's             metadata setting can overwrite this value. Once a key has more than the configured allowed versions the             oldest version will be permanently deleted. Defaults to 10.         :type max_versions: int         :param cas_required: If true all keys will require the cas parameter to be set on all write requests.         :type cas_required: bool         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Retrieve the secret at the specified location.          Supported methods:             GET: /{mount_point}/data/{path}. Produces: 200 application/json           :param path: Specifies the path of the secret to read. This is specified as part of the URL.         :type path: str | unicode         :param version: Specifies the version to return. If not set the latest version is returned.         :type version: int         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Create a new version of a secret at the specified location.          If the value does not yet exist, the calling token must have an ACL policy granting the create capability. If         the value already exists, the calling token must have an ACL policy granting the update capability.          Supported methods:             POST: /{mount_point}/data/{path}. Produces: 200 application/json          :param path: Path         :type path: str | unicode         :param cas: Set the "cas" value to use a Check-And-Set operation. If not set the write will be allowed. If set             to 0 a write will only be allowed if the key doesn't exist. If the index is non-zero the write will only be             allowed if the key's current version matches the version specified in the cas parameter.         :type cas: int         :param secret: The contents of the "secret" dict will be stored and returned on read.         :type secret: dict         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Set or update data in the KV store without overwriting.          :param path: Path         :type path: str | unicode         :param secret: The contents of the "secret" dict will be stored and returned on read.         :type secret: dict         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the create_or_update_secret request.         :rtype: dict
Issue a soft delete of the specified versions of the secret.          This marks the versions as deleted and will stop them from being returned from reads,         but the underlying data will not be removed. A delete can be undone using the         undelete path.          Supported methods:             POST: /{mount_point}/delete/{path}. Produces: 204 (empty body)           :param path: Specifies the path of the secret to delete. This is specified as part of the URL.         :type path: str | unicode         :param versions: The versions to be deleted. The versioned data will not be deleted, but it will no longer be             returned in normal get requests.         :type versions: int         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Retrieve the metadata and versions for the secret at the specified path.          Supported methods:             GET: /{mount_point}/metadata/{path}. Produces: 200 application/json           :param path: Specifies the path of the secret to read. This is specified as part of the URL.         :type path: str | unicode         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Updates the max_versions of cas_required setting on an existing path.          Supported methods:             POST: /{mount_point}/metadata/{path}. Produces: 204 (empty body)           :param path: Path         :type path: str | unicode         :param max_versions: The number of versions to keep per key. If not set, the backend's configured max version is             used. Once a key has more than the configured allowed versions the oldest version will be permanently             deleted.         :type max_versions: int         :param cas_required: If true the key will require the cas parameter to be set on all write requests. If false,             the backend's configuration will be used.         :type cas_required: bool         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Configure the credentials required for the GCP auth method to perform API calls to Google Cloud.          These credentials will be used to query the status of IAM entities and get service account or other Google         public certificates to confirm signed JWTs passed in during login.          Supported methods:             POST: /auth/{mount_point}/config. Produces: 204 (empty body)           :param credentials: A JSON string containing the contents of a GCP credentials file. The credentials file must             have the following permissions: `iam.serviceAccounts.get`, `iam.serviceAccountKeys.get`.             If this value is empty, Vault will try to use Application Default Credentials from the machine on which the             Vault server is running. The project must have the iam.googleapis.com API enabled.         :type credentials: str | unicode         :param google_certs_endpoint: The Google OAuth2 endpoint from which to obtain public certificates. This is used             for testing and should generally not be set by end users.         :type google_certs_endpoint: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Register a role in the GCP auth method.          Role types have specific entities that can perform login operations against this endpoint. Constraints specific             to the role type must be set on the role. These are applied to the authenticated entities attempting to             login.          Supported methods:             POST: /auth/{mount_point}/role/{name}. Produces: 204 (empty body)           :param name: The name of the role.         :type name: str | unicode         :param role_type: The type of this role. Certain fields correspond to specific roles and will be rejected             otherwise.         :type role_type: str | unicode         :param project_id: The GCP project ID. Only entities belonging to this project can authenticate with this role.         :type project_id: str | unicode         :param ttl: The TTL period of tokens issued using this role. This can be specified as an integer number of             seconds or as a duration value like "5m".         :type ttl: str | unicode         :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role. This can be specified             as an integer number of seconds or as a duration value like "5m".         :type max_ttl: str | unicode         :param period: If set, indicates that the token generated using this role should never expire. The token should             be renewed within the duration specified by this value. At each renewal, the token's TTL will be set to the             value of this parameter. This can be specified as an integer number of seconds or as a duration value like             "5m".         :type period: str | unicode         :param policies: The list of policies to be set on tokens issued using this role.         :type policies: list         :param bound_service_accounts: <required for iam> A list of service account emails or IDs that login is             restricted  to. If set to `*`, all service accounts are allowed (role will still be bound by project). Will be             inferred from service account used to issue metadata token for GCE instances.         :type bound_service_accounts: list         :param max_jwt_exp: <iam only> The number of seconds past the time of authentication that the login param JWT             must expire within. For example, if a user attempts to login with a token that expires within an hour and             this is set to 15 minutes, Vault will return an error prompting the user to create a new signed JWT with a             shorter exp. The GCE metadata tokens currently do not allow the exp claim to be customized.         :type max_jwt_exp: str | unicode         :param allow_gce_inference: <iam only> A flag to determine if this role should allow GCE instances to             authenticate by inferring service accounts from the GCE identity metadata token.         :type allow_gce_inference: bool         :param bound_zones: <gce only> The list of zones that a GCE instance must belong to in order to be             authenticated. If bound_instance_groups is provided, it is assumed to be a zonal group and the group must             belong to this zone.         :type bound_zones: list         :param bound_regions: <gce only> The list of regions that a GCE instance must belong to in order to be             authenticated. If bound_instance_groups is provided, it is assumed to be a regional group and the group             must belong to this region. If bound_zones are provided, this attribute is ignored.         :type bound_regions: list         :param bound_instance_groups: <gce only> The instance groups that an authorized instance must belong to in             order to be authenticated. If specified, either bound_zones or bound_regions must be set too.         :type bound_instance_groups: list         :param bound_labels: <gce only> A list of GCP labels formatted as "key:value" strings that must be set on             authorized GCE instances. Because GCP labels are not currently ACL'd, we recommend that this be used in             conjunction with other restrictions.         :type bound_labels: list         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The data key from the JSON response of the request.         :rtype: requests.Response
Edit labels for an existing GCE role in the backend.          This allows you to add or remove labels (keys, values, or both) from the list of keys on the role.          Supported methods:             POST: /auth/{mount_point}/role/{name}/labels. Produces: 204 (empty body)           :param name: The name of an existing gce role. This will return an error if role is not a gce type role.         :type name: str | unicode         :param add: The list of key:value labels to add to the GCE role's bound labels.         :type add: list         :param remove: The list of label keys to remove from the role's bound labels. If any of the specified keys do             not exist, no error is returned (idempotent).         :type remove: list         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the edit_labels_on_gce_role request.         :rtype: requests.Response
Delete the previously registered role.          Supported methods:             DELETE: /auth/{mount_point}/role/{role}. Produces: 204 (empty body)           :param role: The name of the role to delete.         :type role: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Login to retrieve a Vault token via the GCP auth method.          This endpoint takes a signed JSON Web Token (JWT) and a role name for some entity. It verifies the JWT             signature with Google Cloud to authenticate that entity and then authorizes the entity for the given role.          Supported methods:             POST: /auth/{mount_point}/login. Produces: 200 application/json           :param role: The name of the role against which the login is being attempted.         :type role: str | unicode         :param jwt: A signed JSON web token         :type jwt: str | unicode         :param use_token: if True, uses the token in the response received from the auth request to set the "token"             attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.         :type use_token: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Configure the connection parameters for Okta.          This path honors the distinction between the create and update capabilities inside ACL policies.          Supported methods:             POST: /auth/{mount_point}/config. Produces: 204 (empty body)           :param org_name: Name of the organization to be used in the Okta API.         :type org_name: str | unicode         :param api_token: Okta API token. This is required to query Okta for user group membership. If this is not             supplied only locally configured groups will be enabled.         :type api_token: str | unicode         :param base_url:  If set, will be used as the base domain for API requests.  Examples are okta.com,             oktapreview.com, and okta-emea.com.         :type base_url: str | unicode         :param ttl: Duration after which authentication will be expired.         :type ttl: str | unicode         :param max_ttl: Maximum duration after which authentication will be expired.         :type max_ttl: str | unicode         :param bypass_okta_mfa: Whether to bypass an Okta MFA request. Useful if using one of Vault's built-in MFA             mechanisms, but this will also cause certain other statuses to be ignored, such as PASSWORD_EXPIRED.         :type bypass_okta_mfa: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Register a new user and maps a set of policies to it.          Supported methods:             POST: /auth/{mount_point}/users/{username}. Produces: 204 (empty body)          :param username: Name of the user.         :type username: str | unicode         :param groups: List or comma-separated string of groups associated with the user.         :type groups: list         :param policies: List or comma-separated string of policies associated with the user.         :type policies: list         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Read the properties of an existing username.          Supported methods:             GET: /auth/{mount_point}/users/{username}. Produces: 200 application/json          :param username: Username for this user.         :type username: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Delete an existing username from the method.          Supported methods:             DELETE: /auth/{mount_point}/users/{username}. Produces: 204 (empty body)          :param username: Username for this user.         :type username: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Register a new group and maps a set of policies to it.          Supported methods:             POST: /auth/{mount_point}/groups/{name}. Produces: 204 (empty body)          :param name: The name of the group.         :type name: str | unicode         :param policies: The list or comma-separated string of policies associated with the group.         :type policies: list         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Sets the adapter instance under the "_adapter" property in use by this class.          Also sets the adapter property for all implemented classes under this category.          :param adapter: New adapter instance to set for this class and all implemented classes under this category.         :type adapter: hvac.adapters.Adapter
Configure the connection parameters for GitHub.          This path honors the distinction between the create and update capabilities inside ACL policies.          Supported methods:             POST: /auth/{mount_point}/config. Produces: 204 (empty body)           :param organization: The organization users must be part of.         :type organization: str | unicode         :param base_url: The API endpoint to use. Useful if you are running GitHub Enterprise or an API-compatible             authentication server.         :type base_url: str | unicode         :param ttl: Duration after which authentication will be expired.         :type ttl: str | unicode         :param max_ttl: Maximum duration after which authentication will             be expired.         :type max_ttl: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the configure_method request.         :rtype: requests.Response
Map a list of policies to a team that exists in the configured GitHub organization.          Supported methods:             POST: /auth/{mount_point}/map/teams/{team_name}. Produces: 204 (empty body)           :param team_name: GitHub team name in "slugified" format         :type team_name: str | unicode         :param policies: Comma separated list of policies to assign         :type policies: List[str]         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the map_github_teams request.         :rtype: requests.Response
Read the GitHub team policy mapping.          Supported methods:             GET: /auth/{mount_point}/map/teams/{team_name}. Produces: 200 application/json           :param team_name: GitHub team name         :type team_name: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the read_team_mapping request.         :rtype: dict
Read the GitHub user policy mapping.          Supported methods:             GET: /auth/{mount_point}/map/users/{user_name}. Produces: 200 application/json           :param user_name: GitHub user name         :type user_name: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the read_user_mapping request.         :rtype: dict
Login using GitHub access token.          Supported methods:             POST: /auth/{mount_point}/login. Produces: 200 application/json           :param token: GitHub personal API token.         :type token: str | unicode         :param use_token: if True, uses the token in the response received from the auth request to set the "token"             attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.         :type use_token: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the login request.         :rtype: dict
Helper function to prepare a AWS API request to subsequently generate a "AWS Signature Version 4" header.      :param header_value: Vault allows you to require an additional header, X-Vault-AWS-IAM-Server-ID, to be present         to mitigate against different types of replay attacks. Depending on the configuration of the AWS auth         backend, providing a argument to this optional parameter may be required.     :type header_value: str     :return: A PreparedRequest instance, optionally containing the provided header value under a         'X-Vault-AWS-IAM-Server-ID' header name pointed to AWS's simple token service with action "GetCallerIdentity"     :rtype: requests.PreparedRequest
Retrieve lease metadata.          Supported methods:             PUT: /sys/leases/lookup. Produces: 200 application/json          :param lease_id: the ID of the lease to lookup.         :type lease_id: str | unicode         :return: Parsed JSON response from the leases PUT request         :rtype: dict.
Retrieve a list of lease ids.          Supported methods:             LIST: /sys/leases/lookup/{prefix}. Produces: 200 application/json          :param prefix: Lease prefix to filter list by.         :type prefix: str | unicode         :return: The JSON response of the request.         :rtype: dict
Renew a lease, requesting to extend the lease.          Supported methods:             PUT: /sys/leases/renew. Produces: 200 application/json          :param lease_id: The ID of the lease to extend.         :type lease_id: str | unicode         :param increment: The requested amount of time (in seconds) to extend the lease.         :type increment: int         :return: The JSON response of the request         :rtype: dict
Revoke a lease immediately.          Supported methods:             PUT: /sys/leases/revoke. Produces: 204 (empty body)          :param lease_id: Specifies the ID of the lease to revoke.         :type lease_id: str | unicode         :return: The response of the request.         :rtype: requests.Response
Revoke all secrets (via a lease ID prefix) or tokens (via the tokens' path property) generated under a given         prefix immediately.          This requires sudo capability and access to it should be tightly controlled as it can be used to revoke very         large numbers of secrets/tokens at once.          Supported methods:             PUT: /sys/leases/revoke-prefix/{prefix}. Produces: 204 (empty body)           :param prefix: The prefix to revoke.         :type prefix: str | unicode         :return: The response of the request.         :rtype: requests.Response
Create or update an Entity.          Supported methods:             POST: /{mount_point}/entity. Produces: 200 application/json          :param entity_id: ID of the entity. If set, updates the corresponding existing entity.         :type entity_id: str | unicode         :param name: Name of the entity.         :type name: str | unicode         :param metadata: Metadata to be associated with the entity.         :type metadata: dict         :param policies: Policies to be tied to the entity.         :type policies: str | unicode         :param disabled: Whether the entity is disabled. Disabled entities' associated tokens cannot be used, but are             not revoked.         :type disabled: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response for creates, the generic response object for updates, of the request.         :rtype: dict | requests.Response
Query an entity by its identifier.          Supported methods:             GET: /auth/{mount_point}/entity/id/{id}. Produces: 200 application/json          :param entity_id: Identifier of the entity.         :type entity_id: str         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Delete an entity and all its associated aliases.          Supported methods:             DELETE: /{mount_point}/entity/id/:id. Produces: 204 (empty body)          :param entity_id: Identifier of the entity.         :type entity_id: str         :param mount_point: The "path" the secret engine was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Merge many entities into one entity.          Supported methods:             POST: /{mount_point}/entity/merge. Produces: 204 (empty body)          :param from_entity_ids: Entity IDs which needs to get merged.         :type from_entity_ids: array         :param to_entity_id: Entity ID into which all the other entities need to get merged.         :type to_entity_id: str | unicode         :param force: Setting this will follow the 'mine' strategy for merging MFA secrets. If there are secrets of the             same type both in entities that are merged from and in entity into which all others are getting merged,             secrets in the destination will be unaltered. If not set, this API will throw an error containing all the             conflicts.         :type force: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Query the entity alias by its identifier.          Supported methods:             GET: /{mount_point}/entity-alias/id/{id}. Produces: 200 application/json          :param alias_id: Identifier of entity alias.         :type alias_id: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Update an existing entity alias.          Supported methods:             POST: /{mount_point}/entity-alias/id/{id}. Produces: 200 application/json          :param alias_id: Identifier of the entity alias.         :type alias_id: str | unicode         :param name: Name of the alias. Name should be the identifier of the client in the authentication source. For             example, if the alias belongs to userpass backend, the name should be a valid username within userpass             backend. If alias belongs to GitHub, it should be the GitHub username.         :type name: str | unicode         :param canonical_id: Entity ID to which this alias belongs to.         :type canonical_id: str | unicode         :param mount_accessor: Accessor of the mount to which the alias should belong to.         :type mount_accessor: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response where available, otherwise the generic response object, of the request.         :rtype: dict | requests.Response
List available entity aliases by their identifiers.          :param method: Supported methods:             LIST: /{mount_point}/entity-alias/id. Produces: 200 application/json             GET: /{mount_point}/entity-alias/id?list=true. Produces: 200 application/json         :type method: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The the JSON response of the request.         :rtype: dict
Delete a entity alias.          Supported methods:             DELETE: /{mount_point}/entity-alias/id/{alias_id}. Produces: 204 (empty body)          :param alias_id: Identifier of the entity.         :type alias_id: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Determine whether member ID parameters can be sent with a group create / update request.          These parameters are only allowed for the internal group type. If they're set for an external group type, Vault         returns a "error" response.          :param group_type: Type of the group, internal or external         :type group_type: str | unicode         :param params: Params dict to conditionally add the member entity/group ID's to.         :type params: dict         :param member_group_ids:  Group IDs to be assigned as group members.         :type member_group_ids: str | unicode         :param member_entity_ids: Entity IDs to be assigned as  group members.         :type member_entity_ids: str | unicode         :return: Params dict with conditionally added member entity/group ID's.         :rtype: dict
Query the group by its identifier.          Supported methods:             GET: /{mount_point}/group/id/{id}. Produces: 200 application/json          :param group_id: Identifier of the group.         :type group_id: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Delete a group.          Supported methods:             DELETE: /{mount_point}/group/id/{id}. Produces: 204 (empty body)          :param group_id: Identifier of the entity.         :type group_id: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Create or update a group by its name.          Supported methods:             POST: /{mount_point}/group/name/{name}. Produces: 200 application/json          :param name: Name of the group.         :type name: str | unicode         :param group_type: Type of the group, internal or external. Defaults to internal.         :type group_type: str | unicode         :param metadata: Metadata to be associated with the group.         :type metadata: dict         :param policies: Policies to be tied to the group.         :type policies: str | unicode         :param member_group_ids: Group IDs to be assigned as group members.         :type member_group_ids: str | unicode         :param member_entity_ids: Entity IDs to be assigned as group members.         :type member_entity_ids: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Creates or update a group alias.          Supported methods:             POST: /{mount_point}/group-alias. Produces: 200 application/json          :param alias_id: ID of the group alias. If set, updates the corresponding existing group alias.         :type alias_id: str | unicode         :param name: Name of the group alias.         :type name: str | unicode         :param mount_accessor: Mount accessor to which this alias belongs to         :type mount_accessor: str | unicode         :param canonical_id: ID of the group to which this is an alias.         :type canonical_id: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Update an existing group alias.          Supported methods:             POST: /{mount_point}/group-alias/id/{id}. Produces: 200 application/json          :param entity_id: ID of the group alias.         :type entity_id: str | unicode         :param name: Name of the group alias.         :type name: str | unicode         :param mount_accessor: Mount accessor to which this alias belongs             toMount accessor to which this alias belongs to.         :type mount_accessor: str | unicode         :param canonical_id: ID of the group to which this is an alias.         :type canonical_id: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Query an entity based on the given criteria.          The criteria can be name, id, alias_id, or a combination of alias_name and alias_mount_accessor.          Supported methods:             POST: /{mount_point}/lookup/entity. Produces: 200 application/json          :param name: Name of the entity.         :type name: str | unicode         :param entity_id: ID of the entity.         :type entity_id: str | unicode         :param alias_id: ID of the alias.         :type alias_id: str | unicode         :param alias_name: Name of the alias. This should be supplied in conjunction with alias_mount_accessor.         :type alias_name: str | unicode         :param alias_mount_accessor: Accessor of the mount to which the alias belongs to. This should be supplied in conjunction with alias_name.         :type alias_mount_accessor: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request if a entity / entity alias is found in the lookup, None otherwise.         :rtype: dict | None
Create a new named encryption key of the specified type.          The values set here cannot be changed after key creation.          Supported methods:             POST: /{mount_point}/keys/{name}. Produces: 204 (empty body)          :param name: Specifies the name of the encryption key to create. This is specified as part of the URL.         :type name: str | unicode         :param convergent_encryption: If enabled, the key will support convergent encryption, where the same plaintext             creates the same ciphertext. This requires derived to be set to true. When enabled, each             encryption(/decryption/rewrap/datakey) operation will derive a nonce value rather than randomly generate it.         :type convergent_encryption: bool         :param derived: Specifies if key derivation is to be used. If enabled, all encrypt/decrypt requests to this             named key must provide a context which is used for key derivation.         :type derived: bool         :param exportable: Enables keys to be exportable. This allows for all the valid keys in the key ring to be             exported. Once set, this cannot be disabled.         :type exportable: bool         :param allow_plaintext_backup: If set, enables taking backup of named key in the plaintext format. Once set,             this cannot be disabled.         :type allow_plaintext_backup: bool         :param key_type: Specifies the type of key to create. The currently-supported types are:              * **aes256-gcm96**: AES-256 wrapped with GCM using a 96-bit nonce size AEAD             * **chacha20-poly1305**: ChaCha20-Poly1305 AEAD (symmetric, supports derivation and convergent encryption)             * **ed25519**: ED25519 (asymmetric, supports derivation).             * **ecdsa-p256**: ECDSA using the P-256 elliptic curve (asymmetric)             * **rsa-2048**: RSA with bit size of 2048 (asymmetric)             * **rsa-4096**: RSA with bit size of 4096 (asymmetric)         :type key_type: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Tune configuration values for a given key.          These values are returned during a read operation on the named key.          Supported methods:             POST: /{mount_point}/keys/{name}/config. Produces: 204 (empty body)          :param name: Specifies the name of the encryption key to update configuration for.         :type name: str | unicode         :param min_decryption_version: Specifies the minimum version of ciphertext allowed to be decrypted. Adjusting             this as part of a key rotation policy can prevent old copies of ciphertext from being decrypted, should they             fall into the wrong hands. For signatures, this value controls the minimum version of signature that can be             verified against. For HMACs, this controls the minimum version of a key allowed to be used as the key for             verification.         :type min_decryption_version: int         :param min_encryption_version: Specifies the minimum version of the key that can be used to encrypt plaintext,             sign payloads, or generate HMACs. Must be 0 (which will use the latest version) or a value greater or equal             to min_decryption_version.         :type min_encryption_version: int         :param deletion_allowed: Specifies if the key is allowed to be deleted.         :type deletion_allowed: bool         :param exportable: Enables keys to be exportable. This allows for all the valid keys in the key ring to be             exported. Once set, this cannot be disabled.         :type exportable: bool         :param allow_plaintext_backup: If set, enables taking backup of named key in the plaintext format. Once set,             this cannot be disabled.         :type allow_plaintext_backup: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Rotate the version of the named key.          After rotation, new plaintext requests will be encrypted with the new version of the key. To upgrade ciphertext         to be encrypted with the latest version of the key, use the rewrap endpoint. This is only supported with keys         that support encryption and decryption operations.          Supported methods:             POST: /{mount_point}/keys/{name}/rotate. Produces: 204 (empty body)          :param name: Specifies the name of the key to read information about. This is specified as part of the URL.         :type name: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Return the named key.          The keys object shows the value of the key for each version. If version is specified, the specific version will         be returned. If latest is provided as the version, the current key will be provided. Depending on the type of         key, different information may be returned. The key must be exportable to support this operation and the version         must still be valid.          Supported methods:             GET: /{mount_point}/export/{key_type}/{name}(/{version}). Produces: 200 application/json          :param name: Specifies the name of the key to read information about. This is specified as part of the URL.         :type name: str | unicode         :param key_type: Specifies the type of the key to export. This is specified as part of the URL. Valid values are:             encryption-key             signing-key             hmac-key         :type key_type: str | unicode         :param version: Specifies the version of the key to read. If omitted, all versions of the key will be returned.             If the version is set to latest, the current key will be returned.         :type version: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Encrypt the provided plaintext using the named key.          This path supports the create and update policy capabilities as follows: if the user has the create capability         for this endpoint in their policies, and the key does not exist, it will be upserted with default values         (whether the key requires derivation depends on whether the context parameter is empty or not). If the user only         has update capability and the key does not exist, an error will be returned.          Supported methods:             POST: /{mount_point}/encrypt/{name}. Produces: 200 application/json          :param name: Specifies the name of the encryption key to encrypt against. This is specified as part of the URL.         :type name: str | unicode         :param plaintext: Specifies base64 encoded plaintext to be encoded.         :type plaintext: str | unicode         :param context: Specifies the base64 encoded context for key derivation. This is required if key derivation is             enabled for this key.         :type context: str | unicode         :param key_version: Specifies the version of the key to use for encryption. If not set, uses the latest version.             Must be greater than or equal to the key's min_encryption_version, if set.         :type key_version: int         :param nonce: Specifies the base64 encoded nonce value. This must be provided if convergent encryption is             enabled for this key and the key was generated with Vault 0.6.1. Not required for keys created in 0.6.2+.             The value must be exactly 96 bits (12 bytes) long and the user must ensure that for any given context (and             thus, any given encryption key) this nonce value is never reused.         :type nonce: str | unicode         :param batch_input: Specifies a list of items to be encrypted in a single batch. When this parameter is set, if             the parameters 'plaintext', 'context' and 'nonce' are also set, they will be ignored. The format for the             input is: [dict(context="b64_context", plaintext="b64_plaintext"), ...]         :type batch_input: List[dict]         :param type: This parameter is required when encryption key is expected to be created. When performing an             upsert operation, the type of key to create.         :type type: str | unicode         :param convergent_encryption: This parameter will only be used when a key is expected to be created. Whether to             support convergent encryption. This is only supported when using a key with key derivation enabled and will             require all requests to carry both a context and 96-bit (12-byte) nonce. The given nonce will be used in             place of a randomly generated nonce. As a result, when the same context and nonce are supplied, the same             ciphertext is generated. It is very important when using this mode that you ensure that all nonces are             unique for a given context. Failing to do so will severely impact the ciphertext's security.         :type convergent_encryption: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Decrypt the provided ciphertext using the named key.          Supported methods:             POST: /{mount_point}/decrypt/{name}. Produces: 200 application/json          :param name: Specifies the name of the encryption key to decrypt against. This is specified as part of the URL.         :type name: str | unicode         :param ciphertext: the ciphertext to decrypt.         :type ciphertext: str | unicode         :param context: Specifies the base64 encoded context for key derivation. This is required if key derivation is             enabled.         :type context: str | unicode         :param nonce: Specifies a base64 encoded nonce value used during encryption. Must be provided if convergent             encryption is enabled for this key and the key was generated with Vault 0.6.1. Not required for keys created             in 0.6.2+.         :type nonce: str | unicode         :param batch_input: Specifies a list of items to be decrypted in a single batch. When this parameter is set, if             the parameters 'ciphertext', 'context' and 'nonce' are also set, they will be ignored. Format for the input             goes like this: [dict(context="b64_context", ciphertext="b64_plaintext"), ...]         :type batch_input: List[dict]         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Generates a new high-entropy key and the value encrypted with the named key.          Optionally return the plaintext of the key as well. Whether plaintext is returned depends on the path; as a         result, you can use Vault ACL policies to control whether a user is allowed to retrieve the plaintext value of a         key. This is useful if you want an untrusted user or operation to generate keys that are then made available to         trusted users.          Supported methods:             POST: /{mount_point}/datakey/{key_type}/{name}. Produces: 200 application/json          :param name: Specifies the name of the encryption key to use to encrypt the datakey. This is specified as part             of the URL.         :type name: str | unicode         :param key_type: Specifies the type of key to generate. If plaintext, the plaintext key will be returned along             with the ciphertext. If wrapped, only the ciphertext value will be returned. This is specified as part of             the URL.         :type key_type: str | unicode         :param context: Specifies the key derivation context, provided as a base64-encoded string. This must be provided             if derivation is enabled.         :type context: str | unicode         :param nonce: Specifies a nonce value, provided as base64 encoded. Must be provided if convergent encryption is             enabled for this key and the key was generated with Vault 0.6.1. Not required for keys created in 0.6.2+.             The value must be exactly 96 bits (12 bytes) long and the user must ensure that for any given context (and             thus, any given encryption key) this nonce value is never reused.         :type nonce: str | unicode         :param bits: Specifies the number of bits in the desired key. Can be 128, 256, or 512.         :type bits: int         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Return high-quality random bytes of the specified length.          Supported methods:             POST: /{mount_point}/random(/{bytes}). Produces: 200 application/json          :param n_bytes: Specifies the number of bytes to return. This value can be specified either in the request body,             or as a part of the URL.         :type n_bytes: int         :param output_format: Specifies the output encoding. Valid options are hex or base64.         :type output_format: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Return the cryptographic hash of given data using the specified algorithm.          Supported methods:             POST: /{mount_point}/hash(/{algorithm}). Produces: 200 application/json          :param hash_input: Specifies the base64 encoded input data.         :type hash_input: str | unicode         :param algorithm: Specifies the hash algorithm to use. This can also be specified as part of the URL.             Currently-supported algorithms are: sha2-224, sha2-256, sha2-384, sha2-512         :type algorithm: str | unicode         :param output_format: Specifies the output encoding. This can be either hex or base64.         :type output_format: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Return the digest of given data using the specified hash algorithm and the named key.          The key can be of any type supported by transit; the raw key will be marshaled into bytes to be used for the         HMAC function. If the key is of a type that supports rotation, the latest (current) version will be used.          Supported methods:             POST: /{mount_point}/hmac/{name}(/{algorithm}). Produces: 200 application/json          :param name: Specifies the name of the encryption key to generate hmac against. This is specified as part of the             URL.         :type name: str | unicode         :param hash_input: Specifies the base64 encoded input data.         :type input: str | unicode         :param key_version: Specifies the version of the key to use for the operation. If not set, uses the latest             version. Must be greater than or equal to the key's min_encryption_version, if set.         :type key_version: int         :param algorithm: Specifies the hash algorithm to use. This can also be specified as part of the URL.             Currently-supported algorithms are: sha2-224, sha2-256, sha2-384, sha2-512         :type algorithm: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Return the cryptographic signature of the given data using the named key and the specified hash algorithm.          The key must be of a type that supports signing.          Supported methods:             POST: /{mount_point}/sign/{name}(/{hash_algorithm}). Produces: 200 application/json          :param name: Specifies the name of the encryption key to use for signing. This is specified as part of the URL.         :type name: str | unicode         :param hash_input: Specifies the base64 encoded input data.         :type hash_input: str | unicode         :param key_version: Specifies the version of the key to use for signing. If not set, uses the latest version.             Must be greater than or equal to the key's min_encryption_version, if set.         :type key_version: int         :param hash_algorithm: Specifies the hash algorithm to use for supporting key types (notably, not including             ed25519 which specifies its own hash algorithm). This can also be specified as part of the URL.             Currently-supported algorithms are: sha2-224, sha2-256, sha2-384, sha2-512         :type hash_algorithm: str | unicode         :param context: Base64 encoded context for key derivation. Required if key derivation is enabled; currently only             available with ed25519 keys.         :type context: str | unicode         :param prehashed: Set to true when the input is already hashed. If the key type is rsa-2048 or rsa-4096, then             the algorithm used to hash the input should be indicated by the hash_algorithm parameter. Just as the value             to sign should be the base64-encoded representation of the exact binary data you want signed, when set, input             is expected to be base64-encoded binary hashed data, not hex-formatted. (As an example, on the command line,             you could generate a suitable input via openssl dgst -sha256 -binary | base64.)         :type prehashed: bool         :param signature_algorithm: When using a RSA key, specifies the RSA signature algorithm to use for signing.             Supported signature types are: pss, pkcs1v15         :type signature_algorithm: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Return a plaintext backup of a named key.          The backup contains all the configuration data and keys of all the versions along with the HMAC key. The         response from this endpoint can be used with the /restore endpoint to restore the key.          Supported methods:             GET: /{mount_point}/backup/{name}. Produces: 200 application/json          :param name: Name of the key.         :type name: str | unicode         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Restore the backup as a named key.          This will restore the key configurations and all the versions of the named key along with HMAC keys. The input         to this endpoint should be the output of /backup endpoint. For safety, by default the backend will refuse to         restore to an existing key. If you want to reuse a key name, it is recommended you delete the key before         restoring. It is a good idea to attempt restoring to a different key name first to verify that the operation         successfully completes.          Supported methods:             POST: /{mount_point}/restore(/name). Produces: 204 (empty body)          :param backup: Backed up key data to be restored. This should be the output from the /backup endpoint.         :type backup: str | unicode         :param name: If set, this will be the name of the restored key.         :type name: str | unicode         :param force: If set, force the restore to proceed even if a key by this name already exists.         :type force: bool         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Trims older key versions setting a minimum version for the keyring.          Once trimmed, previous versions of the key cannot be recovered.          Supported methods:             POST: /{mount_point}/keys/{name}/trim. Produces: 200 application/json          :param name: Specifies the name of the key to be trimmed.         :type name: str | unicode         :param min_version: The minimum version for the key ring. All versions before this version will be permanently             deleted. This value can at most be equal to the lesser of min_decryption_version and min_encryption_version.             This is not allowed to be set when either min_encryption_version or min_decryption_version is set to zero.         :type min_version: int         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Helper method to raise exceptions based on the status code of a response received back from Vault.      :param status_code: Status code received in a response from Vault.     :type status_code: int     :param message: Optional message to include in a resulting exception.     :type message: str     :param errors: Optional errors to include in a resulting exception.     :type errors: list | str      :raises: hvac.exceptions.InvalidRequest | hvac.exceptions.Unauthorized | hvac.exceptions.Forbidden |         hvac.exceptions.InvalidPath | hvac.exceptions.RateLimitExceeded | hvac.exceptions.InternalServerError |         hvac.exceptions.VaultNotInitialized | hvac.exceptions.VaultDown | hvac.exceptions.UnexpectedError
Generate a message to be used when warning about the use of deprecated methods.      :param to_be_removed_in_version: Version of this module the deprecated method will be removed in.     :type to_be_removed_in_version: str     :param old_method_name: Deprecated method name.     :type old_method_name:  str     :param method_name:  Method intended to replace the deprecated method indicated. This method's docstrings are         included in the decorated method's docstring.     :type method_name: str     :param module_name: Name of the module containing the new method to use.     :type module_name: str     :return: Full deprecation warning message for the indicated method.     :rtype: str
Generate a message to be used when warning about the use of deprecated properties.      :param to_be_removed_in_version: Version of this module the deprecated property will be removed in.     :type to_be_removed_in_version: str     :param old_name: Deprecated property name.     :type old_name: str     :param new_name: Name of the new property name to use.     :type new_name: str     :param new_attribute: The new attribute where the new property can be found.     :type new_attribute: str     :param module_name: Name of the module containing the new method to use.     :type module_name: str     :return: Full deprecation warning message for the indicated property.     :rtype: str
Helper method to use in the getattr method of a class with deprecated properties.      :param obj: Instance of the Class containing the deprecated properties in question.     :type obj: object     :param item: Name of the attribute being requested.     :type item: str     :param deprecated_properties: List of deprecated properties. Each item in the list is a dict with at least a         "to_be_removed_in_version" and "client_property" key to be used in the displayed deprecation warning.     :type deprecated_properties: List[dict]     :return: The new property indicated where available.     :rtype: object
This is a decorator which can be used to mark methods as deprecated. It will result in a warning being emitted     when the function is used.      :param to_be_removed_in_version: Version of this module the decorated method will be removed in.     :type to_be_removed_in_version: str     :param new_method: Method intended to replace the decorated method. This method's docstrings are included in the         decorated method's docstring.     :type new_method: function     :return: Wrapped function that includes a deprecation warning and update docstrings from the replacement method.     :rtype: types.FunctionType
Validate that an argument is a list of strings.      :param param_name: The name of the parameter being validated. Used in any resulting exception messages.     :type param_name: str | unicode     :param param_argument: The argument to validate.     :type param_argument: list     :return: True if the argument is validated, False otherwise.     :rtype: bool
Get the token from env var, VAULT_TOKEN. If not set, attempt to get the token from, ~/.vault-token      :return: The vault token if set, else None     :rtype: str | None
Convert comma-delimited list / string into a list of strings      :param list_param: Comma-delimited string     :type list_param: str | unicode     :return: A list of strings     :rtype: list
Validate that an argument is a PEM-formatted public key or certificate      :param param_name: The name of the parameter being validate. Used in any resulting exception messages.     :type param_name: str | unicode     :param param_argument: The argument to validate     :type param_argument: str | unicode     :return: True if the argument is validate False otherwise     :rtype: bool
List all enabled auth methods.          Supported methods:             GET: /sys/auth. Produces: 200 application/json          :return: The JSON response of the request.         :rtype: dict
Enable a new auth method.          After enabling, the auth method can be accessed and configured via the auth path specified as part of the URL.         This auth path will be nested under the auth prefix.          Supported methods:             POST: /sys/auth/{path}. Produces: 204 (empty body)          :param method_type: The name of the authentication method type, such as "github" or "token".         :type method_type: str | unicode         :param description: A human-friendly description of the auth method.         :type description: str | unicode         :param config: Configuration options for this auth method. These are the possible values:              * **default_lease_ttl**: The default lease duration, specified as a string duration like "5s" or "30m".             * **max_lease_ttl**: The maximum lease duration, specified as a string duration like "5s" or "30m".             * **audit_non_hmac_request_keys**: Comma-separated list of keys that will not be HMAC'd by audit devices in               the request data object.             * **audit_non_hmac_response_keys**: Comma-separated list of keys that will not be HMAC'd by audit devices in               the response data object.             * **listing_visibility**: Speficies whether to show this mount in the UI-specific listing endpoint.             * **passthrough_request_headers**: Comma-separated list of headers to whitelist and pass from the request to               the backend.         :type config: dict         :param plugin_name: The name of the auth plugin to use based from the name in the plugin catalog. Applies only             to plugin methods.         :type plugin_name: str | unicode         :param local: <Vault enterprise only> Specifies if the auth method is a local only. Local auth methods are not             replicated nor (if a secondary) removed by replication.         :type local: bool         :param path: The path to mount the method on. If not provided, defaults to the value of the "method_type"             argument.         :type path: str | unicode         :return: The response of the request.         :rtype: requests.Response
Disable the auth method at the given auth path.          Supported methods:             DELETE: /sys/auth/{path}. Produces: 204 (empty body)          :param path: The path the method was mounted on. If not provided, defaults to the value of the "method_type"             argument.         :type path: str | unicode         :return: The response of the request.         :rtype: requests.Response
Read the given auth path's configuration.          This endpoint requires sudo capability on the final path, but the same functionality can be achieved without         sudo via sys/mounts/auth/[auth-path]/tune.          Supported methods:             GET: /sys/auth/{path}/tune. Produces: 200 application/json          :param path: The path the method was mounted on. If not provided, defaults to the value of the "method_type"             argument.         :type path: str | unicode         :return: The JSON response of the request.         :rtype: dict
Tune configuration parameters for a given auth path.          This endpoint requires sudo capability on the final path, but the same functionality can be achieved without         sudo via sys/mounts/auth/[auth-path]/tune.          Supported methods:             POST: /sys/auth/{path}/tune. Produces: 204 (empty body)          :param path: The path the method was mounted on. If not provided, defaults to the value of the "method_type"             argument.         :type path: str | unicode         :param default_lease_ttl: Specifies the default time-to-live. If set on a specific auth path, this overrides the             global default.         :type default_lease_ttl: int         :param max_lease_ttl: The maximum time-to-live. If set on a specific auth path, this overrides the global             default.         :type max_lease_ttl: int         :param description: Specifies the description of the mount. This overrides the current stored value, if any.         :type description: str | unicode         :param audit_non_hmac_request_keys: Specifies the list of keys that will not be HMAC'd by audit devices in the             request data object.         :type audit_non_hmac_request_keys: array         :param audit_non_hmac_response_keys: Specifies the list of keys that will not be HMAC'd by audit devices in the             response data object.         :type audit_non_hmac_response_keys: list         :param listing_visibility: Specifies whether to show this mount in the UI-specific listing endpoint. Valid             values are "unauth" or "".         :type listing_visibility: list         :param passthrough_request_headers: List of headers to whitelist and pass from the request to the backend.         :type passthrough_request_headers: list         :return: The response of the request.         :rtype: requests.Response
Read the seal status of the Vault.          This is an unauthenticated endpoint.          Supported methods:             GET: /sys/seal-status. Produces: 200 application/json          :return: The JSON response of the request.         :rtype: dict
Enter a single master key share to progress the unsealing of the Vault.          If the threshold number of master key shares is reached, Vault will attempt to unseal the Vault. Otherwise, this         API must be called multiple times until that threshold is met.          Either the key or reset parameter must be provided; if both are provided, reset takes precedence.          Supported methods:             PUT: /sys/unseal. Produces: 200 application/json          :param key: Specifies a single master key share. This is required unless reset is true.         :type key: str | unicode         :param reset: Specifies if previously-provided unseal keys are discarded and the unseal process is reset.         :type reset: bool         :param migrate: Available in 1.0 Beta - Used to migrate the seal from shamir to autoseal or autoseal to shamir.             Must be provided on all unseal key calls.         :type: migrate: bool         :return: The JSON response of the request.         :rtype: dict
Enter multiple master key share to progress the unsealing of the Vault.          :param keys: List of master key shares.         :type keys: List[str]         :param migrate: Available in 1.0 Beta - Used to migrate the seal from shamir to autoseal or autoseal to shamir.             Must be provided on all unseal key calls.         :type: migrate: bool         :return: The JSON response of the last unseal request.         :rtype: dict
Perform a login request.          Associated request is typically to a path prefixed with "/v1/auth") and optionally stores the client token sent             in the resulting Vault response for use by the :py:meth:`hvac.adapters.Adapter` instance under the _adapater             Client attribute.          :param url: Path to send the authentication request to.         :type url: str | unicode         :param use_token: if True, uses the token in the response received from the auth request to set the "token"             attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.         :type use_token: bool         :param kwargs: Additional keyword arguments to include in the params sent with the request.         :type kwargs: dict         :return: The response of the auth request.         :rtype: requests.Response
Main method for routing HTTP requests to the configured Vault base_uri.          :param method: HTTP method to use with the request. E.g., GET, POST, etc.         :type method: str         :param url: Partial URL path to send the request to. This will be joined to the end of the instance's base_uri             attribute.         :type url: str | unicode         :param headers: Additional headers to include with the request.         :type headers: dict         :param raise_exception: If True, raise an exception via utils.raise_for_error(). Set this parameter to False to             bypass this functionality.         :type raise_exception: bool         :param kwargs: Additional keyword arguments to include in the requests call.         :type kwargs: dict         :return: The response of the request.         :rtype: requests.Response
Read the configuration and process of the current root generation attempt.          Supported methods:             GET: /sys/generate-root/attempt. Produces: 200 application/json          :return: The JSON response of the request.         :rtype: dict
Initialize a new root generation attempt.          Only a single root generation attempt can take place at a time. One (and only one) of otp or pgp_key are         required.          Supported methods:             PUT: /sys/generate-root/attempt. Produces: 200 application/json          :param otp: Specifies a base64-encoded 16-byte value. The raw bytes of the token will be XOR'd with this value             before being returned to the final unseal key provider.         :type otp: str | unicode         :param pgp_key: Specifies a base64-encoded PGP public key. The raw bytes of the token will be encrypted with             this value before being returned to the final unseal key provider.         :type pgp_key: str | unicode         :return: The JSON response of the request.         :rtype: dict
Enter a single master key share to progress the root generation attempt.          If the threshold number of master key shares is reached, Vault will complete the root generation and issue the         new token. Otherwise, this API must be called multiple times until that threshold is met. The attempt nonce must         be provided with each call.          Supported methods:             PUT: /sys/generate-root/update. Produces: 200 application/json          :param key: Specifies a single master key share.         :type key: str | unicode         :param nonce: The nonce of the attempt.         :type nonce: str | unicode         :return: The JSON response of the request.         :rtype: dict
Cancel any in-progress root generation attempt.          This clears any progress made. This must be called to change the OTP or PGP key being used.          Supported methods:             DELETE: /sys/generate-root/attempt. Produces: 204 (empty body)          :return: The response of the request.         :rtype: request.Response
Read information about the current encryption key used by Vault.          Supported methods:             GET: /sys/key-status. Produces: 200 application/json          :return: JSON response with information regarding the current encryption key used by Vault.         :rtype: dict
Trigger a rotation of the backend encryption key.          This is the key that is used to encrypt data written to the storage backend, and is not provided to operators.         This operation is done online. Future values are encrypted with the new key, while old values are decrypted with         previous encryption keys.          This path requires sudo capability in addition to update.          Supported methods:             PUT: /sys/rorate. Produces: 204 (empty body)          :return: The response of the request.         :rtype: requests.Response
Initializes a new rekey attempt.          Only a single recovery key rekeyattempt can take place at a time, and changing the parameters of a rekey         requires canceling and starting a new rekey, which will also provide a new nonce.          Supported methods:             PUT: /sys/rekey/init. Produces: 204 (empty body)             PUT: /sys/rekey-recovery-key/init. Produces: 204 (empty body)          :param secret_shares: Specifies the number of shares to split the master key into.         :type secret_shares: int         :param secret_threshold: Specifies the number of shares required to reconstruct the master key. This must be             less than or equal to secret_shares.         :type secret_threshold: int         :param pgp_keys: Specifies an array of PGP public keys used to encrypt the output unseal keys. Ordering is             preserved. The keys must be base64-encoded from their original binary representation. The size of this array             must be the same as secret_shares.         :type pgp_keys: list         :param backup: Specifies if using PGP-encrypted keys, whether Vault should also store a plaintext backup of the             PGP-encrypted keys at core/unseal-keys-backup in the physical storage backend. These can then be retrieved             and removed via the sys/rekey/backup endpoint.         :type backup: bool         :param require_verification: This turns on verification functionality. When verification is turned on, after             successful authorization with the current unseal keys, the new unseal keys are returned but the master key             is not actually rotated. The new keys must be provided to authorize the actual rotation of the master key.             This ensures that the new keys have been successfully saved and protects against a risk of the keys being             lost after rotation but before they can be persisted. This can be used with without pgp_keys, and when used             with it, it allows ensuring that the returned keys can be successfully decrypted before committing to the             new shares, which the backup functionality does not provide.         :param recovery_key: If true, send requests to "rekey-recovery-key" instead of "rekey" api path.         :type recovery_key: bool         :type require_verification: bool         :return: The JSON dict of the response.         :rtype: dict | request.Response
Cancel any in-progress rekey.          This clears the rekey settings as well as any progress made. This must be called to change the parameters of the         rekey.          Note: Verification is still a part of a rekey. If rekeying is canceled during the verification flow, the current         unseal keys remain valid.          Supported methods:             DELETE: /sys/rekey/init. Produces: 204 (empty body)             DELETE: /sys/rekey-recovery-key/init. Produces: 204 (empty body)          :param recovery_key: If true, send requests to "rekey-recovery-key" instead of "rekey" api path.         :type recovery_key: bool         :return: The response of the request.         :rtype: requests.Response
Enter a single recovery key share to progress the rekey of the Vault.          If the threshold number of recovery key shares is reached, Vault will complete the rekey. Otherwise, this API         must be called multiple times until that threshold is met. The rekey nonce operation must be provided with each         call.          Supported methods:             PUT: /sys/rekey/update. Produces: 200 application/json             PUT: /sys/rekey-recovery-key/update. Produces: 200 application/json          :param key: Specifies a single recovery share key.         :type key: str | unicode         :param nonce: Specifies the nonce of the rekey operation.         :type nonce: str | unicode         :param recovery_key: If true, send requests to "rekey-recovery-key" instead of "rekey" api path.         :type recovery_key: bool         :return: The JSON response of the request.         :rtype: dict
Enter multiple recovery key shares to progress the rekey of the Vault.          If the threshold number of recovery key shares is reached, Vault will complete the rekey.          :param keys: Specifies multiple recovery share keys.         :type keys: list         :param nonce: Specifies the nonce of the rekey operation.         :type nonce: str | unicode         :param recovery_key: If true, send requests to "rekey-recovery-key" instead of "rekey" api path.         :type recovery_key: bool         :return: The last response of the rekey request.         :rtype: response.Request
Retrieve the backup copy of PGP-encrypted unseal keys.          The returned value is the nonce of the rekey operation and a map of PGP key fingerprint to hex-encoded         PGP-encrypted key.          Supported methods:             PUT: /sys/rekey/backup. Produces: 200 application/json             PUT: /sys/rekey-recovery-key/backup. Produces: 200 application/json          :param recovery_key: If true, send requests to "rekey-recovery-key" instead of "rekey" api path.         :type recovery_key: bool         :return: The JSON response of the request.         :rtype: dict
Enable a new secrets engine at the given path.          Supported methods:             POST: /sys/mounts/{path}. Produces: 204 (empty body)          :param backend_type: The name of the backend type, such as "github" or "token".         :type backend_type: str | unicode         :param path: The path to mount the method on. If not provided, defaults to the value of the "method_type"             argument.         :type path: str | unicode         :param description: A human-friendly description of the mount.         :type description: str | unicode         :param config: Configuration options for this mount. These are the possible values:              * **default_lease_ttl**: The default lease duration, specified as a string duration like "5s" or "30m".             * **max_lease_ttl**: The maximum lease duration, specified as a string duration like "5s" or "30m".             * **force_no_cache**: Disable caching.             * **plugin_name**: The name of the plugin in the plugin catalog to use.             * **audit_non_hmac_request_keys**: Comma-separated list of keys that will not be HMAC'd by audit devices in               the request data object.             * **audit_non_hmac_response_keys**: Comma-separated list of keys that will not be HMAC'd by audit devices in               the response data object.             * **listing_visibility**: Specifies whether to show this mount in the UI-specific listing endpoint. ("unauth" or "hidden")             * **passthrough_request_headers**: Comma-separated list of headers to whitelist and pass from the request to               the backend.         :type config: dict         :param options: Specifies mount type specific options that are passed to the backend.              * **version**: <KV> The version of the KV to mount. Set to "2" for mount KV v2.         :type options: dict         :param plugin_name: Specifies the name of the plugin to use based from the name in the plugin catalog. Applies only to plugin backends.         :type plugin_name: str | unicode         :param local: <Vault enterprise only> Specifies if the auth method is a local only. Local auth methods are not             replicated nor (if a secondary) removed by replication.         :type local: bool         :param seal_wrap: <Vault enterprise only> Enable seal wrapping for the mount.         :type seal_wrap: bool         :return: The response of the request.         :rtype: requests.Response
Disable the mount point specified by the provided path.          Supported methods:             DELETE: /sys/mounts/{path}. Produces: 204 (empty body)          :param path: Specifies the path where the secrets engine will be mounted. This is specified as part of the URL.         :type path: str | unicode         :return: The response of the request.         :rtype: requests.Response
Tune configuration parameters for a given mount point.          Supported methods:             POST: /sys/mounts/{path}/tune. Produces: 204 (empty body)          :param path: Specifies the path where the secrets engine will be mounted. This is specified as part of the URL.         :type path: str | unicode         :param mount_point: The path the associated secret backend is mounted         :type mount_point: str         :param description: Specifies the description of the mount. This overrides the current stored value, if any.         :type description: str         :param default_lease_ttl: Default time-to-live. This overrides the global default. A value of 0 is equivalent to             the system default TTL         :type default_lease_ttl: int         :param max_lease_ttl: Maximum time-to-live. This overrides the global default. A value of 0 are equivalent and             set to the system max TTL.         :type max_lease_ttl: int         :param audit_non_hmac_request_keys: Specifies the comma-separated list of keys that will not be HMAC'd by audit             devices in the request data object.         :type audit_non_hmac_request_keys: list         :param audit_non_hmac_response_keys: Specifies the comma-separated list of keys that will not be HMAC'd by audit             devices in the response data object.         :type audit_non_hmac_response_keys: list         :param listing_visibility: Speficies whether to show this mount in the UI-specific listing endpoint. Valid             values are "unauth" or "".         :type listing_visibility: str         :param passthrough_request_headers: Comma-separated list of headers to whitelist and pass from the request             to the backend.         :type passthrough_request_headers: str         :param options: Specifies mount type specific options that are passed to the backend.              * **version**: <KV> The version of the KV to mount. Set to "2" for mount KV v2.         :type options: dict         :return: The response from the request.         :rtype: request.Response
Move an already-mounted backend to a new mount point.          Supported methods:             POST: /sys/remount. Produces: 204 (empty body)          :param from_path: Specifies the previous mount point.         :type from_path: str | unicode         :param to_path: Specifies the new destination mount point.         :type to_path: str | unicode         :return: The response of the request.         :rtype: requests.Response
Configure the connection parameters for Kubernetes.          This path honors the distinction between the create and update capabilities inside ACL policies.          Supported methods:             POST: /auth/{mount_point}/config. Produces: 204 (empty body)          :param kubernetes_host: Host must be a host string, a host:port pair, or a URL to the base of the             Kubernetes API server. Example: https://k8s.example.com:443         :type kubernetes_host: str | unicode         :param kubernetes_ca_cert: PEM encoded CA cert for use by the TLS client used to talk with the Kubernetes API.             NOTE: Every line must end with a newline: \n         :type kubernetes_ca_cert: str | unicode         :param token_reviewer_jwt: A service account JWT used to access the TokenReview API to validate other             JWTs during login. If not set the JWT used for login will be used to access the API.         :type token_reviewer_jwt: str | unicode         :param pem_keys: Optional list of PEM-formatted public keys or certificates used to verify the signatures of             Kubernetes service account JWTs. If a certificate is given, its public key will be extracted. Not every             installation of Kubernetes exposes these keys.         :type pem_keys: list         :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :return: The response of the configure_method request.         :rtype: requests.Response
Create a role in the method.          Registers a role in the auth method. Role types have specific entities that can perform login operations         against this endpoint. Constraints specific to the role type must be set on the role. These are applied to         the authenticated entities attempting to login.          Supported methods:             POST: /auth/{mount_point}/role/{name}. Produces: 204 (empty body)          :param name: Name of the role.         :type name: str | unicode         :param bound_service_account_names: List of service account names able to access this role. If set to "*"             all names are allowed, both this and bound_service_account_namespaces can not be "*".         :type bound_service_account_names: list | str | unicode         :param bound_service_account_namespaces: List of namespaces allowed to access this role. If set to "*" all             namespaces are allowed, both this and bound_service_account_names can not be set to "*".         :type bound_service_account_namespaces: list | str | unicode         :param ttl: The TTL period of tokens issued using this role in seconds.         :type ttl: str | unicode         :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role.         :type max_ttl: str | unicode         :param period: If set, indicates that the token generated using this role should never expire. The token should             be renewed within the duration specified by this value. At each renewal, the token's TTL will be set to the             value of this parameter.         :type period: str | unicode         :param policies: Policies to be set on tokens issued using this role.         :type policies: list | str | unicode         :param mount_point: The "path" the azure auth method was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
Create a role in the method.          Role types have specific entities that can perform login operations against this endpoint. Constraints specific         to the role type must be set on the role. These are applied to the authenticated entities attempting to login.          Supported methods:             POST: /auth/{mount_point}/role/{name}. Produces: 204 (empty body)           :param name: Name of the role.         :type name: str | unicode         :param policies: Policies to be set on tokens issued using this role.         :type policies: list         :param ttl: The TTL period of tokens issued using this role in seconds.         :type ttl: str | unicode         :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role.         :type max_ttl: str | unicode         :param period: If set, indicates that the token generated using this role should never expire. The token should             be renewed within the duration specified by this value. At each renewal, the token's TTL will be set to the             value of this parameter.         :type period: str | unicode         :param bound_service_principal_ids: The list of Service Principal IDs that login is restricted to.         :type bound_service_principal_ids: list         :param bound_group_ids: The list of group ids that login is restricted to.         :type bound_group_ids: list         :param bound_location: The list of locations that login is restricted to.         :type bound_location: list         :param bound_subscription_ids: The list of subscription IDs that login is restricted to.         :type bound_subscription_ids: list         :param bound_resource_group_names: The list of resource groups that login is restricted to.         :type bound_resource_group_names: list         :param bound_scale_sets: The list of scale set names that the login is restricted to.         :type bound_scale_sets: list         :param mount_point: The "path" the azure auth method was mounted on.         :type mount_point: str | unicode         :return: The response of the request.         :rtype: requests.Response
List all the roles that are registered with the plugin.          Supported methods:             LIST: /auth/{mount_point}/roles. Produces: 200 application/json           :param mount_point: The "path" the azure auth method was mounted on.         :type mount_point: str | unicode         :return: The "data" key from the JSON response of the request.         :rtype: dict
Fetch a token.          This endpoint takes a signed JSON Web Token (JWT) and a role name for some entity. It verifies the JWT signature         to authenticate that entity and then authorizes the entity for the given role.          Supported methods:             POST: /auth/{mount_point}/login. Produces: 200 application/json           :param role: Name of the role against which the login is being attempted.         :type role: str | unicode         :param jwt: Signed JSON Web Token (JWT) from Azure MSI.         :type jwt: str | unicode         :param subscription_id: The subscription ID for the machine that generated the MSI token. This information can             be obtained through instance metadata.         :type subscription_id: str | unicode         :param resource_group_name: The resource group for the machine that generated the MSI token. This information             can be obtained through instance metadata.         :type resource_group_name: str | unicode         :param vm_name: The virtual machine name for the machine that generated the MSI token. This information can be             obtained through instance metadata.  If vmss_name is provided, this value is ignored.         :type vm_name: str | unicode         :param vmss_name: The virtual machine scale set name for the machine that generated the MSI token. This             information can be obtained through instance metadata.         :type vmss_name: str | unicode         :param use_token: if True, uses the token in the response received from the auth request to set the "token"             attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.         :type use_token: bool         :param mount_point: The "path" the azure auth method was mounted on.         :type mount_point: str | unicode         :return: The JSON response of the request.         :rtype: dict
Enable a new audit device at the supplied path.          The path can be a single word name or a more complex, nested path.          Supported methods:             PUT: /sys/audit/{path}. Produces: 204 (empty body)          :param device_type: Specifies the type of the audit device.         :type device_type: str | unicode         :param description: Human-friendly description of the audit device.         :type description: str | unicode         :param options: Configuration options to pass to the audit device itself. This is             dependent on the audit device type.         :type options: str | unicode         :param path: Specifies the path in which to enable the audit device. This is part of             the request URL.         :type path: str | unicode         :return: The response of the request.         :rtype: requests.Response
Disable the audit device at the given path.          Supported methods:             DELETE: /sys/audit/{path}. Produces: 204 (empty body)          :param path: The path of the audit device to delete. This is part of the request URL.         :type path: str | unicode         :return: The response of the request.         :rtype: requests.Response
Hash the given input data with the specified audit device's hash function and salt.          This endpoint can be used to discover whether a given plaintext string (the input parameter) appears in the         audit log in obfuscated form.          Supported methods:             POST: /sys/audit-hash/{path}. Produces: 204 (empty body)          :param path: The path of the audit device to generate hashes for. This is part of the request URL.         :type path: str | unicode         :param input_to_hash: The input string to hash.         :type input_to_hash: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
List all configured policies.          Supported methods:             GET: /sys/policy. Produces: 200 application/json          :return: The JSON response of the request.         :rtype: dict
Retrieve the policy body for the named policy.          Supported methods:             GET: /sys/policy/{name}. Produces: 200 application/json          :param name: The name of the policy to retrieve.         :type name: str | unicode         :return: The response of the request         :rtype: dict
Add a new or update an existing policy.          Once a policy is updated, it takes effect immediately to all associated users.          Supported methods:             PUT: /sys/policy/{name}. Produces: 204 (empty body)          :param name: Specifies the name of the policy to create.         :type name: str | unicode         :param policy: Specifies the policy document.         :type policy: str | unicode | dict         :param pretty_print: If True, and provided a dict for the policy argument, send the policy JSON to Vault with             "pretty" formatting.         :type pretty_print: bool         :return: The response of the request.         :rtype: requests.Response
Delete the policy with the given name.          This will immediately affect all users associated with this policy.          Supported methods:             DELETE: /sys/policy/{name}. Produces: 204 (empty body)          :param name: Specifies the name of the policy to delete.         :type name: str | unicode         :return: The response of the request.         :rtype: requests.Response
GET /<path>          :param path:         :type path:         :param wrap_ttl:         :type wrap_ttl:         :return:         :rtype:
GET /<path>?list=true          :param path:         :type path:         :return:         :rtype:
POST /<path>          :param path:         :type path:         :param wrap_ttl:         :type wrap_ttl:         :param kwargs:         :type kwargs:         :return:         :rtype:
Retrieve the policy body for the named policy.          :param name: The name of the policy to retrieve.         :type name: str | unicode         :param parse: Specifies whether to parse the policy body using pyhcl or not.         :type parse: bool         :return: The (optionally parsed) policy body for the specified policy.         :rtype: str | dict
POST /auth/token/create          POST /auth/token/create/<role>          POST /auth/token/create-orphan          :param role:         :type role:         :param token_id:         :type token_id:         :param policies:         :type policies:         :param meta:         :type meta:         :param no_parent:         :type no_parent:         :param lease:         :type lease:         :param display_name:         :type display_name:         :param num_uses:         :type num_uses:         :param no_default_policy:         :type no_default_policy:         :param ttl:         :type ttl:         :param orphan:         :type orphan:         :param wrap_ttl:         :type wrap_ttl:         :param renewable:         :type renewable:         :param explicit_max_ttl:         :type explicit_max_ttl:         :param period:         :type period:         :param token_type:         :type token_type:         :return:         :rtype:
GET /auth/token/lookup/<token>          GET /auth/token/lookup-accessor/<token-accessor>          GET /auth/token/lookup-self          :param token:         :type token: str.         :param accessor:         :type accessor: str.         :param wrap_ttl:         :type wrap_ttl: int.         :return:         :rtype:
POST /auth/token/revoke          POST /auth/token/revoke-orphan          POST /auth/token/revoke-accessor          :param token:         :type token:         :param orphan:         :type orphan:         :param accessor:         :type accessor:         :return:         :rtype:
POST /auth/token/renew          POST /auth/token/renew-self          :param token:         :type token:         :param increment:         :type increment:         :param wrap_ttl:         :type wrap_ttl:         :return:         :rtype:
POST /auth/token/roles/<role>          :param role:         :type role:         :param allowed_policies:         :type allowed_policies:         :param disallowed_policies:         :type disallowed_policies:         :param orphan:         :type orphan:         :param period:         :type period:         :param renewable:         :type renewable:         :param path_suffix:         :type path_suffix:         :param explicit_max_ttl:         :type explicit_max_ttl:         :return:         :rtype:
Helper method which returns the authentication status of the client          :return:         :rtype:
POST /auth/<mount point>/login          :param app_id:         :type app_id:         :param user_id:         :type user_id:         :param mount_point:         :type mount_point:         :param use_token:         :type use_token:         :return:         :rtype:
POST /auth/<mount point>/login          :param mount_point:         :type mount_point:         :param use_token:         :type use_token:         :return:         :rtype:
POST /auth/<mount point>/login/<username>          :param username:         :type username:         :param password:         :type password:         :param mount_point:         :type mount_point:         :param use_token:         :type use_token:         :param kwargs:         :type kwargs:         :return:         :rtype:
POST /auth/<mount point>/login          :param access_key: AWS IAM access key ID         :type access_key: str         :param secret_key: AWS IAM secret access key         :type secret_key: str         :param session_token: Optional AWS IAM session token retrieved via a GetSessionToken AWS API request.             see: https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html         :type session_token: str         :param header_value: Vault allows you to require an additional header, X-Vault-AWS-IAM-Server-ID, to be present             to mitigate against different types of replay attacks. Depending on the configuration of the AWS auth             backend, providing a argument to this optional parameter may be required.         :type header_value: str         :param mount_point: The "path" the AWS auth backend was mounted on. Vault currently defaults to "aws". "aws-ec2"             is the default argument for backwards comparability within this module.         :type mount_point: str         :param role: Name of the role against which the login is being attempted. If role is not specified, then the             login endpoint looks for a role bearing the name of the AMI ID of the EC2 instance that is trying to login             if using the ec2 auth method, or the "friendly name" (i.e., role name or username) of the IAM principal             authenticated. If a matching role is not found, login fails.         :type role: str         :param use_token: If True, uses the token in the response received from the auth request to set the "token"             attribute on the current Client class instance.         :type use_token: bool.         :return: The response from the AWS IAM login request attempt.         :rtype: requests.Response
POST /auth/<mount point>/login          :param pkcs7: PKCS#7 version of an AWS Instance Identity Document from the EC2 Metadata Service.         :type pkcs7: str.         :param nonce: Optional nonce returned as part of the original authentication request. Not required if the backend             has "allow_instance_migration" or "disallow_reauthentication" options turned on.         :type nonce: str.         :param role: Identifier for the AWS auth backend role being requested.         :type role: str.         :param use_token: If True, uses the token in the response received from the auth request to set the "token"             attribute on the current Client class instance.         :type use_token: bool.         :param mount_point: The "path" the AWS auth backend was mounted on. Vault currently defaults to "aws". "aws-ec2"             is the default argument for backwards comparability within this module.         :type mount_point: str.         :return: parsed JSON response from the auth POST request         :rtype: dict.
POST /auth/<mount point>/users/<username>          :param username:         :type username:         :param password:         :type password:         :param policies:         :type policies:         :param mount_point:         :type mount_point:         :param kwargs:         :type kwargs:         :return:         :rtype:
GET /auth/<mount point>/users?list=true          :param mount_point:         :type mount_point:         :return:         :rtype:
GET /auth/<mount point>/users/<username>          :param username:         :type username:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount point>/users/<username>/policies          :param username:         :type username:         :param policies:         :type policies:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount point>/users/<username>/password          :param username:         :type username:         :param password:         :type password:         :param mount_point:         :type mount_point:         :return:         :rtype:
DELETE /auth/<mount point>/users/<username>          :param username:         :type username:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount point>/map/app-id/<app_id>          :param app_id:         :type app_id:         :param policies:         :type policies:         :param display_name:         :type display_name:         :param mount_point:         :type mount_point:         :param kwargs:         :type kwargs:         :return:         :rtype:
GET /auth/<mount_point>/map/app-id/<app_id>          :param app_id:         :type app_id:         :param mount_point:         :type mount_point:         :param wrap_ttl:         :type wrap_ttl:         :return:         :rtype:
DELETE /auth/<mount_point>/map/app-id/<app_id>          :param app_id:         :type app_id:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount point>/map/user-id/<user_id>          :param user_id:         :type user_id:         :param app_id:         :type app_id:         :param cidr_block:         :type cidr_block:         :param mount_point:         :type mount_point:         :param kwargs:         :type kwargs:         :return:         :rtype:
GET /auth/<mount_point>/map/user-id/<user_id>          :param user_id:         :type user_id:         :param mount_point:         :type mount_point:         :param wrap_ttl:         :type wrap_ttl:         :return:         :rtype:
DELETE /auth/<mount_point>/map/user-id/<user_id>          :param user_id:         :type user_id:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/config/client          Configure the credentials required to perform API calls to AWS as well as custom endpoints to talk to AWS APIs.         The instance identity document fetched from the PKCS#7 signature will provide the EC2 instance ID. The         credentials configured using this endpoint will be used to query the status of the instances via         DescribeInstances API. If static credentials are not provided using this endpoint, then the credentials will be         retrieved from the environment variables AWS_ACCESS_KEY, AWS_SECRET_KEY and AWS_REGION respectively. If the         credentials are still not found and if the method is configured on an EC2 instance with metadata querying         capabilities, the credentials are fetched automatically          :param access_key: AWS Access key with permissions to query AWS APIs. The permissions required depend on the             specific configurations. If using the iam auth method without inferencing, then no credentials are             necessary. If using the ec2 auth method or using the iam auth method with inferencing, then these             credentials need access to ec2:DescribeInstances. If additionally a bound_iam_role is specified, then these             credentials also need access to iam:GetInstanceProfile. If, however, an alternate sts configuration is set             for the target account, then the credentials must be permissioned to call sts:AssumeRole on the configured             role, and that role must have the permissions described here.         :type access_key: str|unicode         :param secret_key: AWS Secret key with permissions to query AWS APIs.         :type secret_key: str|unicode         :param endpoint: URL to override the default generated endpoint for making AWS EC2 API calls.         :type endpoint: str|unicode         :param mount_point: The "path" the AWS auth backend was mounted on. Vault currently defaults to "aws". "aws-ec2"             is the default argument for backwards comparability within this module.         :type mount_point: str|unicode         :return: The response of the request.         :rtype: requests.Response
POST /auth/<mount_point>/config/certificate/<cert_name>          :param cert_name:         :type cert_name:         :param aws_public_cert:         :type aws_public_cert:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /auth/<mount_point>/config/certificate/<cert_name>          :param cert_name:         :type cert_name:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /auth/<mount_point>/config/certificates?list=true          :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/role/<role>          :param role:         :type role:         :param bound_ami_id:         :type bound_ami_id:         :param bound_account_id:         :type bound_account_id:         :param bound_iam_role_arn:         :type bound_iam_role_arn:         :param bound_iam_instance_profile_arn:         :type bound_iam_instance_profile_arn:         :param bound_ec2_instance_id:         :type bound_ec2_instance_id:         :param bound_region:         :type bound_region:         :param bound_vpc_id:         :type bound_vpc_id:         :param bound_subnet_id:         :type bound_subnet_id:         :param role_tag:         :type role_tag:         :param ttl:         :type ttl:         :param max_ttl:         :type max_ttl:         :param period:         :type period:         :param policies:         :type policies:         :param allow_instance_migration:         :type allow_instance_migration:         :param disallow_reauthentication:         :type disallow_reauthentication:         :param resolve_aws_unique_ids:         :type resolve_aws_unique_ids:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /auth/<mount_point>/role/<role>          :param role:         :type role:         :param mount_point:         :type mount_point:         :return:         :rtype:
DELETE /auth/<mount_point>/role/<role>          :param role:         :type role:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/role/<role>/tag          :param role:         :type role:         :param policies:         :type policies:         :param max_ttl:         :type max_ttl:         :param instance_id:         :type instance_id:         :param disallow_reauthentication:         :type disallow_reauthentication:         :param allow_instance_migration:         :type allow_instance_migration:         :param mount_point:         :type mount_point:         :return:         :rtype:
Perform a login request.          Associated request is typically to a path prefixed with "/v1/auth") and optionally stores the client token sent             in the resulting Vault response for use by the :py:meth:`hvac.adapters.Adapter` instance under the _adapater             Client attribute.          :param url: Path to send the authentication request to.         :type url: str | unicode         :param use_token: if True, uses the token in the response received from the auth request to set the "token"             attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.         :type use_token: bool         :param kwargs: Additional keyword arguments to include in the params sent with the request.         :type kwargs: dict         :return: The response of the auth request.         :rtype: requests.Response
POST /auth/<mount_point>/role/<role name>          :param role_name:         :type role_name:         :param mount_point:         :type mount_point:         :param kwargs:         :type kwargs:         :return:         :rtype:
DELETE /auth/<mount_point>/role/<role name>          :param role_name:         :type role_name:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /auth/<mount_point>/role/<role name>/role-id          :param role_name:         :type role_name:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/role/<role name>/role-id          :param role_name:         :type role_name:         :param role_id:         :type role_id:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /auth/<mount_point>/role/<role name>          :param role_name:         :type role_name:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/role/<role name>/secret-id          :param role_name:         :type role_name:         :param meta:         :type meta:         :param cidr_list:         :type cidr_list:         :param wrap_ttl:         :type wrap_ttl:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/role/<role name>/secret-id/lookup          :param role_name:         :type role_name:         :param secret_id:         :type secret_id:         :param mount_point:         :type mount_point:         :return:         :rtype:
LIST /auth/<mount_point>/role/<role name>/secret-id          :param role_name: Name of the AppRole.         :type role_name: str|unicode         :param mount_point: The "path" the AppRole auth backend was mounted on. Vault currently defaults to "approle".         :type mount_point: str|unicode         :return: The JSON response of the request.         :rtype: dict
POST /auth/<mount_point>/role/<role name>/secret-id-accessor/lookup          :param role_name:         :type role_name:         :param secret_id_accessor:         :type secret_id_accessor:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/role/<role name>/custom-secret-id          :param role_name:         :type role_name:         :param secret_id:         :type secret_id:         :param meta:         :type meta:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /auth/<mount_point>/login          :param role_id:         :type role_id:         :param secret_id:         :type secret_id:         :param mount_point:         :type mount_point:         :param use_token:         :type use_token:         :return:         :rtype:
POST /auth/<mount_point>/config          :param kubernetes_host: A host:port pair, or a URL to the base of the Kubernetes API server.         :type kubernetes_host: str.         :param kubernetes_ca_cert: PEM encoded CA cert for use by the TLS client used to talk with the Kubernetes API.         :type kubernetes_ca_cert: str.         :param token_reviewer_jwt: A service account JWT used to access the TokenReview API to validate other             JWTs during login. If not set the JWT used for login will be used to access the API.         :type token_reviewer_jwt: str.         :param pem_keys: Optional list of PEM-formated public keys or certificates used to verify the signatures of             Kubernetes service account JWTs. If a certificate is given, its public key will be extracted. Not every             installation of Kubernetes exposes these keys.         :type pem_keys: list.         :param mount_point: The "path" the k8s auth backend was mounted on. Vault currently defaults to "kubernetes".         :type mount_point: str.         :return: Will be an empty body with a 204 status code upon success         :rtype: requests.Response.
GET /auth/<mount_point>/config          :param mount_point: The "path" the k8s auth backend was mounted on. Vault currently defaults to "kubernetes".         :type mount_point: str.         :return: Parsed JSON response from the config GET request         :rtype: dict.
POST /auth/<mount_point>/role/:name          :param name: Name of the role.         :type name: str.         :param bound_service_account_names: List of service account names able to access this role. If set to "*" all             names are allowed, both this and bound_service_account_namespaces can not be "*".         :type bound_service_account_names: list.         :param bound_service_account_namespaces: List of namespaces allowed to access this role. If set to "*" all             namespaces are allowed, both this and bound_service_account_names can not be set to "*".         :type bound_service_account_namespaces: list.         :param ttl: The TTL period of tokens issued using this role in seconds.         :type ttl: str.         :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role.         :type max_ttl: str.         :param period: If set, indicates that the token generated using this role should never expire.             The token should be renewed within the duration specified by this value. At each renewal, the token's TTL will             be set to the value of this parameter.         :type period: str.         :param policies: Policies to be set on tokens issued using this role         :type policies: list.         :param mount_point: The "path" the k8s auth backend was mounted on. Vault currently defaults to "kubernetes".         :type mount_point: str.         :return: Will be an empty body with a 204 status code upon success         :rtype: requests.Response.
GET /auth/<mount_point>/role?list=true          :param mount_point: The "path" the k8s auth backend was mounted on. Vault currently defaults to "kubernetes".         :type mount_point: str.         :return: Parsed JSON response from the list roles GET request.         :rtype: dict.
DELETE /auth/<mount_point>/role/:role          :type role: Name of the role.         :param role: str.         :param mount_point: The "path" the k8s auth backend was mounted on. Vault currently defaults to "kubernetes".         :type mount_point: str.         :return: Will be an empty body with a 204 status code upon success.         :rtype: requests.Response.
POST /auth/<mount_point>/login          :param role: Name of the role against which the login is being attempted.         :type role: str.         :param jwt: Signed JSON Web Token (JWT) for authenticating a service account.         :type jwt: str.         :param use_token: if True, uses the token in the response received from the auth request to set the "token"             attribute on the current Client class instance.         :type use_token: bool.         :param mount_point: The "path" the k8s auth backend was mounted on. Vault currently defaults to "kubernetes".         :type mount_point: str.         :return: Parsed JSON response from the config POST request.         :rtype: dict.
POST /<mount_point>/keys/<name>          :param name:         :type name:         :param convergent_encryption:         :type convergent_encryption:         :param derived:         :type derived:         :param exportable:         :type exportable:         :param key_type:         :type key_type:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /<mount_point>/keys/<name>          :param name:         :type name:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /<mount_point>/keys?list=true          :param mount_point:         :type mount_point:         :return:         :rtype:
DELETE /<mount_point>/keys/<name>          :param name:         :type name:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/keys/<name>/config          :param name:         :type name:         :param min_decryption_version:         :type min_decryption_version:         :param min_encryption_version:         :type min_encryption_version:         :param deletion_allowed:         :type deletion_allowed:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/keys/<name>/rotate          :param name:         :type name:         :param mount_point:         :type mount_point:         :return:         :rtype:
GET /<mount_point>/export/<key_type>/<name>(/<version>)          :param name:         :type name:         :param key_type:         :type key_type:         :param version:         :type version:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/encrypt/<name>          :param name:         :type name:         :param plaintext:         :type plaintext:         :param context:         :type context:         :param key_version:         :type key_version:         :param nonce:         :type nonce:         :param batch_input:         :type batch_input:         :param key_type:         :type key_type:         :param convergent_encryption:         :type convergent_encryption:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/decrypt/<name>          :param name:         :type name:         :param ciphertext:         :type ciphertext:         :param context:         :type context:         :param nonce:         :type nonce:         :param batch_input:         :type batch_input:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/datakey/<type>/<name>          :param name:         :type name:         :param key_type:         :type key_type:         :param context:         :type context:         :param nonce:         :type nonce:         :param bits:         :type bits:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/random(/<data_bytes>)          :param data_bytes:         :type data_bytes:         :param output_format:         :type output_format:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/hash(/<algorithm>)          :param hash_input:         :type hash_input:         :param algorithm:         :type algorithm:         :param output_format:         :type output_format:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/hmac/<name>(/<algorithm>)          :param name:         :type name:         :param hmac_input:         :type hmac_input:         :param key_version:         :type key_version:         :param algorithm:         :type algorithm:         :param mount_point:         :type mount_point:         :return:         :rtype:
POST /<mount_point>/sign/<name>(/<algorithm>)          :param name:         :type name:         :param input_data:         :type input_data:         :param key_version:         :type key_version:         :param algorithm:         :type algorithm:         :param context:         :type context:         :param prehashed:         :type prehashed:         :param mount_point:         :type mount_point:         :param signature_algorithm:         :type signature_algorithm:         :return:         :rtype:
POST /<mount_point>/verify/<name>(/<algorithm>)          :param name:         :type name:         :param input_data:         :type input_data:         :param algorithm:         :type algorithm:         :param signature:         :type signature:         :param hmac:         :type hmac:         :param context:         :type context:         :param prehashed:         :type prehashed:         :param mount_point:         :type mount_point:         :param signature_algorithm:         :type signature_algorithm:         :return:         :rtype:
Add a new or update an existing policy.          Once a policy is updated, it takes effect immediately to all associated users.          Supported methods:             PUT: /sys/policy/{name}. Produces: 204 (empty body)          :param name: Specifies the name of the policy to create.         :type name: str | unicode         :param policy: Specifies the policy document.         :type policy: str | unicode | dict
GET /sys/mounts/<mount point>/tune          :param backend_type: Name of the secret engine. E.g. "aws".         :type backend_type: str | unicode         :param mount_point: Alternate argument for backend_type.         :type mount_point: str | unicode         :return: The specified mount's configuration.         :rtype: dict
Read the health status of Vault.          This matches the semantics of a Consul HTTP health check and provides a simple way to monitor the health of a         Vault instance.           :param standby_ok: Specifies if being a standby should still return the active status code instead of the             standby status code. This is useful when Vault is behind a non-configurable load balance that just wants a             200-level response.         :type standby_ok: bool         :param active_code: The status code that should be returned for an active node.         :type active_code: int         :param standby_code: Specifies the status code that should be returned for a standby node.         :type standby_code: int         :param dr_secondary_code: Specifies the status code that should be returned for a DR secondary node.         :type dr_secondary_code: int         :param performance_standby_code: Specifies the status code that should be returned for a performance standby             node.         :type performance_standby_code: int         :param sealed_code: Specifies the status code that should be returned for a sealed node.         :type sealed_code: int         :param uninit_code: Specifies the status code that should be returned for a uninitialized node.         :type uninit_code: int         :param method: Supported methods:             HEAD: /sys/health. Produces: 000 (empty body)             GET: /sys/health. Produces: 000 application/json         :type method: str | unicode         :return: The JSON response of the request.         :rtype: requests.Response
Read the high availability status and current leader instance of Vault.          Supported methods:             GET: /sys/leader. Produces: 200 application/json          :return: The JSON response of the request.         :rtype: dict
Configure MFA for a supported method.          This endpoint allows you to turn on multi-factor authentication with a given backend.         Currently only Duo is supported.          Supported methods:             POST: /auth/{mount_point}/mfa_config. Produces: 204 (empty body)          :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :param mfa_type: Enables MFA with given backend (available: duo)         :type mfa_type: str | unicode         :param force: If True, make the "mfa_config" request regardless of circumstance. If False (the default), verify             the provided mount_point is available and one of the types of methods supported by this feature.         :type force: bool         :return: The response of the configure MFA request.         :rtype: requests.Response
Configure the access keys and host for Duo API connections.          To authenticate users with Duo, the backend needs to know what host to connect to and must authenticate with an         integration key and secret key. This endpoint is used to configure that information.          Supported methods:             POST: /auth/{mount_point}/duo/access. Produces: 204 (empty body)          :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :param host: Duo API host         :type host: str | unicode         :param integration_key: Duo integration key         :type integration_key: Duo secret key         :param secret_key: The "path" the method/backend was mounted on.         :type secret_key: str | unicode         :return: The response of the configure_duo_access request.         :rtype: requests.Response
Configure Duo second factor behavior.          This endpoint allows you to configure how the original auth method username maps to the Duo username by         providing a template format string.          Supported methods:             POST: /auth/{mount_point}/duo/config. Produces: 204 (empty body)           :param mount_point: The "path" the method/backend was mounted on.         :type mount_point: str | unicode         :param push_info: A string of URL-encoded key/value pairs that provides additional context about the             authentication attempt in the Duo Mobile app         :type push_info: str | unicode         :param user_agent: User agent to connect to Duo (default "")         :type user_agent: str | unicode         :param username_format: Format string given auth method username as argument to create Duo username             (default '%s')         :type username_format: str | unicode         :return: The response of the configure_duo_behavior request.         :rtype: requests.Response
Runs the kik connection thread, which creates an encrypted (SSL based) TCP connection         to the kik servers.
Gets called when the TCP connection to kik's servers is done and we are connected.         Now we might initiate a login request or an auth request.
Updates the kik node and creates a new connection to kik servers.         This new connection will be initiated with another payload which proves         we have the credentials for a specific user. This is how authentication is done.          :param kik_node: The user's kik node (everything before '@' in JID).
Sends a login request with the given kik username and password          :param username: Your kik username         :param password: Your kik password         :param captcha_result: If this parameter is provided, it is the answer to the captcha given in the previous         login attempt.
Sends a register request to sign up a new user to kik with the given details.
Sends a text chat message to another person or a group with the given JID/username.          :param peer_jid: The Jabber ID for which to send the message (looks like username_ejs@talk.kik.com)                          If you don't know the JID of someone, you can also specify a kik username here.         :param message: The actual message body         :param bot_mention_jid: If an official bot is referenced, their jid must be embedded as mention for them         to respond.
Sends a read receipt for a previously sent message, to a specific user or group.          :param peer_jid: The JID of the user to which to send the receipt.         :param receipt_message_id: The message ID that the receipt is sent for         :param group_jid If the receipt is sent for a message that was sent in a group,                          this parameter should contain the group's JID
Sends a receipt indicating that a specific message was received, to another person.          :param peer_jid: The other peer's JID to send to receipt to         :param receipt_message_id: The message ID for which to generate the receipt
Updates the 'is typing' status of the bot during a conversation.          :param peer_jid: The JID that the notification will be sent to         :param is_typing: If true, indicates that we're currently typing, or False otherwise.
Requests basic information (username, display name, picture) of some peer JIDs.         When the information arrives, the callback on_peer_info_received() will fire.          :param peer_jids: The JID(s) for which to request the information. If you want to request information for                           more than one JID, supply a list of strings. Otherwise, supply a string
Calls the new format xiphias message to request user data such as profile creation date         and background picture URL.          :param peer_jids: one jid, or a list of jids
Like xiphias_get_users, but for aliases instead of jids.          :param alias_jids: one jid, or a list of jids
Changes the a group's name to something new          :param group_jid: The JID of the group whose name should be changed         :param new_name: The new name to give to the group
Adds someone to a group          :param group_jid: The JID of the group into which to add a user         :param peer_jid: The JID of the user to add
Kicks someone out of a group          :param group_jid: The group JID from which to remove the user         :param peer_jid: The JID of the user to remove
Bans a member from the group          :param group_jid: The JID of the relevant group         :param peer_jid: The JID of the user to ban
Undos a ban of someone from a group          :param group_jid: The JID of the relevant group         :param peer_jid: The JID of the user to un-ban from the gorup
Tries to join into a specific group, using a cryptographic token that was received earlier from a search          :param group_hashtag: The public hashtag of the group into which to join (like '#Music')         :param group_jid: The JID of the same group         :param join_token: a token that can be extracted in the callback on_group_search_response, after calling                            search_group()
Leaves a specific group          :param group_jid: The JID of the group to leave
Turns some group member into an admin          :param group_jid: The group JID for which the member will become an admin         :param peer_jid: The JID of user to turn into an admin
Turns an admin of a group into a regular user with no amidships capabilities.          :param group_jid: The group JID in which the rights apply         :param peer_jid: The admin user to demote         :return:
Adds multiple users to a specific group at once          :param group_jid: The group into which to join the users         :param peer_jids: a list (or a single string) of JIDs to add to the group
Searches for public groups using a query         Results will be returned using the on_group_search_response() callback          :param search_query: The query that contains some of the desired groups' name.
Checks if the given username is available for registration.         Results are returned in the on_username_uniqueness_received() callback          :param username: The username to check for its existence
Sets the profile picture          :param filename: The filename on disk of the image to set
In case a captcha was encountered, solves it using an element ID and a response parameter.         The stc_id can be extracted from a CaptchaElement, and the captcha result needs to be extracted manually with         a browser. Please see solve_captcha_wizard() for the steps needed to solve the captcha          :param stc_id: The stc_id from the CaptchaElement that was encountered         :param captcha_result: The answer to the captcha (which was generated after solved by a human)
Changes the display name          :param first_name: The first name         :param last_name: The last name
Changes the login password          :param new_password: The new login password to set for the account         :param email: The current email of the account
Changes the email of the current account          :param old_email: The current email         :param new_email: The new email to set
Serializes and sends the given XMPP element to kik servers          :param xmpp_element: The XMPP element to send         :return: The UUID of the element that was sent
Gets called whenever we get a whole new XML element from kik's servers.         :param data: The data received (bytes)
The 'k' element appears to be kik's connection-related stanza.         It lets us know if a connection or a login was successful or not.          :param k_element: The XML element we just received from kik.
a XMPP 'message' in the case of Kik is the actual stanza we receive when someone sends us a message         (weather groupchat or not), starts typing, stops typing, reads our message, etc.         Examples: http://slixmpp.readthedocs.io/api/stanza/message.html         :param xmpp_message: The XMPP 'message' element we received
The Kik Connection thread main function.         Initiates the asyncio loop and actually connects.
If the input is a string of cancellation word, finish the command session.
Decorator to register a function as a command.      :param name: command name (e.g. 'echo' or ('random', 'number'))     :param aliases: aliases of command name, for convenient access     :param permission: permission required by the command     :param only_to_me: only handle messages to me     :param privileged: can be run even when there is already a session     :param shell_like: use shell-like syntax to split arguments
Parse a command string (typically from a message).      :param bot: NoneBot instance     :param cmd_string: command string     :return: (Command object, current arg string)
Handle a message as a command.      This function is typically called by "handle_message".      :param bot: NoneBot instance     :param ctx: message context     :return: the message is handled as a command
Call a command internally.      This function is typically called by some other commands     or "handle_natural_language" when handling NLPResult object.      Note: If disable_interaction is not True, after calling this function,     any previous command session will be overridden, even if the command     being called here does not need further interaction (a.k.a asking     the user for more info).      :param bot: NoneBot instance     :param ctx: message context     :param name: command name     :param current_arg: command current argument string     :param args: command args     :param check_perm: should check permission before running command     :param disable_interaction: disable the command's further interaction     :return: the command is successfully called
Force kill current session of the given context,     despite whether it is running or not.      :param ctx: message context
Run the command in a given session.          :param session: CommandSession object         :param check_perm: should check permission before running         :param dry: just check any prerequisite, without actually running         :return: the command is finished (or can be run, given dry == True)
Check if the session has sufficient permission to         call the command.          :param session: CommandSession object         :return: the session has the permission
Decorator to register a function as the arguments parser of         the corresponding command.
Check if the session is expired or not.
Plain text part in the current argument, without any CQ codes.
Images (as list of urls) in the current argument.
Refill the session with a new message context.          :param ctx: new message context         :param current_arg: new command argument as a string
Get an argument with a given key.          If the argument does not exist in the current session,         a pause exception will be raised, and the caller of         the command will know it should keep the session for         further interaction with the user.          :param key: argument key         :param prompt: prompt to ask the user         :param arg_filters: argument filters for the next user input         :return: the argument value
Simply get a argument with given key.          Deprecated. Use `session.state.get()` instead.
Pause the session for further interaction.
Finish the session.
Finish the session and switch to a new (fake) message context.          The user may send another command (or another intention as natural         language) when interacting with the current session. In this case,         the session may not understand what the user is saying, so it         should call this method and pass in that message, then NoneBot will         handle the situation properly.
Approve the request.          :param remark: remark of friend (only works in friend request)
Reject the request.          :param reason: reason to reject (only works in group request)
Initialize NoneBot instance.      This function must be called at the very beginning of code,     otherwise the get_bot() function will return None and nothing     is gonna work properly.      :param config_object: configuration object
Run the NoneBot instance.
Calculate a unique id representing the current context.      mode:       default: one id for one context       group: one id for one group or discuss       user: one id for one user      :param ctx: the context dict     :param mode: unique id mode: "default", "group", or "user"     :param use_hash: use md5 to hash the id or not
Send a message ignoring failure by default.
Render an expression to message string.      :param expr: expression to render     :param escape_args: should escape arguments or not     :param args: positional arguments used in str.format()     :param kwargs: keyword arguments used in str.format()     :return: the rendered message
Load a module as a plugin.      :param module_name: name of module to import     :return: successful or not
Find all non-hidden modules or packages in a given directory,     and import them with the given module prefix.      :param plugin_dir: plugin directory to search     :param module_prefix: module prefix used while importing     :return: number of plugins successfully loaded
Load built-in plugins distributed along with "nonebot" package.
Extract all image urls from a message-like object.
Extract all numbers (integers and floats) from a message-like object.
Decorator to register a function as a natural language processor.      :param keywords: keywords to respond to, if None, respond to all messages     :param permission: permission required by the processor     :param only_to_me: only handle messages to me     :param only_short_message: only handle short messages     :param allow_empty_message: handle empty messages
Handle a message as natural language.      This function is typically called by "handle_message".      :param bot: NoneBot instance     :param ctx: message context     :return: the message is handled as natural language
Send a message ignoring failure by default.          :param message: message to send         :param at_sender: @ the sender if in group or discuss chat         :param ensure_private: ensure the message is sent to private chat         :param ignore_failure: if any CQHttpError raised, ignore it         :return: the result returned by CQHTTP
Convert a chinese text to boolean.      Examples:          是的 -> True         好的呀 -> True         不要 -> False         不用了 -> False         你好呀 -> None
Check if the context has the permission required.      :param bot: NoneBot instance     :param ctx: message context     :param permission_required: permission required     :return: the context has the permission
Validate any object to ensure it's not empty (is None or has no elements).
Validate any sized object to ensure the size/length     is in a given range [min_length, max_length].
Validate any string object to ensure it matches a given pattern.
Validate any object to ensure the result of applying     a boolean function to it is True.
Validate any comparable object to ensure it's between     `start` and `end` inclusively.
Returns nonzero if the source has what looks like a docstring that is     not at the beginning of the source.      A string will be considered a docstring if it is a STRING token with a     col offset of 0.
Extract AWS secrets from environment variables.
Extract AWS secrets from configuration files.      Read an ini-style configuration file and return a set with all found AWS     secret access keys.
Check if files contain AWS secrets.      Return a list of all files containing AWS secrets and keys found, with all     but the first four characters obfuscated to ease debugging.
python3.7+ breakpoint()
Sort a YAML file in alphabetical order, keeping blocks together.      :param lines: array of strings (without newlines)     :return: sorted array of strings
Parse and return a single block, popping off the start of `lines`.      If parsing a header block, we stop after we reach a line that is not a     comment. Otherwise, we stop after reaching an empty line.      :param lines: list of lines     :param header: whether we are parsing a header block     :return: list of lines that form the single block
Parse and return all possible blocks, popping off the start of `lines`.      :param lines: list of lines     :return: list of blocks, where each block is a list of lines
Returns a string representing the sort key of a block.      The sort key is the first YAML key we encounter, ignoring comments, and     stripping leading quotes.      >>> print(test)     # some comment     'foo': true     >>> first_key(test)     'foo'
Creates Redis Sentinel client.      `sentinels` is a list of sentinel nodes.
Execute Sentinel command.          It will be prefixed with SENTINEL automatically.
Returns a dictionary containing the specified masters state.
Returns a (host, port) pair for the given ``name``.
Returns a list of dictionaries containing each master's state.
Returns a list of slaves for ``name``.
Returns a list of sentinels for ``name``.
Add a new master to Sentinel to be monitored.
Remove a master from Sentinel's monitoring.
Set Sentinel monitoring parameters for a given master.
Force a failover of a named master.
Remove and get the first element in a list, or block until         one is available.          :raises TypeError: if timeout is not int         :raises ValueError: if timeout is less than 0
Remove and get the last element in a list, or block until one         is available.          :raises TypeError: if timeout is not int         :raises ValueError: if timeout is less than 0
Remove and get the last element in a list, or block until one         is available.          :raises TypeError: if timeout is not int         :raises ValueError: if timeout is less than 0
Get an element from a list by its index.          :raises TypeError: if index is not int
Inserts value in the list stored at key either before or         after the reference value pivot.
Removes and returns the first element of the list stored at key.
Insert all the specified values at the head of the list         stored at key.
Returns the specified elements of the list stored at key.          :raises TypeError: if start or stop is not int
Removes the first count occurrences of elements equal to value         from the list stored at key.          :raises TypeError: if count is not int
Sets the list element at index to value.          :raises TypeError: if index is not int
Trim an existing list so that it will contain only the specified         range of elements specified.          :raises TypeError: if start or stop is not int
Removes and returns the last element of the list stored at key.
Atomically returns and removes the last element (tail) of the         list stored at source, and pushes the element at the first element         (head) of the list stored at destination.
Insert all the specified values at the tail of the list         stored at key.
Watch the given keys to determine execution of the MULTI/EXEC block.
Returns MULTI/EXEC pipeline wrapper.          Usage:          >>> tr = redis.multi_exec()         >>> fut1 = tr.incr('foo')   # NO `await` as it will block forever!         >>> fut2 = tr.incr('bar')         >>> result = await tr.execute()         >>> result         [1, 1]         >>> await asyncio.gather(fut1, fut2)         [1, 1]
Returns :class:`Pipeline` object to execute bulk of commands.          It is provided for convenience.         Commands can be pipelined without it.          Example:          >>> pipe = redis.pipeline()         >>> fut1 = pipe.incr('foo') # NO `await` as it will block forever!         >>> fut2 = pipe.incr('bar')         >>> result = await pipe.execute()         >>> result         [1, 1]         >>> await asyncio.gather(fut1, fut2)         [1, 1]         >>> #         >>> # The same can be done without pipeline:         >>> #         >>> fut1 = redis.incr('foo')    # the 'INCRY foo' command already sent         >>> fut2 = redis.incr('bar')         >>> await asyncio.gather(fut1, fut2)         [2, 2]
Execute all buffered commands.          Any exception that is raised by any command is caught and         raised later when processing results.          Exceptions can also be returned in result if         `return_exceptions` flag is set to True.
Return a parsed Redis object or an exception         when something wrong happened.
Get the list of client connections.          Returns list of ClientInfo named tuples.
Get the current connection name.
Stop processing commands from clients for *timeout* milliseconds.          :raises TypeError: if timeout is not int         :raises ValueError: if timeout is less than 0
Set the current connection name.
Extract keys given a full Redis command.
Get array of specific Redis command details.
Get the value of a configuration parameter(s).          If called without argument will return all parameters.          :raises TypeError: if parameter is not string
Set a configuration parameter to the given value.
Suspend connection for timeout seconds.
Remove all keys from all databases.          :param async_op: lets the entire dataset to be freed asynchronously. \         Defaults to False
Remove all keys from the current database.          :param async_op: lets a single database to be freed asynchronously. \         Defaults to False
Get information and statistics about the server.          If called without argument will return default set of sections.         For available sections, see http://redis.io/commands/INFO          :raises ValueError: if section is invalid
Return the role of the server instance.          Returns named tuples describing role of the instance.         For fields information see http://redis.io/commands/role#output-format
Synchronously save the dataset to disk and then         shut down the server.
Make the server a slave of another instance,         or promote it as master.          Calling ``slaveof(None)`` will send ``SLAVEOF NO ONE``.          .. versionchanged:: v0.2.6            ``slaveof()`` form deprecated            in favour of explicit ``slaveof(None)``.
Returns the Redis slow queries log.
Post a JSON-encoded message to channel.
Switch connection to Pub/Sub mode and         subscribe to specified channels.          Arguments can be instances of :class:`~aioredis.Channel`.          Returns :func:`asyncio.gather()` coroutine which when done will return         a list of :class:`~aioredis.Channel` objects.
Unsubscribe from specific channels.          Arguments can be instances of :class:`~aioredis.Channel`.
Switch connection to Pub/Sub mode and         subscribe to specified patterns.          Arguments can be instances of :class:`~aioredis.Channel`.          Returns :func:`asyncio.gather()` coroutine which when done will return         a list of subscribed :class:`~aioredis.Channel` objects with         ``is_pattern`` property set to ``True``.
Unsubscribe from specific patterns.          Arguments can be instances of :class:`~aioredis.Channel`.
Lists the currently active channels.
Coroutine that waits for and returns a message.          :raises aioredis.ChannelClosedError: If channel is unsubscribed             and has no messages.
Shortcut to get JSON messages.
Same as get method but its native coroutine.          Usage example:          >>> async for msg in ch.iter():         ...     print(msg)
Create a channel.          Returns ``_Sender`` object implementing         :class:`~aioredis.abc.AbcChannel`.
Create a pattern channel.          Returns ``_Sender`` object implementing         :class:`~aioredis.abc.AbcChannel`.
Read-only channels dict.
Wait for and return pub/sub message from one of channels.          Return value is either:          * tuple of two elements: channel & message;          * tuple of three elements: pattern channel, (target channel & message);          * or None in case Receiver is not active or has just been stopped.          :raises aioredis.ChannelClosedError: If listener is stopped             and all messages have been received.
Blocks until new message appear.
Returns True if listener has any active subscription.
Returns async iterator.          Usage example:          >>> async for ch, msg in mpsc.iter():         ...     print(ch, msg)
Creates Redis Pool.      By default it creates pool of Redis instances, but it is     also possible to create pool of plain connections by passing     ``lambda conn: conn`` as commands_factory.      *commands_factory* parameter is deprecated since v0.2.9      All arguments are the same as for create_connection.      Returns RedisPool instance or a pool_cls if it is given.
Executes redis command in a free connection and returns         future waiting for result.          Picks connection from free pool and send command through         that connection.         If no connection is found, returns coroutine waiting for         free connection to execute command.
Executes Redis (p)subscribe/(p)unsubscribe commands.          ConnectionsPool picks separate connection for pub/sub         and uses it until explicitly closed or disconnected         (unsubscribing from all channels/patterns will leave connection          locked for pub/sub use).          There is no auto-reconnect for this PUB/SUB connection.          Returns asyncio.gather coroutine waiting for all channels/patterns         to receive answers.
Get free connection from pool.          Returns connection.
Acquire connection and execute command.
Changes db index for all free connections.          All previously acquired connections will be closed when released.
Acquires a connection from free pool.          Creates new connection if needed.
Returns used connection back into pool.          When returned connection has db index that differs from one in pool         the connection will be closed and dropped.         When queue of free connections is full the connection will be dropped.
Feed data to parser.
Encodes arguments into redis bulk-strings array.      Raises TypeError if any of args not of bytearray, bytes, float, int, or str     type.
Parse Redis connection URI.      Parse according to IANA specs:     * https://www.iana.org/assignments/uri-schemes/prov/redis     * https://www.iana.org/assignments/uri-schemes/prov/rediss      Also more rules applied:      * empty scheme is treated as unix socket path no further parsing is done.      * 'unix://' scheme is treated as unix socket path and parsed.      * Multiple query parameter values and blank values are considered error.      * DB number specified as path and as query parameter is considered error.      * Password specified in userinfo and as query parameter is       considered error.
Creates redis connection.      Opens connection to Redis server specified by address argument.     Address argument can be one of the following:     * A tuple representing (host, port) pair for TCP connections;     * A string representing either Redis URI or unix domain socket path.      SSL argument is passed through to asyncio.create_connection.     By default SSL/TLS is not used.      By default any timeout is applied at the connection stage, however     you can set a limitted time used trying to open a connection via     the `timeout` Kw.      Encoding argument can be used to decode byte-replies to strings.     By default no decoding is done.      Parser parameter can be used to pass custom Redis protocol parser class.     By default hiredis.Reader is used (unless it is missing or platform     is not CPython).      Return value is RedisConnection instance or a connection_cls if it is     given.      This function is a coroutine.
Response reader task.
Processes command results.
Processes pubsub messages.
Executes redis command and returns Future waiting for the answer.          Raises:         * TypeError if any of args can not be encoded as bytes.         * ReplyError on redis '-ERR' responses.         * ProtocolError when response can not be decoded meaning connection           is broken.         * ConnectionClosedError when either client or server has closed the           connection.
Executes redis (p)subscribe/(p)unsubscribe commands.          Returns asyncio.gather coroutine waiting for all channels/patterns         to receive answers.
True if connection is closed.
Change the selected database for the current connection.
Add one or more geospatial items in the geospatial index represented         using a sorted set.          :rtype: int
Returns members of a geospatial index as standard geohash strings.          :rtype: list[str or bytes or None]
Returns longitude and latitude of members of a geospatial index.          :rtype: list[GeoPoint or None]
Returns the distance between two members of a geospatial index.          :rtype: list[float or None]
Query a sorted set representing a geospatial index to fetch members         matching a given maximum distance from a point.          Return value follows Redis convention:          * if none of ``WITH*`` flags are set -- list of strings returned:              >>> await redis.georadius('Sicily', 15, 37, 200, 'km')             [b"Palermo", b"Catania"]          * if any flag (or all) is set -- list of named tuples returned:              >>> await redis.georadius('Sicily', 15, 37, 200, 'km',             ...                       with_dist=True)             [GeoMember(name=b"Palermo", dist=190.4424, hash=None, coord=None),              GeoMember(name=b"Catania", dist=56.4413, hash=None, coord=None)]          :raises TypeError: radius is not float or int         :raises TypeError: count is not int         :raises ValueError: if unit not equal ``m``, ``km``, ``mi`` or ``ft``         :raises ValueError: if sort not equal ``ASC`` or ``DESC``          :rtype: list[str] or list[GeoMember]
Query a sorted set representing a geospatial index to fetch members         matching a given maximum distance from a member.          Return value follows Redis convention:          * if none of ``WITH*`` flags are set -- list of strings returned:              >>> await redis.georadiusbymember('Sicily', 'Palermo', 200, 'km')             [b"Palermo", b"Catania"]          * if any flag (or all) is set -- list of named tuples returned:              >>> await redis.georadiusbymember('Sicily', 'Palermo', 200, 'km',             ...                               with_dist=True)             [GeoMember(name=b"Palermo", dist=190.4424, hash=None, coord=None),              GeoMember(name=b"Catania", dist=56.4413, hash=None, coord=None)]          :raises TypeError: radius is not float or int         :raises TypeError: count is not int         :raises ValueError: if unit not equal ``m``, ``km``, ``mi`` or ``ft``         :raises ValueError: if sort not equal ``ASC`` or ``DESC``          :rtype: list[str] or list[GeoMember]
Execute a Lua script server side.
Execute a Lua script server side by its SHA1 digest.
Check existence of scripts in the script cache.
Count set bits in a string.          :raises TypeError: if only start or end specified.
Perform bitwise AND operations between strings.
Perform bitwise OR operations between strings.
Perform bitwise XOR operations between strings.
Perform bitwise NOT operations between strings.
Find first bit set or clear in a string.          :raises ValueError: if bit is not 0 or 1
Decrement the integer value of a key by the given number.          :raises TypeError: if decrement is not int
Get the value of a key.
Returns the bit value at offset in the string value stored at key.          :raises TypeError: if offset is not int         :raises ValueError: if offset is less than 0
Get a substring of the string stored at a key.          :raises TypeError: if start or end is not int
Set the string value of a key and return its old value.
Increment the integer value of a key by the given amount.          :raises TypeError: if increment is not int
Increment the float value of a key by the given amount.          :raises TypeError: if increment is not int
Get the values of all the given keys.
Set multiple keys to multiple values or unpack dict to keys & values.          :raises TypeError: if len of args is not event number         :raises TypeError: if len of args equals 1 and it is not a dict
Set multiple keys to multiple values,         only if none of the keys exist.          :raises TypeError: if len of pairs is not event number
Set the value and expiration in milliseconds of a key.          :raises TypeError: if milliseconds is not int
Set the string value of a key.          :raises TypeError: if expire or pexpire is not int
Sets or clears the bit at offset in the string value stored at key.          :raises TypeError: if offset is not int         :raises ValueError: if offset is less than 0 or value is not 0 or 1
Set the value and expiration of a key.          If seconds is float it will be multiplied by 1000         coerced to int and passed to `psetex` method.          :raises TypeError: if seconds is neither int nor float
Set the value of a key, only if the key does not exist.
Overwrite part of a string at key starting at the specified offset.          :raises TypeError: if offset is not int         :raises ValueError: if offset less than 0
Delete a key.
Set a timeout on key.          if timeout is float it will be multiplied by 1000         coerced to int and passed to `pexpire` method.          Otherwise raises TypeError if timeout argument is not int.
Set expire timestamp on a key.          if timeout is float it will be multiplied by 1000         coerced to int and passed to `pexpireat` method.          Otherwise raises TypeError if timestamp argument is not int.
Returns all keys matching pattern.
Atomically transfer a key from a Redis instance to another one.
Atomically transfer keys from one Redis instance to another one.          Keys argument must be list/tuple of keys to migrate.
Move key from currently selected database to specified destination.          :raises TypeError: if db is not int         :raises ValueError: if db is less than 0
Remove the existing timeout on key.
Set a milliseconds timeout on key.          :raises TypeError: if timeout is not int
Set expire timestamp on key, timestamp in milliseconds.          :raises TypeError: if timeout is not int
Renames key to newkey.          :raises ValueError: if key == newkey
Renames key to newkey only if newkey does not exist.          :raises ValueError: if key == newkey
Creates a key associated with a value that is obtained via DUMP.
Incrementally iterate the keys space.          Usage example:          >>> match = 'something*'         >>> cur = b'0'         >>> while cur:         ...     cur, keys = await redis.scan(cur, match=match)         ...     for key in keys:         ...         print('Matched:', key)
Incrementally iterate the keys space using async for.          Usage example:          >>> async for key in redis.iscan(match='something*'):         ...     print('Matched:', key)
Sort the elements in a list, set or sorted set.
Delete a key asynchronously in another thread.
Delete one or more hash fields.
Determine if hash field exists.
Get the value of a hash field.
Get all the fields and values in a hash.
Increment the integer value of a hash field by the given number.
Increment the float value of a hash field by the given number.
Get all the fields in a hash.
Get the values of all the given fields.
Set multiple hash fields to multiple values.
Set multiple hash fields to multiple values.          dict can be passed as first positional argument:          >>> await redis.hmset_dict(         ...     'key', {'field1': 'value1', 'field2': 'value2'})          or keyword arguments can be used:          >>> await redis.hmset_dict(         ...     'key', field1='value1', field2='value2')          or dict argument can be mixed with kwargs:          >>> await redis.hmset_dict(         ...     'key', {'field1': 'value1'}, field2='value2')          .. note:: ``dict`` and ``kwargs`` not get mixed into single dictionary,            if both specified and both have same key(s) -- ``kwargs`` will win:             >>> await redis.hmset_dict('key', {'foo': 'bar'}, foo='baz')            >>> await redis.hget('key', 'foo', encoding='utf-8')            'baz'
Set the string value of a hash field.
Set the value of a hash field, only if the field does not exist.
Get all the values in a hash.
Incrementally iterate hash fields and associated values.
Incrementally iterate sorted set items using async for.          Usage example:          >>> async for name, val in redis.ihscan(key, match='something*'):         ...     print('Matched:', name, '->', val)
Add one or more members to a sorted set or update its score.          :raises TypeError: score not int or float         :raises TypeError: length of pairs is not even number
Count the members in a sorted set with scores         within the given values.          :raises TypeError: min or max is not float or int         :raises ValueError: if min greater than max
Increment the score of a member in a sorted set.          :raises TypeError: increment is not float or int
Count the number of members in a sorted set between a given         lexicographical range.          :raises TypeError: if min is not bytes         :raises TypeError: if max is not bytes
Return a range of members in a sorted set, by lexicographical range.          :raises TypeError: if min is not bytes         :raises TypeError: if max is not bytes         :raises TypeError: if both offset and count are not specified         :raises TypeError: if offset is not bytes         :raises TypeError: if count is not bytes
Remove one or more members from a sorted set.
Remove all members in a sorted set between the given         lexicographical range.          :raises TypeError: if min is not bytes         :raises TypeError: if max is not bytes
Remove all members in a sorted set within the given indexes.          :raises TypeError: if start is not int         :raises TypeError: if stop is not int
Remove all members in a sorted set within the given scores.          :raises TypeError: if min or max is not int or float
Return a range of members in a sorted set, by index,         with scores ordered from high to low.          :raises TypeError: if start or stop is not int
Return a range of members in a sorted set, by score,         with scores ordered from high to low.          :raises TypeError: if min or max is not float or int         :raises TypeError: if both offset and count are not specified         :raises TypeError: if offset is not int         :raises TypeError: if count is not int
Get the score associated with the given member in a sorted set.
Add multiple sorted sets and store result in a new key.
Incrementally iterate sorted sets elements and associated scores.
Incrementally iterate sorted set items using async for.          Usage example:          >>> async for val, score in redis.izscan(key, match='something*'):         ...     print('Matched:', val, ':', score)
Removes and returns up to count members with the lowest scores         in the sorted set stored at key.          :raises TypeError: if count is not int
Removes and returns up to count members with the highest scores         in the sorted set stored at key.          :raises TypeError: if count is not int
Convert a flat list of key/values into an OrderedDict
Parse messages returned by stream      Messages returned by XREAD arrive in the form:         [stream_name,             [                 [message_id, [key1, value1, key2, value2, ...]],                 ...             ],             ...         ]      Here we parse this into (with the help of the above parse_messages()     function):          [             [stream_name, message_id, OrderedDict(                 (key1, value1),                 (key2, value2),.                 ...             )],             ...         ]
Add a message to a stream.
Retrieve messages from a stream.
Retrieve messages from a stream in reverse order.
Perform a blocking read on the given stream          :raises ValueError: if the length of streams and latest_ids do                             not match
Perform a blocking read on the given stream as part of a consumer group          :raises ValueError: if the length of streams and latest_ids do                             not match
Create a consumer group
Set the latest ID for a consumer group
Delete a consumer group
Delete a specific consumer from a group
Get information on pending messages for a stream          Returned data will vary depending on the presence (or not)         of the start/stop/count parameters. For more details see:         https://redis.io/commands/xpending          :raises ValueError: if the start/stop/count parameters are only                             partially specified
Claim a message for a given consumer
Acknowledge a message for a given consumer group
Retrieve consumers of a consumer group
Retrieve the consumer groups for a stream
Retrieve information about the given stream.
Retrieve help regarding the ``XINFO`` sub-commands
Wraps up common functionality between ``xread()``         and ``xread_group()``          You should probably be using ``xread()`` or ``xread_group()`` directly.
Acquire a lock.         This method blocks until the lock is unlocked, then sets it to         locked and returns True.
Wake up the first waiter who isn't cancelled.
Assign new hash slots to receiving node.
Return the number of local keys in the specified hash slot.
Set hash slots as unbound in receiving node.
Remove a node from the nodes table.
Return local key names in the specified hash slot.
Force a node cluster to handshake with another node.
Reconfigure a node as a slave of the specified master node.
Reset a Redis Cluster node.
Set the configuration epoch in a new node.
Create SentinelPool.
Returns wrapper to master's pool for requested service.
Returns wrapper to slave's pool for requested service.
Execute sentinel command.
Discover sentinels and all monitored services within given timeout.          If no sentinels discovered within timeout: TimeoutError is raised.         If some sentinels were discovered but not all — it is ok.         If not all monitored services (masters/slaves) discovered         (or connections established) — it is ok.         TBD: what if some sentinels/services unreachable;
Try to connect to specified Sentinel returning either         connections pool or exception.
Perform Master discovery for specified service.
Perform Slave discovery for specified service.
Creates high-level Redis interface.      This function is a coroutine.
Creates high-level Redis interface.      This function is a coroutine.
Echo the given string.
Ping the server.          Accept optional echo message.
Add one or more members to a set.
Subtract multiple sets and store the resulting set in a key.
Intersect multiple sets and store the resulting set in a key.
Get all the members in a set.
Move a member from one set to another.
Remove and return one or multiple random members from a set.
Get one or multiple random members from a set.
Remove one or more members from a set.
Add multiple sets and store the resulting set in a key.
Incrementally iterate Set elements.
Incrementally iterate set elements using async for.          Usage example:          >>> async for val in redis.isscan(key, match='something*'):         ...     print('Matched:', val)
Adds the specified elements to the specified HyperLogLog.
Merge N different HyperLogLogs into a single one.
Scan command example.
Load a schema from ./schemas/``name``.json and return it.
Return the set of additional properties for the given ``instance``.      Weeds out properties that should have been validated by ``properties`` and     / or ``patternProperties``.      Assumes ``instance`` is dict-like already.
Create an error message for extra items or properties.
Create an error message for a failure to match the given types.      If the ``instance`` is an object and contains a ``name`` property, it will     be considered to be a description of that object and used as its type.      Otherwise the message is simply the reprs of the given ``types``.
isinstance() can accept a bunch of really annoying different types:         * a single type         * a tuple of types         * an arbitrary nested tree of tuples      Return a flattened tuple of the given argument.
A hack to make True and 1 and False and 0 unique for ``uniq``.
Check if all of a container's elements are unique.      Successively tries first to rely that the elements are hashable, then     falls back on them being sortable, and finally falls back on brute     force.
Register a decorated function as validating a new format.          Arguments:              format (str):                  The format that the decorated function will check.              raises (Exception):                  The exception(s) raised by the decorated function when an                 invalid instance is found.                  The exception object will be accessible as the                 `jsonschema.exceptions.ValidationError.cause` attribute of the                 resulting validation error.
Check whether the instance conforms to the given format.          Arguments:              instance (*any primitive type*, i.e. str, number, bool):                  The instance to check              format (str):                  The format that instance should conform to           Raises:              FormatError: if the instance does not conform to ``format``
Check whether the instance conforms to the given format.          Arguments:              instance (*any primitive type*, i.e. str, number, bool):                  The instance to check              format (str):                  The format that instance should conform to          Returns:              bool: whether it conformed
Generate newer-style type checks out of JSON-type-name-to-type mappings.      Arguments:          types (dict):              A mapping of type names to their Python types      Returns:          A dictionary of definitions to pass to `TypeChecker`
Register the decorated validator for a ``version`` of the specification.      Registered validators and their meta schemas will be considered when     parsing ``$schema`` properties' URIs.      Arguments:          version (str):              An identifier to use as the version's name      Returns:          callable: a class decorator to decorate the validator with the version
Create a new validator class.      Arguments:          meta_schema (collections.Mapping):              the meta schema for the new validator class          validators (collections.Mapping):              a mapping from names to callables, where each callable will             validate the schema property with the given name.              Each callable should take 4 arguments:                  1. a validator instance,                 2. the value of the property being validated within the                    instance                 3. the instance                 4. the schema          version (str):              an identifier for the version that this validator class will             validate. If provided, the returned validator class will have its             ``__name__`` set to include the version, and also will have             `jsonschema.validators.validates` automatically called for the             given version.          type_checker (jsonschema.TypeChecker):              a type checker, used when applying the :validator:`type` validator.              If unprovided, a `jsonschema.TypeChecker` will be created with             a set of default types typical of JSON Schema drafts.          default_types (collections.Mapping):              .. deprecated:: 3.0.0                  Please use the type_checker argument instead.              If set, it provides mappings of JSON types to Python types that             will be converted to functions and redefined in this object's             `jsonschema.TypeChecker`.          id_of (callable):              A function that given a schema, returns its ID.      Returns:          a new `jsonschema.IValidator` class
Create a new validator class by extending an existing one.      Arguments:          validator (jsonschema.IValidator):              an existing validator class          validators (collections.Mapping):              a mapping of new validator callables to extend with, whose             structure is as in `create`.              .. note::                  Any validator callables with the same name as an existing one                 will (silently) replace the old validator callable entirely,                 effectively overriding any validation done in the "parent"                 validator class.                  If you wish to instead extend the behavior of a parent's                 validator callable, delegate and call it directly in the new                 validator function by retrieving it using                 ``OldValidator.VALIDATORS["validator_name"]``.          version (str):              a version for the new validator class          type_checker (jsonschema.TypeChecker):              a type checker, used when applying the :validator:`type` validator.              If unprovided, the type checker of the extended             `jsonschema.IValidator` will be carried along.`      Returns:          a new `jsonschema.IValidator` class extending the one provided      .. note:: Meta Schemas          The new validator class will have its parent's meta schema.          If you wish to change or extend the meta schema in the new         validator class, modify ``META_SCHEMA`` directly on the returned         class. Note that no implicit copying is done, so a copy should         likely be made before modifying it, in order to not affect the         old validator.
Validate an instance under the given schema.          >>> validate([2, 3, 4], {"maxItems": 2})         Traceback (most recent call last):             ...         ValidationError: [2, 3, 4] is too long      :func:`validate` will first verify that the provided schema is itself     valid, since not doing so can lead to less obvious error messages and fail     in less obvious or consistent ways.      If you know you have a valid schema already, especially if you     intend to validate multiple instances with the same schema, you     likely would prefer using the `IValidator.validate` method directly     on a specific validator (e.g. ``Draft7Validator.validate``).       Arguments:          instance:              The instance to validate          schema:              The schema to validate with          cls (IValidator):              The class that will be used to validate the instance.      If the ``cls`` argument is not provided, two things will happen in     accordance with the specification. First, if the schema has a     :validator:`$schema` property containing a known meta-schema [#]_ then the     proper validator will be used.  The specification recommends that all     schemas contain :validator:`$schema` properties for this reason. If no     :validator:`$schema` property is found, the default validator class is     the latest released draft.      Any other provided positional and keyword arguments will be passed on when     instantiating the ``cls``.      Raises:          `jsonschema.exceptions.ValidationError` if the instance             is invalid          `jsonschema.exceptions.SchemaError` if the schema itself             is invalid      .. rubric:: Footnotes     .. [#] known by a validator registered with         `jsonschema.validators.validates`
Retrieve the validator class appropriate for validating the given schema.      Uses the :validator:`$schema` property that should be present in the given     schema to look up the appropriate validator class.      Arguments:          schema (collections.Mapping or bool):              the schema to look at          default:              the default to return if the appropriate validator class cannot be             determined.              If unprovided, the default is to return             the latest supported draft.
Construct a resolver from a JSON schema object.          Arguments:              schema:                  the referring schema          Returns:              `RefResolver`
Resolve the given ``ref`` and enter its resolution scope.          Exits the scope on exit of this context manager.          Arguments:              ref (str):                  The reference to resolve
Resolve a remote ``uri``.          If called directly, does not check the store first, but after         retrieving the document at the specified URI it will be saved in         the store if :attr:`cache_remote` is True.          .. note::              If the requests_ library is present, ``jsonschema`` will use it to             request the remote ``uri``, so that the correct encoding is             detected and used.              If it isn't, or if the scheme of the ``uri`` is not ``http`` or             ``https``, UTF-8 is assumed.          Arguments:              uri (str):                  The URI to resolve          Returns:              The retrieved document          .. _requests: https://pypi.org/project/requests/
The total number of errors in the entire tree, including children.
Install the plugin.      Arguments:          app (sphinx.application.Sphinx):              the Sphinx application context
Fetch a new specification or use the cache if it's current.      Arguments:          cache_path:              the path to a cached specification
Yeah.      It doesn't allow using a class because it does stupid stuff like try to set     attributes on the callable object rather than just keeping a dict.
Import the given name as a module, then walk the stack to determine whether     the failure was the module not existing, or some code in the module (for     example a dependent import) failing.  This can be helpful to determine     whether any actual application code was run.  For example, to distiguish     administrative error (entering the wrong module name), from programmer     error (writing buggy code in a module that fails to import).      @param importName: The name of the module to import.     @type importName: C{str}     @raise Exception: if something bad happens.  This can be any type of         exception, since nobody knows what loading some arbitrary code might         do.     @raise _NoModuleFound: if no module was found.
Retrieve a Python object by its fully qualified name from the global Python     module namespace.  The first part of the name, that describes a module,     will be discovered and imported.  Each subsequent part of the name is     treated as the name of an attribute of the object specified by all of the     name which came before it.  For example, the fully-qualified name of this     object is 'twisted.python.reflect.namedAny'.      @type name: L{str}     @param name: The name of the object to return.      @raise InvalidName: If the name is an empty string, starts or ends with         a '.', or is otherwise syntactically incorrect.      @raise ModuleNotFound: If the name is syntactically correct but the         module it specifies cannot be imported because it does not appear to         exist.      @raise ObjectNotFound: If the name is syntactically correct, includes at         least one '.', but the module it specifies cannot be imported because         it does not appear to exist.      @raise AttributeError: If an attribute of an object along the way cannot be         accessed, or a module along the way is not found.      @return: the Python object identified by 'name'.
Returns an (n,2) list of face indices.     Each pair of faces in the list shares an edge, making them adjacent.       Parameters     ----------     faces : (n, 3) int, or None         List of vertex indices representing triangles     mesh : Trimesh object         If passed will used cached edges instead of faces     return_edges : bool         Return the edges shared by adjacent faces      Returns     ---------     adjacency : (m,2) int         Indexes of faces that are adjacent     edges: (m,2) int         Only returned if return_edges is True         Indexes of vertices which make up the         edges shared by the adjacent faces      Examples     ----------     This is useful for lots of things such as finding     face- connected components:     >>> graph = nx.Graph()     >>> graph.add_edges_from(mesh.face_adjacency)     >>> groups = nx.connected_components(graph_connected)
Return the vertex index of the two vertices not in the shared     edge between two adjacent faces      Parameters     ----------     mesh : Trimesh object      Returns     -----------     vid_unshared : (len(mesh.face_adjacency), 2) int         Indexes of mesh.vertices
Compute an approximate radius between adjacent faces.      Parameters     --------------     mesh : trimesh.Trimesh      Returns     -------------     radii : (len(self.face_adjacency),) float         Approximate radius between faces         Parallel faces will have a value of np.inf     span :  (len(self.face_adjacency),) float         Perpendicular projection distance of two         unshared vertices onto the shared edge
Returns a networkx graph representing the vertices and     their connections in the mesh.      Parameters     ----------     mesh : Trimesh object      Returns     ---------     graph : networkx.Graph         Graph representing vertices and edges between         them where vertices are nodes and edges are edges      Examples     ----------     This is useful for getting nearby vertices for a given vertex,     potentially for some simple smoothing techniques.     >>> graph = mesh.vertex_adjacency_graph     >>> graph.neighbors(0)     > [1,3,4]
Given two sets of faces, find the edges which are in both sets.      Parameters     ---------     faces_a: (n,3) int, set of faces     faces_b: (m,3) int, set of faces      Returns     ---------     shared: (p, 2) int, set of edges
Given graph G and list of nodes, return the list of edges that     are connected to nodes
Find the list of parallel adjacent faces.      Parameters     ---------     mesh :  trimesh.Trimesh     engine : str        Which graph engine to use:        ('scipy', 'networkx', 'graphtool')      Returns     ---------     facets : sequence of (n,) int         Groups of face indexes of         parallel adjacent faces.
Split a mesh into multiple meshes from face connectivity.      If only_watertight is true, it will only return watertight meshes     and will attempt single triangle/quad repairs.      Parameters     ----------     mesh: Trimesh     only_watertight: if True, only return watertight components     adjacency: (n,2) list of face adjacency to override using the plain                adjacency calculated automatically.     engine: str, which engine to use. ('networkx', 'scipy', or 'graphtool')      Returns     ----------     meshes: list of Trimesh objects
Find groups of connected nodes from an edge list.      Parameters     -----------     edges:      (n,2) int, edges between nodes     nodes:      (m, ) int, list of nodes that exist     min_len:    int, minimum length of a component group to return     engine:     str, which graph engine to use.                 ('networkx', 'scipy', or 'graphtool')                 If None, will automatically choose fastest available.      Returns     -----------     components: (n,) sequence of lists, nodes which are connected
Label graph nodes from an edge list, using scipy.sparse.csgraph      Parameters     ----------     edges : (n, 2) int        Edges of a graph     node_count : int, or None         The largest node in the graph.      Returns     ---------     labels : (node_count,) int         Component labels for each node
Given a traversal as a list of nodes, split the traversal     if a sequential index pair is not in the given edges.      Parameters     --------------     edges : (n, 2) int        Graph edge indexes     traversal : (m,) int        Traversal through edges     edge_hash : (n,)        Edges sorted on axis=1 and        passed to grouping.hashable_rows      Returns     ---------------     split : sequence of (p,) int
Convert a traversal of a list of edges into a sequence of     traversals where every pair of consecutive node indexes     is an edge in a passed edge list      Parameters     -------------     traversals : sequence of (m,) int        Node indexes of traversals of a graph     edges : (n, 2) int        Pairs of connected node indexes     edges_hash : None, or (n,) int        Edges sorted along axis 1 then hashed        using grouping.hashable_rows      Returns     --------------     splits : sequence of (p,) int        Node indexes of connected traversals
Given an edge list, generate a sequence of ordered     depth first search traversals, using scipy.csgraph routines.      Parameters     ------------     edges : (n,2) int, undirected edges of a graph     mode :  str, 'bfs', or 'dfs'      Returns     -----------     traversals: (m,) sequence of (p,) int,                 ordered DFS or BFS traversals of the graph.
Given an edge list, return a boolean scipy.sparse.coo_matrix     representing the edges in matrix form.      Parameters     ------------     edges : (n,2) int       Edges of a graph     count : int       The total number of nodes in the graph       if None: count = edges.max() + 1     data : (n,) any       Assign data to each edge, if None will       be bool True for each specified edge      Returns     ------------     matrix: (count, count) scipy.sparse.coo_matrix       Sparse COO
Return a non- watertight version of the mesh which will     render nicely with smooth shading by disconnecting faces     at sharp angles to each other.      Parameters     ---------     mesh : trimesh.Trimesh       Source geometry     angle : float       Angle in radians, adjacent faces which have normals       below this angle will be smoothed      Returns     ---------     smooth : trimesh.Trimesh       Geometry with disconnected face patches
Parameters     ---------     edges : (n, 2) int       List of vertex indices     edges_sorted : (n, 2) int       Pass vertex indices sorted on axis 1 as a speedup      Returns     ---------     watertight : boolean       Whether every edge is shared by an even       number of faces     winding : boolean       Whether every shared edge is reversed
Turn a networkx graph into an SVG string, using graphviz dot.      Parameters     ----------     graph: networkx graph      Returns     ---------     svg: string, pictoral layout in SVG format
For a networkx MultiDiGraph, find all paths from a source node     to leaf nodes. This function returns edge instance numbers     in addition to nodes, unlike networkx.all_simple_paths.      Parameters     ---------------     G : networkx.MultiDiGraph       Graph to evaluate     source : hashable       Node to start traversal at     cutoff : int       Number of nodes to visit       If None will visit all nodes      Returns     ----------     traversals : (n,) list of [(node, edge instance index), ] paths       Traversals of the multigraph
Given a MultiDiGraph traversal, collect attributes along it.      Parameters     -------------     G:          networkx.MultiDiGraph     traversal:  (n) list of (node, instance) tuples     attrib:     dict key, name to collect. If None, will return all      Returns     -------------     collected: (len(traversal) - 1) list of attributes
Turn a set of keyword arguments into a transformation matrix.
Update a transform in the tree.          Parameters         ---------         frame_from : hashable object           Usually a string (eg 'world').           If left as None it will be set to self.base_frame         frame_to :  hashable object           Usually a string (eg 'mesh_0')         matrix : (4,4) float           Homogenous transformation matrix         quaternion :  (4,) float           Quaternion ordered [w, x, y, z]         axis : (3,) float           Axis of rotation         angle :  float           Angle of rotation, in radians         translation : (3,) float           Distance to translate         geometry : hashable           Geometry object name, e.g. 'mesh_0'
"Hash" of transforms          Returns         -----------         md5 : str           Approximate hash of transforms
Return a copy of the current TransformForest          Returns         ------------         copied: TransformForest
Export the current transform graph as a flattened
Export a transforms as the 'nodes' section of a GLTF dict.         Flattens tree.          Returns         --------         gltf : dict           with keys:                   'nodes': list of dicts
Export the current transforms as a list of edge tuples, with         each tuple having the format:         (node_a, node_b, {metadata})          Returns         -------         edgelist: (n,) list of tuples
Load transform data from an edge list into the current         scene graph.          Parameters         -------------         edgelist : (n,) tuples             (node_a, node_b, {key: value})         strict : bool             If true, raise a ValueError when a             malformed edge is passed in a tuple.
A list of every node in the graph.          Returns         -------------         nodes: (n,) array, of node names
The nodes in the scene graph with geometry attached.          Returns         ------------         nodes_geometry: (m,) array, of node names
Get the transform from one frame to another, assuming they are connected         in the transform tree.          If the frames are not connected a NetworkXNoPath error will be raised.          Parameters         ---------         frame_from: hashable object, usually a string (eg 'world').                     If left as None it will be set to self.base_frame         frame_to:   hashable object, usually a string (eg 'mesh_0')          Returns         ---------         transform:  (4,4) homogenous transformation matrix
Plot the graph layout of the scene.
Find a path between two frames, either from cached paths or         from the transform graph.          Parameters         ---------         frame_from: a frame key, usually a string                     eg, 'world'         frame_to:   a frame key, usually a string                     eg, 'mesh_0'          Returns         ----------         path: (n) list of frame keys               eg, ['mesh_finger', 'mesh_hand', 'world']
Check if connected planar points are counterclockwise.      Parameters     -----------     points: (n,2) float, connected points on a plane      Returns     ----------     ccw: bool, True if points are counterclockwise
Concatenate multiple paths into a single path.      Parameters     -------------     paths: list of Path, Path2D, or Path3D objects      Returns     -------------     concat: Path, Path2D, or Path3D object
Smooth a mesh in-place using laplacian smoothing     and Humphrey filtering.      Articles     "Improved Laplacian Smoothing of Noisy Surface Meshes"     J. Vollmer, R. Mencl, and H. Muller      Parameters     ------------     mesh : trimesh.Trimesh       Mesh to be smoothed in place     alpha : float       Controls shrinkage, range is 0.0 - 1.0       If 0.0, not considered       If 1.0, no smoothing     beta : float       Controls how aggressive smoothing is       If 0.0, no smoothing       If 1.0, full aggressiveness     iterations : int       Number of passes to run filter     laplacian_operator : None or scipy.sparse.coo.coo_matrix       Sparse matrix laplacian operator       Will be autogenerated if None
Smooth a mesh in-place using laplacian smoothing     and taubin filtering.      Articles     "Improved Laplacian Smoothing of Noisy Surface Meshes"     J. Vollmer, R. Mencl, and H. Muller      Parameters     ------------     mesh : trimesh.Trimesh       Mesh to be smoothed in place.     lamb : float       Controls shrinkage, range is 0.0 - 1.0     nu : float       Controls dilation, range is 0.0 - 1.0       Nu shall be between 0.0 < 1.0/lambda - 1.0/nu < 0.1     iterations : int       Number of passes to run the filter     laplacian_operator : None or scipy.sparse.coo.coo_matrix       Sparse matrix laplacian operator       Will be autogenerated if None
Calculate a sparse matrix for laplacian operations.      Parameters     -------------     mesh : trimesh.Trimesh       Input geometry     equal_weight : bool       If True, all neighbors will be considered equally       If False, all neightbors will be weighted by inverse distance      Returns     ----------     laplacian : scipy.sparse.coo.coo_matrix       Laplacian operator
Find the intersection between two lines.     Uses terminology from:     http://geomalgorithms.com/a05-_intersect-1.html      line 1:    P(s) = p_0 + sU     line 2:    Q(t) = q_0 + tV      Parameters     ---------     origins:      (2, d) float, points on lines (d in [2,3])     directions:   (2, d) float, direction vectors     plane_normal: (3, ) float, if not passed computed from cross      Returns     ---------     intersects:   boolean, whether the lines intersect.                   In 2D, false if the lines are parallel                   In 3D, false if lines are not coplanar     intersection: if intersects: (d) length point of intersection                   else:          None
Fit a circle, and reject the fit if:     * the radius is larger than tol.radius_min*scale or tol.radius_max*scale     * any segment spans more than tol.seg_angle     * any segment is longer than tol.seg_frac*scale     * the fit deviates by more than tol.radius_frac*radius     * the segments on the ends deviate from tangent by more than tol.tangent      Parameters     ---------     points:  (n, d) set of points which represent a path     prior:   (center, radius) tuple for best guess, or None if unknown     scale:   float, what is the overall scale of the set of points     verbose: boolean, if True output log.debug messages for the reasons              for fit rejection. Potentially generates hundreds of thousands of              messages so only suggested in manual debugging.      Returns     ---------     if fit is acceptable:         (center, radius) tuple     else:         None
Given a set of points, quickly determine if they represent     a circle or not.      Parameters     -------------     points: (n,2) float, points in space     scale:  float, scale of overall drawing     verbose: bool, print all fit messages or not      Returns     -------------     control: (3,2) float, points in space, OR               None, if not a circle
Given a set of points representing a path in space,     merge points which are colinear.      Parameters     ----------     points: (n, d) set of points (where d is dimension)     scale:  float, scale of drawing      Returns     ----------     merged: (j, d) set of points with colinear and duplicate              points merged, where (j < n)
Resample a path in space, smoothing along a b-spline.      Parameters     -----------     points: (n, dimension) float, points in space     smooth: float, smoothing amount     count:  number of samples in output     degree: int, degree of spline polynomial      Returns     ---------     resampled: (count, dimension) float, points in space
Create a spline entity from a curve in space      Parameters     -----------     points: (n, dimension) float, points in space     smooth: float, smoothing amount     count:  int, number of samples in result      Returns     ---------     entity: entities.BSpline object with points indexed at zero     control: (m, dimension) float, new vertices for entity
Merge colinear segments and fit circles.      Parameters     -----------     drawing: Path2D object, will not be modified.      Returns     -----------     simplified: Path2D with circles.
Replace discrete curves with b-spline or Arc and     return the result as a new Path2D object.      Parameters     ------------     path : trimesh.path.Path2D       Input geometry     smooth : float       Distance to smooth      Returns     ------------     simplified : Path2D       Consists of Arc and BSpline entities
A way to interface with openSCAD which is itself an interface     to the CGAL CSG bindings.     CGAL is very stable if difficult to install/use, so this function provides a     tempfile- happy solution for getting the basic CGAL CSG functionality.      Parameters     ---------     meshes: list of Trimesh objects     script: string of the script to send to scad.             Trimesh objects can be referenced in the script as             $mesh_0, $mesh_1, etc.
Run an operation on a set of meshes
Parse a loaded MTL file.      Parameters     -------------     mtl : str or bytes       Data from an MTL file      Returns     ------------     mtllibs : list of dict       Each dict has keys: newmtl, map_Kd, Kd
Loads an ascii Wavefront OBJ file_obj into kwargs     for the Trimesh constructor.      Vertices with the same position but different normals or uvs     are split into multiple vertices.      Colors are discarded.      Parameters     ----------     file_obj : file object       Containing a wavefront file     resolver : trimesh.visual.Resolver or None       For loading referenced files, like MTL or textures     kwargs : **       Passed to trimesh.Trimesh.__init__      Returns     ----------     loaded : dict       kwargs for Trimesh constructor
Export a mesh as a Wavefront OBJ file      Parameters     -----------     mesh: Trimesh object      Returns     -----------     export: str, string of OBJ format output
Get a resource from the trimesh/resources folder.      Parameters     -------------     name: str, location relative to trimesh/resources      Returns     -------------     resource: str, string of file data
Scaling factor for precision.
A cached version of the pyembree scene.
Return the location of where a ray hits a surface.          Parameters         ----------         ray_origins:    (n,3) float, origins of rays         ray_directions: (n,3) float, direction (vector) of rays           Returns         ---------         locations: (n) sequence of (m,3) intersection points         index_ray: (n,) int, list of ray index         index_tri: (n,) int, list of triangle (face) indexes
Find the triangles hit by a list of rays, including         optionally multiple hits along a single ray.          Parameters         ----------         ray_origins:      (n,3) float, origins of rays         ray_directions:   (n,3) float, direction (vector) of rays         multiple_hits:    bool, if True will return every hit along the ray                                 if False will only return first hit         return_locations: bool, should we return hit locations or not          Returns         ----------         index_tri: (m,) int, index of triangle the ray hit         index_ray: (m,) int, index of ray         locations: (m,3) float, locations in space
Find the index of the first triangle a ray hits.           Parameters         ----------         ray_origins:    (n,3) float, origins of rays         ray_directions: (n,3) float, direction (vector) of rays          Returns         ----------         triangle_index: (n,) int, index of triangle ray hit, or -1 if not hit
Check if a list of rays hits the surface.           Parameters         ----------         ray_origins:    (n,3) float, origins of rays         ray_directions: (n,3) float, direction (vector) of rays          Returns         ----------         hit:            (n,) bool, did each ray hit the surface
Load a 3MF formatted file into a Trimesh scene.      Parameters     ------------     file_obj:       file object      Returns     ------------     kwargs: dict, with keys 'graph', 'geometry', 'base_frame'
Extract a homogenous transform from a dictionary.      Parameters     ------------     attrib: dict, optionally containing 'transform'      Returns     ------------     transform: (4, 4) float, homogeonous transformation
Check input ranges, convert them to vector form,     and get a fixed precision integer version of them.      Parameters     --------------     a : (2, ) or (2, n) float       Start and end of a 1D interval     b : (2, ) or (2, n) float       Start and end of a 1D interval     digits : int       How many digits to consider      Returns     --------------     a : (2, n) float       Ranges as vector     b : (2, n) float       Ranges as vector     a_int : (2, n) int64       Ranges rounded to digits, as vector     b_int : (2, n) int64       Ranges rounded to digits, as vector     is_1D : bool       If True, input was single pair of ranges
Given a pair of ranges, merge them in to     one range if they overlap at all      Parameters     --------------     a : (2, ) float       Start and end of a 1D interval     b : (2, ) float       Start and end of a 1D interval     digits : int       How many digits to consider      Returns     --------------     intersects : bool or (n,) bool       Indicates if the ranges overlap at all     new_range : (2, ) or (2, 2) float       The unioned range from the two inputs,       or both of the original ranges if not overlapping
Get an MD5 for a geometry object      Parameters     ------------     geometry : object      Returns     ------------     MD5 : str
Render a preview of a scene to a PNG.      Parameters     ------------     scene : trimesh.Scene       Geometry to be rendered     resolution : (2,) int       Resolution in pixels     kwargs : **       Passed to SceneViewer      Returns     ---------     render : bytes       Image in PNG format
Add a geometry to the viewer.          Parameters         --------------         name : hashable           Name that references geometry         geometry : Trimesh, Path2D, Path3D, PointCloud           Geometry to display in the viewer window         kwargs **           Passed to rendering.convert_to_vertexlist
Set view to the default view.          Parameters         --------------         flags : None or dict           If any view key passed override the default           e.g. {'cull': False}
Perform the magic incantations to create an         OpenGL scene using pyglet.
Take the lights defined in scene.lights and         apply them as openGL lights.
Toggle a rendered XYZ/RGB axis marker:         off, world frame, every frame
Check the view flags, and call required GL functions.
Handle resized windows.
Set the start point of the drag.
Pan or rotate the view.
Zoom the view.
Call appropriate functions given key presses.
Run the actual draw calls.
Save the current color buffer to a file object         in PNG format.          Parameters         -------------         file_obj: file name, or file- like object
Calculate the conversion from one set of units to another.      Parameters     ---------     current : str         Unit system values are in now (eg 'millimeters')     desired : str         Unit system we'd like values in (eg 'inches')      Returns     ---------     conversion : float         Number to multiply by to put values into desired units
Try to extract hints from metadata and if that fails     guess based on the object scale.       Parameters     ------------     obj: object         Has attributes 'metadata' (dict) and 'scale' (float)     guess : bool         If metadata doesn't indicate units, guess from scale      Returns     ------------     units: str         A guess of what the units might be
Given an object with scale and units try to scale     to different units via the object's `apply_scale`.      Parameters     ---------     obj :  object         With apply_scale method (i.e. Trimesh, Path2D, etc)     desired : str         Units desired (eg 'inches')     guess:   bool         Whether we are allowed to guess the units         if they are not specified.
Export a Path object to a file- like object, or to a filename      Parameters     ---------     file_obj:  None, str, or file object       A filename string or a file-like object     file_type: None or str       File type, e.g.: 'svg', 'dxf'     kwargs : passed to loader      Returns     ---------     exported : str or bytes       Data exported
Export a path as a dict of kwargs for the Path constructor.
Write a string to a file.     If file_obj isn't specified, return the string      Parameters     ---------     export: a string of the export data     file_obj: a file-like object or a filename
Sample the surface of a mesh, returning the specified     number of points      For individual triangle sampling uses this method:     http://mathworld.wolfram.com/TrianglePointPicking.html      Parameters     ---------     mesh: Trimesh object     count: number of points to return      Returns     ---------     samples: (count,3) points in space on the surface of mesh     face_index: (count,) indices of faces for each sampled point
Use rejection sampling to produce points randomly distributed     in the volume of a mesh.      Parameters     ----------     mesh: Trimesh object     count: int, number of samples desired      Returns     ----------     samples: (n,3) float, points in the volume of the mesh.               where: n <= count
Return random samples inside a rectangular volume.      Parameters     ----------     extents:   (3,) float, side lengths of rectangular solid     count:     int, number of points to return     transform: (4,4) float, transformation matrix      Returns     ---------     samples: (count, 3) float, points in volume
Sample the surface of a mesh, returning samples which are     approximately evenly spaced.       Parameters     ---------     mesh: Trimesh object     count: number of points to return      Returns     ---------     samples: (count,3) points in space on the surface of mesh     face_index: (count,) indices of faces for each sampled point
Correctly pick random points on the surface of a unit sphere      Uses this method:     http://mathworld.wolfram.com/SpherePointPicking.html      Parameters     ----------     count: int, number of points to return      Returns     ----------     points: (count,3) float, list of random points on a unit sphere
For 3D line segments defined by two points, turn     them in to an origin defined as the closest point along     the line to the zero origin as well as a direction vector     and start and end parameter.      Parameters     ------------     segments : (n, 2, 3) float        Line segments defined by start and end points      Returns     --------------     origins : (n, 3) float        Point on line closest to [0, 0, 0]     vectors : (n, 3) float        Unit line directions     parameters : (n, 2) float        Start and end distance pairs for each line
Convert a parametric line segment representation to     a two point line segment representation      Parameters     ------------     origins : (n, 3) float        Line origin point     vectors : (n, 3) float        Unit line directions     parameters : (n, 2) float        Start and end distance pairs for each line      Returns     --------------     segments : (n, 2, 3) float        Line segments defined by start and end points
Find pairs of segments which are colinear.      Parameters     -------------     segments : (n, 2, (2, 3)) float       Two or three dimensional line segments     radius : float       Maximum radius line origins can differ       and be considered colinear     angle : float       Maximum angle in radians segments can       differ and still be considered colinear     length : None or float       If specified, will additionally require       that pairs have a mean vertex distance less       than this value from each other to qualify.      Returns     ------------     pairs : (m, 2) int       Indexes of segments which are colinear
Find any points that lie on a segment (not an endpoint)     and then split that segment into two segments.      We are basically going to find the distance between     point and both segment vertex, and see if it is with     tolerance of the segment length.      Parameters     --------------     segments : (n, 2, (2, 3) float       Line segments in space     points : (n, (2, 3)) float       Points in space     atol : float       Absolute tolerance for distances      Returns     -------------     split : (n, 2, (3 | 3) float       Line segments in space, split at vertices
Find unique line segments.      Parameters     ------------     segments : (n, 2, (2|3)) float       Line segments in space     digits : int       How many digits to consider when merging vertices      Returns     -----------     unique : (m, 2, (2|3)) float       Segments with duplicates merged
Find the overlap of two parallel line segments.      Parameters     ------------     origins : (2, 3) float        Origin points of lines in space     vectors : (2, 3) float        Unit direction vectors of lines     params : (2, 2) float        Two (start, end) distance pairs      Returns     ------------     length : float        Overlapping length     overlap : (n, 2, 3) float        Line segments for overlapping distance
Load a COLLADA (.dae) file into a list of trimesh kwargs.      Parameters     ----------     file_obj : file object       Containing a COLLADA file     resolver : trimesh.visual.Resolver or None       For loading referenced files, like texture images     kwargs : **       Passed to trimesh.Trimesh.__init__      Returns     -------     loaded : list of dict       kwargs for Trimesh constructor
Export a mesh or a list of meshes as a COLLADA .dae file.      Parameters     -----------     mesh: Trimesh object or list of Trimesh objects         The mesh(es) to export.      Returns     -----------     export: str, string of COLLADA format output
Recursively parse COLLADA scene nodes.
Load a texture from a file into a PIL image.
Turn a COLLADA effect into a trimesh material.
Turn a trimesh material into a COLLADA material.
Load a ZAE file, which is just a zipped DAE file.      Parameters     -------------     file_obj : file object       Contains ZAE data     resolver : trimesh.visual.Resolver       Resolver to load additional assets     kwargs : dict       Passed to load_collada      Returns     ------------     loaded : dict       Results of loading
Load an OFF file into the kwargs for a Trimesh constructor       Parameters     ----------     file_obj : file object                  Contains an OFF file      Returns     ----------     loaded : dict               kwargs for Trimesh constructor
Load a dict packed with msgpack into kwargs for     a Trimesh constructor      Parameters     ----------     blob : bytes       msgpack packed dict containing       keys 'vertices' and 'faces'      Returns     ----------     loaded : dict      Keyword args for Trimesh constructor, aka      mesh=trimesh.Trimesh(**loaded)
Load multiple input types into kwargs for a Trimesh constructor.     Tries to extract keys:     'faces'     'vertices'     'face_normals'     'vertex_normals'      Parameters     ----------     data: accepts multiple forms           -dict: has keys for vertices and faces as (n,3) numpy arrays           -dict: has keys for vertices/faces (n,3) arrays encoded as dicts/base64                  with trimesh.util.array_to_encoded/trimesh.util.encoded_to_array           -str:  json blob as dict with either straight array or base64 values           -file object: json blob of dict     file_type: not used      Returns     -----------     loaded: dict with keys             -vertices: (n,3) float             -faces:    (n,3) int             -face_normals: (n,3) float (optional)
Parameters     ----------     points : (order, dimension) float       Control points of the bezier curve       For a 2D cubic bezier, order=3, dimension=2     count : int, or None       Number of segments     scale : float       Scale of curve     Returns     ----------     discrete: (n,d) list of points, a polyline representation               of the bezier curve which respects constants.RES_LENGTH
Given a B-Splines control points and knot vector, return     a sampled version of the curve.      Parameters     ----------     control : (o, d) float       Control points of the b- spline     knots : (j,) float       B-spline knots     count : int       Number of line segments to discretize the spline       If not specified will be calculated as something reasonable      Returns     ----------     discrete : (count, dimension) float        Points on a polyline version of the B-spline
Return all binomial coefficients for a given order.      For n > 5, scipy.special.binom is used, below we hardcode     to avoid the scipy.special dependency.      Parameters     --------------     n : int       Order      Returns     ---------------     binom : (n + 1,) int       Binomial coefficients of a given order
Apply basic cleaning functions to the Path object, in- place.
If entities have a layer defined, return it.          Returns         ---------         layers: (len(entities), ) list of str
A CRC of the current vertices and entities.          Returns         ------------         crc: int, CRC of entity points and vertices
An MD5 hash of the current vertices and entities.          Returns         ------------         md5: str, two appended MD5 hashes
Sequence of closed paths, encoded by entity index.          Returns         ---------         paths: (n,) sequence of (*,) int referencing self.entities
List of entities that aren't included in a closed path          Returns         ----------         dangling: (n,) int, index of self.entities
A KDTree object holding the vertices of the path.          Returns         ----------         kdtree: scipy.spatial.cKDTree object holding self.vertices
What is a representitive number that reflects the magnitude         of the world holding the paths, for numerical comparisons.          Returns         ----------         scale : float             Approximate size of the world holding this path
Return the axis aligned bounding box of the current path.          Returns         ----------         bounds: (2, dimension) float, (min, max) coordinates
Turn every multi- segment entity into single segment entities, in- place
Are all entities connected to other entities.          Returns         -----------         closed : bool           Every entity is connected at its ends
Get a list of which vertex indices are nodes,         which are either endpoints or points where the         entity makes a direction change.          Returns         --------------         nodes : (n, 2) int           Indexes of self.vertices which are nodes
Apply a transformation matrix to the current path in- place          Parameters         -----------         transform : (d+1, d+1) float           Homogenous transformation for vertices
Apply a transformation matrix to the current path in- place          Parameters         -----------         scale : float or (3,) float           Scale to be applied to mesh
Apply a transformation matrix to the current path in- place          Parameters         -----------         offset : float or (3,) float           Translation to be applied to mesh
Translate so that every vertex is positive in the current         mesh is positive.          Returns         -----------         matrix : (dimension + 1, dimension + 1) float           Homogenous transformation that was applied           to the current Path object.
Merges vertices which are identical and replace references.          Parameters         --------------         digits : None, or int           How many digits to consider when merging vertices          Alters         -----------         self.entities : entity.points re- referenced         self.vertices : duplicates removed
Replace the vertex index references in every entity.          Parameters         ------------         mask : (len(self.vertices), ) int           Contains new vertex indexes          Alters         ------------         entity.points in self.entities           Replaced by mask[entity.points]
Remove entities by index.          Parameters         -----------         entity_ids : (n,) int           Indexes of self.entities to remove
Remove entities which declare themselves invalid          Alters         ----------         self.entities: shortened
Remove entities that are duplicated          Alters         -------         self.entities: length same or shorter
Which vertices are referenced by an entity.          Returns         -----------         referenced_vertices: (n,) int, indexes of self.vertices
Removes all vertices which aren't used by an entity.          Alters         ---------         self.vertices: reordered and shortened         self.entities: entity.points references updated
Given a list of entities, return a list of connected points.          Parameters         -----------         path: (n,) int, indexes of self.entities          Returns         -----------         discrete: (m, dimension)
A sequence of connected vertices in space, corresponding to         self.paths.          Returns         ---------         discrete : (len(self.paths),)             A sequence of (m*, dimension) float
Export the path to a file object or return data.          Parameters         ---------------         file_obj : None, str, or file object           File object or string to export to         file_type : None or str           Type of file: dxf, dict, svg          Returns         ---------------         exported : bytes or str           Exported as specified type
Get a copy of the current mesh          Returns         ---------         copied: Path object, copy of self
Check to see if current vectors are all coplanar.          If they are, return a Path2D and a transform which will         transform the 2D representation back into 3 dimensions          Parameters         -----------         to_2D: (4,4) float             Homogenous transformation matrix to apply,             If not passed a plane will be fitted to vertices.         normal: (3,) float, or None            Approximate normal of direction of plane            If to_2D is not specified sign            will be applied to fit plane normal         check:  bool             If True: Raise a ValueError if             points aren't coplanar          Returns         -----------         planar : trimesh.path.Path2D                    Current path transformed onto plane         to_3D :  (4,4) float                    Homeogenous transformation to move planar                    back into 3D space
Plot closed curves          Parameters         ------------         show : bool            If False will not execute matplotlib.pyplot.show
Plot discrete version of entities without regards         for connectivity.          Parameters         -------------         show : bool            If False will not execute matplotlib.pyplot.show
Plot the current Path2D object using matplotlib.
Transform the current path so that its OBB is axis aligned         and OBB center is at the origin.
Rasterize a Path2D object into a boolean image ("mode 1").          Parameters         ------------         pitch:      float, length in model space of a pixel edge         origin:     (2,) float, origin position in model space         resolution: (2,) int, resolution in pixel space         fill:       bool, if True will return closed regions as filled         width:      int, if not None will draw outline this wide (pixels)          Returns         ------------         raster: PIL.Image object, mode 1
Use rejection sampling to generate random points inside a         polygon.          Parameters         -----------         count   : int                     Number of points to return                     If there are multiple bodies, there will                     be up to count * bodies points returned         factor  : float                     How many points to test per loop                     IE, count * factor         max_iter : int,                     Maximum number of intersection loops                     to run, total points sampled is                     count * factor * max_iter          Returns         -----------         hit : (n, 2) float                Random points inside polygon
Convert 2D path to 3D path on the XY plane.          Parameters         -------------         transform : (4, 4) float             If passed, will transform vertices.             If not passed and 'to_3D' is in metadata             that transform will be used.          Returns         -----------         path_3D: Path3D version of current path
A list of shapely.geometry.Polygon objects with interiors created         by checking which closed polygons enclose which other polygons.          Returns         ---------         full : (len(self.root),) shapely.geometry.Polygon             Polygons containing interiors
Return the area of the polygons interior.          Returns         ---------         area: float, total area of polygons minus interiors
The total discretized length of every entity.          Returns         --------         length: float, summed length of every entity
Extrude the current 2D path into a 3D mesh.          Parameters         ----------         height: float, how far to extrude the profile         kwargs: passed directly to meshpy.triangle.build:                 triangle.build(mesh_info,                                verbose=False,                                refinement_func=None,                                attributes=False,                                volume_constraints=True,                                max_volume=None,                                allow_boundary_steiner=True,                                allow_volume_steiner=True,                                quality_meshing=True,                                generate_edges=None,                                generate_faces=False,                                min_angle=None)         Returns         --------         mesh: trimesh object representing extruded polygon
Create a region- aware triangulation of the 2D path.          Parameters         -------------         **kwargs : dict           Passed to trimesh.creation.triangulate_polygon          Returns         -------------         vertices : (n, 2) float           2D vertices of triangulation         faces : (n, 3) int           Indexes of vertices for triangles
Find the approximate medial axis based         on a voronoi diagram of evenly spaced points on the         boundary of the polygon.          Parameters         ----------         resolution : None or float           Distance between each sample on the polygon boundary         clip : None, or (2,) float           Min, max number of samples          Returns         ----------         medial : Path2D object           Contains only medial axis of Path
Given an index of self.paths find other paths which         overlap with that path.          Parameters         -----------         path_id : int           Index of self.paths         include_self : bool           Should the result include path_id or not          Returns         -----------         path_ids :  (n, ) int           Indexes of self.paths that overlap input path_id
Convert paths into b-splines.          Parameters         -----------         path_indexes : (n) int           List of indexes of self.paths to convert         smooth : float           How much the spline should smooth the curve          Returns         ------------         simplified: Path2D object
Plot the closed curves of the path.
Plot the entities of the path, with no notion of topology
A unique identifier for the path.          Returns         ---------         identifier: (5,) float, unique identifier
Return an MD5 of the identifier
Returns         ----------         path_valid: (n,) bool, indexes of self.paths self.polygons_closed                          which are valid polygons
Networkx DiGraph of polygon enclosure
A dictionary of path indexes which are 'shell' paths, and values         of 'hole' paths.          Returns         ----------         corresponding: dict, {index of self.paths of shell : [indexes of holes]}
A decorator for methods which will time the method     and then emit a log.debug message with the method name     and how long it took to execute.
For each point find nearby faces relatively quickly.      The closest point on the mesh to the queried point is guaranteed to be     on one of the faces listed.      Does this by finding the nearest vertex on the mesh to each point, and     then returns all the faces that intersect the axis aligned bounding box     centered at the queried point and extending to the nearest vertex.      Parameters     ----------     mesh : Trimesh object     points : (n,3) float , points in space      Returns     -----------     candidates : (points,) int, sequence of indexes for mesh.faces
Given a mesh and a list of points find the closest point     on any triangle.      Does this by constructing a very large intermediate array and     comparing every point to every triangle.      Parameters     ----------     mesh : Trimesh       Takes mesh to have same interfaces as `closest_point`     points : (m, 3) float       Points in space      Returns     ----------     closest : (m, 3) float       Closest point on triangles for each point     distance : (m,) float       Distances between point and triangle     triangle_id : (m,) int       Index of triangle containing closest point
Given a mesh and a list of points, find the closest point on any triangle.      Parameters     ----------     mesh   : Trimesh object     points : (m,3)   float, points in space      Returns     ----------     closest     : (m,3) float, closest point on triangles for each point     distance    : (m,)  float, distance     triangle_id : (m,)  int, index of triangle containing closest point
Find the signed distance from a mesh to a list of points.      * Points OUTSIDE the mesh will have NEGATIVE distance     * Points within tol.merge of the surface will have POSITIVE distance     * Points INSIDE the mesh will have POSITIVE distance      Parameters     -----------     mesh   : Trimesh object     points : (n,3) float, list of points in space      Returns     ----------     signed_distance : (n,3) float, signed distance from point to mesh
Find the lengths of the longest rays which do not intersect the mesh     cast from a list of points in the provided directions.      Parameters     -----------     points : (n,3) float, list of points in space     directions : (n,3) float, directions of rays      Returns     ----------     signed_distance : (n,) float, length of rays
Find the center and radius of the sphere which is tangent to     the mesh at the given point and at least one more point with no     non-tangential intersections with the mesh.      Masatomo Inui, Nobuyuki Umezu & Ryohei Shimane (2016)     Shrinking sphere:     A parallel algorithm for computing the thickness of 3D objects,     Computer-Aided Design and Applications, 13:2, 199-207,     DOI: 10.1080/16864360.2015.1084186      Parameters     ----------     points : (n,3) float, list of points in space     inwards : bool, whether to have the sphere inside or outside the mesh     normals : (n,3) float, normals of the mesh at the given points               None, compute this automatically.      Returns     ----------     centers : (n,3) float, centers of spheres     radii : (n,) float, radii of spheres
Find the thickness of the mesh at the given points.      Parameters     ----------     points : (n,3) float, list of points in space     exterior : bool, whether to compute the exterior thickness                      (a.k.a. reach)     normals : (n,3) float, normals of the mesh at the given points               None, compute this automatically.     method : string, one of 'max_sphere' or 'ray'      Returns     ----------     thickness : (n,) float, thickness
Given a set of points, return the closest vertex index to each point          Parameters         ----------         points : (n,3) float, list of points in space          Returns         ----------         distance  : (n,) float, distance from source point to vertex         vertex_id : (n,) int, index of mesh.vertices which is closest
Permutate a mesh, record the maximum it deviates from the original mesh     and the resulting value of an identifier function.      Parameters     ----------     mesh:     Trimesh object     function:     function which takes a single mesh as an argument                   and returns an (n,) float vector     subdivisions: the maximum number of times to subdivide the mesh     count: int, number of times to permutate each subdivision step      Returns     -----------     identifiers: numpy array of identifiers
Get a list of single- body meshes to test identifiers on.      Parameters     ------------     path:   str, location of models     cutoff: int, number of meshes to stop loading at      Returns     ------------     meshes: (n,) list of Trimesh objects
Align a mesh with another mesh or a PointCloud using     the principal axes of inertia as a starting point which     is refined by iterative closest point.      Parameters     ------------     mesh : trimesh.Trimesh object       Mesh to align with other     other : trimesh.Trimesh or (n, 3) float       Mesh or points in space     samples : int       Number of samples from mesh surface to align     scale : bool       Allow scaling in transform     icp_first : int       How many ICP iterations for the 9 possible       combinations of sign flippage     icp_final : int       How many ICP iterations for the closest       candidate from the wider search      Returns     -----------     mesh_to_other : (4, 4) float       Transform to align mesh to the other object     cost : float       Average squared distance per point
Perform Procrustes' analysis subject to constraints. Finds the     transformation T mapping a to b which minimizes the square sum     distances between Ta and b, also called the cost.      Parameters     ----------     a : (n,3) float       List of points in space     b : (n,3) float       List of points in space     reflection : bool       If the transformation is allowed reflections     translation : bool       If the transformation is allowed translations     scale : bool       If the transformation is allowed scaling     return_cost : bool       Whether to return the cost and transformed a as well      Returns     ----------     matrix : (4,4) float       The transformation matrix sending a to b     transformed : (n,3) float       The image of a under the transformation     cost : float       The cost of the transformation
Apply the iterative closest point algorithm to align a point cloud with     another point cloud or mesh. Will only produce reasonable results if the     initial transformation is roughly correct. Initial transformation can be     found by applying Procrustes' analysis to a suitable set of landmark     points (often picked manually).      Parameters     ----------     a : (n,3) float       List of points in space.     b : (m,3) float or Trimesh       List of points in space or mesh.     initial : (4,4) float       Initial transformation.     threshold : float       Stop when change in cost is less than threshold     max_iterations : int       Maximum number of iterations     kwargs : dict       Args to pass to procrustes      Returns     ----------     matrix : (4,4) float       The transformation matrix sending a to b     transformed : (n,3) float       The image of a under the transformation     cost : float       The cost of the transformation
Concatenate multiple visual objects.      Parameters     ----------     visuals: ColorVisuals object, or list of same     *args:  ColorVisuals object, or list of same      Returns     ----------     concat: ColorVisuals object
Convert a single or multiple RGB colors to RGBA colors.      Parameters     ----------     colors: (n,[3|4]) list of RGB or RGBA colors      Returns     ----------     colors: (n,4) list of RGBA colors             (4,)  single RGBA color
Turn a string hex color to a (4,) RGBA color.      Parameters     -----------     color: str, hex color      Returns     -----------     rgba: (4,) np.uint8, RGBA color
Return a random RGB color using datatype specified.      Parameters     ----------     dtype: numpy dtype of result      Returns     ----------     color: (4,) dtype, random color that looks OK
Convert a list of vertex colors to face colors.      Parameters     ----------     vertex_colors: (n,(3,4)),  colors     faces:         (m,3) int, face indexes      Returns     -----------     face_colors: (m,4) colors
Convert a list of face colors into a list of vertex colors.      Parameters     -----------     mesh : trimesh.Trimesh object     face_colors: (n, (3,4)) int, face colors     dtype:       data type of output      Returns     -----------     vertex_colors: (m,4) dtype, colors for each vertex
Convert a list of colors into a list of unique materials     and material indexes.      Parameters     -----------     colors : (n, 3) or (n, 4) float       RGB or RGBA colors     count : int       Number of entities to apply color to      Returns     -----------     diffuse : (m, 4) int       Colors     index : (count,) int       Index of each color
Linearly interpolate between two colors.      If colors are not specified the function will     interpolate between  0.0 values as red and 1.0 as green.      Parameters     --------------     values : (n, ) float       Values to interpolate     color_range : None, or (2, 4) uint8       What colors should extrema be set to      Returns     ---------------     colors : (n, 4) uint8       RGBA colors for interpolated values
Given a 1D list of values, return interpolated colors     for the range.      Parameters     ---------------     values : (n, ) float       Values to be interpolated over     color_map : None, or str       Key to a colormap contained in:       matplotlib.pyplot.colormaps()       e.g: 'viridis'      Returns     -------------     interpolated : (n, 4) dtype       Interpolated RGBA colors
Does the current object contain any transparency.          Returns         ----------         transparency: bool, does the current visual contain transparency
What color mode has been set.          Returns         ----------         mode: 'face', 'vertex', or None
A checksum for the current visual object and its parent mesh.          Returns         ----------         crc: int, checksum of data in visual object and its parent mesh
Return a copy of the current ColorVisuals object.          Returns         ----------         copied : ColorVisuals           Contains the same information as self
Set the colors for each face of a mesh.          This will apply these colors and delete any previously specified         color information.          Parameters         ------------         colors: (len(mesh.faces), 3), set each face to the specified color                 (len(mesh.faces), 4), set each face to the specified color                 (3,) int, set the whole mesh this color                 (4,) int, set the whole mesh this color
Set the colors for each vertex of a mesh          This will apply these colors and delete any previously specified         color information.          Parameters         ------------         colors: (len(mesh.vertices), 3), set each face to the color                 (len(mesh.vertices), 4), set each face to the color                 (3,) int, set the whole mesh this color                 (4,) int, set the whole mesh this color
A magical function which maintains the sanity of vertex and face colors.          * If colors have been explicitly stored or changed, they are considered         user data, stored in self._data (DataStore), and are returned immediately         when requested.         * If colors have never been set, a (count,4) tiled copy of the default diffuse         color will be stored in the cache         ** the CRC on creation for these cached default colors will also be stored         ** if the cached color array is altered (different CRC than when it was         created) we consider that now to be user data and the array is moved from         the cache to the DataStore.          Parameters         -----------         name: str, 'face', or 'vertex'          Returns         -----------         colors: (count, 4) uint8, RGBA colors
Verify the checksums of cached face and vertex color, to verify         that a user hasn't altered them since they were generated from         defaults.          If the colors have been altered since creation, move them into         the DataStore at self._data since the user action has made them         user data.
Given a mask of face indices, return a sliced version.          Parameters         ----------         face_index: (n,) int, mask for faces                     (n,) bool, mask for faces          Returns         ----------         visual: ColorVisuals object containing a subset of faces.
What is the most commonly occurring color.          Returns         ------------         color: (4,) uint8, most common color
Concatenate two or more ColorVisuals objects into a single object.          Parameters         -----------         other : ColorVisuals           Object to append         *args: ColorVisuals objects          Returns         -----------         result: ColorVisuals object containing information from current                 object and others in the order it was passed.
Mask the value contained in the DataStore at a specified key.          Parameters         -----------         mask: (n,) int               (n,) bool         key: hashable object, in self._data
Do the bare minimum processing to make a mesh useful.          Does this by:             1) removing NaN and Inf values              2) merging duplicate vertices          If self._validate:             3) Remove triangles which have one edge of their rectangular 2D                oriented bounding box shorter than tol.merge              4) remove duplicated triangles          Returns         ------------         self: trimesh.Trimesh           Current mesh
Set the vertex indexes that make up triangular faces.          Parameters         --------------         values : (n, 3) int           Indexes of self.vertices
A sparse matrix representation of the faces.          Returns         ----------         sparse : scipy.sparse.coo_matrix           Has properties:           dtype : bool           shape : (len(self.vertices), len(self.faces))
Return the unit normal vector for each face.          If a face is degenerate and a normal can't be generated         a zero magnitude unit vector will be returned for that face.          Returns         -----------         normals : (len(self.faces), 3) np.float64           Normal vectors of each face
Assign values to face normals.          Parameters         -------------         values : (len(self.faces), 3) float           Unit face normals
Assign vertex values to the mesh.          Parameters         --------------         values : (n, 3) float           Points in space
The vertex normals of the mesh. If the normals were loaded         we check to make sure we have the same number of vertex         normals and vertices before returning them. If there are         no vertex normals defined or a shape mismatch we  calculate         the vertex normals from the mean normals of the faces the         vertex is used in.          Returns         ----------         vertex_normals : (n,3) float           Represents the surface normal at each vertex.           Where n == len(self.vertices)
Assign values to vertex normals          Parameters         -------------         values : (len(self.vertices), 3) float           Unit normal vectors for each vertex
The axis aligned bounds of the faces of the mesh.          Returns         -----------         bounds : (2, 3) float           Bounding box with [min, max] coordinates
The length, width, and height of the bounding box of the mesh.          Returns         -----------         extents : (3,) float           Array containing axis aligned [length, width, height]
The point in space which is the average of the triangle centroids         weighted by the area of each triangle.          This will be valid even for non- watertight meshes,         unlike self.center_mass          Returns         ----------         centroid : (3,) float           The average vertex weighted by face area
Set the density of the mesh.          Parameters         -------------         density : float           Specify the density of the mesh to be used in inertia calculations
Return the principal components of inertia          Ordering corresponds to mesh.principal_inertia_vectors          Returns         ----------         components : (3,) float           Principal components of inertia
A transform which moves the current mesh so the principal         inertia vectors are on the X,Y, and Z axis, and the centroid is         at the origin.          Returns         ----------         transform : (4, 4) float           Homogenous transformation matrix
Check whether a mesh has rotational symmetry.          Returns         -----------         symmetry: None         No rotational symmetry                   'radial'     Symmetric around an axis                   'spherical'  Symmetric around a point
Actual triangles of the mesh (points, not indexes)          Returns         ---------         triangles : (n, 3, 3) float           Points of triangle vertices
Edges of the mesh (derived from faces).          Returns         ---------         edges : (n, 2) int           List of vertex indices making up edges
The unique edges of the mesh.          Returns         ----------         edges_unique : (n, 2) int           Vertex indices for unique edges
How long is each unique edge.          Returns         ----------         length : (len(self.edges_unique), ) float           Length of each unique edge
Edges in sparse bool COO graph format where connected         vertices are True.          Returns         ----------         sparse: (len(self.vertices), len(self.vertices)) bool           Sparse graph in COO format
How many connected groups of vertices exist in this mesh.          Note that this number may differ from result in mesh.split,         which is calculated from FACE rather than vertex adjacency.          Returns         -----------         count : int           Number of connected vertex groups
For each face return which indexes in mesh.unique_edges constructs         that face.          Returns         ---------         faces_unique_edges : (len(self.faces), 3) int           Indexes of self.edges_unique that           construct self.faces          Examples         ---------         In [0]: mesh.faces[0:2]         Out[0]:         TrackedArray([[    1,  6946, 24224],                       [ 6946,  1727, 24225]])          In [1]: mesh.edges_unique[mesh.faces_unique_edges[0:2]]         Out[1]:         array([[[    1,  6946],                 [ 6946, 24224],                 [    1, 24224]],                [[ 1727,  6946],                 [ 1727, 24225],                 [ 6946, 24225]]])
Return the Euler characteristic (a topological invariant) for the mesh         In order to guarantee correctness, this should be called after         remove_unreferenced_vertices          Returns         ----------         euler_number : int           Topological invariant
Which vertices in the current mesh are referenced by a face.          Returns         -------------         referenced : (len(self.vertices),) bool           Which vertices are referenced by a face
Convert the units of the mesh into a specified unit.          Parameters         ----------         desired : string           Units to convert to (eg 'inches')         guess : boolean           If self.units are not defined should we           guess the current units of the document and then convert?
If a mesh has vertices that are closer than         trimesh.constants.tol.merge reindex faces to reference         the same index for both vertices.          Parameters         --------------         digits : int           If specified overrides tol.merge         textured : bool           If True avoids merging vertices with different UV           coordinates. No effect on untextured meshes.
Update vertices with a mask.          Parameters         ----------         vertex_mask : (len(self.vertices)) bool           Array of which vertices to keep         inverse : (len(self.vertices)) int           Array to reconstruct vertex references           such as output by np.unique
In many cases, we will want to remove specific faces.         However, there is additional bookkeeping to do this cleanly.         This function updates the set of faces with a validity mask,         as well as keeping track of normals and colors.          Parameters         ---------         valid : (m) int or (len(self.faces)) bool           Mask to remove faces
Ensure that every vertex and face consists of finite numbers.          This will remove vertices or faces containing np.nan and np.inf          Alters         ----------         self.faces : masked to remove np.inf/np.nan         self.vertices : masked to remove np.inf/np.nan
On the current mesh remove any faces which are duplicates.          Alters         ----------         self.faces : removes duplicates
Returns a list of Trimesh objects, based on face connectivity.         Splits into individual components, sometimes referred to as 'bodies'          Parameters         ---------         only_watertight : bool           Only return watertight meshes and discard remainder         adjacency : None or (n, 2) int           Override face adjacency with custom values          Returns         ---------         meshes : (n,) trimesh.Trimesh           Separate bodies from original mesh
Find faces that share an edge, which we call here 'adjacent'.          Returns         ----------         adjacency : (n,2) int           Pairs of faces which share an edge          Examples         ---------          In [1]: mesh = trimesh.load('models/featuretype.STL')          In [2]: mesh.face_adjacency         Out[2]:         array([[   0,    1],                [   2,    3],                [   0,    3],                ...,                [1112,  949],                [3467, 3475],                [1113, 3475]])          In [3]: mesh.faces[mesh.face_adjacency[0]]         Out[3]:         TrackedArray([[   1,    0,  408],                       [1239,    0,    1]], dtype=int64)          In [4]: import networkx as nx          In [5]: graph = nx.from_edgelist(mesh.face_adjacency)          In [6]: groups = nx.connected_components(graph)
Return the angle between adjacent faces          Returns         --------         adjacency_angle : (n,) float           Angle between adjacent faces           Each value corresponds with self.face_adjacency
The approximate radius of a cylinder that fits inside adjacent faces.          Returns         ------------         radii : (len(self.face_adjacency),) float           Approximate radius formed by triangle pair
The vertex neighbors of each vertex of the mesh, determined from         the cached vertex_adjacency_graph, if already existent.          Returns         ----------         vertex_neighbors : (len(self.vertices),) int           Represents immediate neighbors of each vertex along           the edge of a triangle          Examples         ----------         This is useful for getting nearby vertices for a given vertex,         potentially for some simple smoothing techniques.          >>> mesh = trimesh.primitives.Box()         >>> mesh.vertex_neighbors[0]         [1,2,3,4]
Does the mesh have consistent winding or not.         A mesh with consistent winding has each shared edge         going in an opposite direction from the other in the pair.          Returns         --------         consistent : bool           Is winding is consistent or not
Check if a mesh is watertight by making sure every edge is         included in two faces.          Returns         ----------         is_watertight : bool           Is mesh watertight or not
Check if a mesh has all the properties required to represent         a valid volume, rather than just a surface.          These properties include being watertight, having consistent         winding and outward facing normals.          Returns         ---------         valid : bool           Does the mesh represent a volume
Check if a mesh is convex or not.          Returns         ----------         is_convex: bool           Is mesh convex or not
Return a scipy.spatial.cKDTree of the vertices of the mesh.         Not cached as this lead to observed memory issues and segfaults.          Returns         ---------         tree : scipy.spatial.cKDTree           Contains mesh.vertices
Remove degenerate faces (faces without 3 unique vertex indices)         from the current mesh.          If a height is specified, it will remove any face with a 2D oriented         bounding box with one edge shorter than that height.          If not specified, it will remove any face with a zero normal.          Parameters         ------------         height : float           If specified removes faces with an oriented bounding           box shorter than this on one side.          Returns         -------------         nondegenerate : (len(self.faces),) bool           Mask used to remove faces
Return an array containing the area of each facet.          Returns         ---------         area : (len(self.facets),) float           Total area of each facet (group of faces)
Return the normal of each facet          Returns         ---------         normals: (len(self.facets), 3) float           A unit normal vector for each facet
Return the edges which represent the boundary of each facet          Returns         ---------         edges_boundary : sequence of (n, 2) int           Indices of self.vertices
Find which facets of the mesh are on the convex hull.          Returns         ---------         on_hull : (len(mesh.facets),) bool           is A facet on the meshes convex hull or not
Find and fix problems with self.face_normals and self.faces         winding direction.          For face normals ensure that vectors are consistently pointed         outwards, and that self.faces is wound in the correct direction         for all connected components.          Parameters         -------------         multibody : None or bool           Fix normals across multiple bodies           if None automatically pick from body_count
Align a mesh with another mesh or a PointCloud using         the principal axes of inertia as a starting point which         is refined by iterative closest point.          Parameters         ------------         mesh : trimesh.Trimesh object           Mesh to align with other         other : trimesh.Trimesh or (n, 3) float           Mesh or points in space         samples : int           Number of samples from mesh surface to align         icp_first : int           How many ICP iterations for the 9 possible           combinations of         icp_final : int           How many ICP itertations for the closest           candidate from the wider search          Returns         -----------         mesh_to_other : (4, 4) float           Transform to align mesh to the other object         cost : float           Average square distance per point
Computes stable orientations of a mesh and their quasi-static probabilites.          This method samples the location of the center of mass from a multivariate         gaussian (mean at com, cov equal to identity times sigma) over n_samples.         For each sample, it computes the stable resting poses of the mesh on a         a planar workspace and evaulates the probabilities of landing in         each pose if the object is dropped onto the table randomly.          This method returns the 4x4 homogenous transform matrices that place         the shape against the planar surface with the z-axis pointing upwards         and a list of the probabilities for each pose.         The transforms and probabilties that are returned are sorted, with the         most probable pose first.          Parameters         ----------         center_mass : (3,) float           The object center of mass (if None, this method           assumes uniform density and watertightness and           computes a center of mass explicitly)         sigma : float           The covariance for the multivariate gaussian used           to sample center of mass locations         n_samples : int           The number of samples of the center of mass location         threshold : float           The probability value at which to threshold           returned stable poses          Returns         -------         transforms : (n, 4, 4) float           The homogenous matrices that transform the           object to rest in a stable pose, with the           new z-axis pointing upwards from the table           and the object just touching the table.          probs : (n,) float           A probability ranging from 0.0 to 1.0 for each pose
Subdivide a mesh, with each subdivided face replaced with four         smaller faces.          Parameters         ----------         face_index: (m,) int or None           If None all faces of mesh will be subdivided           If (m,) int array of indices: only specified faces will be           subdivided. Note that in this case the mesh will generally           no longer be manifold, as the additional vertex on the midpoint           will not be used by the adjacent faces to the faces specified,           and an additional postprocessing step will be required to           make resulting mesh watertight
Return a version of the current mesh which will render         nicely, without changing source mesh.          Parameters         -------------         angle : float           Angle in radians, face pairs with angles smaller than           this value will appear smoothed          Returns         ---------         smoothed : trimesh.Trimesh           Non watertight version of current mesh           which will render nicely with smooth shading
Returns a 3D cross section of the current mesh and a plane         defined by origin and normal.          Parameters         ---------         plane_normal: (3) vector for plane normal           Normal vector of section plane         plane_origin : (3,) float           Point on the cross section plane          Returns         ---------         intersections: Path3D or None           Curve of intersection
Return multiple parallel cross sections of the current         mesh in 2D.          Parameters         ---------         plane_normal: (3) vector for plane normal           Normal vector of section plane         plane_origin : (3,) float           Point on the cross section plane         heights : (n,) float           Each section is offset by height along           the plane normal.          Returns         ---------         paths : (n,) Path2D or None           2D cross sections at specified heights.           path.metadata['to_3D'] contains transform           to return 2D section back into 3D space.
Returns another mesh that is the current mesh         sliced by the plane defined by origin and normal.          Parameters         ---------         plane_normal: (3) vector for plane normal           Normal vector of slicing plane         plane_origin : (3,) float           Point on the slicing plane          Returns         ---------         new_mesh: trimesh.Trimesh or None           Subset of current mesh sliced by plane
Return random samples distributed normally across the         surface of the mesh          Parameters         ---------         count : int           Number of points to sample         return_index : bool           If True will also return the index of which face each           sample was taken from.          Returns         ---------         samples : (count, 3) float           Points on surface of mesh         face_index : (count, ) int           Index of self.faces
Remove all vertices in the current mesh which are not         referenced by a face.
Removes all face references so that every face contains         three unique vertex indices and no faces are adjacent.
Apply the oriented bounding box transform to the current mesh.          This will result in a mesh with an AABB centered at the         origin and the same dimensions as the OBB.          Returns         ----------         matrix : (4, 4) float           Transformation matrix that was applied           to mesh to move it into OBB frame
Transform mesh by a homogenous transformation matrix.          Does the bookkeeping to avoid recomputing things so this function         should be used rather than directly modifying self.vertices         if possible.          Parameters         ----------         matrix : (4, 4) float           Homogenous transformation matrix
Return a Voxel object representing the current mesh         discretized into voxels at the specified pitch          Parameters         ----------         pitch : float           The edge length of a single voxel          Returns         ----------         voxelized : Voxel object           Representing the current mesh
Given a list of face indexes find the outline of those         faces and return it as a Path3D.          The outline is defined here as every edge which is only         included by a single triangle.          Note that this implies a non-watertight mesh as the         outline of a watertight mesh is an empty path.          Parameters         ----------         face_ids : (n,) int           Indices to compute the outline of.           If None, outline of full mesh will be computed.         **kwargs: passed to Path3D constructor          Returns         ----------         path : Path3D           Curve in 3D of the outline
The area of each face in the mesh.          Returns         ---------         area_faces : (n,) float           Area of each face
Returns the mass properties of the current mesh.          Assumes uniform density, and result is probably garbage if mesh         isn't watertight.          Returns         ----------         properties : dict           With keys:           'volume'      : in global units^3           'mass'        : From specified density           'density'     : Included again for convenience (same as kwarg density)           'inertia'     : Taken at the center of mass and aligned with global                          coordinate system           'center_mass' : Center of mass location, in global coordinate system
Invert the mesh in- place by reversing the winding of every         face and negating normals without dumping the cache.          Alters         ---------         self.faces :          columns reversed         self.face_normals :   negated if defined         self.vertex_normals : negated if defined
Return a subset of the mesh.          Parameters         ----------         faces_sequence : sequence (m,) int           Face indices of mesh         only_watertight : bool           Only return submeshes which are watertight         append : bool           Return a single mesh which has the faces appended.           if this flag is set, only_watertight is ignored          Returns         ---------         if append : trimesh.Trimesh object         else :      list of trimesh.Trimesh objects
Export the current mesh to a file object.         If file_obj is a filename, file will be written there.          Supported formats are stl, off, ply, collada, json, dict, glb,         dict64, msgpack.          Parameters         ---------         file_obj: open writeable file object           str, file name where to save the mesh           None, if you would like this function to return the export blob         file_type: str           Which file type to export as.           If file name is passed this is not required
Compute an approximate convex decomposition of a mesh.          testVHACD Parameters which can be passed as kwargs:          Name                                        Default         -----------------------------------------------------         resolution                                  100000         max. concavity                              0.001         plane down-sampling                         4         convex-hull down-sampling                   4         alpha                                       0.05         beta                                        0.05         maxhulls                                    10         pca                                         0         mode                                        0         max. vertices per convex-hull               64         min. volume to add vertices to convex-hulls 0.0001         convex-hull approximation                   1         OpenCL acceleration                         1         OpenCL platform ID                          0         OpenCL device ID                            0         output                                      output.wrl         log                                         log.txt           Parameters         ----------         maxhulls :  int           Maximum number of convex hulls to return         **kwargs :  testVHACD keyword arguments          Returns         -------         meshes : list of trimesh.Trimesh           List of convex meshes that approximate the original
Boolean intersection between this mesh and n other meshes          Parameters         ---------         other : trimesh.Trimesh, or list of trimesh.Trimesh objects           Meshes to calculate intersections with          Returns         ---------         intersection : trimesh.Trimesh           Mesh of the volume contained by all passed meshes
Given a set of points, determine whether or not they are inside the mesh.         This raises an error if called on a non- watertight mesh.          Parameters         ---------         points : (n, 3) float           Points in cartesian space          Returns         ---------         contains : (n, ) bool           Whether or not each point is inside the mesh
An R-tree of face adjacencies.          Returns         --------         tree: rtree.index           Where each edge in self.face_adjacency has a           rectangular cell
Safely get a copy of the current mesh.          Copied objects will have emptied caches to avoid memory         issues and so may be slow on initial operations until         caches are regenerated.          Current object will *not* have its cache cleared.          Returns         ---------         copied : trimesh.Trimesh           Copy of current mesh
Evaluate a statement and cache the result before returning.          Statements are evaluated inside the Trimesh object, and          Parameters         -----------         statement : str           Statement of valid python code         *args : list           Available inside statement as args[0], etc          Returns         -----------         result : result of running eval on statement with args          Examples         -----------         r = mesh.eval_cached('np.dot(self.vertices, args[0])', [0,0,1])
Traverse and change mesh faces in-place to make sure winding     is correct, with edges on adjacent faces in     opposite directions.      Parameters     -------------     mesh: Trimesh object      Alters     -------------     mesh.face: will reverse columns of certain faces
Check to see if a mesh has normals pointing "out."      Parameters     -------------     mesh:      Trimesh object     multibody: bool, if True will try to fix normals on every body      Alters     -------------     mesh.face: may reverse faces
Fix the winding and direction of a mesh face and     face normals in-place.      Really only meaningful on watertight meshes, but will orient all     faces and winding in a uniform way for non-watertight face     patches as well.      Parameters     -------------     mesh:      Trimesh object     multibody: bool, if True try to correct normals direction                      on every body.      Alters     --------------     mesh.faces: will flip columns on inverted faces
Return the index of faces in the mesh which break the     watertight status of the mesh.      Parameters     --------------     mesh: Trimesh object     color: (4,) uint8, will set broken faces to this color            None,       will not alter mesh colors      Returns     ---------------     broken: (n, ) int, indexes of mesh.faces
Fill single- triangle holes on triangular meshes by adding     new triangles to fill the holes. New triangles will have     proper winding and normals, and if face colors exist the color     of the last face will be assigned to the new triangles.      Parameters     ---------     mesh : trimesh.Trimesh       Mesh will be repaired in- place
Find a the intersections between a mesh and a plane,     returning a set of line segments on that plane.      Parameters     ---------     mesh : Trimesh object         Source mesh to slice     plane_normal : (3,) float         Normal vector of plane to intersect with mesh     plane_origin:  (3,) float         Point on plane to intersect with mesh     return_faces:  bool         If True return face index each line is from     cached_dots : (n, 3) float         If an external function has stored dot         products pass them here to avoid recomputing      Returns     ----------     lines :  (m, 2, 3) float         List of 3D line segments in space     face_index : (m,) int         Index of mesh.faces for each line         Only returned if return_faces was True
A utility function for slicing a mesh by multiple     parallel planes, which caches the dot product operation.      Parameters     -------------     mesh : trimesh.Trimesh         Geometry to be sliced by planes     plane_normal : (3,) float         Normal vector of plane     plane_origin : (3,) float         Point on a plane     heights : (m,) float         Offset distances from plane to slice at      Returns     --------------     lines : (m,) sequence of (n, 2, 2) float         Lines in space for m planes     to_3D : (m, 4, 4) float         Transform to move each section back to 3D     face_index : (m,) sequence of (n,) int         Indexes of mesh.faces for each segment
Calculate plane-line intersections      Parameters     ---------     plane_origin : (3,) float         Point on plane     plane_normal : (3,) float         Plane normal vector     endpoints : (2, n, 3) float         Points defining lines to be tested     line_segments : bool         If True, only returns intersections as valid if         vertices from endpoints are on different sides         of the plane.      Returns     ---------     intersections : (m, 3) float         Cartesian intersection points     valid : (n, 3) bool         Indicate whether a valid intersection exists         for each input line segment
Given one line per plane, find the intersection points.      Parameters     -----------     plane_origins : (n,3) float         Point on each plane     plane_normals : (n,3) float         Normal vector of each plane     line_origins : (n,3) float         Point at origin of each line     line_directions : (n,3) float         Direction vector of each line      Returns     ----------     on_plane : (n,3) float         Points on specified planes     valid : (n,) bool         Did plane intersect line or not
Slice a mesh (given as a set of faces and vertices) with a plane, returning a     new mesh (again as a set of faces and vertices) that is the     portion of the original mesh to the positive normal side of the plane.      Parameters     ---------     vertices : (n, 3) float         Vertices of source mesh to slice     faces : (n, 3) int         Faces of source mesh to slice     plane_normal : (3,) float         Normal vector of plane to intersect with mesh     plane_origin:  (3,) float         Point on plane to intersect with mesh     cached_dots : (n, 3) float         If an external function has stored dot         products pass them here to avoid recomputing     Returns     ----------     new_vertices : (n, 3) float         Vertices of sliced mesh     new_faces : (n, 3) int         Faces of sliced mesh
Slice a mesh with a plane, returning a new mesh that is the     portion of the original mesh to the positive normal side of the plane      Parameters     ---------     mesh : Trimesh object         Source mesh to slice     plane_normal : (3,) float         Normal vector of plane to intersect with mesh     plane_origin:  (3,) float         Point on plane to intersect with mesh     cached_dots : (n, 3) float         If an external function has stored dot         products pass them here to avoid recomputing     Returns     ----------     new_mesh : Trimesh object         Sliced mesh
Create a scene with a Fuze bottle, some cubes, and an axis.      Returns     ----------     scene : trimesh.Scene       Object with geometry
A sparse matrix representation of the face angles.      Returns     ----------     sparse: scipy.sparse.coo_matrix with:             dtype: float             shape: (len(mesh.vertices), len(mesh.faces))
Return the vertex defects, or (2*pi) minus the sum of the angles     of every face that includes that vertex.      If a vertex is only included by coplanar triangles, this     will be zero. For convex regions this is positive, and     concave negative.      Returns     --------     vertex_defect : (len(self.vertices), ) float                      Vertex defect at the every vertex
Return the discrete gaussian curvature measure of a sphere centered     at a point as detailed in 'Restricted Delaunay triangulations and normal     cycle', Cohen-Steiner and Morvan.      Parameters     ----------     points : (n,3) float, list of points in space     radius : float, the sphere radius      Returns     --------     gaussian_curvature: (n,) float, discrete gaussian curvature measure.
Return the discrete mean curvature measure of a sphere centered     at a point as detailed in 'Restricted Delaunay triangulations and normal     cycle', Cohen-Steiner and Morvan.      Parameters     ----------     points : (n,3) float, list of points in space     radius : float, the sphere radius      Returns     --------     mean_curvature: (n,) float, discrete mean curvature measure.
Compute the length of the intersection of a line segment with a ball.      Parameters     ----------     start_points : (n,3) float, list of points in space     end_points   : (n,3) float, list of points in space     center       : (3,) float, the sphere center     radius       : float, the sphere radius      Returns     --------     lengths: (n,) float, the lengths.
Compute the surface area of the intersection of sphere of radius R centered     at (0, 0, 0) with a ball of radius r centered at (R, 0, 0).      Parameters     ----------     R : float, sphere radius     r : float, ball radius      Returns     --------     area: float, the surface are.
Get the color in a texture image.      Parameters     -------------     uv : (n, 2) float       UV coordinates on texture image     image : PIL.Image       Texture image      Returns     ----------     colors : (n, 4) float       RGBA color at each of the UV coordinates
Set the UV coordinates.          Parameters         --------------         values : (n, 2) float           Pixel locations on a texture per- vertex
Return a copy of the current TextureVisuals object.          Returns         ----------         copied : TextureVisuals           Contains the same information in a new object
Convert textured visuals to a ColorVisuals with vertex         color calculated from texture.          Returns         -----------         vis : trimesh.visuals.ColorVisuals           Contains vertex color from texture
Apply a mask to remove or duplicate vertex properties.
Load an STL file from a file object.      Parameters     ----------     file_obj: open file- like object     file_type: not used      Returns     ----------     loaded: kwargs for a Trimesh constructor with keys:               vertices:     (n,3) float, vertices               faces:        (m,3) int, indexes of vertices               face_normals: (m,3) float, normal vector of each face
Load a binary STL file from a file object.      Parameters     ----------     file_obj: open file- like object      Returns     ----------     loaded: kwargs for a Trimesh constructor with keys:               vertices:     (n,3) float, vertices               faces:        (m,3) int, indexes of vertices               face_normals: (m,3) float, normal vector of each face
Load an ASCII STL file from a file object.      Parameters     ----------     file_obj: open file- like object      Returns     ----------     loaded: kwargs for a Trimesh constructor with keys:               vertices:     (n,3) float, vertices               faces:        (m,3) int, indexes of vertices               face_normals: (m,3) float, normal vector of each face
Convert a Trimesh object into a binary STL file.      Parameters     ---------     mesh: Trimesh object      Returns     ---------     export: bytes, representing mesh in binary STL form
Convert a Trimesh object into an ASCII STL file.      Parameters     ---------     mesh : trimesh.Trimesh      Returns     ---------     export : str         Mesh represented as an ASCII STL file
Given a set of entity objects generate a networkx.Graph     that represents their vertex nodes.      Parameters     --------------     entities : list        Objects with 'closed' and 'nodes' attributes      Returns     -------------     graph : networkx.Graph         Graph where node indexes represent vertices     closed : (n,) int         Indexes of entities which are 'closed'
Convert a path of vertex indices to a path of entity indices.      Parameters     ----------     vertex_path : (n,) int         Ordered list of vertex indices representing a path     graph : nx.Graph         Vertex connectivity     entities : (m,) list         Entity objects     vertices :  (p, dimension) float         Vertex points in space      Returns     ----------     entity_path : (q,) int         Entity indices which make up vertex_path
Paths are lists of entity indices.     We first generate vertex paths using graph cycle algorithms,     and then convert them to entity paths.      This will also change the ordering of entity.points in place     so a path may be traversed without having to reverse the entity.      Parameters     -------------     entities : (n,) entity objects         Entity objects     vertices : (m, dimension) float         Vertex points in space      Returns     -------------     entity_paths : sequence of (n,) int         Ordered traversals of entities
Turn a list of entity indices into a path of connected points.      Parameters     -----------     entities : (j,) entity objects        Objects like 'Line', 'Arc', etc.     vertices: (n, dimension) float         Vertex points in space.     path : (m,) int         Indexes of entities     scale : float         Overall scale of drawing used for         numeric tolerances in certain cases      Returns     -----------     discrete : (p, dimension) float        Connected points in space that lie on the        path and can be connected with line segments.
Given a path along (n,d) points, resample them such that the     distance traversed along the path is constant in between each     of the resampled points. Note that this can produce clipping at     corners, as the original vertices are NOT guaranteed to be in the     new, resampled path.      ONLY ONE of count or step can be specified     Result can be uniformly distributed (np.linspace) by specifying count     Result can have a specific distance (np.arange) by specifying step       Parameters     ----------     points:   (n, d) float         Points in space     count : int,         Number of points to sample evenly (aka np.linspace)     step : float         Distance each step should take along the path (aka np.arange)      Returns     ----------     resampled : (j,d) float         Points on the path
Split a Path2D into multiple Path2D objects where each     one has exactly one root curve.      Parameters     --------------     self : trimesh.path.Path2D       Input geometry      Returns     -------------     split : list of trimesh.path.Path2D       Original geometry as separate paths
Return a truncated version of the path.         Only one vertex (at the endpoint) will be added.
Find the intersections between a group of triangles and rays      Parameters     -------------     triangles : (n, 3, 3) float       Triangles in space     ray_origins : (m, 3) float       Ray origin points     ray_directions : (m, 3) float       Ray direction vectors     triangles_normal : (n, 3) float       Normal vector of triangles, optional     tree : rtree.Index       Rtree object holding triangle bounds      Returns     -----------     index_triangle : (h,) int       Index of triangles hit     index_ray : (h,) int       Index of ray that hit triangle     locations : (h, 3) float       Position of intersection in space
Do broad- phase search for triangles that the rays     may intersect.      Does this by creating a bounding box for the ray as it     passes through the volume occupied by the tree      Parameters     ------------     ray_origins:      (m,3) float, ray origin points     ray_directions:   (m,3) float, ray direction vectors     tree:             rtree object, contains AABB of each triangle      Returns     ----------     ray_candidates: (n,) int, triangle indexes     ray_id:         (n,) int, corresponding ray index for a triangle candidate
Given a set of rays and a bounding box for the volume of interest     where the rays will be passing through, find the bounding boxes     of the rays as they pass through the volume.      Parameters     ------------     ray_origins:      (m,3) float, ray origin points     ray_directions:   (m,3) float, ray direction vectors     bounds:           (2,3) bounding box (min, max)     buffer_dist:      float, distance to pad zero width bounding boxes      Returns     ---------     ray_bounding: (n) set of AABB of rays passing through volume
Find the intersections between the current mesh and a list of rays.          Parameters         ------------         ray_origins:      (m,3) float, ray origin points         ray_directions:   (m,3) float, ray direction vectors         multiple_hits:    bool, consider multiple hits of each ray or not         return_locations: bool, return hit locations or not          Returns         -----------         index_triangle: (h,) int,    index of triangles hit         index_ray:      (h,) int,    index of ray that hit triangle         locations:      (h,3) float, (optional) position of intersection in space
Find out if each ray hit any triangle on the mesh.          Parameters         ------------         ray_origins:      (m,3) float, ray origin points         ray_directions:   (m,3) float, ray direction vectors          Returns         ---------         hit: boolean, whether any ray hit any triangle on the mesh
Turn a pure dict into a dict containing entity objects that     can be sent directly to a Path constructor.      Parameters     -----------     as_dict : dict       Has keys: 'vertices', 'entities'      Returns     ------------     kwargs : dict       Has keys: 'vertices', 'entities'
Turn line segments into a Path2D or Path3D object.      Parameters     ------------     lines : (n, 2, dimension) or (n, dimension) float       Line segments or connected polyline curve in 2D or 3D      Returns     -----------     kwargs : dict       kwargs for Path constructor
Load shapely Polygon objects into a trimesh.path.Path2D object      Parameters     -------------     polygon : shapely.geometry.Polygon       Input geometry      Returns     -------------     kwargs : dict       Keyword arguments for Path2D constructor
Load shapely LineString objects into a trimesh.path.Path2D object      Parameters     -------------     multi : shapely.geometry.LineString or MultiLineString       Input 2D geometry      Returns     -------------     kwargs : dict       Keyword arguments for Path2D constructor
Given a mesh and face indices find the outline edges and     turn them into a Path3D.      Parameters     ---------     mesh : trimesh.Trimesh       Triangulated surface in 3D     face_ids : (n,) int       Indexes referencing mesh.faces      Returns     ---------     kwargs : dict       Kwargs for Path3D constructor
Given an edge list of indices and associated vertices     representing lines, generate kwargs for a Path object.      Parameters     -----------     edges : (n, 2) int       Vertex indices of line segments     vertices : (m, dimension) float       Vertex positions where dimension is 2 or 3      Returns     ----------     kwargs: dict, kwargs for Path constructor
The minimum perpendicular distance of a point to a plane.      Parameters     -----------     points:       (n, 3) float, points in space     plane_normal: (3,) float, normal vector     plane_origin: (3,) float, plane origin in space      Returns     ------------     distances:     (n,) float, distance from point to plane
Returns an approximate vector representing the major axis of points      Parameters     -------------     points: (n, dimension) float, points in space      Returns     -------------     axis: (dimension,) float, vector along approximate major axis
Given a set of points, find an origin and normal using SVD.      Parameters     ---------     points : (n,3) float         Points in 3D space      Returns     ---------     C : (3,) float         Point on the plane     N : (3,) float         Normal vector of plane
Sorts a set of points radially (by angle) around an     origin/normal.      Parameters     --------------     points: (n,3) float, points in space     origin: (3,)  float, origin to sort around     normal: (3,)  float, vector to sort around      Returns     --------------     ordered: (n,3) flot, re- ordered points in space
Projects a set of (n,3) points onto a plane.      Parameters     ---------     points:           (n,3) array of points     plane_normal:     (3) normal vector of plane     plane_origin:     (3) point on plane     transform:        None or (4,4) matrix. If specified, normal/origin are ignored     return_transform: bool, if true returns the (4,4) matrix used to project points                       onto a plane     return_planar:    bool, if True, returns (n,2) points. If False, returns                       (n,3), where the Z column consists of zeros
Given an (n, m) set of points where n=(2|3) return a list of points     where no point is closer than radius.      Parameters     ------------     points : (n, dimension) float       Points in space     radius : float       Minimum radius between result points      Returns     ------------     culled : (m, dimension) float       Points in space     mask : (n,) bool       Which points from the original set were returned
Find k centroids that attempt to minimize the k- means problem:     https://en.wikipedia.org/wiki/Metric_k-center      Parameters     ----------     points:  (n, d) float         Points in a space     k : int          Number of centroids to compute     **kwargs : dict         Passed directly to scipy.cluster.vq.kmeans      Returns     ----------     centroids : (k, d) float          Points in some space     labels: (n) int         Indexes for which points belong to which centroid
Find an ordering of points where each is visited and     the next point is the closest in euclidean distance,     and if there are multiple points with equal distance     go to an arbitrary one.      Assumes every point is visitable from every other point,     i.e. the travelling salesman problem on a fully connected     graph. It is not a MINIMUM traversal; rather it is a     "not totally goofy traversal, quickly." On random points     this traversal is often ~20x shorter than random ordering.      Parameters     ---------------     points : (n, dimension) float       ND points in space     start : int       The index of points we should start at      Returns     ---------------     traversal : (n,) int       Ordered traversal visiting every point     distances : (n - 1,) float       The euclidean distance between points in traversal
Plot an (n,3) list of points using matplotlib      Parameters     -------------     points : (n, 3) float       Points in space     show : bool       If False, will not show until plt.show() is called
Safely get a copy of the current point cloud.          Copied objects will have emptied caches to avoid memory         issues and so may be slow on initial operations until         caches are regenerated.          Current object will *not* have its cache cleared.          Returns         ---------         copied : trimesh.PointCloud           Copy of current point cloud
Merge vertices closer than tol.merge (default: 1e-8)
Apply a homogenous transformation to the PointCloud         object in- place.          Parameters         --------------         transform : (4, 4) float           Homogenous transformation to apply to PointCloud
The axis aligned bounds of the PointCloud          Returns         ------------         bounds : (2, 3) float           Miniumum, Maximum verteex
Return matrix to mirror at plane defined by point and normal vector.      >>> v0 = np.random.random(4) - 0.5     >>> v0[3] = 1.     >>> v1 = np.random.random(3) - 0.5     >>> R = reflection_matrix(v0, v1)     >>> np.allclose(2, np.trace(R))     True     >>> np.allclose(v0, np.dot(R, v0))     True     >>> v2 = v0.copy()     >>> v2[:3] += v1     >>> v3 = v0.copy()     >>> v2[:3] -= v1     >>> np.allclose(v2, np.dot(R, v3))     True
Return mirror plane point and normal vector from reflection matrix.      >>> v0 = np.random.random(3) - 0.5     >>> v1 = np.random.random(3) - 0.5     >>> M0 = reflection_matrix(v0, v1)     >>> point, normal = reflection_from_matrix(M0)     >>> M1 = reflection_matrix(point, normal)     >>> is_same_transform(M0, M1)     True
Return matrix to rotate about axis defined by point and     direction.      Parameters     -------------     angle     : float, or sympy.Symbol                 Angle, in radians or symbolic angle     direction : (3,) float                 Unit vector along rotation axis     point     : (3, ) float, or None                 Origin point of rotation axis      Returns     -------------     matrix : (4, 4) float, or (4, 4) sympy.Matrix              Homogenous transformation matrix      Examples     -------------     >>> R = rotation_matrix(math.pi/2, [0, 0, 1], [1, 0, 0])     >>> np.allclose(np.dot(R, [0, 0, 0, 1]), [1, -1, 0, 1])     True     >>> angle = (random.random() - 0.5) * (2*math.pi)     >>> direc = np.random.random(3) - 0.5     >>> point = np.random.random(3) - 0.5     >>> R0 = rotation_matrix(angle, direc, point)     >>> R1 = rotation_matrix(angle-2*math.pi, direc, point)     >>> is_same_transform(R0, R1)     True     >>> R0 = rotation_matrix(angle, direc, point)     >>> R1 = rotation_matrix(-angle, -direc, point)     >>> is_same_transform(R0, R1)     True     >>> I = np.identity(4, np.float64)     >>> np.allclose(I, rotation_matrix(math.pi*2, direc))     True     >>> np.allclose(2, np.trace(rotation_matrix(math.pi/2,direc,point)))     True
Return matrix to obtain normalized device coordinates from frustum.      The frustum bounds are axis-aligned along x (left, right),     y (bottom, top) and z (near, far).      Normalized device coordinates are in range [-1, 1] if coordinates are     inside the frustum.      If perspective is True the frustum is a truncated pyramid with the     perspective point at origin and direction along z axis, otherwise an     orthographic canonical view volume (a box).      Homogeneous coordinates transformed by the perspective clip matrix     need to be dehomogenized (divided by w coordinate).      >>> frustum = np.random.rand(6)     >>> frustum[1] += frustum[0]     >>> frustum[3] += frustum[2]     >>> frustum[5] += frustum[4]     >>> M = clip_matrix(perspective=False, *frustum)     >>> a = np.dot(M, [frustum[0], frustum[2], frustum[4], 1])     >>> np.allclose(a, [-1., -1., -1.,  1.])     True     >>> b = np.dot(M, [frustum[1], frustum[3], frustum[5], 1])     >>> np.allclose(b, [ 1.,  1.,  1.,  1.])     True     >>> M = clip_matrix(perspective=True, *frustum)     >>> v = np.dot(M, [frustum[0], frustum[2], frustum[4], 1])     >>> c = v / v[3]     >>> np.allclose(c, [-1., -1., -1.,  1.])     True     >>> v = np.dot(M, [frustum[1], frustum[3], frustum[4], 1])     >>> d = v / v[3]     >>> np.allclose(d, [ 1.,  1., -1.,  1.])     True
Return matrix to shear by angle along direction vector on shear plane.      The shear plane is defined by a point and normal vector. The direction     vector must be orthogonal to the plane's normal vector.      A point P is transformed by the shear matrix into P" such that     the vector P-P" is parallel to the direction vector and its extent is     given by the angle of P-P'-P", where P' is the orthogonal projection     of P onto the shear plane.      >>> angle = (random.random() - 0.5) * 4*math.pi     >>> direct = np.random.random(3) - 0.5     >>> point = np.random.random(3) - 0.5     >>> normal = np.cross(direct, np.random.random(3))     >>> S = shear_matrix(angle, direct, point, normal)     >>> np.allclose(1, np.linalg.det(S))     True
Return shear angle, direction and plane from shear matrix.      >>> angle  = np.pi / 2.0     >>> direct = [0.0, 1.0, 0.0]     >>> point  = [0.0, 0.0, 0.0]     >>> normal = np.cross(direct, np.roll(direct,1))     >>> S0 = shear_matrix(angle, direct, point, normal)     >>> angle, direct, point, normal = shear_from_matrix(S0)     >>> S1 = shear_matrix(angle, direct, point, normal)     >>> is_same_transform(S0, S1)     True
Return transformation matrix from sequence of transformations.      This is the inverse of the decompose_matrix function.      Sequence of transformations:         scale : vector of 3 scaling factors         shear : list of shear factors for x-y, x-z, y-z axes         angles : list of Euler angles about static x, y, z axes         translate : translation vector along x, y, z axes         perspective : perspective partition of matrix      >>> scale = np.random.random(3) - 0.5     >>> shear = np.random.random(3) - 0.5     >>> angles = (np.random.random(3) - 0.5) * (2*math.pi)     >>> trans = np.random.random(3) - 0.5     >>> persp = np.random.random(4) - 0.5     >>> M0 = compose_matrix(scale, shear, angles, trans, persp)     >>> result = decompose_matrix(M0)     >>> M1 = compose_matrix(*result)     >>> is_same_transform(M0, M1)     True
Return homogeneous rotation matrix from Euler angles and axis sequence.      ai, aj, ak : Euler's roll, pitch and yaw angles     axes : One of 24 axis sequences as string or encoded tuple      >>> R = euler_matrix(1, 2, 3, 'syxz')     >>> np.allclose(np.sum(R[0]), -1.34786452)     True     >>> R = euler_matrix(1, 2, 3, (0, 1, 0, 1))     >>> np.allclose(np.sum(R[0]), -0.383436184)     True     >>> ai, aj, ak = (4*math.pi) * (np.random.random(3) - 0.5)     >>> for axes in _AXES2TUPLE.keys():     ...    R = euler_matrix(ai, aj, ak, axes)     >>> for axes in _TUPLE2AXES.keys():     ...    R = euler_matrix(ai, aj, ak, axes)
Return Euler angles from rotation matrix for specified axis sequence.      axes : One of 24 axis sequences as string or encoded tuple      Note that many Euler angle triplets can describe one matrix.      >>> R0 = euler_matrix(1, 2, 3, 'syxz')     >>> al, be, ga = euler_from_matrix(R0, 'syxz')     >>> R1 = euler_matrix(al, be, ga, 'syxz')     >>> np.allclose(R0, R1)     True     >>> angles = (4*math.pi) * (np.random.random(3) - 0.5)     >>> for axes in _AXES2TUPLE.keys():     ...    R0 = euler_matrix(axes=axes, *angles)     ...    R1 = euler_matrix(axes=axes, *euler_from_matrix(R0, axes))     ...    if not np.allclose(R0, R1): print(axes, "failed")
Return quaternion from Euler angles and axis sequence.      ai, aj, ak : Euler's roll, pitch and yaw angles     axes : One of 24 axis sequences as string or encoded tuple      >>> q = quaternion_from_euler(1, 2, 3, 'ryxz')     >>> np.allclose(q, [0.435953, 0.310622, -0.718287, 0.444435])     True
Return quaternion for rotation about axis.      >>> q = quaternion_about_axis(0.123, [1, 0, 0])     >>> np.allclose(q, [0.99810947, 0.06146124, 0, 0])     True
Return conjugate of quaternion.      >>> q0 = random_quaternion()     >>> q1 = quaternion_conjugate(q0)     >>> q1[0] == q0[0] and all(q1[1:] == -q0[1:])     True
Return inverse of quaternion.      >>> q0 = random_quaternion()     >>> q1 = quaternion_inverse(q0)     >>> np.allclose(quaternion_multiply(q0, q1), [1, 0, 0, 0])     True
Return imaginary part of quaternion.      >>> quaternion_imag([3, 0, 1, 2])     array([0., 1., 2.])
Return uniform random unit quaternion.      rand: array like or None         Three independent random variables that are uniformly distributed         between 0 and 1.      >>> q = random_quaternion()     >>> np.allclose(1, vector_norm(q))     True     >>> q = random_quaternion(np.random.random(3))     >>> len(q.shape), q.shape[0]==4     (1, True)
Return sphere point perpendicular to axis.
Return axis, which arc is nearest to point.
Return length, i.e. Euclidean norm, of ndarray along axis.      >>> v = np.random.random(3)     >>> n = vector_norm(v)     >>> np.allclose(n, np.linalg.norm(v))     True     >>> v = np.random.rand(6, 5, 3)     >>> n = vector_norm(v, axis=-1)     >>> np.allclose(n, np.sqrt(np.sum(v*v, axis=2)))     True     >>> n = vector_norm(v, axis=1)     >>> np.allclose(n, np.sqrt(np.sum(v*v, axis=1)))     True     >>> v = np.random.rand(5, 4, 3)     >>> n = np.empty((5, 3))     >>> vector_norm(v, axis=1, out=n)     >>> np.allclose(n, np.sqrt(np.sum(v*v, axis=1)))     True     >>> vector_norm([])     0.0     >>> vector_norm([1])     1.0
Return vector perpendicular to vectors.      >>> v = vector_product([2, 0, 0], [0, 3, 0])     >>> np.allclose(v, [0, 0, 6])     True     >>> v0 = [[2, 0, 0, 2], [0, 2, 0, 2], [0, 0, 2, 2]]     >>> v1 = [[3], [0], [0]]     >>> v = vector_product(v0, v1)     >>> np.allclose(v, [[0, 0, 0, 0], [0, 0, 6, 6], [0, -6, 0, -6]])     True     >>> v0 = [[2, 0, 0], [2, 0, 0], [0, 2, 0], [2, 0, 0]]     >>> v1 = [[0, 3, 0], [0, 0, 3], [0, 0, 3], [3, 3, 3]]     >>> v = vector_product(v0, v1, axis=1)     >>> np.allclose(v, [[0, 0, 6], [0, -6, 0], [6, 0, 0], [0, -6, 6]])     True
Return concatenation of series of transformation matrices.      >>> M = np.random.rand(16).reshape((4, 4)) - 0.5     >>> np.allclose(M, concatenate_matrices(M))     True     >>> np.allclose(np.dot(M, M.T), concatenate_matrices(M, M.T))     True
Return True if two matrices perform same transformation.      >>> is_same_transform(np.identity(4), np.identity(4))     True     >>> is_same_transform(np.identity(4), random_rotation_matrix())     False
Return True if two quaternions are equal.
Given a transformation matrix, apply its rotation     around a point in space.      Parameters     ----------     matrix: (4,4) or (3, 3) float, transformation matrix     point:  (3,) or (2,)  float, point in space      Returns     ---------     result: (4,4) transformation matrix
2D homogeonous transformation matrix      Parameters     ----------     offset : (2,) float       XY offset     theta : float       Rotation around Z in radians     point :  (2, ) float       point to rotate around      Returns     ----------     matrix : (3, 3) flat       Homogenous 2D transformation matrix
Given a 2D homogenous rotation matrix convert it to a 3D rotation     matrix that is rotating around the Z axis      Parameters     ----------     matrix_2D: (3,3) float, homogenous 2D rotation matrix      Returns     ----------     matrix_3D: (4,4) float, homogenous 3D rotation matrix
Give a spherical coordinate vector, find the rotation that will     transform a [0,0,1] vector to those coordinates      Parameters     -----------     theta: float, rotation angle in radians     phi:   float, rotation angle in radians      Returns     ----------     matrix: (4,4) rotation matrix where the following will              be a cartesian vector in the direction of the              input spherical coordinats:                 np.dot(matrix, [0,0,1,0])
Returns points, rotated by transformation matrix      If points is (n,2), matrix must be (3,3)     if points is (n,3), matrix must be (4,4)      Parameters     ----------     points : (n, d) float       Points where d is 2 or 3     matrix : (3,3) or (4,4) float       Homogenous rotation matrix     translate : bool       Apply translation from matrix or not      Returns     ----------     transformed : (n,d) float       Transformed points
Check to make sure a homogeonous transformation matrix is     a rigid body transform.      Parameters     -----------     matrix: possibly a transformation matrix      Returns     -----------     check: bool, True if matrix is a valid (4,4) rigid body transform.
Place Arcball, e.g. when window size changes.          center : sequence[2]             Window coordinates of trackball center.         radius : float             Radius of trackball in window coordinates.
Set axes to constrain rotations.
Set initial cursor window coordinates and pick constrain-axis.
Continue rotation in direction of last drag.
Make sure an input can be returned as a valid polygon.      Parameters     -------------     obj : shapely.geometry.Polygon, str (wkb), or (n, 2) float       Object which might be a polygon      Returns     ------------     polygon : shapely.geometry.Polygon       Valid polygon object      Raises     -------------     ValueError       If a valid finite- area polygon isn't available
Extrude a 2D shapely polygon into a 3D mesh      Parameters     ----------     polygon : shapely.geometry.Polygon       2D geometry to extrude     height : float       Distance to extrude polygon along Z     **kwargs:         passed to Trimesh      Returns     ----------     mesh : trimesh.Trimesh       Resulting extrusion as watertight body
Extrude a 2D shapely polygon into a 3D mesh along an     arbitrary 3D path. Doesn't handle sharp curvature.       Parameters     ----------     polygon : shapely.geometry.Polygon       Profile to sweep along path     path : (n, 3) float       A path in 3D     angles :  (n,) float       Optional rotation angle relative to prior vertex       at each vertex      Returns     -------     mesh : trimesh.Trimesh       Geometry of result
Turn a 2D triangulation into a watertight Trimesh.      Parameters     ----------     vertices : (n, 2) float       2D vertices     faces : (m, 3) int       Triangle indexes of vertices     height : float       Distance to extrude triangulation     **kwargs:         passed to Trimesh      Returns     ---------     mesh : trimesh.Trimesh       Mesh created from extrusion
Given a shapely polygon create a triangulation using one     of the python interfaces to triangle.c:     > pip install meshpy     > pip install triangle      Parameters     ---------     polygon : Shapely.geometry.Polygon         Polygon object to be triangulated     triangle_args : str         Passed to triangle.triangulate     engine : str         'meshpy', 'triangle', or 'auto'     kwargs: passed directly to meshpy.triangle.build:             triangle.build(mesh_info,                            verbose=False,                            refinement_func=None,                            attributes=False,                            volume_constraints=True,                            max_volume=None,                            allow_boundary_steiner=True,                            allow_volume_steiner=True,                            quality_meshing=True,                            generate_edges=None,                            generate_faces=False,                            min_angle=None)      Returns     --------------     vertices : (n, 2) float        Points in space     faces :    (n, 3) int        Index of vertices that make up triangles
Given a shapely polygon generate the data to pass to     the triangle mesh generator      Parameters     ---------     polygon : Shapely.geometry.Polygon       Input geometry      Returns     --------     result : dict       Has keys: vertices, segments, holes
Return a cuboid.      Parameters     ------------     extents : float, or (3,) float       Edge lengths     transform: (4, 4) float       Transformation matrix     **kwargs:         passed to Trimesh to create box      Returns     ------------     geometry : trimesh.Trimesh       Mesh of a cuboid
Create an icosahedron, a 20 faced polyhedron.      Returns     -------------     ico : trimesh.Trimesh       Icosahederon centered at the origin.
Create an isophere centered at the origin.      Parameters     ----------     subdivisions : int       How many times to subdivide the mesh.       Note that the number of faces will grow as function of       4 ** subdivisions, so you probably want to keep this under ~5     radius : float       Desired radius of sphere     color: (3,) float or uint8       Desired color of sphere      Returns     ---------     ico : trimesh.Trimesh       Meshed sphere
Create a UV sphere (latitude + longitude) centered at the     origin. Roughly one order of magnitude faster than an     icosphere but slightly uglier.      Parameters     ----------     radius : float       Radius of sphere     count : (2,) int       Number of latitude and longitude lines     theta : (n,) float       Optional theta angles in radians     phi :   (n,) float       Optional phi angles in radians      Returns     ----------     mesh : trimesh.Trimesh        Mesh of UV sphere with specified parameters
Create a mesh of a capsule, or a cylinder with hemispheric ends.      Parameters     ----------     height : float       Center to center distance of two spheres     radius : float       Radius of the cylinder and hemispheres     count : (2,) int       Number of sections on latitude and longitude      Returns     ----------     capsule : trimesh.Trimesh       Capsule geometry with:         - cylinder axis is along Z         - one hemisphere is centered at the origin         - other hemisphere is centered along the Z axis at height
Create a mesh of a cylinder along Z centered at the origin.      Parameters     ----------     radius : float       The radius of the cylinder     height : float       The height of the cylinder     sections : int       How many pie wedges should the cylinder have     segment : (2, 3) float       Endpoints of axis, overrides transform and height     transform : (4, 4) float       Transform to apply     **kwargs:         passed to Trimesh to create cylinder      Returns     ----------     cylinder: trimesh.Trimesh       Resulting mesh of a cylinder
Create a mesh of an annular cylinder along Z,     centered at the origin.      Parameters     ----------     r_min : float       The inner radius of the annular cylinder     r_max : float       The outer radius of the annular cylinder     height : float       The height of the annular cylinder     sections : int       How many pie wedges should the annular cylinder have     **kwargs:         passed to Trimesh to create annulus      Returns     ----------     annulus : trimesh.Trimesh       Mesh of annular cylinder
Return random triangles as a Trimesh      Parameters     -----------     face_count : int       Number of faces desired in mesh      Returns     -----------     soup : trimesh.Trimesh       Geometry with face_count random faces
Return an XYZ axis marker as a  Trimesh, which represents position     and orientation. If you set the origin size the other parameters     will be set relative to it.      Parameters     ----------     transform : (4, 4) float       Transformation matrix     origin_size : float       Radius of sphere that represents the origin     origin_color : (3,) float or int, uint8 or float       Color of the origin     axis_radius : float       Radius of cylinder that represents x, y, z axis     axis_length: float       Length of cylinder that represents x, y, z axis      Returns     -------     marker : trimesh.Trimesh       Mesh geometry of axis indicators
Create a visual marker for a camera object, including an axis and FOV.      Parameters     ---------------     camera : trimesh.scene.Camera       Camera object with FOV and transform defined     marker_height : float       How far along the camera Z should FOV indicators be     origin_size : float       Sphere radius of the origin (default: marker_height / 10.0)      Returns     ------------     meshes : list       Contains Trimesh and Path3D objects which can be visualized
Get a new Trimesh object representing the convex hull of the     current mesh, with proper normals and watertight.     Requires scipy >.12.      Arguments     --------     obj : Trimesh, or (n,3) float       Mesh or cartesian points      Returns     --------     convex : Trimesh       Mesh of convex hull
Test if a mesh is convex by projecting the vertices of     a triangle onto the normal of its adjacent face.      Parameters     ----------     mesh : Trimesh       Input geometry      Returns     ----------     projection : (len(mesh.face_adjacency),) float       Distance of projection of adjacent vertex onto plane
Check if a mesh is convex.      Parameters     -----------     mesh : Trimesh       Input geometry      Returns     -----------     convex : bool       Was passed mesh convex or not
Try to extract a convex set of points from multiple input formats.      Parameters     ---------     obj: Trimesh object          (n,d) points          (m,) Trimesh objects      Returns     --------     points: (o,d) convex set of points
If the first point is the same as the end point         the entity is closed
Returns an (n,2) list of nodes, or vertices on the path.         Note that this generic class function assumes that all of the         reference points are on the path which is true for lines and         three point arcs.          If you were to define another class where that wasn't the case         (for example, the control points of a bezier curve),         you would need to implement an entity- specific version of this         function.          The purpose of having a list of nodes is so that they can then be         added as edges to a graph so we can use functions to check         connectivity, extract paths, etc.          The slicing on this function is essentially just tiling points         so the first and last vertices aren't repeated. Example:          self.points = [0,1,2]         returns:      [[0,1], [1,2]]
Return the total length of the entity.          Returns         ---------         length: float, total length of entity
Plot the text using matplotlib.          Parameters         --------------         vertices : (n, 2) float           Vertices in space         show : bool           If True, call plt.show()
If Text is 2D, get the rotation angle in radians.          Parameters         -----------         vertices : (n, 2) float           Vertices in space referenced by self.points          Returns         ---------         angle : float           Rotation angle in radians
Discretize into a world- space path.          Parameters         ------------         vertices: (n, dimension) float           Points in space         scale : float           Size of overall scene for numerical comparisons          Returns         -------------         discrete: (m, dimension) float           Path in space composed of line segments
Is the current entity valid.          Returns         -----------         valid : bool           Is the current entity well formed
If the current Line entity consists of multiple line         break it up into n Line entities.          Returns         ----------         exploded: (n,) Line entities
Discretize the arc entity into line sections.          Parameters         ------------         vertices : (n, dimension) float             Points in space         scale : float             Size of overall scene for numerical comparisons          Returns         -------------         discrete: (m, dimension) float, linear path in space
Return the AABB of the arc entity.          Parameters         -----------         vertices: (n,dimension) float, vertices in space          Returns         -----------         bounds: (2, dimension) float, (min, max) coordinate of AABB
Discretize the Bezier curve.          Parameters         -------------         vertices : (n, 2) or (n, 3) float           Points in space         scale : float           Scale of overall drawings (for precision)         count : int           Number of segments to return          Returns         -------------         discrete : (m, 2) or (m, 3) float           Curve as line segments
Discretize the B-Spline curve.          Parameters         -------------         vertices : (n, 2) or (n, 3) float           Points in space         scale : float           Scale of overall drawings (for precision)         count : int           Number of segments to return          Returns         -------------         discrete : (m, 2) or (m, 3) float           Curve as line segments
Returns a dictionary with all of the information         about the entity.
For 3D line segments defined by two points, turn     them in to an origin defined as the closest point along     the line to the zero origin as well as a direction vector     and start and end parameter.      Parameters     ------------     segments : (n, 2, 3) float        Line segments defined by start and end points      Returns     --------------     origins : (n, 3) float        Point on line closest to [0, 0, 0]     vectors : (n, 3) float        Unit line directions     parameters : (n, 2) float        Start and end distance pairs for each line
Load a 3D XAML file.      Parameters     ----------     file_obj : file object                 Open, containing XAML file      Returns     ----------     result : dict                 kwargs for a trimesh constructor, including:                 vertices:       (n,3) np.float64, points in space                 faces:          (m,3) np.int64, indices of vertices                 face_colors:    (m,4) np.uint8, RGBA colors                 vertex_normals: (n,3) np.float64, vertex normals
Load a 3DXML scene into kwargs.      Parameters     ------------     file_obj : file object       Open and containing 3DXML data      Returns     -----------     kwargs : dict       Can be passed to trimesh.exchange.load.load_kwargs
Pretty- print an lxml.etree element.      Parameters     ------------     element : etree element
Removes duplicate vertices based on integer hashes of     each row.      Parameters     -------------     mesh : Trimesh object       Mesh to merge vertices on     digits : int       How many digits to consider for vertices       If not specified uses tol.merge     textured : bool       If True, for textured meshes only merge vertices       with identical positions AND UV coordinates.       No effect on untextured meshes     uv_digits : int       Number of digits to consider for UV coordinates.
Return the indices of values that are identical      Parameters     ----------     values:     1D array     min_len:    int, the shortest group allowed                 All groups will have len >= min_length     max_len:    int, the longest group allowed                 All groups will have len <= max_length      Returns     ----------     groups: sequence of indices to form groups             IE [0,1,0,1] returns [[0,2], [1,3]]
We turn our array into integers based on the precision     given by digits and then put them in a hashable format.      Parameters     ---------     data : (n, m) array       Input data     digits : int or None       How many digits to add to hash if data is floating point       If None, tol.merge will be used      Returns     ---------     hashable : (n,) array       Custom data type which can be sorted       or used as hash keys
Given a numpy array of float/bool/int, return as integers.      Parameters     -------------     data :  (n, d) float, int, or bool       Input data     digits : float or int       Precision for float conversion     dtype : numpy.dtype       What datatype should result be returned as      Returns     -------------     as_int : (n, d) int       Data as integers
Returns the same as np.unique, but ordered as per the     first occurrence of the unique value in data.      Examples     ---------     In [1]: a = [0, 3, 3, 4, 1, 3, 0, 3, 2, 1]      In [2]: np.unique(a)     Out[2]: array([0, 1, 2, 3, 4])      In [3]: trimesh.grouping.unique_ordered(a)     Out[3]: array([0, 3, 4, 1, 2])
For arrays of integers find unique values using bin counting.     Roughly 10x faster for correct input than np.unique      Parameters     --------------     values : (n,) int       Values to find unique members of     minlength : int       Maximum value that will occur in values (values.max())     return_inverse : bool       If True, return an inverse such that unique[inverse] == values      Returns     ------------     unique : (m,) int       Unique values in original array     inverse : (n,) int       An array such that unique[inverse] == values       Only returned if return_inverse is True
Merge duplicate sequential values. This differs from unique_ordered     in that values can occur in multiple places in the sequence, but     only consecutive repeats are removed      Parameters     -----------     data: (n,) float or int      Returns     --------     merged: (m,) float or int      Examples     ---------     In [1]: a     Out[1]:     array([-1, -1, -1,  0,  0,  1,  1,  2,  0,             3,  3,  4,  4,  5,  5,  6,  6,  7,             7,  8,  8,  9,  9,  9])      In [2]: trimesh.grouping.merge_runs(a)     Out[2]: array([-1,  0,  1,  2,  0,  3,  4,  5,  6,  7,  8,  9])
Identical to the numpy.unique command, except evaluates floating point     numbers, using a specified number of digits.      If digits isn't specified, the library default TOL_MERGE will be used.
Returns indices of unique rows. It will return the     first occurrence of a row that is duplicated:     [[1,2], [3,4], [1,2]] will return [0,1]      Parameters     ---------     data: (n,m) set of floating point data     digits: how many digits to consider for the purposes of uniqueness      Returns     --------     unique:  (j) array, index in data which is a unique row     inverse: (n) length array to reconstruct original                  example: unique[inverse] == data
For a 2D array of integers find the position of a value in each     row which only occurs once. If there are more than one value per     row which occur once, the last one is returned.      Parameters     ----------     data:   (n,d) int     unique: (m) int, list of unique values contained in data.              speedup purposes only, generated from np.unique if not passed      Returns     ---------     result: (n,d) bool, with one or zero True values per row.       Examples     -------------------------------------     In [0]: r = np.array([[-1,  1,  1],                           [-1,  1, -1],                           [-1,  1,  1],                           [-1,  1, -1],                           [-1,  1, -1]], dtype=np.int8)      In [1]: unique_value_in_row(r)     Out[1]:            array([[ True, False, False],                   [False,  True, False],                   [ True, False, False],                   [False,  True, False],                   [False,  True, False]], dtype=bool)      In [2]: unique_value_in_row(r).sum(axis=1)     Out[2]: array([1, 1, 1, 1, 1])      In [3]: r[unique_value_in_row(r)]     Out[3]: array([-1,  1, -1,  1,  1], dtype=int8)
Returns index groups of duplicate rows, for example:     [[1,2], [3,4], [1,2]] will return [[0,2], [1]]      Parameters     ----------     data:          (n,m) array     require_count: only returns groups of a specified length, eg:                    require_count =  2                    [[1,2], [3,4], [1,2]] will return [[0,2]]                     Note that using require_count allows numpy advanced indexing                    to be used in place of looping and checking hashes, and as a                    consequence is ~10x faster.      digits:        If data is floating point, how many decimals to look at.                    If this is None, the value in TOL_MERGE will be turned into a                    digit count and used.      Returns     ----------     groups:        List or sequence of indices from data indicating identical rows.                    If require_count != None, shape will be (j, require_count)                    If require_count is None, shape will be irregular (AKA a sequence)
Find the rows in two arrays which occur in both rows.      Parameters     ---------     a: (n, d) int         Array with row vectors     b: (m, d) int         Array with row vectors     operation : function         Numpy boolean set operation function:           -np.intersect1d           -np.setdiff1d      Returns     --------     shared: (p, d) array containing rows in both a and b
Group vectors based on an angle tolerance, with the option to     include negative vectors.      Parameters     -----------     vectors : (n,3) float         Direction vector     angle : float         Group vectors closer than this angle in radians     include_negative : bool         If True consider the same:         [0,0,1] and [0,0,-1]      Returns     ------------     new_vectors : (m,3) float         Direction vector     groups : (m,) sequence of int         Indices of source vectors
Find groups of points which have neighbours closer than radius,     where no two points in a group are farther than distance apart.      Parameters     ---------     points :   (n, d) float         Points of dimension d     distance : float         Max distance between points in a cluster      Returns     ----------     unique : (m, d) float         Median value of each group     groups : (m) sequence of int         Indexes of points that make up a group
Find clusters of points which have neighbours closer than radius      Parameters     ---------     points : (n, d) float         Points of dimension d     radius : float         Max distance between points in a cluster      Returns     ----------     groups : (m,) sequence of int         Indices of points in a cluster
Given an array, find the indices of contiguous blocks     of equal values.      Parameters     ---------     data:    (n) array     min_len: int, the minimum length group to be returned     max_len: int, the maximum length group to be retuurned     digits:  if dealing with floats, how many digits to use     only_nonzero: bool, only return blocks of non- zero values      Returns     ---------     blocks: (m) sequence of indices referencing data
Given a list of groups, find the minimum element of data within each group      Parameters     -----------     groups : (n,) sequence of (q,) int         Indexes of each group corresponding to each element in data     data : (m,)         The data that groups indexes reference      Returns     -----------     minimums : (n,)         Minimum value of data per group
Compute the minimum n- sphere for a mesh or a set of points.      Uses the fact that the minimum n- sphere will be centered at one of     the vertices of the furthest site voronoi diagram, which is n*log(n)     but should be pretty fast due to using the scipy/qhull implementations     of convex hulls and voronoi diagrams.      Parameters     ----------     obj : (n, d) float or trimesh.Trimesh       Points or mesh to find minimum bounidng nsphere      Returns     ----------     center : (d,) float       Center of fitted n- sphere     radius : float       Radius of fitted n-sphere
Fit an n-sphere to a set of points using least squares.      Parameters     ---------     points : (n, d) float       Points in space     prior : (d,) float       Best guess for center of nsphere      Returns     ---------     center : (d,) float       Location of center     radius : float       Mean radius across circle     error : float       Peak to peak value of deviation from mean radius
Check if a list of points is an nsphere.      Parameters     -----------     points : (n, dimension) float       Points in space      Returns     -----------     check : bool       True if input points are on an nsphere
Check if a mesh contains a set of points, using ray tests.      If the point is on the surface of the mesh, behavior is     undefined.      Parameters     ---------     mesh: Trimesh object     points: (n,3) points in space      Returns     ---------     contains : (n) bool                   Whether point is inside mesh or not
Create a BVHModel object from a Trimesh object      Parameters     -----------     mesh : Trimesh       Input geometry      Returns     ------------     bvh : fcl.BVHModel       BVH of input geometry
Create collision objects from a trimesh.Scene object.      Parameters     ------------     scene : trimesh.Scene       Scene to create collision objects for      Returns     ------------     manager : CollisionManager       CollisionManager for objects in scene     objects: {node name: CollisionObject}       Collision objects for nodes in scene
Add an object to the collision manager.          If an object with the given name is already in the manager,         replace it.          Parameters         ----------         name : str           An identifier for the object         mesh : Trimesh object           The geometry of the collision object         transform : (4,4) float           Homogenous transform matrix for the object
Delete an object from the collision manager.          Parameters         ----------         name : str           The identifier for the object
Set the transform for one of the manager's objects.         This replaces the prior transform.          Parameters         ----------         name : str           An identifier for the object already in the manager         transform : (4,4) float           A new homogenous transform matrix for the object
Check a single object for collisions against all objects in the         manager.          Parameters         ----------         mesh : Trimesh object           The geometry of the collision object         transform : (4,4) float           Homogenous transform matrix         return_names : bool           If true, a set is returned containing the names           of all objects in collision with the object         return_data :  bool           If true, a list of ContactData is returned as well          Returns         ------------         is_collision : bool           True if a collision occurs and False otherwise         names : set of str           The set of names of objects that collided with the           provided one         contacts : list of ContactData           All contacts detected
Check if any object from this manager collides with any object         from another manager.          Parameters         -------------------         other_manager : CollisionManager           Another collision manager object         return_names : bool           If true, a set is returned containing the names           of all pairs of objects in collision.         return_data : bool           If true, a list of ContactData is returned as well          Returns         -------------         is_collision : bool           True if a collision occurred between any pair of objects           and False otherwise         names : set of 2-tup           The set of pairwise collisions. Each tuple           contains two names (first from this manager,           second from the other_manager) indicating           that the two corresponding objects are in collision.         contacts : list of ContactData           All contacts detected
Get the minimum distance between a single object and any         object in the manager.          Parameters         ---------------         mesh : Trimesh object           The geometry of the collision object         transform : (4,4) float           Homogenous transform matrix for the object         return_names : bool           If true, return name of the closest object         return_data : bool           If true, a DistanceData object is returned as well          Returns         -------------         distance : float           Min distance between mesh and any object in the manager         name : str           The name of the object in the manager that was closest         data : DistanceData           Extra data about the distance query
Get the minimum distance between any pair of objects in the manager.          Parameters         -------------         return_names : bool           If true, a 2-tuple is returned containing the names           of the closest objects.         return_data : bool           If true, a DistanceData object is returned as well          Returns         -----------         distance : float           Min distance between any two managed objects         names : (2,) str           The names of the closest objects         data : DistanceData           Extra data about the distance query
Get the minimum distance between any pair of objects,         one in each manager.          Parameters         ----------         other_manager : CollisionManager           Another collision manager object         return_names : bool           If true, a 2-tuple is returned containing           the names of the closest objects.         return_data : bool           If true, a DistanceData object is returned as well          Returns         -----------         distance : float           The min distance between a pair of objects,           one from each manager.         names : 2-tup of str           A 2-tuple containing two names (first from this manager,           second from the other_manager) indicating           the two closest objects.         data : DistanceData           Extra data about the distance query
Return the inertia tensor of a cylinder.      Parameters     ------------     mass : float       Mass of cylinder     radius : float       Radius of cylinder     height : float       Height of cylinder     transform : (4,4) float       Transformation of cylinder      Returns     ------------     inertia : (3,3) float       Inertia tensor
Find the principal components and principal axis     of inertia from the inertia tensor.      Parameters     ------------     inertia : (3,3) float       Inertia tensor      Returns     ------------     components : (3,) float       Principal components of inertia     vectors : (3,3) float       Row vectors pointing along the       principal axes of inertia
Transform an inertia tensor to a new frame.      More details in OCW PDF:     MIT16_07F09_Lec26.pdf      Parameters     ------------     transform : (3, 3) or (4, 4) float       Transformation matrix     inertia_tensor : (3, 3) float       Inertia tensor      Returns     ------------     transformed : (3, 3) float       Inertia tensor in new frame
Check whether a mesh has rotational symmetry.      Returns     -----------     symmetry : None or str          None         No rotational symmetry          'radial'     Symmetric around an axis          'spherical'  Symmetric around a point     axis : None or (3,) float       Rotation axis or point     section : None or (3, 2) float       If radial symmetry provide vectors       to get cross section
Generate a list of lights for a scene that looks decent.      Parameters     --------------     scene : trimesh.Scene       Scene with geometry      Returns     --------------     lights : [Light]       List of light objects     transforms : (len(lights), 4, 4) float       Transformation matrices for light positions.
Properly subclass a numpy ndarray to track changes.      Avoids some pitfalls of subclassing by forcing contiguous     arrays, and does a view into a TrackedArray.      Parameters     ------------     array : array- like object       To be turned into a TrackedArray     dtype : np.dtype       Which dtype to use for the array      Returns     ------------     tracked : TrackedArray       Contains input array data
A decorator for class methods, replaces @property     but will store and retrieve function return values     in object cache.      Parameters     ------------     function : method       This is used as a decorator:       ```       @cache_decorator       def foo(self, things):         return 'happy days'       ```
On certain platforms/builds zlib.adler32 is substantially     faster than zlib.crc32, but it is not consistent across     Windows/Linux/OSX.      This function runs a quick check (2ms on my machines) to     determine the fastest hashing function available in zlib.      Parameters     ------------     count: int, number of repetitions to do on the speed trial      Returns     ----------     crc32: function, either zlib.adler32 or zlib.crc32
Return an MD5 hash of the current array.          Returns         -----------         md5: str, hexadecimal MD5 of the array
A zlib.crc32 or zlib.adler32 checksum         of the current data.          Returns         -----------         crc: int, checksum from zlib.crc32 or zlib.adler32
An xxhash.b64 hash of the array.          Returns         -------------         xx: int, xxhash.xxh64 hash of array.
Remove a key from the cache.
Verify that the cached values are still for the same         value of id_function and delete all stored items if         the value of id_function has changed.
Remove all elements in the cache.
Is the current DataStore empty or not.          Returns         ----------         empty: bool, False if there are items in the DataStore
Get an MD5 reflecting everything in the DataStore.          Returns         ----------         md5: str, MD5 in hexadecimal
Get a CRC reflecting everything in the DataStore.          Returns         ----------         crc: int, CRC of data
Get a CRC32 or xxhash.xxh64 reflecting the DataStore.          Returns         ------------         hashed: int, checksum of data
Return a basic identifier for a mesh, consisting of properties     that have been hand tuned to be somewhat robust to rigid     transformations and different tesselations.      Parameters     ----------     mesh : Trimesh object       Source geometry      Returns     ----------     identifier : (6,) float       Identifying values of the mesh
Hash an identifier array to a specified number of     significant figures.      Parameters     ----------     identifier : (n,) float       Vector of properties     sigfig : (n,) int       Number of sigfigs per property      Returns     ----------     md5 : str       MD5 hash of identifier
Triangles with three different length sides are     ordered in two ways:     [small edge, medium edge, large edge] (SML)     [small edge, large edge,  medium edge] (SLM)      This function returns [-1, 0, 1], depending on whether     the triangle is SML or SLM, and 0 if M == L.      The reason this is useful as it as a rare property that is     invariant to translation and rotation but changes when a     mesh is reflected or inverted. It is NOT invariant to     different tesselations of the same surface.      Parameters     -------------     mesh : trimesh.Trimesh       Source geometry to calculate ordering on      Returns     --------------     order : (len(mesh.faces), ) int       Is each face SML (-1), SLM (+1), or M==L (0)
Try to convert various geometry objects to the constructor     args for a pyglet indexed vertex list.      Parameters     ------------     obj : Trimesh, Path2D, Path3D, (n,2) float, (n,3) float       Object to render      Returns     ------------     args : tuple       Args to be passed to pyglet indexed vertex list       constructor.
Convert a Trimesh object to arguments for an     indexed vertex list constructor.      Parameters     -------------     mesh : trimesh.Trimesh       Mesh to be rendered     group : str       Rendering group for the vertex list     smooth : bool       Should we try to smooth shade the mesh     smooth_threshold : int       Maximum number of faces to smooth shade      Returns     --------------     args : (7,) tuple       Args for vertex list constructor
Convert a Path3D object to arguments for an     indexed vertex list constructor.      Parameters     -------------     path : trimesh.path.Path3D object       Mesh to be rendered     group : str       Rendering group for the vertex list      Returns     --------------     args : (7,) tuple       Args for vertex list constructor
Convert a numpy array of 3D points to args for     a vertex list constructor.      Parameters     -------------     points : (n, 3) float       Points to be rendered     colors : (n, 3) or (n, 4) float       Colors for each point     group : str       Rendering group for the vertex list      Returns     --------------     args : (7,) tuple       Args for vertex list constructor
Given a list of colors (or None) return a GL- acceptable list of colors      Parameters     ------------     colors: (count, (3 or 4)) float       Input colors as an array      Returns     ---------     colors_type : str       Color type     colors_gl : (count,) list       Colors to pass to pyglet
Convert a trimesh.visual.texture.Material object into     a pyglet- compatible texture object.      Parameters     --------------     material : trimesh.visual.texture.Material       Material to be converted      Returns     ---------------     texture : pyglet.image.Texture       Texture loaded into pyglet form
Convert a numpy row- major homogenous transformation matrix     to a flat column- major GLfloat transformation.      Parameters     -------------     matrix : (4,4) float       Row- major homogenous transform      Returns     -------------     glmatrix : (16,) gl.GLfloat       Transform in pyglet format
Convert an array and an optional set of args into a     flat vector of gl.GLfloat
Convert trimesh.scene.lighting.Light objects into     args for gl.glLightFv calls      Parameters     --------------     light : trimesh.scene.lighting.Light       Light object to be converted to GL     transform : (4, 4) float       Transformation matrix of light     lightN : int       Result of gl.GL_LIGHT0, gl.GL_LIGHT1, etc      Returns     --------------     multiarg : [tuple]       List of args to pass to gl.glLightFv eg:       [gl.glLightfb(*a) for a in multiarg]
Record an initial mouse press at a given point.          Parameters         ----------         point : (2,) int             The x and y pixel coordinates of the mouse press.
Update the tracball during a drag.          Parameters         ----------         point : (2,) int             The current x and y pixel coordinates of the mouse during a drag.             This will compute a movement for the trackball with the relative             motion between this point and the one marked by down().
Zoom using a mouse scroll wheel motion.          Parameters         ----------         clicks : int             The number of clicks. Positive numbers indicate forward wheel             movement.
Rotate the trackball about the "Up" axis by azimuth radians.          Parameters         ----------         azimuth : float             The number of radians to rotate.
Symbolically integrate a function(x,y,z) across a triangle or mesh.      Parameters     ----------     function: string or sympy expression               x, y, z will be replaced with a barycentric representation               and the the function is integrated across the triangle.      Returns     ----------     evaluator: numpy lambda function of result which takes a mesh     expr:      sympy expression of result      Examples     -----------      In [1]: function = '1'      In [2]: integrator, expr = integrate_barycentric(function)      In [3]: integrator     Out[3]: <__main__.evaluator instance at 0x7f66cd2a6200>      In [4]: expr     Out[4]: 1/2      In [5]: result  = integrator(mesh)      In [6]: mesh.area     Out[6]: 34.641016151377542      In [7]: result.sum()     Out[7]: 34.641016151377542
Generate transform for a camera to keep a list     of points in the camera's field of view.      Parameters     -------------     points : (n, 3) float       Points in space     fov : (2,) float       Field of view, in DEGREES     rotation : None, or (4, 4) float       Rotation matrix for initial rotation     center : None, or (3,) float       Center of field of view.      Returns     --------------     transform : (4, 4) float       Transformation matrix with points in view
Convert a trimesh.scene.Camera object to ray origins     and direction vectors. Will return one ray per pixel,     as set in camera.resolution.      Parameters     --------------     camera : trimesh.scene.Camera       Camera with transform defined      Returns     --------------     origins : (n, 3) float       Ray origins in space     vectors : (n, 3) float       Ray direction unit vectors     angles : (n, 2) float       Ray spherical coordinate angles in radians
Set the camera resolution in pixels.          Parameters         ------------         resolution (2,) float           Camera resolution in pixels
Set the reference to the scene that this camera is in.          Parameters         -------------         scene : None, or trimesh.Scene           Scene where this camera is attached
Get the (4, 4) homogenous transformation from the         world frame to this camera object.          Returns         ------------         transform : (4, 4) float           Transform from world to camera
Get the focal length in pixels for the camera.          Returns         ------------         focal : (2,) float           Focal length in pixels
Get the intrinsic matrix for the Camera object.          Returns         -----------         K : (3, 3) float           Intrinsic matrix for camera
Get the field of view in degrees.          Returns         -------------         fov : (2,) float           XY field of view in degrees
Set the field of view in degrees.          Parameters         -------------         values : (2,) float           Size of FOV to set in degrees
A callback passed to a scene viewer which will update     transforms in the viewer periodically.      Parameters     -------------     scene : trimesh.Scene       Scene containing geometry
Returns a surface mesh from CAD model in Open Cascade     Breap (.brep), Step (.stp or .step) and Iges formats     Or returns a surface mesh from 3D volume mesh using gmsh.      For a list of possible options to pass to GMSH, check:     http://gmsh.info/doc/texinfo/gmsh.html      An easy way to install the GMSH SDK is through the `gmsh-sdk`     package on PyPi, which downloads and sets up gmsh:         >>> pip install gmsh-sdk      Parameters     --------------     file_name : str       Location of the file to be imported     gmsh_args : (n, 2) list       List of (parameter, value) pairs to be passed to       gmsh.option.setNumber     max_element : float or None       Maximum length of an element in the volume mesh      Returns     ------------     mesh : trimesh.Trimesh       Surface mesh of input geometry
Convert a surface mesh to a 3D volume mesh generated by gmsh.      An easy way to install the gmsh sdk is through the gmsh-sdk     package on pypi, which downloads and sets up gmsh:         pip install gmsh-sdk      Algorithm details, although check gmsh docs for more information:     The "Delaunay" algorithm is split into three separate steps.     First, an initial mesh of the union of all the volumes in the model is performed,     without inserting points in the volume. The surface mesh is then recovered using H.     Si's boundary recovery algorithm Tetgen/BR. Then a three-dimensional version of the     2D Delaunay algorithm described above is applied to insert points in the volume to     respect the mesh size constraints.      The Frontal" algorithm uses J. Schoeberl's Netgen algorithm.     The "HXT" algorithm is a new efficient and parallel reimplementaton     of the Delaunay algorithm.     The "MMG3D" algorithm (experimental) allows to generate     anisotropic tetrahedralizations       Parameters     --------------     mesh : trimesh.Trimesh       Surface mesh of input geometry     file_name : str or None       Location to save output, in .msh (gmsh) or .bdf (Nastran) format     max_element : float or None       Maximum length of an element in the volume mesh     mesher_id : int       3D unstructured algorithms:       1: Delaunay, 4: Frontal, 7: MMG3D, 10: HXT      Returns     ------------     data : None or bytes       MSH data, only returned if file_name is None
Convert a Trimesh object into a URDF package for physics simulation.     This breaks the mesh into convex pieces and writes them to the same     directory as the .urdf file.      Parameters     ---------     mesh      : Trimesh object     directory : str                   The directory path for the URDF package      Returns     ---------     mesh : Trimesh object              Multi-body mesh containing convex decomposition
Voxelize a surface by subdividing a mesh until every edge is     shorter than: (pitch / edge_factor)      Parameters     -----------     mesh:        Trimesh object     pitch:       float, side length of a single voxel cube     max_iter:    int, cap maximum subdivisions or None for no limit.     edge_factor: float,      Returns     -----------     voxels_sparse:   (n,3) int, (m,n,p) indexes of filled cells     origin_position: (3,) float, position of the voxel                                  grid origin in space
Voxelize a mesh in the region of a cube around a point. When fill=True,     uses proximity.contains to fill the resulting voxels so may be meaningless     for non-watertight meshes. Useful to reduce memory cost for small values of     pitch as opposed to global voxelization.      Parameters     -----------     mesh : trimesh.Trimesh       Source geometry     point : (3, ) float       Point in space to voxelize around     pitch :  float       Side length of a single voxel cube     radius : int       Number of voxel cubes to return in each direction.     kwargs : parameters to pass to voxelize_subdivide      Returns     -----------     voxels : (m, m, m) bool       Array of local voxels where m=2*radius+1     origin_position : (3,) float       Position of the voxel grid origin in space
Voxelize a mesh using ray queries.      Parameters     -------------     mesh     : Trimesh object                  Mesh to be voxelized     pitch    : float                  Length of voxel cube     per_cell : (2,) int                  How many ray queries to make per cell      Returns     -------------     voxels : (n, 3) int                  Voxel positions     origin : (3, ) int                  Origin of voxels
Given a sparse surface voxelization, fill in between columns.      Parameters     --------------     occupied: (n, 3) int, location of filled cells      Returns     --------------     filled: (m, 3) int, location of filled cells
Convert center points of an (n,m,p) matrix into its indices.      Parameters     ----------     points: (q, 3) float, center points of voxel matrix (n,m,p)     pitch: float, what pitch was the voxel matrix computed with     origin: (3,) float, what is the origin of the voxel matrix      Returns     ----------     indices: (q, 3) int, list of indices
Convert indices of an (n,m,p) matrix into a set of voxel center points.      Parameters     ----------     indices: (q, 3) int, index of voxel matrix (n,m,p)     pitch: float, what pitch was the voxel matrix computed with     origin: (3,) float, what is the origin of the voxel matrix      Returns     ----------     points: (q, 3) float, list of points
Convert an (n,m,p) matrix into a set of points for each voxel center.      Parameters     -----------     matrix: (n,m,p) bool, voxel matrix     pitch: float, what pitch was the voxel matrix computed with     origin: (3,) float, what is the origin of the voxel matrix      Returns     ----------     points: (q, 3) list of points
Convert an (n,m,p) matrix into a mesh, using marching_cubes.      Parameters     -----------     matrix: (n,m,p) bool, voxel matrix     pitch: float, what pitch was the voxel matrix computed with     origin: (3,) float, what is the origin of the voxel matrix      Returns     ----------     mesh: Trimesh object, generated by meshing voxels using                           the marching cubes algorithm in skimage
Take a sparse (n,3) list of integer indexes of filled cells,     turn it into a dense (m,o,p) matrix.      Parameters     -----------     sparse: (n,3) int, index of filled cells      Returns     ------------     dense: (m,o,p) bool, matrix of filled cells
Return a Trimesh object with a box at every center.      Doesn't do anything nice or fancy.      Parameters     -----------     centers: (n,3) float, center of boxes that are occupied     pitch:   float, the edge length of a voxel     colors: (3,) or (4,) or (n,3) or (n, 4) float, color of boxes      Returns     ---------     rough: Trimesh object representing inputs
Find common rows between two arrays very quickly     using 3D boolean sparse matrices.      Parameters     -----------     a: (n, d)  int, coordinates in space     b: (m, d)  int, coordinates in space     operation: numpy operation function, ie:                   np.logical_and                   np.logical_or      Returns     -----------     coords: (q, d) int, coordinates in space
A marching cubes Trimesh representation of the voxels.          No effort was made to clean or smooth the result in any way;         it is merely the result of applying the scikit-image         measure.marching_cubes function to self.matrix.          Returns         ---------         meshed: Trimesh object representing the current voxel                         object, as returned by marching cubes algorithm.
The center of each filled cell as a list of points.          Returns         ----------         points: (self.filled, 3) float, list of points
Convert a point to an index in the matrix array.          Parameters         ----------         point: (3,) float, point in space          Returns         ---------         index: (3,) int tuple, index in self.matrix
Query a point to see if the voxel cell it lies in is filled or not.          Parameters         ----------         point: (3,) float, point in space          Returns         ---------         is_filled: bool, is cell occupied or not
A rough Trimesh representation of the voxels with a box         for each filled voxel.          Parameters         ----------         colors : (3,) or (4,) float or uint8                  (X, Y, Z, 3) or (X, Y, Z, 4) float or uint8          Where matrix.shape == (X, Y, Z)          Returns         ---------         mesh : trimesh.Trimesh           Mesh with one box per filled cell.
Filled cells on the surface of the mesh.          Returns         ----------------         voxels: (n, 3) int, filled cells on mesh surface
A rough Trimesh representation of the voxels with a box         for each filled voxel.          Parameters         -----------         solid: bool, if True return boxes for sparse_solid          Returns         ---------         mesh: Trimesh object made up of one box per filled cell.
Produce a mesh that is a rectangular solid with noise     with a random transform.      Parameters     -------------     face_count : int       Approximate number of faces desired     extents : (n,3) float       Dimensions of brick     noise : float       Magnitude of vertex noise to apply
Unitize a vector or an array or row- vectors.      Parameters     ---------     vectors : (n,m) or (j) float        Vector or vectors to be unitized     check_valid :  bool        If set, will return mask of nonzero vectors     threshold : float        Cutoff for a value to be considered zero.      Returns     ---------     unit :  (n,m) or (j) float        Input vectors but unitized     valid : (n,) bool or bool         Mask of nonzero vectors returned if `check_valid`
Euclidean distance between vectors a and b.      Parameters     ------------     a : (n,) float        First vector     b : (n,) float        Second vector      Returns     ------------     distance : float         Euclidean distance between A and B
Check to see if an object is None or not.      Handles the case of np.array(None) as well.      Parameters     -------------     obj : object       Any object type to be checked      Returns     -------------     is_none : bool         True if obj is None or numpy None-like
Check if an object is a sequence or not.      Parameters     -------------     obj : object       Any object type to be checked      Returns     -------------     is_sequence : bool         True if object is sequence
Compare the shape of a numpy.ndarray to a target shape,     with any value less than zero being considered a wildcard      Note that if a list- like object is passed that is not a numpy     array, this function will not convert it and will return False.      Parameters     ---------     obj :   np.ndarray        Array to check the shape on     shape : list or tuple        Any negative term will be considered a wildcard        Any tuple term will be evaluated as an OR      Returns     ---------     shape_ok: bool, True if shape of obj matches query shape      Examples     ------------------------     In [1]: a = np.random.random((100, 3))      In [2]: a.shape     Out[2]: (100, 3)      In [3]: trimesh.util.is_shape(a, (-1, 3))     Out[3]: True      In [4]: trimesh.util.is_shape(a, (-1, 3, 5))     Out[4]: False      In [5]: trimesh.util.is_shape(a, (100, -1))     Out[5]: True      In [6]: trimesh.util.is_shape(a, (-1, (3, 4)))     Out[6]: True      In [7]: trimesh.util.is_shape(a, (-1, (4, 5)))     Out[7]: False
Given an object, if it is a sequence return, otherwise     add it to a length 1 sequence and return.      Useful for wrapping functions which sometimes return single     objects and other times return lists of objects.      Parameters     --------------     obj : object       An object to be made a sequence      Returns     --------------     as_sequence : (n,) sequence        Contains input value
For a set of 3D vectors alter the sign so they are all in the     upper hemisphere.      If the vector lies on the plane all vectors with negative Y     will be reversed.      If the vector has a zero Z and Y value vectors with a     negative X value will be reversed.      Parameters     ----------     vectors : (n,3) float       Input vectors     return_sign : bool       Return the sign mask or not      Returns     ----------     oriented: (n, 3) float        Vectors with same magnitude as source        but possibly reversed to ensure all vectors        are in the same hemisphere.     sign : (n,) float
Convert a set of cartesian points to (n,2) spherical unit     vectors.      Parameters     ------------     cartesian : (n, 3) float        Points in space      Returns     ------------     spherical : (n, 2) float        Angles, in radians
Convert a set of (n,2) spherical vectors to (n,3) vectors      Parameters     -----------     spherical : (n , 2) float        Angles, in radians      Returns     -----------     vectors : (n, 3) float       Unit vectors
For an iterable, group values into pairs.      Parameters     -----------     iterable : (m, ) list        A sequence of values      Returns     -----------     pairs: (n, 2)       Pairs of sequential values      Example     -----------     In [1]: data     Out[1]: [0, 1, 2, 3, 4, 5, 6]      In [2]: list(trimesh.util.pairwise(data))     Out[2]: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]
Dot product by row of a and b.      There are a lot of ways to do this though     performance varies very widely. This method     uses the dot product to sum the row and avoids     function calls if at all possible.      Comparing performance of some equivalent versions:     ```     In [1]: import numpy as np; import trimesh      In [2]: a = np.random.random((10000, 3))      In [3]: b = np.random.random((10000, 3))      In [4]: %timeit (a * b).sum(axis=1)     1000 loops, best of 3: 181 us per loop      In [5]: %timeit np.einsum('ij,ij->i', a, b)     10000 loops, best of 3: 62.7 us per loop      In [6]: %timeit np.diag(np.dot(a, b.T))     1 loop, best of 3: 429 ms per loop      In [7]: %timeit np.dot(a * b, np.ones(a.shape[1]))     10000 loops, best of 3: 61.3 us per loop      In [8]: %timeit trimesh.util.diagonal_dot(a, b)     10000 loops, best of 3: 55.2 us per loop     ```      Parameters     ------------     a : (m, d) float       First array     b : (m, d) float       Second array      Returns     -------------     result : (m,) float       Dot product of each row
For a list of (n, 2) or (n, 3) points return them     as (n, 3) 3D points, 2D points on the XY plane.      Parameters     ----------     points :  (n, 2) or (n, 3) float       Points in either 2D or 3D space     return_2D : bool       Were the original points 2D?      Returns     ----------     points : (n, 3) float       Points in space     is_2D : bool       Only returned if return_2D       If source points were (n, 2) True
Return a grid from an (2,dimension) bounds with samples step distance apart.      Parameters     ---------     bounds: (2,dimension) list of [[min x, min y, etc], [max x, max y, etc]]     step:   float, or (dimension) floats, separation between points      Returns     -------     grid: (n, dimension), points inside the specified bounds
Return a grid spaced inside a bounding box with edges spaced using np.linspace.      Parameters     ---------     bounds: (2,dimension) list of [[min x, min y, etc], [max x, max y, etc]]     count:  int, or (dimension,) int, number of samples per side      Returns     -------     grid: (n, dimension) float, points in the specified bounds
Given a set of key value pairs, create a dictionary.     If a key occurs multiple times, stack the values into an array.      Can be called like the regular dict(pairs) constructor      Parameters     ----------     pairs: (n,2) array of key, value pairs      Returns     ----------     result: dict, with all values stored (rather than last with regular dict)
Returns True if file has non-ASCII characters (> 0x7F, or 127)     Should work in both Python 2 and 3
For an open file object how far is it to the end      Parameters     ----------     file_obj: open file- like object      Returns     ----------     distance: int, bytes to end of file
Return the number of digits to the first nonzero decimal.      Parameters     -----------     decimal:    float     min_digits: int, minimum number of digits to return      Returns     -----------      digits: int, number of digits to the first nonzero decimal
Get the hash of an open file- like object.      Parameters     ---------     file_obj: file like object     hash_function: function to use to hash data      Returns     ---------     hashed: str, hex version of result
If an object is hashable, return the string of the MD5.      Parameters     -----------     obj: object      Returns     ----------     md5: str, MD5 hash
Attach a stream handler to all loggers.      Parameters     ------------     level:     logging level     handler:   log handler object     loggers:   list of loggers to attach to                  if None, will try to attach to all available     colors:    bool, if True try to use colorlog formatter     blacklist: list of str, names of loggers NOT to attach to
Stack a list of values that represent a polyline into     individual line segments with duplicated consecutive values.      Parameters     ----------     indices: sequence of items      Returns     ---------     stacked: (n,2) set of items      In [1]: trimesh.util.stack_lines([0,1,2])     Out[1]:     array([[0, 1],            [1, 2]])      In [2]: trimesh.util.stack_lines([0,1,2,4,5])     Out[2]:     array([[0, 1],            [1, 2],            [2, 4],            [4, 5]])      In [3]: trimesh.util.stack_lines([[0,0],[1,1],[2,2], [3,3]])     Out[3]:     array([[0, 0],            [1, 1],            [1, 1],            [2, 2],            [2, 2],            [3, 3]])
Given a sequence of zero- indexed faces and vertices     combine them into a single array of faces and     a single array of vertices.      Parameters     -----------     vertices_seq : (n, ) sequence of (m, d) float       Multiple arrays of verticesvertex arrays     faces_seq : (n, ) sequence of (p, j) int       Zero indexed faces for matching vertices      Returns     ----------     vertices : (i, d) float       Points in space     faces : (j, 3) int       Reference vertex indices
Convert a 1 or 2D array into a string with a specified number     of digits and delimiter. The reason this exists is that the     basic numpy array to string conversions are surprisingly bad.      Parameters     ----------     array : (n,) or (n, d) float or int        Data to be converted        If shape is (n,) only column delimiter will be used     col_delim : str       What string should separate values in a column     row_delim : str       What string should separate values in a row     digits : int       How many digits should floating point numbers include     value_format : str        Format string for each value or sequence of values        If multiple values per value_format it must divide        into array evenly.      Returns     ----------     formatted : str        String representation of original array
Export a numpy array to a compact serializable dictionary.      Parameters     ------------     array : array       Any numpy array     dtype : str or None       Optional dtype to encode array     encoding : str       'base64' or 'binary'      Returns     ---------     encoded : dict       Has keys:       'dtype':  str, of dtype       'shape':  tuple of shape       'base64': str, base64 encoded string
If a dictionary has keys that are bytes decode them to a str.      Parameters     ---------     store : dict       Dictionary with data      Returns     ---------     result : dict       Values are untouched but keys that were bytes       are converted to ASCII strings.      Example     -----------     In [1]: d     Out[1]: {1020: 'nah', b'hi': 'stuff'}      In [2]: trimesh.util.decode_keys(d)     Out[2]: {1020: 'nah', 'hi': 'stuff'}
Turn a dictionary with base64 encoded strings back into a numpy array.      Parameters     ------------     encoded : dict       Has keys:         dtype: string of dtype         shape: int tuple of shape         base64: base64 encoded string of flat array         binary:  decode result coming from numpy.tostring      Returns     ----------     array: numpy array
Return the bases of the object passed.
Similar to the type() builtin, but looks in class bases     for named instance.      Parameters     ----------     obj: object to look for class of     name : str, name of class      Returns     ----------     named class, or None
Concatenate two or more meshes.      Parameters     ----------     a: Trimesh object, or list of such     b: Trimesh object, or list of such      Returns     ----------     result: Trimesh object containing concatenated mesh
Return a subset of a mesh.      Parameters     ----------     mesh : Trimesh        Source mesh to take geometry from     faces_sequence : sequence (p,) int         Indexes of mesh.faces     only_watertight : bool         Only return submeshes which are watertight.     append : bool         Return a single mesh which has the faces appended,         if this flag is set, only_watertight is ignored      Returns     ---------     if append : Trimesh object     else        list of Trimesh objects
Parameters     --------     data : (n,)       1D array     count : int       Minimum length of result array      Returns     --------     padded : (m,)       1D array where m >= count
A version of json.dumps that can handle numpy arrays     by creating a custom encoder for numpy dtypes.      Parameters     --------------     obj : JSON- serializable blob     **kwargs :         Passed to json.dumps      Returns     --------------     dumped : str       JSON dump of obj
Convert an item to have the dtype of another item      Parameters     ----------     item: item to be converted     like: object with target dtype. If None, item is returned unmodified      Returns     --------     result: item, but in dtype of like
Given a set of axis aligned bounds, create an r-tree for broad- phase     collision detection      Parameters     ---------     bounds: (n, dimension*2) list of non- interleaved bounds              for a 2D bounds tree:              [(minx, miny, maxx, maxy), ...]      Returns     ---------     tree: Rtree object
Wrap a string or bytes object as a file object.      Parameters     ----------     item: str or bytes       Item to be wrapped      Returns     ---------     wrapped: file-like object
Round a single value to a specified number of significant figures.      Parameters     ----------     values: float, value to be rounded     sigfig: int, number of significant figures to reduce to       Returns     ----------     rounded: values, but rounded to the specified number of significant figures       Examples     ----------     In [1]: trimesh.util.round_sigfig(-232453.00014045456, 1)     Out[1]: -200000.0      In [2]: trimesh.util.round_sigfig(.00014045456, 1)     Out[2]: 0.0001      In [3]: trimesh.util.round_sigfig(.00014045456, 4)     Out[3]: 0.0001405
Convert a set of floating point values into integers with a specified number     of significant figures and an exponent.      Parameters     ------------     values: (n,) float or int, array of values     sigfig: (n,) int, number of significant figures to keep      Returns     ------------     as_int:      (n,) int, every value[i] has sigfig[i] digits     multiplier:  (n, int), exponent, so as_int * 10 ** multiplier is                  the same order of magnitude as the input
Given an open file object and a file type, return all components     of the archive as open file objects in a dict.      Parameters     -----------     file_obj : file-like       Containing compressed data     file_type : str       File extension, 'zip', 'tar.gz', etc      Returns     ---------     decompressed : dict       Data from archive in format {file name : file-like}
Compress data stored in a dict.      Parameters     -----------     info : dict       Data to compress in form:       {file name in archive: bytes or file-like object}      Returns     -----------     compressed : bytes       Compressed file data
Find the file extension of a file name, including support for     special case multipart file extensions (like .tar.gz)      Parameters     ----------     file_name: str, file name     special:   list of str, multipart extensions                eg: ['tar.bz2', 'tar.gz']      Returns     ----------     extension: str, last characters after a period, or                a value from 'special'
Given a sequence of triangle strips, convert them to (n,3) faces.      Processes all strips at once using np.concatenate and is significantly     faster than loop- based methods.      From the OpenGL programming guide describing a single triangle     strip [v0, v1, v2, v3, v4]:      Draws a series of triangles (three-sided polygons) using vertices     v0, v1, v2, then v2, v1, v3  (note the order), then v2, v3, v4,     and so on. The ordering is to ensure that the triangles are all     drawn with the same orientation so that the strip can correctly form     part of a surface.      Parameters     ------------     strips: (n,) list of (m,) int vertex indices      Returns     ------------     faces: (m,3) int, vertex indices representing triangles
A thin wrapper for numpy.vstack that ignores empty lists.      Parameters     ------------     tup: tuple or list of arrays with the same number of columns      Returns     ------------     stacked: (n,d) array, with same number of columns as               constituent arrays.
If a file is open in binary mode and a string is passed, encode and write     If a file is open in text   mode and bytes are passed, decode and write      Parameters     -----------     file_obj: file object,  with 'write' and 'mode'     stuff:    str or bytes, stuff to be written     encoding: str,          encoding of text
Generate a decent looking alphanumeric unique identifier.     First 16 bits are time- incrementing, followed by randomness.      This function is used as a nicer looking alternative to:     >>> uuid.uuid4().hex      Follows the advice in:     https://eager.io/blog/how-long-does-an-id-need-to-be/      Parameters     ------------     length:    int, length of resulting identifier     increment: int, number to add to header uint16                     useful if calling this function repeatedly                     in a tight loop executing faster than time                     can increment the header     Returns     ------------     unique: str, unique alphanumeric identifier
Generate an arbitrary basis (also known as a coordinate frame)     from a given z-axis vector.      Parameters     ----------     z: (3,) float       A vector along the positive z-axis      Returns     -------     x : (3,) float       Vector along x axis     y : (3,) float       Vector along y axis     z : (3,) float       Vector along z axis
A replacement for np.isclose that does fewer checks     and validation and as a result is roughly 4x faster.      Note that this is used in tight loops, and as such     a and b MUST be np.ndarray, not list or "array-like"      Parameters     ----------     a : np.ndarray       To be compared     b : np.ndarray       To be compared     atol : float       Acceptable distance between `a` and `b` to be "close"      Returns     -----------     close : np.ndarray, bool       Per- element closeness
Load an SVG file into a Path2D object.      Parameters     -----------     file_obj : open file object       Contains SVG data     file_type: None       Not used      Returns     -----------     loaded : dict       With kwargs for Path2D constructor
Convert an SVG transform string to an array of matrices.       > transform = "rotate(-10 50 100)                    translate(-36 45.5)                    skewX(40)                    scale(1 0.5)"      Parameters     -----------     transform : str       Contains transformation information in SVG form      Returns     -----------     matrices : (n, 3, 3) float       Multiple transformation matrices from input transform string
Convert an SVG path string into a Path2D object      Parameters     -------------     paths: list of tuples       Containing (path string, (3,3) matrix)      Returns     -------------     drawing : dict       Kwargs for Path2D constructor
Export a Path2D object into an SVG file.      Parameters     -----------     drawing : Path2D      Source geometry     return_path : bool       If True return only path string     layers : None, or [str]       Only export specified layers      Returns     -----------     as_svg: str, XML formatted as SVG
Computes stable orientations of a mesh and their quasi-static probabilites.      This method samples the location of the center of mass from a multivariate     gaussian with the mean at the center of mass, and a covariance     equal to and identity matrix times sigma, over n_samples.      For each sample, it computes the stable resting poses of the mesh on a     a planar workspace and evaulates the probabilities of landing in     each pose if the object is dropped onto the table randomly.      This method returns the 4x4 homogenous transform matrices that place     the shape against the planar surface with the z-axis pointing upwards     and a list of the probabilities for each pose.      The transforms and probabilties that are returned are sorted, with the     most probable pose first.      Parameters     ----------     mesh : trimesh.Trimesh       The target mesh     com : (3,) float       Rhe object center of mass. If None, this method       assumes uniform density and watertightness and       computes a center of mass explicitly     sigma : float       Rhe covariance for the multivariate gaussian used       to sample center of mass locations     n_samples : int       The number of samples of the center of mass location     threshold : float       The probability value at which to threshold       returned stable poses      Returns     -------     transforms : (n, 4, 4) float       The homogenous matrices that transform the       object to rest in a stable pose, with the       new z-axis pointing upwards from the table       and the object just touching the table.     probs : (n,) float       Probability in (0, 1) for each pose
Performs a fast 3D orientation test.      Parameters     ----------     plane: (3,3) float, three points in space that define a plane     pd:    (3,)  float, a single point      Returns     -------     result: float, if greater than zero then pd is above the plane through                    the given three points, if less than zero then pd is below                    the given plane, and if equal to zero then pd is on the                    given plane.
For an object with the given center of mass, compute     the probability that the given tri would be the first to hit the     ground if the object were dropped with a pose chosen uniformly at random.      Parameters     ----------     tri: (3,3) float, the vertices of a triangle     cm:  (3,) float, the center of mass of the object      Returns     -------     prob: float, the probability in [0,1] for the given triangle
Constructs a toppling digraph for the given convex hull mesh and     center of mass.      Each node n_i in the digraph corresponds to a face f_i of the mesh and is     labelled with the probability that the mesh will land on f_i if dropped     randomly. Not all faces are stable, and node n_i has a directed edge to     node n_j if the object will quasi-statically topple from f_i to f_j if it     lands on f_i initially.      This computation is described in detail in     http://goldberg.berkeley.edu/pubs/eps.pdf.      Parameters     ----------     cvh_mesh : trimesh.Trimesh       Rhe convex hull of the target shape     com : (3,) float       The 3D location of the target shape's center of mass      Returns     -------     graph : networkx.DiGraph       Graph representing static probabilities and toppling       order for the convex hull
Return a permutated variant of a mesh by randomly reording faces     and rotatating + translating a mesh by a random matrix.      Parameters     ----------     mesh:   Trimesh object (input will not be altered by this function)      Returns     ----------     permutated: Trimesh object, same faces as input mesh but                 rotated and reordered.
Add gaussian noise to every vertex of a mesh.     Makes no effort to maintain topology or sanity.      Parameters     ----------     mesh:      Trimesh object (will not be mutated)     magnitude: float, what is the maximum distance per axis we can displace a vertex.                Default value is mesh.scale/100.0      Returns     ----------     permutated: Trimesh object, input mesh with noise applied
Subdivide each face of a mesh into three faces with the new vertex     randomly placed inside the old face.      This produces a mesh with exactly the same surface area and volume     but with different tessellation.      Parameters     ----------     mesh: Trimesh object      Returns     ----------     permutated: Trimesh object with remeshed facets
Load a PLY file from an open file object.      Parameters     ---------     file_obj : an open file- like object       Source data, ASCII or binary PLY     resolver : trimesh.visual.resolvers.Resolver       Object which can resolve assets     fix_texture : bool       If True, will re- index vertices and faces       so vertices with different UV coordinates       are disconnected.      Returns     ---------     mesh_kwargs : dict       Data which can be passed to       Trimesh constructor, eg: a = Trimesh(**mesh_kwargs)
Export a mesh in the PLY format.      Parameters     ----------     mesh : Trimesh object     encoding : ['ascii'|'binary_little_endian']     vertex_normal : include vertex normals      Returns     ----------     export : bytes of result
Read the ASCII header of a PLY file, and leave the file object     at the position of the start of data but past the header.      Parameters     -----------     file_obj : open file object       Positioned at the start of the file      Returns     -----------     elements : collections.OrderedDict       Fields and data types populated     is_ascii : bool       Whether the data is ASCII or binary     image_name : None or str       File name of TextureFile
Given an elements data structure, extract the keyword     arguments that a Trimesh object constructor will expect.      Parameters     ------------     elements: OrderedDict object, with fields and data loaded      Returns     -----------     kwargs: dict, with keys for Trimesh constructor.             eg: mesh = trimesh.Trimesh(**kwargs)
Given an element, try to extract RGBA color from     properties and return them as an (n,3|4) array.      Parameters     -------------     element: dict, containing color keys      Returns     ------------     colors: (n,(3|4)     signal: float, estimate of range
Load data from an ASCII PLY file into an existing elements data structure.      Parameters     ------------     elements: OrderedDict object, populated from the file header.               object will be modified to add data by this function.      file_obj: open file object, with current position at the start               of the data section (past the header)
Load the data from a binary PLY file into the elements data structure.      Parameters     ------------     elements: OrderedDict object, populated from the file header.               object will be modified to add data by this function.      file_obj: open file object, with current position at the start               of the data section (past the header)
Export a mesh using Google's Draco compressed format.      Only works if draco_encoder is in your PATH:     https://github.com/google/draco      Parameters     ----------     mesh : Trimesh object      Returns     ----------     data : str or bytes       DRC file bytes
Load a mesh from Google's Draco format.      Parameters     ----------     file_obj  : file- like object       Contains data      Returns     ----------     kwargs : dict       Keyword arguments to construct a Trimesh object
Compute an approximate convex decomposition of a mesh.      Parameters     ----------     mesh : trimesh.Trimesh       Mesh to be decomposed into convex parts      Returns     -------     mesh_args : list       List of **kwargs for Trimeshes that are nearly       convex and approximate the original.
Automatically pick an engine for booleans based on availability.      Parameters     --------------     meshes : list of Trimesh       Meshes to be booleaned     operation : str       Type of boolean, i.e. 'union', 'intersection', 'difference'      Returns     ---------------     result : trimesh.Trimesh       Result of boolean operation
Rasterize a Path2D object into a boolean image ("mode 1").      Parameters     ------------     path:       Path2D object     pitch:      float, length in model space of a pixel edge     origin:     (2,) float, origin position in model space     resolution: (2,) int, resolution in pixel space     fill:       bool, if True will return closed regions as filled     width:      int, if not None will draw outline this wide (pixels)      Returns     ------------     raster: PIL.Image object, mode 1
Subdivide a mesh into smaller triangles.      Note that if `face_index` is passed, only those faces will     be subdivided and their neighbors won't be modified making     the mesh no longer "watertight."      Parameters     ----------     vertices : (n, 3) float       Vertices in space     faces : (n, 3) int       Indexes of vertices which make up triangular faces     face_index : faces to subdivide.       if None: all faces of mesh will be subdivided       if (n,) int array of indices: only specified faces      Returns     ----------     new_vertices : (n, 3) float       Vertices in space     new_faces : (n, 3) int       Remeshed faces
Subdivide a mesh until every edge is shorter than a     specified length.      Will return a triangle soup, not a nicely structured mesh.      Parameters     ------------     vertices : (n, 3) float       Vertices in space     faces : (m, 3) int       Indices of vertices which make up triangles     max_edge : float       Maximum length of any edge in the result     max_iter : int       The maximum number of times to run subdivision      Returns     ------------     vertices : (j, 3) float       Vertices in space     faces : (q, 3) int       Indices of vertices
Load a DXF file to a dictionary containing vertices and     entities.      Parameters     ----------     file_obj: file or file- like object (has object.read method)      Returns     ----------     result: dict, keys are  entities, vertices and metadata
Export a 2D path object to a DXF file      Parameters     ----------     path: trimesh.path.path.Path2D      Returns     ----------     export: str, path formatted as a DXF file
Load DWG files by converting them to DXF files using     TeighaFileConverter.      Parameters     -------------     file_obj : file- like object      Returns     -------------     loaded : dict         kwargs for a Path2D constructor
Polylines can have "vertex bulge," which means the polyline     has an arc tangent to segments, rather than meeting at a     vertex.      From Autodesk reference:     The bulge is the tangent of one fourth the included     angle for an arc segment, made negative if the arc     goes clockwise from the start point to the endpoint.     A bulge of 0 indicates a straight segment, and a     bulge of 1 is a semicircle.      Parameters     ----------------     lines : (n, 2) float       Polyline vertices in order     bulge : (m,) float       Vertex bulge value     bulge_idx : (m,) float       Which index of lines is bulge associated with     is_closed : bool       Is segment closed     metadata : None, or dict       Entity metadata to add      Returns     ---------------     vertices : (a, 2) float       New vertices for poly-arc     entities : (b,) entities.Entity       New entities, either line or arc
Given a loaded (n, 2) blob and a field name     get a value by code.
Convert any DXF/DWG to R14 ASCII DXF using Teigha Converter.      Parameters     ---------------     data : str or bytes        The contents of a DXF or DWG file     extension : str        The format of data: 'dwg' or 'dxf'      Returns     --------------     converted : str        Result as R14 ASCII DXF
Returns the cross product of two edges from input triangles      Parameters     --------------     triangles: (n, 3, 3) float       Vertices of triangles      Returns     --------------     crosses : (n, 3) float       Cross product of two edge vectors
Calculates the sum area of input triangles      Parameters     ----------     triangles : (n, 3, 3) float       Vertices of triangles     crosses : (n, 3) float or None       As a speedup don't re- compute cross products     sum : bool       Return summed area or individual triangle area      Returns     ----------     area : (n,) float or float       Individual or summed area depending on `sum` argument
Calculates the normals of input triangles      Parameters     ------------     triangles : (n, 3, 3) float       Vertex positions     crosses : (n, 3) float       Cross products of edge vectors      Returns     ------------     normals : (m, 3) float       Normal vectors     valid : (n,) bool       Was the face nonzero area or not
Calculates the angles of input triangles.      Parameters     ------------     triangles : (n, 3, 3) float       Vertex positions      Returns     ------------     angles : (n, 3) float       Angles at vertex positions, in radians
Check to see if a list of triangles are all coplanar      Parameters     ----------------     triangles: (n, 3, 3) float       Vertices of triangles      Returns     ---------------     all_coplanar : bool       True if all triangles are coplanar
Calculate the mass properties of a group of triangles.      Implemented from:     http://www.geometrictools.com/Documentation/PolyhedralMassProperties.pdf      Parameters     ----------     triangles : (n, 3, 3) float       Triangle vertices in space     crosses : (n,) float       Optional cross products of triangles     density : float       Optional override for density     center_mass :  (3,) float       Optional override for center mass     skip_inertia : bool       if True will not return moments matrix      Returns     ---------     info : dict       Mass properties
Given a list of triangles and a list of normals determine if the     two are aligned      Parameters     ----------     triangles : (n, 3, 3) float       Vertex locations in space     normals_compare : (n, 3) float       List of normals to compare      Returns     ----------     aligned : (n,) bool       Are normals aligned with triangles
Given a list of triangles, create an r-tree for broad- phase     collision detection      Parameters     ---------     triangles : (n, 3, 3) float       Triangles in space      Returns     ---------     tree : rtree.Rtree       One node per triangle
Find all triangles which have an oriented bounding box     where both of the two sides is larger than a specified height.      Degenerate triangles can be when:     1) Two of the three vertices are colocated     2) All three vertices are unique but colinear       Parameters     ----------     triangles : (n, 3, 3) float       Triangles in space     height : float       Minimum edge length of a triangle to keep      Returns     ----------     nondegenerate : (n,) bool       True if a triangle meets required minimum height
Return the 2D bounding box size of each triangle.      Parameters     ----------     triangles : (n, 3, 3) float       Triangles in space     areas : (n,) float       Optional area of input triangles      Returns     ----------     box :  (n, 2) float       The size of each triangle's 2D oriented bounding box
Convert a list of barycentric coordinates on a list of triangles     to cartesian points.      Parameters     ------------     triangles : (n, 3, 3) float       Triangles in space     barycentric : (n, 2) float       Barycentric coordinates      Returns     -----------     points : (m, 3) float       Points in space
Find the barycentric coordinates of points relative to triangles.      The Cramer's rule solution implements:         http://blackpawn.com/texts/pointinpoly      The cross product solution implements:         https://www.cs.ubc.ca/~heidrich/Papers/JGT.05.pdf       Parameters     -----------     triangles : (n, 3, 3) float       Triangles vertices in space     points : (n, 3) float       Point in space associated with a triangle     method :  str       Which method to compute the barycentric coordinates with:         - 'cross': uses a method using cross products, roughly 2x slower but                   different numerical robustness properties         - anything else: uses a cramer's rule solution      Returns     -----------     barycentric : (n, 3) float       Barycentric coordinates of each point
Return the closest point on the surface of each triangle for a     list of corresponding points.      Implements the method from "Real Time Collision Detection" and     use the same variable names as "ClosestPtPointTriangle" to avoid     being any more confusing.       Parameters     ----------     triangles : (n, 3, 3) float       Triangle vertices in space     points : (n, 3) float       Points in space      Returns     ----------     closest : (n, 3) float       Point on each triangle closest to each point
Convert a list of triangles to the kwargs for the Trimesh     constructor.      Parameters     ---------     triangles : (n, 3, 3) float       Triangles in space      Returns     ---------     kwargs : dict       Keyword arguments for the trimesh.Trimesh constructor       Includes keys 'vertices' and 'faces'      Examples     ---------     >>> mesh = trimesh.Trimesh(**trimesh.triangles.to_kwargs(triangles))
Return a copy of the Primitive object.
Return a copy of the Primitive object as a Trimesh object.
Apply a transform to the current primitive (sets self.transform)          Parameters         -----------         matrix: (4,4) float, homogenous transformation
The analytic volume of the cylinder primitive.          Returns         ---------         volume : float           Volume of the cylinder
The analytic inertia tensor of the cylinder primitive.          Returns         ----------         tensor: (3,3) float, 3D inertia tensor
A line segment which if inflated by cylinder radius         would represent the cylinder primitive.          Returns         -------------         segment : (2, 3) float           Points representing a single line segment
Return a cylinder primitive which covers the source cylinder         by distance: radius is inflated by distance, height by twice         the distance.          Parameters         ------------         distance : float           Distance to inflate cylinder radius and height          Returns         -------------         buffered : Cylinder          Cylinder primitive inflated by distance
Apply a transform to the sphere primitive          Parameters         ------------         matrix: (4,4) float, homogenous transformation
The analytic inertia tensor of the sphere primitive.          Returns         ----------         tensor: (3,3) float, 3D inertia tensor
Return random samples from inside the volume of the box.          Parameters         -------------         count : int           Number of samples to return          Returns         ----------         samples : (count, 3) float           Points inside the volume
Return a 3D grid which is contained by the box.         Samples are either 'step' distance apart, or there are         'count' samples per box side.          Parameters         -----------         count : int or (3,) int           If specified samples are spaced with np.linspace         step : float or (3,) float           If specified samples are spaced with np.arange          Returns         -----------         grid : (n, 3) float           Points inside the box
Returns whether or not the current box is rotated at all.
Volume of the box Primitive.          Returns         --------         volume: float, volume of box
The surface area of the primitive extrusion.          Calculated from polygon and height to avoid mesh creation.          Returns         ----------         area: float, surface area of 3D extrusion
The volume of the primitive extrusion.          Calculated from polygon and height to avoid mesh creation.          Returns         ----------         volume: float, volume of 3D extrusion
Based on the extrudes transform, what is the vector along         which the polygon will be extruded          Returns         ---------         direction: (3,) float vector. If self.primitive.transform is an                    identity matrix this will be [0.0, 0.0, 1.0]
Alter the transform of the current extrusion to slide it         along its extrude_direction vector          Parameters         -----------         distance: float, distance along self.extrude_direction to move
Return a new Extrusion object which is expanded in profile and         in height by a specified distance.          Returns         ----------         buffered: Extrusion object
Given a list of shapely polygons with only exteriors,     find which curves represent the exterior shell or root curve     and which represent holes which penetrate the exterior.      This is done with an R-tree for rough overlap detection,     and then exact polygon queries for a final result.      Parameters     -----------     polygons : (n,) shapely.geometry.Polygon        Polygons which only have exteriors and may overlap      Returns     -----------     roots : (m,) int         Index of polygons which are root     contains : networkx.DiGraph        Edges indicate a polygon is        contained by another polygon
Given an edge list of indices and associated vertices     representing lines, generate a list of polygons.      Parameters     -----------     edges : (n, 2) int       Indexes of vertices which represent lines     vertices : (m, 2) float       Vertices in 2D space      Returns     ----------     polygons : (p,) shapely.geometry.Polygon       Polygon objects with interiors
Find the OBBs for a list of shapely.geometry.Polygons
Find the oriented bounding box of a Shapely polygon.      The OBB is always aligned with an edge of the convex hull of the polygon.      Parameters     -------------     polygons: shapely.geometry.Polygon      Returns     -------------     transform: (3,3) float, transformation matrix                which will move input polygon from its original position                to the first quadrant where the AABB is the OBB     extents:   (2,) float, extents of transformed polygon
Transform a polygon by a a 2D homogenous transform.      Parameters     -------------     polygon : shapely.geometry.Polygon                  2D polygon to be transformed.     matrix  : (3, 3) float                  2D homogenous transformation.      Returns     --------------     result : shapely.geometry.Polygon                  Polygon transformed by matrix.
Plot a shapely polygon using matplotlib.      Parameters     ------------     polygon : shapely.geometry.Polygon       Polygon to be plotted     show : bool       If True will display immediately     **kwargs       Passed to plt.plot
Return a version of a polygon with boundaries resampled     to a specified resolution.      Parameters     -------------     polygon:    shapely.geometry.Polygon object     resolution: float, desired distance between points on boundary     clip:       (2,) int, upper and lower bounds to clip                 number of samples to (to avoid exploding counts)      Returns     ------------     kwargs: dict, keyword args for a Polygon(**kwargs)
Stack the boundaries of a polygon into a single     (n, 2) list of vertices.      Parameters     ------------     boundaries: dict, with keys 'shell', 'holes'      Returns     ------------     stacked: (n, 2) float, list of vertices
Given a shapely polygon, find the approximate medial axis     using a voronoi diagram of evenly spaced points on the     boundary of the polygon.      Parameters     ----------     polygon : shapely.geometry.Polygon       The source geometry     resolution : float       Distance between each sample on the polygon boundary     clip : None, or (2,) int       Clip sample count to min of clip[0] and max of clip[1]      Returns     ----------     edges : (n, 2) int       Vertex indices representing line segments       on the polygon's medial axis     vertices : (m, 2) float       Vertex positions in space
Return a vector containing values representitive of     a particular polygon.      Parameters     ---------     polygon : shapely.geometry.Polygon       Input geometry      Returns     ---------     hashed: (6), float       Representitive values representing input polygon
Generate a random polygon with a maximum number of sides and approximate radius.      Parameters     ---------     segments: int, the maximum number of sides the random polygon will have     radius:   float, the approximate radius of the polygon desired      Returns     ---------     polygon: shapely.geometry.Polygon object with random exterior, and no interiors.
For a Polygon object, return the diagonal length of the AABB.      Parameters     ------------     polygon: shapely.geometry.Polygon object      Returns     ------------     scale: float, length of AABB diagonal
Given a sequence of connected points turn them into     valid shapely Polygon objects.      Parameters     -----------     paths : (n,) sequence         Of (m,2) float, closed paths     scale: float         Approximate scale of drawing for precision      Returns     -----------     polys: (p,) list         shapely.geometry.Polygon         None
Use rejection sampling to generate random points inside a     polygon.      Parameters     -----------     polygon : shapely.geometry.Polygon                 Polygon that will contain points     count   : int                 Number of points to return     factor  : float                 How many points to test per loop                 IE, count * factor     max_iter : int,                 Maximum number of intersection loops                 to run, total points sampled is                 count * factor * max_iter      Returns     -----------     hit : (n, 2) float            Random points inside polygon            where n <= count
Given a shapely.geometry.Polygon, attempt to return a     valid version of the polygon through buffering tricks.      Parameters     -----------     polygon: shapely.geometry.Polygon object     rtol:    float, how close does a perimeter have to be     scale:   float, or None      Returns     ----------     repaired: shapely.geometry.Polygon object      Raises     ----------     ValueError: if polygon can't be repaired
Export a scene object as a GLTF directory.      This puts each mesh into a separate file (i.e. a `buffer`)     as opposed to one larger file.      Parameters     -----------     scene : trimesh.Scene       Scene to be exported      Returns     ----------     export : dict       Format: {file name : file data}
Export a scene as a binary GLTF (GLB) file.      Parameters     ------------     scene: trimesh.Scene       Input geometry     extras : JSON serializable       Will be stored in the extras field     include_normals : bool       Include vertex normals in output file?      Returns     ----------     exported : bytes       Exported result in GLB 2.0
Load a GLTF file, which consists of a directory structure     with multiple files.      Parameters     -------------     file_obj : None or file-like       Object containing header JSON, or None     resolver : trimesh.visual.Resolver       Object which can be used to load other files by name     **mesh_kwargs : dict       Passed to mesh constructor      Returns     --------------     kwargs : dict       Arguments to create scene
Load a GLTF file in the binary GLB format into a trimesh.Scene.      Implemented from specification:     https://github.com/KhronosGroup/glTF/tree/master/specification/2.0      Parameters     ------------     file_obj : file- like object       Containing GLB data      Returns     ------------     kwargs : dict       Kwargs to instantiate a trimesh.Scene
Create a simple GLTF material for a mesh using the most     commonly occurring color in that mesh.      Parameters     ------------     mesh: trimesh.Trimesh       Mesh to create a material from      Returns     ------------     material: dict       In GLTF material format
Generate a GLTF header.      Parameters     -------------     scene : trimesh.Scene       Input scene data     extras : JSON serializable       Will be stored in the extras field     include_normals : bool       Include vertex normals in output file?      Returns     ---------------     tree : dict       Contains required keys for a GLTF scene     buffer_items : list       Contains bytes of data
Append a mesh to the scene structure and put the     data into buffer_items.      Parameters     -------------     mesh : trimesh.Trimesh       Source geometry     name : str       Name of geometry     tree : dict       Will be updated with data from mesh     buffer_items       Will have buffer appended with mesh data     include_normals : bool       Include vertex normals in export or not
GLTF wants chunks aligned with 4- byte boundaries     so this function will add padding to the end of a     chunk of bytes so that it aligns with a specified     boundary size      Parameters     --------------     data : bytes       Data to be padded     bound : int       Length of desired boundary      Returns     --------------     padded : bytes       Result where: (len(padded) % bound) == 0
Append a 2D or 3D path to the scene structure and put the     data into buffer_items.      Parameters     -------------     path : trimesh.Path2D or trimesh.Path3D       Source geometry     name : str       Name of geometry     tree : dict       Will be updated with data from path     buffer_items       Will have buffer appended with path data
Convert materials and images stored in a GLTF header     and buffer views to PBRMaterial objects.      Parameters     ------------     header : dict       Contains layout of file     views : (n,) bytes       Raw data      Returns     ------------     materials : list       List of trimesh.visual.texture.Material objects
Given a list of binary data and a layout, return the     kwargs to create a scene object.      Parameters     -----------     header : dict       With GLTF keys     buffers : list of bytes       Stored data     passed : dict       Kwargs for mesh constructors      Returns     -----------     kwargs : dict       Can be passed to load_kwargs for a trimesh.Scene
Convert a trimesh camera to a GLTF camera.      Parameters     ------------     camera : trimesh.scene.cameras.Camera       Trimesh camera object      Returns     -------------     gltf_camera : dict       Camera represented as a GLTF dict
Get an asset.          Parameters         -------------         name : str           Name of the asset          Returns         ------------         data : bytes           Loaded data from asset
Get an asset from the ZIP archive.          Parameters         -------------         name : str           Name of the asset          Returns         -------------         data : bytes           Loaded data from asset
Get a resource from the remote site.          Parameters         -------------         name : str           Asset name, i.e. 'quadknot.obj.mtl'
Translate the current mesh.          Parameters         ----------         translation : (3,) float           Translation in XYZ
Scale the mesh equally on all axis.          Parameters         ----------         scaling : float           Scale factor to apply to the mesh
An axis aligned bounding box for the current mesh.          Returns         ----------         aabb : trimesh.primitives.Box           Box object with transform and extents defined           representing the axis aligned bounding box of the mesh
An oriented bounding box for the current mesh.          Returns         ---------         obb : trimesh.primitives.Box           Box object with transform and extents defined           representing the minimum volume oriented bounding box of the mesh
A minimum volume bounding sphere for the current mesh.          Note that the Sphere primitive returned has an unpadded, exact         sphere_radius so while the distance of every vertex of the current         mesh from sphere_center will be less than sphere_radius, the faceted         sphere primitive may not contain every vertex          Returns         --------         minball: trimesh.primitives.Sphere           Sphere primitive containing current mesh
A minimum volume bounding cylinder for the current mesh.          Returns         --------         mincyl : trimesh.primitives.Cylinder           Cylinder primitive containing current mesh
The minimum volume primitive (box, sphere, or cylinder) that         bounds the mesh.          Returns         ---------         bounding_primitive : trimesh.primitives.Sphere                              trimesh.primitives.Box                              trimesh.primitives.Cylinder           Primitive which bounds the mesh with the smallest volume
Export a Trimesh object to a file- like object, or to a filename      Parameters     ---------     file_obj : str, file-like       Where should mesh be exported to     file_type : str or None       Represents file type (eg: 'stl')      Returns     ----------     exported : bytes or str       Result of exporter
Export a mesh as an OFF file, a simple text format      Parameters     -----------     mesh : trimesh.Trimesh       Geometry to export     digits : int       Number of digits to include on floats      Returns     -----------     export : str       OFF format output
Export a mesh to a dict      Parameters     ------------     mesh : Trimesh object              Mesh to be exported     encoding : str, or None                  'base64'      Returns     -------------
Load a javascript file and minify.      Parameters     ------------     path: str, path of resource
Create a Path2D representing a circle pattern.      Parameters     ------------     pattern_radius : float       Radius of circle centers     circle_radius : float       The radius of each circle     count : int       Number of circles in the pattern     center : (2,) float       Center of pattern     angle :  float       If defined pattern will span this angle       If None, pattern will be evenly spaced      Returns     -------------     pattern : trimesh.path.Path2D       Path containing circular pattern
Create a Path2D containing a single or multiple rectangles     with the specified bounds.      Parameters     --------------     bounds : (2, 2) float, or (m, 2, 2) float       Minimum XY, Maximum XY      Returns     -------------     rect : Path2D       Path containing specified rectangles
Create a Path2D containing a single or multiple rectangles     with the specified bounds.      Parameters     --------------     bounds : (2, 2) float, or (m, 2, 2) float       Minimum XY, Maximum XY      Returns     -------------     rect : Path2D       Path containing specified rectangles
Return a cuboid.      Parameters     ------------     extents : float, or (3,) float       Edge lengths     transform: (4, 4) float       Transformation matrix     **kwargs:         passed to Trimesh to create box      Returns     ------------     geometry : trimesh.Path3D       Path outline of a cuboid geometry
Given the origin and normal of a plane find the transform     that will move that plane to be coplanar with the XY plane.      Parameters     ----------     origin : (3,) float         Point that lies on the plane     normal : (3,) float         Vector that points along normal of plane      Returns     ---------     transform: (4,4) float         Transformation matrix to move points onto XY plane
Find a transform between two 3D vectors.      Implements the method described here:     http://ethaneade.com/rot_between_vectors.pdf      Parameters     --------------     a : (3,) float       Source vector     b : (3,) float       Target vector     return_angle : bool       If True return the angle between the two vectors      Returns     -------------     transform : (4, 4) float       Homogenous transform from a to b     angle : float       Angle between vectors in radians       Only returned if return_angle
Given a list of faces (n,3), return a list of edges (n*3,2)      Parameters     -----------     faces : (n, 3) int       Vertex indices representing faces      Returns     -----------     edges : (n*3, 2) int       Vertex indices representing edges
Find the angles between pairs of unit vectors.      Parameters     ----------     pairs : (n, 2, 3) float       Unit vector pairs      Returns     ----------     angles : (n,) float       Angles between vectors in radians
Given a set of quad faces, return them as triangle faces.      Parameters     -----------     quads: (n, 4) int       Vertex indices of quad faces      Returns     -----------     faces : (m, 3) int       Vertex indices of triangular faces
Find vertex normals from the mean of the faces that contain     that vertex.      Parameters     -----------     vertex_count : int       The number of vertices faces refer to     faces : (n, 3) int       List of vertex indices     face_normals : (n, 3) float       Normal vector for each face      Returns     -----------     vertex_normals : (vertex_count, 3) float       Normals for every vertex       Vertices unreferenced by faces will be zero.
Return a sparse matrix for which vertices are contained in which faces.      Returns     ---------     sparse: scipy.sparse.coo_matrix of shape (column_count, len(faces))             dtype is boolean      Examples      ----------     In [1]: sparse = faces_sparse(len(mesh.vertices), mesh.faces)      In [2]: sparse.shape     Out[2]: (12, 20)      In [3]: mesh.faces.shape     Out[3]: (20, 3)      In [4]: mesh.vertices.shape     Out[4]: (12, 3)      In [5]: dense = sparse.toarray().astype(int)      In [6]: dense     Out[6]:     array([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],            [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],            [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],            [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],            [0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0],            [0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1],            [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0],            [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0],            [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1],            [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]])      In [7]: dense.sum(axis=0)     Out[7]: array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])
Load the JSON blob into native objects
Write a native object to a JSON blob
Replace non-strippable whitepace in a string with a safe space
For a dict, write each value to destination/key
For a directory full of files, retrieve it     as a dict with file_name:text
Given three points on an arc find:     center, radius, normal, and angle.      This uses the fact that the intersection of the perp     bisectors of the segments between the control points     is the center of the arc.      Parameters     ---------     points : (3, dimension) float       Points in space, where dimension is either 2 or 3      Returns     ---------     result : dict       Has keys:         'center':   (d,) float, cartesian center of the arc         'radius':   float, radius of the arc         'normal':   (3,) float, the plane normal.         'angle':    (2,) float, angle of start and end, in radians         'span' :    float, angle swept by the arc, in radians
Returns a version of a three point arc consisting of     line segments.      Parameters     ---------     points : (3, d) float       Points on the arc where d in [2,3]     close :  boolean       If True close the arc into a circle     scale : float       What is the approximate overall drawing scale       Used to establish order of magnitude for precision      Returns     ---------     discrete : (m, d) float       Connected points in space
For 2D arcs, given a center and radius convert them to three     points on the arc.      Parameters     -----------     center : (2,) float       Center point on the plane     radius : float       Radius of arc     angles : (2,) float       Angles in radians for start and end angle       if not specified, will default to (0.0, pi)      Returns     ----------     three : (3, 2) float       Arc control points
Take paths relative to the current file and     convert them to absolute paths.      Parameters     ------------     rel     : str               Relative path, IE '../stuff'      Returns     -------------     abspath : str               Absolute path, IE '/home/user/stuff'
Use the pyassimp library to load a mesh from a file object     and type or file name if file_obj is a string      Parameters     ---------     file_obj: str, or file object       File path or object containing mesh data     file_type : str       File extension, aka 'stl'     resolver : trimesh.visual.resolvers.Resolver       Used to load referenced data (like texture files)     kwargs : dict       Passed through to mesh constructor      Returns     ---------     scene : trimesh.Scene       Native trimesh copy of assimp scene
Load a file using the cyassimp bindings.      The easiest way to install these is with conda:     conda install -c menpo/label/master cyassimp      Parameters     ---------     file_obj: str, or file object       File path or object containing mesh data     file_type : str       File extension, aka 'stl'     resolver : trimesh.visual.resolvers.Resolver       Used to load referenced data (like texture files)     kwargs : dict       Passed through to mesh constructor      Returns     ---------     meshes : (n,) list of dict       Contain kwargs for Trimesh constructor
Load a file to a Path object.      Parameters     -----------     obj : One of the following:          - Path, Path2D, or Path3D objects          - open file object (dxf or svg)          - file name (dxf or svg)          - shapely.geometry.Polygon          - shapely.geometry.MultiLineString          - dict with kwargs for Path constructor          - (n,2,(2|3)) float, line segments     file_type : str         Type of file is required if file         object passed.      Returns     ---------     path : Path, Path2D, Path3D object         Data as a native trimesh Path object
Turn entities and vertices into a Path2D or a Path3D     object depending on dimension of vertices.      Parameters     -----------     entities : list         Entity objects that reference vertex indices     vertices : (n, 2) or (n, 3) float         Vertices in space     metadata : dict         Any metadata about the path object      Returns     -----------     as_path : Path2D or Path3D object         Args in native trimesh object form
Given a geometry, list of geometries, or a Scene     return them as a single Scene object.      Parameters     ----------     geometry : splittable      Returns     ---------     scene: trimesh.Scene
Concatenate multiple scene objects into one scene.      Parameters     -------------     iterable : (n,) Trimesh or Scene        Geometries that should be appended     common : (n,) str        Nodes that shouldn't be remapped      Returns     ------------     result : trimesh.Scene        Scene containing all geometry
Add a geometry to the scene.          If the mesh has multiple transforms defined in its         metadata, they will all be copied into the         TransformForest of the current scene automatically.          Parameters         ----------         geometry : Trimesh, Path2D, Path3D PointCloud or list           Geometry to initially add to the scene         base_frame : str or hashable           Name of base frame         metadata : dict           Any metadata about the scene         graph : TransformForest or None           A passed transform graph to use          Returns         ----------         node_name : str           Name of node in self.graph
MD5 of scene which will change when meshes or         transforms are changed          Returns         --------         hashed: str, MD5 hash of scene
Is every geometry connected to the root node.          Returns         -----------         is_valid : bool           Does every geometry have a transform
A list of points that represent the corners of the         AABB of every geometry in the scene.          This can be useful if you want to take the AABB in         a specific frame.          Returns         -----------         corners: (n, 3) float, points in space
Return the overall bounding box of the scene.          Returns         --------         bounds: (2,3) float points for min, max corner
Return a correctly transformed polygon soup of the         current scene.          Returns         ----------         triangles: (n,3,3) float, triangles in space
Look up geometries by identifier MD5          Returns         ---------         identifiers: dict, identifier md5: key in self.geometry
Return a sequence of node keys of identical meshes.          Will combine meshes duplicated by copying in space with different keys in         self.geometry, as well as meshes repeated by self.nodes.          Returns         -----------         duplicates: (m) sequence of keys to self.nodes that represent                      identical geometry
Create a camera object for self.camera, and add         a transform to self.graph for it.          If arguments are not passed sane defaults will be figured         out which show the mesh roughly centered.          Parameters         -----------         angles : (3,) float           Initial euler angles in radians         distance : float           Distance from centroid         center : (3,) float           Point camera should be center on         camera : Camera object           Object that stores camera parameters
Get the single camera for the scene. If not manually         set one will abe automatically generated.          Returns         ----------         camera : trimesh.scene.Camera           Camera object defined for the scene
Get a list of the lights in the scene. If nothing is         set it will generate some automatically.          Returns         -------------         lights : [trimesh.scene.lighting.Light]           Lights in the scene.
Move the current scene so that the AABB of the whole         scene is centered at the origin.          Does this by changing the base frame to a new, offset         base frame.
Append all meshes in scene to a list of meshes.          Returns         ----------         dumped: (n,) list, of Trimesh objects transformed to their                            location the scene.graph
The convex hull of the whole scene          Returns         ---------         hull: Trimesh object, convex hull of all meshes in scene
Export a snapshot of the current scene.          Parameters         ----------         file_type: what encoding to use for meshes                    ie: dict, dict64, stl          Returns         ----------         export: dict with keys:                 meshes: list of meshes, encoded as per file_type                 transforms: edge list of transforms, eg:                              ((u, v, {'matrix' : np.eye(4)}))
Get a PNG image of a scene.          Parameters         -----------         resolution: (2,) int, resolution to render image         **kwargs:  passed to SceneViewer constructor          Returns         -----------         png: bytes, render of scene in PNG form
Get the units for every model in the scene, and         raise a ValueError if there are mixed units.          Returns         -----------         units : str           Units for every model in the scene
Set the units for every model in the scene without         converting any units just setting the tag.          Parameters         ------------         value : str           Value to set every geometry unit value to
If geometry has units defined convert them to new units.          Returns a new scene with geometries and transforms scaled.          Parameters         ----------         desired : str           Desired final unit system: 'inches', 'mm', etc.         guess : bool           Is the converter allowed to guess scale when models           don't have it specified in their metadata.          Returns         ----------         scaled : trimesh.Scene           Copy of scene with scaling applied and units set           for every model
Explode a scene around a point and vector.          Parameters         -----------         vector : (3,) float or float            Explode radially around a direction vector or spherically         origin : (3,) float           Point to explode around
Return a copy of the current scene, with meshes and scene         transforms scaled to the requested factor.          Parameters         -----------         scale : float           Factor to scale meshes and transforms          Returns         -----------         scaled : trimesh.Scene           A copy of the current scene but scaled
Return a deep copy of the current scene          Returns         ----------         copied : trimesh.Scene           Copy of the current scene
Display the current scene.          Parameters         -----------         viewer: str 'gl':       open a pyglet window                 str,'notebook': return ipython.display.HTML                 None: automatically pick based on whether or not                           we are in an ipython notebook         smooth : bool           Turn on or off automatic smooth shading
Get a list of all available loaders      Returns     -----------     loaders : list         Extensions of available loaders         i.e. 'stl', 'ply', 'dxf', etc.
Load a mesh or vectorized path into objects:     Trimesh, Path2D, Path3D, Scene      Parameters     ---------     file_obj : str, or file- like object       The source of the data to be loadeded     file_type: str       What kind of file type do we have (eg: 'stl')     resolver : trimesh.visual.Resolver       Object to load referenced assets like materials and textures     kwargs : **       Passed to geometry __init__      Returns     ---------     geometry : Trimesh, Path2D, Path3D, Scene       Loaded geometry as trimesh classes
Load a mesh file into a Trimesh object      Parameters     -----------     file_obj : str or file object       File name or file with mesh data     file_type : str or None       Which file type, e.g. 'stl'     kwargs : dict       Passed to Trimesh constructor      Returns     ----------     mesh : trimesh.Trimesh or trimesh.Scene       Loaded geometry data
Given a compressed archive load all the geometry that     we can from it.      Parameters     ----------     file_obj : open file-like object       Containing compressed data     file_type : str       Type of the archive file     mixed : bool       If False, for archives containing both 2D and 3D       data will only load the 3D data into the Scene.      Returns     ----------     scene : trimesh.Scene       Geometry loaded in to a Scene object
Load a mesh at a remote URL into a local trimesh object.      This must be called explicitly rather than automatically     from trimesh.load to ensure users don't accidentally make     network requests.      Parameters     ------------     url : string       URL containing mesh file     **kwargs : passed to `load`
Load geometry from a properly formatted dict or kwargs
Given a file_obj and a file_type try to turn them into a file-like     object and a lowercase string of file type.      Parameters     -----------     file_obj:  str: if string represents a file path, returns                     -------------------------------------------                     file_obj:   an 'rb' opened file object of the path                     file_type:  the extension from the file path                 str: if string is NOT a path, but has JSON-like special characters                     -------------------------------------------                     file_obj:   the same string passed as file_obj                     file_type:  set to 'json'                 str: string is a valid URL                     -------------------------------------------                     file_obj: an open 'rb' file object with retrieved data                     file_type: from the extension                 str: string is not an existing path or a JSON-like object                     -------------------------------------------                     ValueError will be raised as we can't do anything with input                 file like object: we cannot grab information on file_type automatically                     -------------------------------------------                     ValueError will be raised if file_type is None                     file_obj:  same as input                     file_type: same as input                 other object: like a shapely.geometry.Polygon, etc:                     -------------------------------------------                     file_obj:  same as input                     file_type: if None initially, set to the class name                                (in lower case), otherwise passed through      file_type: str, type of file and handled according to above      Returns     -----------     file_obj:  loadable object     file_type: str, lower case of the type of file (eg 'stl', 'dae', etc)     metadata:  dict, any metadata     opened:    bool, did we open the file or not
Pack smaller rectangles onto a larger rectangle, using a binary     space partition tree.      Parameters     ----------     rectangles : (n, 2) float       An array of (width, height) pairs       representing the rectangles to be packed.     sheet_size : (2,) float       Width, height of rectangular sheet     shuffle : bool       Whether or not to shuffle the insert order of the       smaller rectangles, as the final packing density depends       on insertion order.      Returns     ---------     density : float       Area filled over total sheet area     offset :  (m,2) float       Offsets to move rectangles to their packed location     inserted : (n,) bool       Which of the original rectangles were packed     consumed_box : (2,) float       Bounding box size of packed result
Pack a list of Path2D objects into a rectangle.      Parameters     ------------     paths: (n,) Path2D       Geometry to be packed      Returns     ------------     packed : trimesh.path.Path2D       Object containing input geometry     inserted : (m,) int       Indexes of paths inserted into result
Pack polygons into a rectangle by taking each Polygon's OBB     and then packing that as a rectangle.      Parameters     ------------     polygons : (n,) shapely.geometry.Polygon       Source geometry     sheet_size : (2,) float       Size of rectangular sheet     iterations : int       Number of times to run the loop     density_escape : float       When to exit early (0.0 - 1.0)     spacing : float       How big a gap to leave between polygons     quantity : (n,) int, or None       Quantity of each Polygon      Returns     -------------     overall_inserted : (m,) int       Indexes of inserted polygons     packed : (m, 3, 3) float       Homogeonous transforms from original frame to packed frame
Insert a rectangle into the bin.          Parameters         -------------         rectangle: (2,) float, size of rectangle to insert
Returns two bounding boxes representing the current         bounds split into two smaller boxes.          Parameters         -------------         length:   float, length to split         vertical: bool, if True will split box vertically          Returns         -------------         box: (2,4) float, two bounding boxes consisting of:                           [minx, miny, maxx, maxy]
Find an oriented bounding box for an array of 2D points.      Parameters     ----------     points : (n,2) float       Points in 2D.      Returns     ----------     transform : (3,3) float       Homogenous 2D transformation matrix to move the       input points so that the axis aligned bounding box       is CENTERED AT THE ORIGIN.     rectangle : (2,) float        Size of extents once input points are transformed        by transform
Find the oriented bounding box for a Trimesh      Parameters     ----------     obj : trimesh.Trimesh, (n, 2) float, or (n, 3) float        Mesh object or points in 2D or 3D space     angle_digits : int        How much angular precision do we want on our result.        Even with less precision the returned extents will cover        the mesh albeit with larger than minimal volume, and may        experience substantial speedups.      Returns     ----------     to_origin : (4,4) float       Transformation matrix which will move the center of the       bounding box of the input mesh to the origin.     extents: (3,) float       The extents of the mesh once transformed with to_origin
Find the approximate minimum volume cylinder which contains     a mesh or a a list of points.      Samples a hemisphere then uses scipy.optimize to pick the     final orientation of the cylinder.      A nice discussion about better ways to implement this is here:     https://www.staff.uni-mainz.de/schoemer/publications/ALGO00.pdf       Parameters     ----------     obj : trimesh.Trimesh, or (n, 3) float       Mesh object or points in space     sample_count : int       How densely should we sample the hemisphere.       Angular spacing is 180 degrees / this number      Returns     ----------     result : dict       With keys:         'radius'    : float, radius of cylinder         'height'    : float, height of cylinder         'transform' : (4,4) float, transform from the origin                       to centered cylinder
Given a pair of axis aligned bounds, return all     8 corners of the bounding box.      Parameters     ----------     bounds : (2,3) or (2,2) float       Axis aligned bounds      Returns     ----------     corners : (8,3) float       Corner vertices of the cube
Do an axis aligned bounding box check on a list of points.      Parameters     -----------     bounds : (2, dimension) float        Axis aligned bounding box     points : (n, dimension) float        Points in space      Returns     -----------     points_inside : (n,) bool       True if points are inside the AABB
Return HTML that will render the scene using     GLTF/GLB encoded to base64 loaded by three.js      Parameters     --------------     scene : trimesh.Scene       Source geometry      Returns     --------------     html : str       HTML containing embedded geometry
Convert a scene to HTML containing embedded geometry     and a three.js viewer that will display nicely in     an IPython/Jupyter notebook.      Parameters     -------------     scene : trimesh.Scene       Source geometry      Returns     -------------     html : IPython.display.HTML       Object containing rendered scene
Check to see if we are in an IPython or Jypyter notebook.      Returns     -----------     in_notebook : bool       Returns True if we are in a notebook
cons(i) is TRUE <=> b[i] is a consonant.
m() measures the number of consonant sequences between k0 and j.         if c is a consonant sequence and v a vowel sequence, and <..>         indicates arbitrary presence,             <c><v>       gives 0            <c>vc<v>     gives 1            <c>vcvc<v>   gives 2            <c>vcvcvc<v> gives 3            ....
vowelinstem(stem) is TRUE <=> stem contains a vowel
doublec(word) is TRUE <=> word ends with a double consonant
cvc(i) is TRUE <=>          a) ( --NEW--) i == 1, and word[0] word[1] is vowel consonant, or          b) word[i - 2], word[i - 1], word[i] has the form consonant -            vowel - consonant and also if the second c is not w, x or y. this            is used when trying to restore an e at the end of a short word.            e.g.                 cav(e), lov(e), hop(e), crim(e), but                snow, box, tray.
step1ab() gets rid of plurals and -ed or -ing. e.g.             caresses  ->  caress            ponies    ->  poni            sties     ->  sti            tie       ->  tie        (--NEW--: see below)            caress    ->  caress            cats      ->  cat             feed      ->  feed            agreed    ->  agree            disabled  ->  disable             matting   ->  mat            mating    ->  mate            meeting   ->  meet            milling   ->  mill            messing   ->  mess             meetings  ->  meet
step1c() turns terminal y to i when there is another vowel in the stem.         --NEW--: This has been modified from the original Porter algorithm so that y->i         is only done when y is preceded by a consonant, but not if the stem         is only a single consonant, i.e.             (*c and not c) Y -> I          So 'happy' -> 'happi', but           'enjoy' -> 'enjoy'  etc          This is a much better rule. Formerly 'enjoy'->'enjoi' and 'enjoyment'->         'enjoy'. Step 1c is perhaps done too soon; but with this modification that         no longer really matters.          Also, the removal of the vowelinstem(z) condition means that 'spy', 'fly',         'try' ... stem to 'spi', 'fli', 'tri' and conflate with 'spied', 'tried',         'flies' ...
step2() maps double suffices to single ones.         so -ization ( = -ize plus -ation) maps to -ize etc. note that the         string before the suffix must give m() > 0.
step3() deals with -ic-, -full, -ness etc. similar strategy to step2.
step4() takes off -ant, -ence etc., in context <c>vcvc<v>.
step5() removes a final -e if m() > 1, and changes -ll to -l if         m() > 1.
Returns the stem of p, or, if i and j are given, the stem of p[i:j+1].
Return the region R1 that is used by the Scandinavian stemmers.          R1 is the region after the first non-vowel following a vowel,         or is the null region at the end of the word if there is no         such non-vowel. But then R1 is adjusted so that the region         before it contains at least three letters.          :param word: The word whose region R1 is determined.         :type word: str or unicode         :param vowels: The vowels of the respective language that are                        used to determine the region R1.         :type vowels: unicode         :return: the region R1 for the respective word.         :rtype: unicode         :note: This helper method is invoked by the respective stem method of                the subclasses DanishStemmer, NorwegianStemmer, and                SwedishStemmer. It is not to be invoked directly!
Return the standard interpretations of the string regions R1 and R2.          R1 is the region after the first non-vowel following a vowel,         or is the null region at the end of the word if there is no         such non-vowel.          R2 is the region after the first non-vowel following a vowel         in R1, or is the null region at the end of the word if there         is no such non-vowel.          :param word: The word whose regions R1 and R2 are determined.         :type word: str or unicode         :param vowels: The vowels of the respective language that are                        used to determine the regions R1 and R2.         :type vowels: unicode         :return: (r1,r2), the regions R1 and R2 for the respective word.         :rtype: tuple         :note: This helper method is invoked by the respective stem method of                the subclasses DutchStemmer, FinnishStemmer,                FrenchStemmer, GermanStemmer, ItalianStemmer,                PortugueseStemmer, RomanianStemmer, and SpanishStemmer.                It is not to be invoked directly!         :note: A detailed description of how to define R1 and R2                can be found at http://snowball.tartarus.org/texts/r1r2.html
Return the standard interpretation of the string region RV.          If the second letter is a consonant, RV is the region after the         next following vowel. If the first two letters are vowels, RV is         the region after the next following consonant. Otherwise, RV is         the region after the third letter.          :param word: The word whose region RV is determined.         :type word: str or unicode         :param vowels: The vowels of the respective language that are                        used to determine the region RV.         :type vowels: unicode         :return: the region RV for the respective word.         :rtype: unicode         :note: This helper method is invoked by the respective stem method of                the subclasses ItalianStemmer, PortugueseStemmer,                RomanianStemmer, and SpanishStemmer. It is not to be                invoked directly!
Stem a Dutch word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Stem an English word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Stem a Finnish word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Stem a French word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Return the region RV that is used by the French stemmer.          If the word begins with two vowels, RV is the region after         the third letter. Otherwise, it is the region after the first         vowel not at the beginning of the word, or the end of the word         if these positions cannot be found. (Exceptionally, u'par',         u'col' or u'tap' at the beginning of a word is also taken to         define RV as the region to their right.)          :param word: The French word whose region RV is determined.         :type word: str or unicode         :param vowels: The French vowels that are used to determine                        the region RV.         :type vowels: unicode         :return: the region RV for the respective French word.         :rtype: unicode         :note: This helper method is invoked by the stem method of                the subclass FrenchStemmer. It is not to be invoked directly!
Stem a German word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Stem an Hungarian word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Return the region R1 that is used by the Hungarian stemmer.          If the word begins with a vowel, R1 is defined as the region         after the first consonant or digraph (= two letters stand for         one phoneme) in the word. If the word begins with a consonant,         it is defined as the region after the first vowel in the word.         If the word does not contain both a vowel and consonant, R1         is the null region at the end of the word.          :param word: The Hungarian word whose region R1 is determined.         :type word: str or unicode         :param vowels: The Hungarian vowels that are used to determine                        the region R1.         :type vowels: unicode         :param digraphs: The digraphs that are used to determine the                          region R1.         :type digraphs: tuple         :return: the region R1 for the respective word.         :rtype: unicode         :note: This helper method is invoked by the stem method of the subclass                HungarianStemmer. It is not to be invoked directly!
Stem an Italian word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Stem a Romanian word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Stem a Russian word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Return the regions RV and R2 which are used by the Russian stemmer.          In any word, RV is the region after the first vowel,         or the end of the word if it contains no vowel.          R2 is the region after the first non-vowel following         a vowel in R1, or the end of the word if there is no such non-vowel.          R1 is the region after the first non-vowel following a vowel,         or the end of the word if there is no such non-vowel.          :param word: The Russian word whose regions RV and R2 are determined.         :type word: str or unicode         :return: the regions RV and R2 for the respective Russian word.         :rtype: tuple         :note: This helper method is invoked by the stem method of the subclass                RussianStemmer. It is not to be invoked directly!
Transliterate a Russian word into the Roman alphabet.          A Russian word whose letters consist of the Cyrillic         alphabet are transliterated into the Roman alphabet         in order to ease the forthcoming stemming process.          :param word: The word that is transliterated.         :type word: unicode         :return: the transliterated word.         :rtype: unicode         :note: This helper method is invoked by the stem method of the subclass                RussianStemmer. It is not to be invoked directly!
Transliterate a Russian word back into the Cyrillic alphabet.          A Russian word formerly transliterated into the Roman alphabet         in order to ease the stemming process, is transliterated back         into the Cyrillic alphabet, its original form.          :param word: The word that is transliterated.         :type word: str or unicode         :return: word, the transliterated word.         :rtype: unicode         :note: This helper method is invoked by the stem method of the subclass                RussianStemmer. It is not to be invoked directly!
Stem a Spanish word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
Stem a Swedish word and return the stemmed form.          :param word: The word that is stemmed.         :type word: str or unicode         :return: The stemmed form.         :rtype: unicode
:param token: string         :return: normalized token type string
Stem an Arabic word and return the stemmed form.         :param word: string         :return: string
Remove accentuation from the given string.
Iteratively yield tokens as unicode strings, optionally also lowercasing them     and removing accent marks.
Tokenizes a given text into sentences, applying filters and lemmatizing them.     Returns a SyntacticUnit list.
Tokenizes a given text into words, applying filters and lemmatizing them.     Returns a dict of word -> syntacticUnit.
Given a list of sentences, returns a list of sentences with a     total word count similar to the word count provided.
:param extracted_lemmas:list of tuples     :param lemma_to_word: dict of {lemma:list of words}     :return: dict of {keyword:score}
:param keywords:dict of keywords:scores     :param split_text: list of strings     :return: combined_keywords:list
:param keywords:dict of keywords:scores     :param combined_keywords:list of word/s
Calculates PageRank for an undirected graph
:rtype: str
Helper for python2         :rtype: unicode
:rtype: bytes
:rtype: bool
:rtype: int
:rtype: float
:returns: Json parsed
:rtype: list
:rtype: tuple
:rtype: dict
:rtype: urlparse.ParseResult
Returns a config dictionary, defaulting to DATABASE_URL.          :rtype: dict
Returns a config dictionary, defaulting to CACHE_URL.          :rtype: dict
Returns a config dictionary, defaulting to EMAIL_URL.          :rtype: dict
Returns a config dictionary, defaulting to SEARCH_URL.          :rtype: dict
:rtype: Path
Return value for given environment variable.          :param var: Name of variable.         :param cast: Type to cast return value as.         :param default: If var not present in environ, return this instead.         :param parse_default: force to parse default..          :returns: Value from environment or default (if set)
Parse and cast provided value          :param value: Stringed value.         :param cast: Type to cast return value as.          :returns: Casted value
Pulled from DJ-Database-URL, parse an arbitrary Database URL.         Support currently exists for PostgreSQL, PostGIS, MySQL, Oracle and SQLite.          SQLite connects to file based databases. The same URL format is used, omitting the hostname,         and using the "file" portion as the filename of the database.         This has the effect of four slashes being present for an absolute file path:          >>> from environ import Env         >>> Env.db_url_config('sqlite:////full/path/to/your/file.sqlite')         {'ENGINE': 'django.db.backends.sqlite3', 'HOST': '', 'NAME': '/full/path/to/your/file.sqlite', 'PASSWORD': '', 'PORT': '', 'USER': ''}         >>> Env.db_url_config('postgres://uf07k1i6d8ia0v:wegauwhgeuioweg@ec2-107-21-253-135.compute-1.amazonaws.com:5431/d8r82722r2kuvn')         {'ENGINE': 'django.db.backends.postgresql', 'HOST': 'ec2-107-21-253-135.compute-1.amazonaws.com', 'NAME': 'd8r82722r2kuvn', 'PASSWORD': 'wegauwhgeuioweg', 'PORT': 5431, 'USER': 'uf07k1i6d8ia0v'}
Pulled from DJ-Cache-URL, parse an arbitrary Cache URL.          :param url:         :param backend:         :return:
Parses an email URL.
Read a .env file into os.environ.          If not given a path to a dotenv path, does filthy magic stack backtracking         to find manage.py and then find the dotenv.          http://www.wellfireinteractive.com/blog/easier-12-factor-django/          https://gist.github.com/bennylope/2999704
Create new Path based on self.root and provided paths.          :param paths: List of sub paths         :param kwargs: required=False         :rtype: Path
Open a file.          :param name: Filename appended to self.root         :param args: passed to open()         :param kwargs: passed to open()          :rtype: file
Display deprecation warning in a standard way.      Parameters     ----------     since : str         The release at which this API became deprecated.      message : str, optional         Override the default deprecation message.  The format         specifier `%(name)s` may be used for the name of the function,         and `%(alternative)s` may be used in the deprecation message         to insert the name of an alternative to the deprecated         function.  `%(obj_type)s` may be used to insert a friendly name         for the type of object being deprecated.      name : str, optional         The name of the deprecated object.      alternative : str, optional         An alternative function that the user may use in place of the         deprecated function.  The deprecation warning will tell the user         about this alternative if provided.      pending : bool, optional         If True, uses a PendingDeprecationWarning instead of a         DeprecationWarning.      obj_type : str, optional         The object type being deprecated.      addendum : str, optional         Additional text appended directly to the final message.      Examples     --------         Basic example::              # To warn of the deprecation of "metpy.name_of_module"             warn_deprecated('0.6.0', name='metpy.name_of_module',                             obj_type='module')
r"""Generate a meshgrid based on bounding box and x & y resolution.      Parameters     ----------     horiz_dim: integer         Horizontal resolution     bbox: dictionary         Dictionary containing coordinates for corners of study area.      Returns     -------     grid_x: (X, Y) ndarray         X dimension meshgrid defined by given bounding box     grid_y: (X, Y) ndarray         Y dimension meshgrid defined by given bounding box
r"""Calculate x,y coordinates of each grid cell.      Parameters     ----------     gx: numeric         x coordinates in meshgrid     gy: numeric         y coordinates in meshgrid      Returns     -------     (X, Y) ndarray         List of coordinates in meshgrid
r"""Return x and y ranges in meters based on bounding box.      bbox: dictionary         dictionary containing coordinates for corners of study area      Returns     -------     x_range: float         Range in meters in x dimension.     y_range: float         Range in meters in y dimension.
r"""Return meshgrid spacing based on bounding box.      bbox: dictionary         Dictionary containing coordinates for corners of study area.     h_dim: integer         Horizontal resolution in meters.      Returns     -------     x_steps, (X, ) ndarray         Number of grids in x dimension.     y_steps: (Y, ) ndarray         Number of grids in y dimension.
r"""Return bounding box based on given x and y coordinates assuming northern hemisphere.      x: numeric         x coordinates.     y: numeric         y coordinates.     spatial_pad: numeric         Number of meters to add to the x and y dimensions to reduce         edge effects.      Returns     -------     bbox: dictionary         dictionary containing coordinates for corners of study area
r"""Generate a natural neighbor interpolation of the given points to a regular grid.      This assigns values to the given grid using the Liang and Hale [Liang2010]_.     approach.      Parameters     ----------     xp: (N, ) ndarray         x-coordinates of observations     yp: (N, ) ndarray         y-coordinates of observations     variable: (N, ) ndarray         observation values associated with (xp, yp) pairs.         IE, variable[i] is a unique observation at (xp[i], yp[i])     grid_x: (M, 2) ndarray         Meshgrid associated with x dimension     grid_y: (M, 2) ndarray         Meshgrid associated with y dimension      Returns     -------     img: (M, N) ndarray         Interpolated values on a 2-dimensional grid      See Also     --------     natural_neighbor_to_points
Wrap natural_neighbor_to_grid for deprecated natural_neighbor function.
r"""Generate an inverse distance interpolation of the given points to a regular grid.      Values are assigned to the given grid using inverse distance weighting based on either     [Cressman1959]_ or [Barnes1964]_. The Barnes implementation used here based on [Koch1983]_.      Parameters     ----------     xp: (N, ) ndarray         x-coordinates of observations.     yp: (N, ) ndarray         y-coordinates of observations.     variable: (N, ) ndarray         observation values associated with (xp, yp) pairs.         IE, variable[i] is a unique observation at (xp[i], yp[i]).     grid_x: (M, 2) ndarray         Meshgrid associated with x dimension.     grid_y: (M, 2) ndarray         Meshgrid associated with y dimension.     r: float         Radius from grid center, within which observations         are considered and weighted.     gamma: float         Adjustable smoothing parameter for the barnes interpolation. Default None.     kappa: float         Response parameter for barnes interpolation. Default None.     min_neighbors: int         Minimum number of neighbors needed to perform barnes or cressman interpolation         for a point. Default is 3.     kind: str         Specify what inverse distance weighting interpolation to use.         Options: 'cressman' or 'barnes'. Default 'cressman'      Returns     -------     img: (M, N) ndarray         Interpolated values on a 2-dimensional grid      See Also     --------     inverse_distance_to_points
Wrap inverse_distance_to_grid for deprecated inverse_distance function.
r"""Interpolate given (x,y), observation (z) pairs to a grid based on given parameters.      Parameters     ----------     x: array_like         x coordinate     y: array_like         y coordinate     z: array_like         observation value     interp_type: str         What type of interpolation to use. Available options include:         1) "linear", "nearest", "cubic", or "rbf" from `scipy.interpolate`.         2) "natural_neighbor", "barnes", or "cressman" from `metpy.interpolate`.         Default "linear".     hres: float         The horizontal resolution of the generated grid, given in the same units as the         x and y parameters. Default 50000.     minimum_neighbors: int         Minimum number of neighbors needed to perform barnes or cressman interpolation for a         point. Default is 3.     gamma: float         Adjustable smoothing parameter for the barnes interpolation. Default 0.25.     kappa_star: float         Response parameter for barnes interpolation, specified nondimensionally         in terms of the Nyquist. Default 5.052     search_radius: float         A search radius to use for the barnes and cressman interpolation schemes.         If search_radius is not specified, it will default to the average spacing of         observations.     rbf_func: str         Specifies which function to use for Rbf interpolation.         Options include: 'multiquadric', 'inverse', 'gaussian', 'linear', 'cubic',         'quintic', and 'thin_plate'. Defualt 'linear'. See `scipy.interpolate.Rbf` for more         information.     rbf_smooth: float         Smoothing value applied to rbf interpolation.  Higher values result in more smoothing.     boundary_coords: dictionary         Optional dictionary containing coordinates of the study area boundary. Dictionary         should be in format: {'west': west, 'south': south, 'east': east, 'north': north}      Returns     -------     grid_x: (N, 2) ndarray         Meshgrid for the resulting interpolation in the x dimension     grid_y: (N, 2) ndarray         Meshgrid for the resulting interpolation in the y dimension ndarray     img: (M, N) ndarray         2-dimensional array representing the interpolated values for each grid.      Notes     -----     This function acts as a wrapper for `interpolate_points` to allow it to generate a regular     grid.      See Also     --------     interpolate_to_points
r"""Linear interpolation of a variable to a given vertical level from given values.      This function assumes that highest vertical level (lowest pressure) is zeroth index.     A classic use of this function would be to compute the potential temperature on the     dynamic tropopause (2 PVU surface).      Parameters     ----------     level_var: array_like (P, M, N)         Level values in 3D grid on common vertical coordinate (e.g., PV values on         isobaric levels). Assumes height dimension is highest to lowest in atmosphere.     interp_var: array_like (P, M, N)         Variable on 3D grid with same vertical coordinate as level_var to interpolate to         given level (e.g., potential temperature on isobaric levels)     level: int or float         Desired interpolated level (e.g., 2 PVU surface)      Other Parameters     ----------------     bottom_up_search : bool, optional         Controls whether to search for levels bottom-up, or top-down. Defaults to         True, which is bottom-up search.      Returns     -------     interp_level: (M, N) ndarray         The interpolated variable (e.g., potential temperature) on the desired level (e.g.,         2 PVU surface)      Notes     -----     This function implements a linear interpolation to estimate values on a given surface.     The prototypical example is interpolation of potential temperature to the dynamic     tropopause (e.g., 2 PVU surface)
Wrap interpolate_to_grid for deprecated interpolate function.
Interpolate NaN values in y.      Interpolate NaN values in the y dimension. Works with unsorted x values.      Parameters     ----------     x : array-like         1-dimensional array of numeric x-values     y : array-like         1-dimensional array of numeric y-values     kind : string         specifies the kind of interpolation x coordinate - 'linear' or 'log', optional.         Defaults to 'linear'.      Returns     -------         An array of the y coordinate data with NaN values interpolated.
r"""Interpolates data with any shape over a specified axis.      Interpolation over a specified axis for arrays of any shape.      Parameters     ----------     x : array-like         1-D array of desired interpolated values.      xp : array-like         The x-coordinates of the data points.      args : array-like         The data to be interpolated. Can be multiple arguments, all must be the same shape as         xp.      axis : int, optional         The axis to interpolate over. Defaults to 0.      fill_value: float, optional         Specify handling of interpolation points out of data bounds. If None, will return         ValueError if points are out of bounds. Defaults to nan.      Returns     -------     array-like         Interpolated values for each point with coordinates sorted in ascending order.      Examples     --------      >>> x = np.array([1., 2., 3., 4.])      >>> y = np.array([1., 2., 3., 4.])      >>> x_interp = np.array([2.5, 3.5])      >>> metpy.calc.interp(x_interp, x, y)      array([2.5, 3.5])      Notes     -----     xp and args must be the same shape.
r"""Interpolates data with logarithmic x-scale over a specified axis.      Interpolation on a logarithmic x-scale for interpolation values in pressure coordintates.      Parameters     ----------     x : array-like         1-D array of desired interpolated values.      xp : array-like         The x-coordinates of the data points.      args : array-like         The data to be interpolated. Can be multiple arguments, all must be the same shape as         xp.      axis : int, optional         The axis to interpolate over. Defaults to 0.      fill_value: float, optional         Specify handling of interpolation points out of data bounds. If None, will return         ValueError if points are out of bounds. Defaults to nan.      Returns     -------     array-like         Interpolated values for each point with coordinates sorted in ascending order.      Examples     --------      >>> x_log = np.array([1e3, 1e4, 1e5, 1e6])      >>> y_log = np.log(x_log) * 2 + 3      >>> x_interp = np.array([5e3, 5e4, 5e5])      >>> metpy.calc.log_interp(x_interp, x_log, y_log)      array([20.03438638, 24.63955657, 29.24472675])      Notes     -----     xp and args must be the same shape.
Calculate the distances in the x and y directions along a cross-section.      Parameters     ----------     cross : `xarray.DataArray`         The input DataArray of a cross-section from which to obtain geometeric distances in         the x and y directions.      Returns     -------     x, y : tuple of `xarray.DataArray`         A tuple of the x and y distances as DataArrays
Calculate the latitude of points in a cross-section.      Parameters     ----------     cross : `xarray.DataArray`         The input DataArray of a cross-section from which to obtain latitudes.      Returns     -------     latitude : `xarray.DataArray`         Latitude of points
r"""Calculate the unit tanget and unit normal vectors from a cross-section.      Given a path described parametrically by :math:`\vec{l}(i) = (x(i), y(i))`, we can find     the unit tangent vector by the formula      .. math:: \vec{T}(i) =         \frac{1}{\sqrt{\left( \frac{dx}{di} \right)^2 + \left( \frac{dy}{di} \right)^2}}         \left( \frac{dx}{di}, \frac{dy}{di} \right)      From this, because this is a two-dimensional path, the normal vector can be obtained by a     simple :math:`\frac{\pi}{2}` rotation.      Parameters     ----------     cross : `xarray.DataArray`         The input DataArray of a cross-section from which to obtain latitudes.     index : `str`, optional         A string denoting the index coordinate of the cross section, defaults to 'index' as         set by `metpy.interpolate.cross_section`.      Returns     -------     unit_tangent_vector, unit_normal_vector : tuple of `numpy.ndarray`         Arrays describing the unit tangent and unit normal vectors (in x,y) for all points         along the cross section.
r"""Obtain the tangential and normal components of a cross-section of a vector field.      Parameters     ----------     data_x : `xarray.DataArray`         The input DataArray of the x-component (in terms of data projection) of the vector         field.     data_y : `xarray.DataArray`         The input DataArray of the y-component (in terms of data projection) of the vector         field.      Returns     -------     component_tangential, component_normal: tuple of `xarray.DataArray`         The components of the vector field in the tangential and normal directions,         respectively.      See Also     --------     tangential_component, normal_component      Notes     -----     The coordinates of `data_x` and `data_y` must match.
r"""Obtain the normal component of a cross-section of a vector field.      Parameters     ----------     data_x : `xarray.DataArray`         The input DataArray of the x-component (in terms of data projection) of the vector         field.     data_y : `xarray.DataArray`         The input DataArray of the y-component (in terms of data projection) of the vector         field.      Returns     -------     component_normal: `xarray.DataArray`         The component of the vector field in the normal directions.      See Also     --------     cross_section_components, tangential_component      Notes     -----     The coordinates of `data_x` and `data_y` must match.
r"""Obtain the tangential component of a cross-section of a vector field.      Parameters     ----------     data_x : `xarray.DataArray`         The input DataArray of the x-component (in terms of data projection) of the vector         field.     data_y : `xarray.DataArray`         The input DataArray of the y-component (in terms of data projection) of the vector         field.      Returns     -------     component_tangential: `xarray.DataArray`         The component of the vector field in the tangential directions.      See Also     --------     cross_section_components, normal_component      Notes     -----     The coordinates of `data_x` and `data_y` must match.
r"""Calculate cross-sectional absolute momentum (also called pseudoangular momentum).      As given in [Schultz1999]_, absolute momentum (also called pseudoangular momentum) is     given by      .. math:: M = v + fx      where :math:`v` is the along-front component of the wind and :math:`x` is the cross-front     distance. Applied to a cross-section taken perpendicular to the front, :math:`v` becomes     the normal component of the wind and :math:`x` the tangential distance.      If using this calculation in assessing symmetric instability, geostrophic wind should be     used so that geostrophic absolute momentum :math:`\left(M_g\right)` is obtained, as     described in [Schultz1999]_.      Parameters     ----------     u_wind : `xarray.DataArray`         The input DataArray of the x-component (in terms of data projection) of the wind.     v_wind : `xarray.DataArray`         The input DataArray of the y-component (in terms of data projection) of the wind.      Returns     -------     absolute_momentum: `xarray.DataArray`         The absolute momentum      Notes     -----     The coordinates of `u_wind` and `v_wind` must match.
r"""Read colortable information from a file.      Reads a colortable, which consists of one color per line of the file, where     a color can be one of: a tuple of 3 floats, a string with a HTML color name,     or a string with a HTML hex color.      Parameters     ----------     fobj : a file-like object         A file-like object to read the colors from      Returns     -------     List of tuples         A list of the RGB color values, where each RGB color is a tuple of 3 floats in the         range of [0, 1].
r"""Convert a GEMPAK color table to one MetPy can read.      Reads lines from a GEMPAK-style color table file, and writes them to another file in     a format that MetPy can parse.      Parameters     ----------     infile : file-like object         The file-like object to read from     outfile : file-like object         The file-like object to write to
r"""Scan a resource directory for colortable files and add them to the registry.          Parameters         ----------         pkg : str             The package containing the resource directory         path : str             The path to the directory with the color tables
r"""Scan a directory on disk for color table files and add them to the registry.          Parameters         ----------         path : str             The path to the directory with the color tables
r"""Add a color table from a file to the registry.          Parameters         ----------         fobj : file-like object             The file to read the color table from         name : str             The name under which the color table will be stored
r"""Get a color table from the registry with a corresponding norm.          Builds a `matplotlib.colors.BoundaryNorm` using `start`, `step`, and         the number of colors, based on the color table obtained from `name`.          Parameters         ----------         name : str             The name under which the color table will be stored         start : float             The starting boundary         step : float             The step between boundaries          Returns         -------         `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`             The boundary norm based on `start` and `step` with the number of colors             from the number of entries matching the color table, and the color table itself.
r"""Get a color table from the registry with a corresponding norm.          Builds a `matplotlib.colors.BoundaryNorm` using `start`, `end`, and         the number of colors, based on the color table obtained from `name`.          Parameters         ----------         name : str             The name under which the color table will be stored         start : float             The starting boundary         end : float             The ending boundary          Returns         -------         `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`             The boundary norm based on `start` and `end` with the number of colors             from the number of entries matching the color table, and the color table itself.
r"""Get a color table from the registry with a corresponding norm.          Builds a `matplotlib.colors.BoundaryNorm` using `boundaries`.          Parameters         ----------         name : str             The name under which the color table will be stored         boundaries : array_like             The list of boundaries for the norm          Returns         -------         `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`             The boundary norm based on `boundaries`, and the color table itself.
r"""Generate a Cressman interpolation value for a point.      The calculated value is based on the given distances and search radius.      Parameters     ----------     sq_dist: (N, ) ndarray         Squared distance between observations and grid point     values: (N, ) ndarray         Observation values in same order as sq_dist     radius: float         Maximum distance to search for observations to use for         interpolation.      Returns     -------     value: float         Interpolation value for grid point.
r"""Generate a single pass barnes interpolation value for a point.      The calculated value is based on the given distances, kappa and gamma values.      Parameters     ----------     sq_dist: (N, ) ndarray         Squared distance between observations and grid point     values: (N, ) ndarray         Observation values in same order as sq_dist     kappa: float         Response parameter for barnes interpolation.     gamma: float         Adjustable smoothing parameter for the barnes interpolation. Default 1.      Returns     -------     value: float         Interpolation value for grid point.
r"""Generate a natural neighbor interpolation of the observations to the given point.      This uses the Liang and Hale approach [Liang2010]_. The interpolation will fail if     the grid point has no natural neighbors.      Parameters     ----------     xp: (N, ) ndarray         x-coordinates of observations     yp: (N, ) ndarray         y-coordinates of observations     variable: (N, ) ndarray         observation values associated with (xp, yp) pairs.         IE, variable[i] is a unique observation at (xp[i], yp[i])     grid_loc: (float, float)         Coordinates of the grid point at which to calculate the         interpolation.     tri: object         Delaunay triangulation of the observations.     neighbors: (N, ) ndarray         Simplex codes of the grid point's natural neighbors. The codes         will correspond to codes in the triangulation.     triangle_info: dictionary         Pre-calculated triangle attributes for quick look ups. Requires         items 'cc' (circumcenters) and 'r' (radii) to be associated with         each simplex code key from the delaunay triangulation.      Returns     -------     value: float        Interpolated value for the grid location
r"""Generate a natural neighbor interpolation to the given points.      This assigns values to the given interpolation points using the Liang and Hale     [Liang2010]_. approach.      Parameters     ----------     points: array_like, shape (n, 2)         Coordinates of the data points.     values: array_like, shape (n,)         Values of the data points.     xi: array_like, shape (M, 2)         Points to interpolate the data onto.      Returns     -------     img: (M,) ndarray         Array representing the interpolated values for each input point in `xi`      See Also     --------     natural_neighbor_to_grid
r"""Generate an inverse distance weighting interpolation to the given points.      Values are assigned to the given interpolation points based on either [Cressman1959]_ or     [Barnes1964]_. The Barnes implementation used here based on [Koch1983]_.      Parameters     ----------     points: array_like, shape (n, 2)         Coordinates of the data points.     values: array_like, shape (n,)         Values of the data points.     xi: array_like, shape (M, 2)         Points to interpolate the data onto.     r: float         Radius from grid center, within which observations         are considered and weighted.     gamma: float         Adjustable smoothing parameter for the barnes interpolation. Default None.     kappa: float         Response parameter for barnes interpolation. Default None.     min_neighbors: int         Minimum number of neighbors needed to perform barnes or cressman interpolation         for a point. Default is 3.     kind: str         Specify what inverse distance weighting interpolation to use.         Options: 'cressman' or 'barnes'. Default 'cressman'      Returns     -------     img: (M,) ndarray         Array representing the interpolated values for each input point in `xi`      See Also     --------     inverse_distance_to_grid
r"""Interpolate unstructured point data to the given points.      This function interpolates the given `values` valid at `points` to the points `xi`. This is     modeled after `scipy.interpolate.griddata`, but acts as a generalization of it by including     the following types of interpolation:      - Linear     - Nearest Neighbor     - Cubic     - Radial Basis Function     - Natural Neighbor (2D Only)     - Barnes (2D Only)     - Cressman (2D Only)      Parameters     ----------     points: array_like, shape (n, D)         Coordinates of the data points.     values: array_like, shape (n,)         Values of the data points.     xi: array_like, shape (M, D)         Points to interpolate the data onto.     interp_type: str         What type of interpolation to use. Available options include:         1) "linear", "nearest", "cubic", or "rbf" from `scipy.interpolate`.         2) "natural_neighbor", "barnes", or "cressman" from `metpy.interpolate`.         Default "linear".     minimum_neighbors: int         Minimum number of neighbors needed to perform barnes or cressman interpolation for a         point. Default is 3.     gamma: float         Adjustable smoothing parameter for the barnes interpolation. Default 0.25.     kappa_star: float         Response parameter for barnes interpolation, specified nondimensionally         in terms of the Nyquist. Default 5.052     search_radius: float         A search radius to use for the barnes and cressman interpolation schemes.         If search_radius is not specified, it will default to the average spacing of         observations.     rbf_func: str         Specifies which function to use for Rbf interpolation.         Options include: 'multiquadric', 'inverse', 'gaussian', 'linear', 'cubic',         'quintic', and 'thin_plate'. Defualt 'linear'. See `scipy.interpolate.Rbf` for more         information.     rbf_smooth: float         Smoothing value applied to rbf interpolation.  Higher values result in more smoothing.      Returns     -------     values_interpolated: (M,) ndarray         Array representing the interpolated values for each input point in `xi`.      Notes     -----     This function primarily acts as a wrapper for the individual interpolation routines. The     individual functions are also available for direct use.      See Also     --------     interpolate_to_grid
r"""Convert 7 bytes from a GINI file to a `datetime` instance.
r"""Convert a 3 byte string to a signed integer value.
r"""Create an io helper to convert an integer to a named value.
Add coordinate variables (projection and lon/lat) to a dataset.
r"""Convert a Variable with projection information to a Proj.4 Projection instance.      The attributes of this Variable must conform to the Climate and Forecasting (CF)     netCDF conventions.      Parameters     ----------     var : Variable         The projection variable with appropriate attributes.
r"""Compute the perturbation from the mean of a time series.      Parameters     ----------     ts : array_like          The time series from which you wish to find the perturbation          time series (perturbation from the mean).      Returns     -------     array_like         The perturbation time series.      Other Parameters     ----------------     axis : int            The index of the time axis. Default is -1      Notes     -----     The perturbation time series produced by this function is defined as     the perturbations about the mean:      .. math:: x(t)^{\prime} = x(t) - \overline{x(t)}
r"""Compute turbulence kinetic energy.      Compute the turbulence kinetic energy (e) from the time series of the     velocity components.      Parameters     ----------     u : array_like         The wind component along the x-axis     v : array_like         The wind component along the y-axis     w : array_like         The wind component along the z-axis      perturbation : {False, True}, optional                    True if the `u`, `v`, and `w` components of wind speed                    supplied to the function are perturbation velocities.                    If False, perturbation velocities will be calculated by                    removing the mean value from each component.      Returns     -------     array_like         The corresponding turbulence kinetic energy value      Other Parameters     ----------------     axis : int            The index of the time axis. Default is -1      See Also     --------     get_perturbation : Used to compute perturbations if `perturbation`                        is False.      Notes     -----     Turbulence Kinetic Energy is computed as:      .. math:: e = 0.5 \sqrt{\overline{u^{\prime2}} +                             \overline{v^{\prime2}} +                             \overline{w^{\prime2}}},      where the velocity components      .. math:: u^{\prime}, v^{\prime}, u^{\prime}      are perturbation velocities. For more information on the subject, please     see [Garratt1994]_.
r"""Compute the kinematic flux from two time series.      Compute the kinematic flux from the time series of two variables `vel`     and b. Note that to be a kinematic flux, at least one variable must be     a component of velocity.      Parameters     ----------     vel : array_like         A component of velocity      b : array_like         May be a component of velocity or a scalar variable (e.g. Temperature)      perturbation : bool, optional         `True` if the `vel` and `b` variables are perturbations. If `False`, perturbations         will be calculated by removing the mean value from each variable. Defaults to `False`.      Returns     -------     array_like         The corresponding kinematic flux      Other Parameters     ----------------     axis : int, optional            The index of the time axis, along which the calculations will be            performed. Defaults to -1      Notes     -----     A kinematic flux is computed as      .. math:: \overline{u^{\prime} s^{\prime}}      where at the prime notation denotes perturbation variables, and at least     one variable is perturbation velocity. For example, the vertical kinematic     momentum flux (two velocity components):      .. math:: \overline{u^{\prime} w^{\prime}}      or the vertical kinematic heat flux (one velocity component, and one     scalar):      .. math:: \overline{w^{\prime} T^{\prime}}      If perturbation variables are passed into this function (i.e.     `perturbation` is True), the kinematic flux is computed using the equation     above.      However, the equation above can be rewritten as      .. math:: \overline{us} - \overline{u}~\overline{s}      which is computationally more efficient. This is how the kinematic flux     is computed in this function if `perturbation` is False.      For more information on the subject, please see [Garratt1994]_.
r"""Compute the friction velocity from the time series of velocity components.      Compute the friction velocity from the time series of the x, z,     and optionally y, velocity components.      Parameters     ----------     u : array_like         The wind component along the x-axis     w : array_like         The wind component along the z-axis     v : array_like, optional         The wind component along the y-axis.      perturbation : {False, True}, optional                    True if the `u`, `w`, and `v` components of wind speed                    supplied to the function are perturbation velocities.                    If False, perturbation velocities will be calculated by                    removing the mean value from each component.      Returns     -------     array_like         The corresponding friction velocity      Other Parameters     ----------------     axis : int            The index of the time axis. Default is -1      See Also     --------     kinematic_flux : Used to compute the x-component and y-component                      vertical kinematic momentum flux(es) used in the                      computation of the friction velocity.      Notes     -----     The Friction Velocity is computed as:      .. math:: u_{*} = \sqrt[4]{\left(\overline{u^{\prime}w^{\prime}}\right)^2 +                                \left(\overline{v^{\prime}w^{\prime}}\right)^2},      where :math: \overline{u^{\prime}w^{\prime}} and     :math: \overline{v^{\prime}w^{\prime}}     are the x-component and y-components of the vertical kinematic momentum     flux, respectively. If the optional v component of velocity is not     supplied to the function, the computation of the friction velocity is     reduced to      .. math:: u_{*} = \sqrt[4]{\left(\overline{u^{\prime}w^{\prime}}\right)^2}      For more information on the subject, please see [Garratt1994]_.
Return a file-object given either a filename or an object.      Handles opening with the right class based on the file extension.
Decompress all frames of zlib-compressed bytes.      Repeatedly tries to decompress `data` until all data are decompressed, or decompression     fails. This will skip over bytes that are not compressed with zlib.      Parameters     ----------     data : bytearray or bytes         Binary data compressed using zlib.      Returns     -------         bytearray             All decompressed bytes
Perform a hexudmp of the buffer.      Returns the hexdump as a canonically-formatted string.
Override the units on the underlying variable.
Parse bytes and return a namedtuple.
Read bytes from a buffer and return as a namedtuple.
Parse bytes and return a namedtuple.
Unpack the next bytes from a file object.
Mark the current location and return its id so that the buffer can return later.
Jump to a previously set mark.
Replace the data after the marked location with the specified data.
Parse and return a structure from the current buffer offset.
Parse data from the current buffer offset using a function.
Parse the current buffer offset as the specified code.
Read and return the specified bytes from the buffer.
Get the next bytes in the buffer without modifying the offset.
Jump the ahead the specified bytes in the buffer.
Draw one of the natural neighbor polygons with some information.
Transpose array or list of arrays if they are 2D.
Wrap a function to ensure all array arguments are y, x ordered, based on kwarg.
r"""Calculate the vertical vorticity of the horizontal wind.      Parameters     ----------     u : (M, N) ndarray         x component of the wind     v : (M, N) ndarray         y component of the wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.      Returns     -------     (M, N) ndarray         vertical vorticity      See Also     --------     divergence      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate the horizontal divergence of the horizontal wind.      Parameters     ----------     u : (M, N) ndarray         x component of the wind     v : (M, N) ndarray         y component of the wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.      Returns     -------     (M, N) ndarray         The horizontal divergence      See Also     --------     vorticity      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate the horizontal total deformation of the horizontal wind.      Parameters     ----------     u : (M, N) ndarray         x component of the wind     v : (M, N) ndarray         y component of the wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.      Returns     -------     (M, N) ndarray         Total Deformation      See Also     --------     shearing_deformation, stretching_deformation      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate the advection of a scalar field by the wind.      The order of the dimensions of the arrays must match the order in which     the wind components are given.  For example, if the winds are given [u, v],     then the scalar and wind arrays must be indexed as x,y (which puts x as the     rows, not columns).      Parameters     ----------     scalar : N-dimensional array         Array (with N-dimensions) with the quantity to be advected.     wind : sequence of arrays         Length M sequence of N-dimensional arrays.  Represents the flow,         with a component of the wind in each dimension.  For example, for         horizontal advection, this could be a list: [u, v], where u and v         are each a 2-dimensional array.     deltas : sequence of float or ndarray         A (length M) sequence containing the grid spacing(s) in each dimension. If using         arrays, in each array there should be one item less than the size of `scalar` along the         applicable axis.      Returns     -------     N-dimensional array         An N-dimensional array containing the advection at all grid points.
r"""Calculate the 2D kinematic frontogenesis of a temperature field.      The implementation is a form of the Petterssen Frontogenesis and uses the formula     outlined in [Bluestein1993]_ pg.248-253.      .. math:: F=\frac{1}{2}\left|\nabla \theta\right|[D cos(2\beta)-\delta]      * :math:`F` is 2D kinematic frontogenesis     * :math:`\theta` is potential temperature     * :math:`D` is the total deformation     * :math:`\beta` is the angle between the axis of dilitation and the isentropes     * :math:`\delta` is the divergence      Parameters     ----------     thta : (M, N) ndarray         Potential temperature     u : (M, N) ndarray         x component of the wind     v : (M, N) ndarray         y component of the wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.      Returns     -------     (M, N) ndarray         2D Frontogenesis in [temperature units]/m/s      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.      Conversion factor to go from [temperature units]/m/s to [temperature units/100km/3h]     :math:`1.08e4*1.e5`
r"""Calculate the geostrophic wind given from the heights or geopotential.      Parameters     ----------     heights : (M, N) ndarray         The height field, with either leading dimensions of (x, y) or trailing dimensions         of (y, x), depending on the value of ``dim_order``.     f : array_like         The coriolis parameter.  This can be a scalar to be applied         everywhere or an array of values.     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `heights` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `heights` along the applicable axis.      Returns     -------     A 2-item tuple of arrays         A tuple of the u-component and v-component of the geostrophic wind.      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate the ageostrophic wind given from the heights or geopotential.      Parameters     ----------     heights : (M, N) ndarray         The height field.     f : array_like         The coriolis parameter.  This can be a scalar to be applied         everywhere or an array of values.     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `heights` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `heights` along the applicable axis.     u : (M, N) ndarray         The u wind field.     v : (M, N) ndarray         The u wind field.      Returns     -------     A 2-item tuple of arrays         A tuple of the u-component and v-component of the ageostrophic wind.      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate storm relative helicity.      Calculates storm relatively helicity following [Markowski2010] 230-231.      .. math:: \int\limits_0^d (\bar v - c) \cdot \bar\omega_{h} \,dz      This is applied to the data from a hodograph with the following summation:      .. math:: \sum_{n = 1}^{N-1} [(u_{n+1} - c_{x})(v_{n} - c_{y}) -                                   (u_{n} - c_{x})(v_{n+1} - c_{y})]      Parameters     ----------     u : array-like         u component winds     v : array-like         v component winds     heights : array-like         atmospheric heights, will be converted to AGL     depth : number         depth of the layer     bottom : number         height of layer bottom AGL (default is surface)     storm_u : number         u component of storm motion (default is 0 m/s)     storm_v : number         v component of storm motion (default is 0 m/s)      Returns     -------     `pint.Quantity, pint.Quantity, pint.Quantity`         positive, negative, total storm-relative helicity
Calculate the absolute vorticity of the horizontal wind.      Parameters     ----------     u : (M, N) ndarray         x component of the wind     v : (M, N) ndarray         y component of the wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     lats : (M, N) ndarray         latitudes of the wind data in radians or with appropriate unit information attached      Returns     -------     (M, N) ndarray         absolute vorticity      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate the baroclinic potential vorticity.      .. math:: PV = -g \left(\frac{\partial u}{\partial p}\frac{\partial \theta}{\partial y}               - \frac{\partial v}{\partial p}\frac{\partial \theta}{\partial x}               + \frac{\partial \theta}{\partial p}(\zeta + f) \right)      This formula is based on equation 4.5.93 [Bluestein1993]_.      Parameters     ----------     potential_temperature : (P, M, N) ndarray         potential temperature     pressure : (P, M, N) ndarray         vertical pressures     u : (P, M, N) ndarray         x component of the wind     v : (P, M, N) ndarray         y component of the wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     lats : (M, N) ndarray         latitudes of the wind data in radians or with appropriate unit information attached     axis : int, optional         The axis corresponding to the vertical dimension in the potential temperature         and pressure arrays, defaults to 0, the first dimension.      Returns     -------     (P, M, N) ndarray         baroclinic potential vorticity      Notes     -----     This function will only work with data that is in (P, Y, X) format. If your data     is in a different order you will need to re-order your data in order to get correct     results from this function.      The same function can be used for isobaric and isentropic PV analysis. Provide winds     for vorticity calculations on the desired isobaric or isentropic surface. At least three     layers of pressure/potential temperature are required in order to calculate the vertical     derivative (one above and below the desired surface). The first two terms will be zero if     isentropic level data is used due to the gradient of theta in both the x and y-directions     will be zero since you are on an isentropic surface.      This function expects pressure/isentropic level to increase with increasing array element     (e.g., from higher in the atmosphere to closer to the surface. If the pressure array is     one-dimensional p[:, None, None] can be used to make it appear multi-dimensional.)
r"""Calculate the barotropic (Rossby) potential vorticity.      .. math:: PV = \frac{f + \zeta}{H}      This formula is based on equation 7.27 [Hobbs2006]_.      Parameters     ----------     heights : (M, N) ndarray         atmospheric heights     u : (M, N) ndarray         x component of the wind     v : (M, N) ndarray         y component of the wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     lats : (M, N) ndarray         latitudes of the wind data in radians or with appropriate unit information attached      Returns     -------     (M, N) ndarray         barotropic potential vorticity      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate the inertial advective wind.      .. math:: \frac{\hat k}{f} \times (\vec V \cdot \nabla)\hat V_g      .. math:: \frac{\hat k}{f} \times \left[ \left( u \frac{\partial u_g}{\partial x} + v               \frac{\partial u_g}{\partial y} \right) \hat i + \left( u \frac{\partial v_g}               {\partial x} + v \frac{\partial v_g}{\partial y} \right) \hat j \right]      .. math:: \left[ -\frac{1}{f}\left(u \frac{\partial v_g}{\partial x} + v               \frac{\partial v_g}{\partial y} \right) \right] \hat i + \left[ \frac{1}{f}               \left( u \frac{\partial u_g}{\partial x} + v \frac{\partial u_g}{\partial y}               \right) \right] \hat j      This formula is based on equation 27 of [Rochette2006]_.      Parameters     ----------     u : (M, N) ndarray         x component of the advecting wind     v : (M, N) ndarray         y component of the advecting wind     u_geostrophic : (M, N) ndarray         x component of the geostrophic (advected) wind     v_geostrophic : (M, N) ndarray         y component of the geostrophic (advected) wind     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     lats : (M, N) ndarray         latitudes of the wind data in radians or with appropriate unit information attached      Returns     -------     (M, N) ndarray         x component of inertial advective wind     (M, N) ndarray         y component of inertial advective wind      Notes     -----     Many forms of the inertial advective wind assume the advecting and advected     wind to both be the geostrophic wind. To do so, pass the x and y components     of the geostrophic with for u and u_geostrophic/v and v_geostrophic.      If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
r"""Calculate Q-vector at a given pressure level using the u, v winds and temperature.      .. math:: \vec{Q} = (Q_1, Q_2)                       =  - \frac{R}{\sigma p}\left(                                \frac{\partial \vec{v}_g}{\partial x} \cdot \nabla_p T,                                \frac{\partial \vec{v}_g}{\partial y} \cdot \nabla_p T                            \right)      This formula follows equation 5.7.55 from [Bluestein1992]_, and can be used with the     the below form of the quasigeostrophic omega equation to assess vertical motion     ([Bluestein1992]_ equation 5.7.54):      .. math:: \left( \nabla_p^2 + \frac{f_0^2}{\sigma} \frac{\partial^2}{\partial p^2}                   \right) \omega =               - 2 \nabla_p \cdot \vec{Q} -                   \frac{R}{\sigma p} \beta \frac{\partial T}{\partial x}.      Parameters     ----------     u : (M, N) ndarray         x component of the wind (geostrophic in QG-theory)     v : (M, N) ndarray         y component of the wind (geostrophic in QG-theory)     temperature : (M, N) ndarray         Array of temperature at pressure level     pressure : `pint.Quantity`         Pressure at level     dx : float or ndarray         The grid spacing(s) in the x-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     dy : float or ndarray         The grid spacing(s) in the y-direction. If an array, there should be one item less than         the size of `u` along the applicable axis.     static_stability : `pint.Quantity`, optional         The static stability at the pressure level. Defaults to 1 if not given to calculate         the Q-vector without factoring in static stability.      Returns     -------     tuple of (M, N) ndarrays         The components of the Q-vector in the u- and v-directions respectively      See Also     --------     static_stability      Notes     -----     If inputs have more than two dimensions, they are assumed to have either leading dimensions     of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.
Make our basic default map for plotting
r"""Get all target_points within a specified radius of a center point.      All data must be in same coordinate system, or you will get undetermined results.      Parameters     ----------     center_points: (X, Y) ndarray         location from which to grab surrounding points within r     target_points: (X, Y) ndarray         points from which to return if they are within r of center_points     r: integer         search radius around center_points to grab target_points      Returns     -------     matches: (X, Y) ndarray         A list of points within r distance of, and in the same         order as, center_points
r"""Get count of target points within a specified radius from center points.      All data must be in same coordinate system, or you will get undetermined results.      Parameters     ----------     center_points: (X, Y) ndarray         locations from which to grab surrounding points within r     target_points: (X, Y) ndarray         points from which to return if they are within r of center_points     r: integer         search radius around center_points to grab target_points      Returns     -------     matches: (N, ) ndarray         A list of point counts within r distance of, and in the same         order as, center_points
r"""Return the area of a triangle.      Parameters     ----------     pt1: (X,Y) ndarray         Starting vertex of a triangle     pt2: (X,Y) ndarray         Second vertex of a triangle     pt3: (X,Y) ndarray         Ending vertex of a triangle      Returns     -------     area: float         Area of the given triangle.
r"""Return the squared distance between two points.      This is faster than calculating distance but should     only be used with comparable ratios.      Parameters     ----------     x0: float         Starting x coordinate     y0: float         Starting y coordinate     x1: float         Ending x coordinate     y1: float         Ending y coordinate      Returns     -------     d2: float         squared distance      See Also     --------     distance
r"""Return the distance between two points.      Parameters     ----------     p0: (X,Y) ndarray         Starting coordinate     p1: (X,Y) ndarray         Ending coordinate      Returns     -------     d: float         distance      See Also     --------     dist_2
r"""Calculate and return the squared radius of a given triangle's circumcircle.      This is faster than calculating radius but should only be used with comparable ratios.      Parameters     ----------     pt0: (x, y)         Starting vertex of triangle     pt1: (x, y)         Second vertex of triangle     pt2: (x, y)         Final vertex of a triangle      Returns     -------     r: float         circumcircle radius      See Also     --------     circumcenter
r"""Calculate and return the circumcenter of a circumcircle generated by a given triangle.      All three points must be unique or a division by zero error will be raised.      Parameters     ----------     pt0: (x, y)         Starting vertex of triangle     pt1: (x, y)         Second vertex of triangle     pt2: (x, y)         Final vertex of a triangle      Returns     -------     cc: (x, y)         circumcenter coordinates      See Also     --------     circumcenter
r"""Return the natural neighbor triangles for each given grid cell.      These are determined by the properties of the given delaunay triangulation.     A triangle is a natural neighbor of a grid cell if that triangles circumcenter     is within the circumradius of the grid cell center.      Parameters     ----------     tri: Object         A Delaunay Triangulation.     grid_points: (X, Y) ndarray         Locations of grids.      Returns     -------     members: dictionary         List of simplex codes for natural neighbor         triangles in 'tri' for each grid cell.     triangle_info: dictionary         Circumcenter and radius information for each         triangle in 'tri'.
r"""Return the natural neighbors of a triangle containing a point.      This is based on the provided Delaunay Triangulation.      Parameters     ----------     tri: Object         A Delaunay Triangulation     cur_tri: int         Simplex code for Delaunay Triangulation lookup of         a given triangle that contains 'position'.     point: (x, y)         Coordinates used to calculate distances to         simplexes in 'tri'.      Returns     -------     nn: (N, ) array         List of simplex codes for natural neighbor         triangles in 'tri'.
r"""Find and return the outside edges of a collection of natural neighbor triangles.      There is no guarantee that this boundary is convex, so ConvexHull is not     sufficient in some situations.      Parameters     ----------     tri: Object         A Delaunay Triangulation     triangles: (N, ) array         List of natural neighbor triangles.      Returns     -------     edges: (2, N) ndarray         List of vertex codes that form outer edges of         a group of natural neighbor triangles.
r"""Find the area of a given polygon using the shoelace algorithm.      Parameters     ----------     poly: (2, N) ndarray         2-dimensional coordinates representing an ordered         traversal around the edge a polygon.      Returns     -------     area: float
r"""Return an ordered traversal of the edges of a two-dimensional polygon.      Parameters     ----------     edges: (2, N) ndarray         List of unordered line segments, where each         line segment is represented by two unique         vertex codes.      Returns     -------     ordered_edges: (2, N) ndarray
r"""Calculate precipitable water through the depth of a sounding.      Formula used is:      .. math::  -\frac{1}{\rho_l g} \int\limits_{p_\text{bottom}}^{p_\text{top}} r dp      from [Salby1996]_, p. 28.      Parameters     ----------     dewpt : `pint.Quantity`         Atmospheric dewpoint profile     pressure : `pint.Quantity`         Atmospheric pressure profile     bottom: `pint.Quantity`, optional         Bottom of the layer, specified in pressure. Defaults to None (highest pressure).     top: `pint.Quantity`, optional         The top of the layer, specified in pressure. Defaults to None (lowest pressure).      Returns     -------     `pint.Quantity`         The precipitable water in the layer
r"""Calculate pressure-weighted mean of an arbitrary variable through a layer.      Layer top and bottom specified in height or pressure.      Parameters     ----------     pressure : `pint.Quantity`         Atmospheric pressure profile     *args : `pint.Quantity`         Parameters for which the pressure-weighted mean is to be calculated.     heights : `pint.Quantity`, optional         Heights from sounding. Standard atmosphere heights assumed (if needed)         if no heights are given.     bottom: `pint.Quantity`, optional         The bottom of the layer in either the provided height coordinate         or in pressure. Don't provide in meters AGL unless the provided         height coordinate is meters AGL. Default is the first observation,         assumed to be the surface.     depth: `pint.Quantity`, optional         The depth of the layer in meters or hPa.      Returns     -------     `pint.Quantity`         u_mean: u-component of layer mean wind.     `pint.Quantity`         v_mean: v-component of layer mean wind.
r"""Calculate the Bunkers right-mover and left-mover storm motions and sfc-6km mean flow.      Uses the storm motion calculation from [Bunkers2000]_.      Parameters     ----------     pressure : array-like         Pressure from sounding     u : array-like         U component of the wind     v : array-like         V component of the wind     heights : array-like         Heights from sounding      Returns     -------     right_mover: `pint.Quantity`         U and v component of Bunkers RM storm motion     left_mover: `pint.Quantity`         U and v component of Bunkers LM storm motion     wind_mean: `pint.Quantity`         U and v component of sfc-6km mean flow
r"""Calculate bulk shear through a layer.      Layer top and bottom specified in meters or pressure.      Parameters     ----------     pressure : `pint.Quantity`         Atmospheric pressure profile     u : `pint.Quantity`         U-component of wind.     v : `pint.Quantity`         V-component of wind.     height : `pint.Quantity`, optional         Heights from sounding     depth: `pint.Quantity`, optional         The depth of the layer in meters or hPa. Defaults to 100 hPa.     bottom: `pint.Quantity`, optional         The bottom of the layer in height or pressure coordinates.         If using a height, it must be in the same coordinates as the given         heights (i.e., don't use meters AGL unless given heights         are in meters AGL.) Defaults to the highest pressure or lowest height given.      Returns     -------     u_shr: `pint.Quantity`         u-component of layer bulk shear     v_shr: `pint.Quantity`         v-component of layer bulk shear
r"""Calculate the supercell composite parameter.      The supercell composite parameter is designed to identify     environments favorable for the development of supercells,     and is calculated using the formula developed by     [Thompson2004]_:      .. math::  \text{SCP} = \frac{\text{MUCAPE}}{1000 \text{J/kg}} *                \frac{\text{Effective SRH}}{50 \text{m}^2/\text{s}^2} *                \frac{\text{Effective Shear}}{20 \text{m/s}}      The effective_shear term is set to zero below 10 m/s and     capped at 1 when effective_shear exceeds 20 m/s.      Parameters     ----------     mucape : `pint.Quantity`         Most-unstable CAPE     effective_storm_helicity : `pint.Quantity`         Effective-layer storm-relative helicity     effective_shear : `pint.Quantity`         Effective bulk shear      Returns     -------     array-like         supercell composite
r"""Calculate the significant tornado parameter (fixed layer).      The significant tornado parameter is designed to identify     environments favorable for the production of significant     tornadoes contingent upon the development of supercells.     It's calculated according to the formula used on the SPC     mesoanalysis page, updated in [Thompson2004]_:      .. math::  \text{SIGTOR} = \frac{\text{SBCAPE}}{1500 \text{J/kg}} * \frac{(2000 \text{m} -                \text{LCL}_\text{SB})}{1000 \text{m}} *                \frac{SRH_{\text{1km}}}{150 \text{m}^\text{s}/\text{s}^2} *                \frac{\text{Shear}_\text{6km}}{20 \text{m/s}}      The lcl height is set to zero when the lcl is above 2000m and     capped at 1 when below 1000m, and the shr6 term is set to 0     when shr6 is below 12.5 m/s and maxed out at 1.5 when shr6     exceeds 30 m/s.      Parameters     ----------     sbcape : `pint.Quantity`         Surface-based CAPE     surface_based_lcl_height : `pint.Quantity`         Surface-based lifted condensation level     storm_helicity_1km : `pint.Quantity`         Surface-1km storm-relative helicity     shear_6km : `pint.Quantity`         Surface-6km bulk shear      Returns     -------     array-like         significant tornado parameter
r"""Calculate the critical angle.      The critical angle is the angle between the 10m storm-relative inflow vector     and the 10m-500m shear vector. A critical angle near 90 degrees indicates     that a storm in this environment on the indicated storm motion vector     is likely ingesting purely streamwise vorticity into its updraft, and [Esterheld2008]_     showed that significantly tornadic supercells tend to occur in environments     with critical angles near 90 degrees.      Parameters     ----------     pressure : `pint.Quantity`         Pressures from sounding.     u : `pint.Quantity`         U-component of sounding winds.     v : `pint.Quantity`         V-component of sounding winds.     heights : `pint.Quantity`         Heights from sounding.     stormu : `pint.Quantity`         U-component of storm motion.     stormv : `pint.Quantity`         V-component of storm motion.      Returns     -------     `pint.Quantity`         critical angle in degrees
Calculate index values to properly broadcast index array within data array.      See usage in interp.
Register a callable with the registry under a particular name.          Parameters         ----------         name : str             The name under which to register a function          Returns         -------         dec : callable             A decorator that takes a function and will register it under the name.
r"""Compute the wind speed from u and v-components.      Parameters     ----------     u : array_like         Wind component in the X (East-West) direction     v : array_like         Wind component in the Y (North-South) direction      Returns     -------     wind speed: array_like         The speed of the wind      See Also     --------     wind_components
r"""Compute the wind direction from u and v-components.      Parameters     ----------     u : array_like         Wind component in the X (East-West) direction     v : array_like         Wind component in the Y (North-South) direction      Returns     -------     direction: `pint.Quantity`         The direction of the wind in interval [0, 360] degrees, specified as the direction from         which it is blowing, with 360 being North.      See Also     --------     wind_components      Notes     -----     In the case of calm winds (where `u` and `v` are zero), this function returns a direction     of 0.
r"""Calculate the U, V wind vector components from the speed and direction.      Parameters     ----------     speed : array_like         The wind speed (magnitude)     wdir : array_like         The wind direction, specified as the direction from which the wind is         blowing (0-2 pi radians or 0-360 degrees), with 360 degrees being North.      Returns     -------     u, v : tuple of array_like         The wind components in the X (East-West) and Y (North-South)         directions, respectively.      See Also     --------     wind_speed     wind_direction      Examples     --------     >>> from metpy.units import units     >>> metpy.calc.wind_components(10. * units('m/s'), 225. * units.deg)     (<Quantity(7.071067811865475, 'meter / second')>,      <Quantity(7.071067811865477, 'meter / second')>)
r"""Calculate the Wind Chill Temperature Index (WCTI).      Calculates WCTI from the current temperature and wind speed using the formula     outlined by the FCM [FCMR192003]_.      Specifically, these formulas assume that wind speed is measured at     10m.  If, instead, the speeds are measured at face level, the winds     need to be multiplied by a factor of 1.5 (this can be done by specifying     `face_level_winds` as `True`.)      Parameters     ----------     temperature : `pint.Quantity`         The air temperature     speed : `pint.Quantity`         The wind speed at 10m.  If instead the winds are at face level,         `face_level_winds` should be set to `True` and the 1.5 multiplicative         correction will be applied automatically.     face_level_winds : bool, optional         A flag indicating whether the wind speeds were measured at facial         level instead of 10m, thus requiring a correction.  Defaults to         `False`.     mask_undefined : bool, optional         A flag indicating whether a masked array should be returned with         values where wind chill is undefined masked.  These are values where         the temperature > 50F or wind speed <= 3 miles per hour. Defaults         to `True`.      Returns     -------     `pint.Quantity`         The corresponding Wind Chill Temperature Index value(s)      See Also     --------     heat_index
r"""Calculate the Heat Index from the current temperature and relative humidity.      The implementation uses the formula outlined in [Rothfusz1990]_. This equation is a     multi-variable least-squares regression of the values obtained in [Steadman1979]_.      Parameters     ----------     temperature : `pint.Quantity`         Air temperature     rh : array_like         The relative humidity expressed as a unitless ratio in the range [0, 1].         Can also pass a percentage if proper units are attached.      Returns     -------     `pint.Quantity`         The corresponding Heat Index value(s)      Other Parameters     ----------------     mask_undefined : bool, optional         A flag indicating whether a masked array should be returned with         values where heat index is undefined masked.  These are values where         the temperature < 80F or relative humidity < 40 percent. Defaults         to `True`.      See Also     --------     windchill
r"""Calculate the current apparent temperature.      Calculates the current apparent temperature based on the wind chill or heat index     as appropriate for the current conditions. Follows [NWS10201]_.      Parameters     ----------     temperature : `pint.Quantity`         The air temperature     rh : `pint.Quantity`         The relative humidity expressed as a unitless ratio in the range [0, 1].         Can also pass a percentage if proper units are attached.     speed : `pint.Quantity`         The wind speed at 10m.  If instead the winds are at face level,         `face_level_winds` should be set to `True` and the 1.5 multiplicative         correction will be applied automatically.     face_level_winds : bool, optional         A flag indicating whether the wind speeds were measured at facial         level instead of 10m, thus requiring a correction.  Defaults to         `False`.      Returns     -------     `pint.Quantity`         The corresponding apparent temperature value(s)      See Also     --------     heat_index, windchill
r"""Convert pressure data to heights using the U.S. standard atmosphere.      The implementation uses the formula outlined in [Hobbs1977]_ pg.60-61.      Parameters     ----------     pressure : `pint.Quantity`         Atmospheric pressure      Returns     -------     `pint.Quantity`         The corresponding height value(s)      Notes     -----     .. math:: Z = \frac{T_0}{\Gamma}[1-\frac{p}{p_0}^\frac{R\Gamma}{g}]
r"""Compute geopotential for a given height.      Parameters     ----------     height : `pint.Quantity`         Height above sea level (array_like)      Returns     -------     `pint.Quantity`         The corresponding geopotential value(s)      Examples     --------     >>> from metpy.constants import g, G, me, Re     >>> import metpy.calc     >>> from metpy.units import units     >>> height = np.linspace(0,10000, num = 11) * units.m     >>> geopot = metpy.calc.height_to_geopotential(height)     >>> geopot     <Quantity([     0.           9817.46806283  19631.85526579  29443.16305888     39251.39289118  49056.54621087  58858.62446525  68657.62910064     78453.56156253  88246.42329545  98036.21574306], 'meter ** 2 / second ** 2')>      Notes     -----     Derived from definition of geopotential in [Hobbs2006]_ pg.14 Eq.1.8.
r"""Compute height from a given geopotential.      Parameters     ----------     geopotential : `pint.Quantity`         Geopotential (array_like)      Returns     -------     `pint.Quantity`         The corresponding height value(s)      Examples     --------     >>> from metpy.constants import g, G, me, Re     >>> import metpy.calc     >>> from metpy.units import units     >>> height = np.linspace(0,10000, num = 11) * units.m     >>> geopot = metpy.calc.height_to_geopotential(height)     >>> geopot     <Quantity([     0.           9817.46806283  19631.85526579  29443.16305888     39251.39289118  49056.54621087  58858.62446525  68657.62910064     78453.56156253  88246.42329545  98036.21574306], 'meter ** 2 / second ** 2')>     >>> height = metpy.calc.geopotential_to_height(geopot)     >>> height     <Quantity([     0.   1000.   2000.   3000.   4000.   5000.   6000.   7000.   8000.     9000.  10000.], 'meter')>      Notes     -----     Derived from definition of geopotential in [Hobbs2006]_ pg.14 Eq.1.8.
r"""Convert height data to pressures using the U.S. standard atmosphere.      The implementation inverts the formula outlined in [Hobbs1977]_ pg.60-61.      Parameters     ----------     height : `pint.Quantity`         Atmospheric height      Returns     -------     `pint.Quantity`         The corresponding pressure value(s)      Notes     -----     .. math:: p = p_0 e^{\frac{g}{R \Gamma} \text{ln}(1-\frac{Z \Gamma}{T_0})}
r"""Calculate the coriolis parameter at each point.      The implementation uses the formula outlined in [Hobbs1977]_ pg.370-371.      Parameters     ----------     latitude : array_like         Latitude at each point      Returns     -------     `pint.Quantity`         The corresponding coriolis force at each point
r"""Calculate pressure from sigma values.      Parameters     ----------     sigma : ndarray         The sigma levels to be converted to pressure levels.      psfc : `pint.Quantity`         The surface pressure value.      ptop : `pint.Quantity`         The pressure value at the top of the model domain.      Returns     -------     `pint.Quantity`         The pressure values at the given sigma levels.      Notes     -----     Sigma definition adapted from [Philips1957]_.      .. math:: p = \sigma * (p_{sfc} - p_{top}) + p_{top}      * :math:`p` is pressure at a given `\sigma` level     * :math:`\sigma` is non-dimensional, scaled pressure     * :math:`p_{sfc}` is pressure at the surface or model floor     * :math:`p_{top}` is pressure at the top of the model domain
Filter with normal distribution of weights.      Parameters     ----------     scalar_grid : `pint.Quantity`         Some n-dimensional scalar grid. If more than two axes, smoothing         is only done across the last two.      n : int         Degree of filtering      Returns     -------     `pint.Quantity`         The filtered 2D scalar grid      Notes     -----     This function is a close replication of the GEMPAK function GWFS,     but is not identical.  The following notes are incorporated from     the GEMPAK source code:      This function smoothes a scalar grid using a moving average     low-pass filter whose weights are determined by the normal     (Gaussian) probability distribution function for two dimensions.     The weight given to any grid point within the area covered by the     moving average for a target grid point is proportional to                      EXP [ -( D ** 2 ) ],      where D is the distance from that point to the target point divided     by the standard deviation of the normal distribution.  The value of     the standard deviation is determined by the degree of filtering     requested.  The degree of filtering is specified by an integer.     This integer is the number of grid increments from crest to crest     of the wave for which the theoretical response is 1/e = .3679.  If     the grid increment is called delta_x, and the value of this integer     is represented by N, then the theoretical filter response function     value for the N * delta_x wave will be 1/e.  The actual response     function will be greater than the theoretical value.      The larger N is, the more severe the filtering will be, because the     response function for all wavelengths shorter than N * delta_x     will be less than 1/e.  Furthermore, as N is increased, the slope     of the filter response function becomes more shallow; so, the     response at all wavelengths decreases, but the amount of decrease     lessens with increasing wavelength.  (The theoretical response     function can be obtained easily--it is the Fourier transform of the     weight function described above.)      The area of the patch covered by the moving average varies with N.     As N gets bigger, the smoothing gets stronger, and weight values     farther from the target grid point are larger because the standard     deviation of the normal distribution is bigger.  Thus, increasing     N has the effect of expanding the moving average window as well as     changing the values of weights.  The patch is a square covering all     points whose weight values are within two standard deviations of the     mean of the two dimensional normal distribution.      The key difference between GEMPAK's GWFS and this function is that,     in GEMPAK, the leftover weight values representing the fringe of the     distribution are applied to the target grid point.  In this     function, the leftover weights are not used.      When this function is invoked, the first argument is the grid to be     smoothed, the second is the value of N as described above:                          GWFS ( S, N )      where N > 1.  If N <= 1, N = 2 is assumed.  For example, if N = 4,     then the 4 delta x wave length is passed with approximate response     1/e.
Filter with normal distribution of weights.      Parameters     ----------     scalar_grid : array-like or `pint.Quantity`         Some 2D scalar grid to be smoothed.      n: int         The number of points to use in smoothing, only valid inputs         are 5 and 9. Defaults to 5.      passes : int         The number of times to apply the filter to the grid. Defaults         to 1.      Returns     -------     array-like or `pint.Quantity`         The filtered 2D scalar grid.      Notes     -----     This function is a close replication of the GEMPAK function SM5S     and SM9S depending on the choice of the number of points to use     for smoothing. This function can be applied multiple times to     create a more smoothed field and will only smooth the interior     points, leaving the end points with their original values. If a     masked value or NaN values exists in the array, it will propagate     to any point that uses that particular grid point in the smoothing     calculation. Applying the smoothing function multiple times will     propogate NaNs further throughout the domain.
Input validation of values that could be in degrees instead of radians.      Parameters     ----------     value : `pint.Quantity`         The input value to check.      max_radians : float         Maximum absolute value of radians before warning.      Returns     -------     `pint.Quantity`         The input value
r"""Remove all x, y, and z where z is less than val.      Will not destroy original values.      Parameters     ----------     x: array_like         x coordinate.     y: array_like         y coordinate.     z: array_like         Observation value.     val: float         Value at which to threshold z.      Returns     -------     x, y, z         List of coordinate observation pairs without         observation values less than val.
r"""Remove all x, y, and z where z is nan.      Will not destroy original values.      Parameters     ----------     x: array_like         x coordinate     y: array_like         y coordinate     z: array_like         observation value      Returns     -------     x, y, z         List of coordinate observation pairs without         nan valued observations.
r"""Remove all x, y, and z where (x,y) is repeated and keep the first occurrence only.      Will not destroy original values.      Parameters     ----------     x: array_like         x coordinate     y: array_like         y coordinate     z: array_like         observation value      Returns     -------     x, y, z         List of coordinate observation pairs without         repeated coordinates.
Set the location of tick in data coords with scalar *loc*.
Control whether the gridline is drawn for this tick.
Set limits and transforms.          This is called once when the plot is created to set up all the         transforms for the data, text and grids.
Decompress all of the bzip2-ed blocks.      Returns the decompressed data as a `bytearray`.
Convert NEXRAD date time format to python `datetime.datetime`.
Convert status integer value to appropriate bitmask.
Replace single item lists in a dictionary with the single item.
Convert a 16-bit floating point value to a standard Python float.
Create a function to parse a datetime from the product-specific blocks.
Create a function to combine two specified product-specific blocks into a single int.
r"""Calculate the relative humidity.      Uses temperature and dewpoint in celsius to calculate relative     humidity using the ratio of vapor pressure to saturation vapor pressures.      Parameters     ----------     temperature : `pint.Quantity`         The temperature     dew point : `pint.Quantity`         The dew point temperature      Returns     -------     `pint.Quantity`         The relative humidity      See Also     --------     saturation_vapor_pressure
r"""Calculate the Exner function.      .. math:: \Pi = \left( \frac{p}{p_0} \right)^\kappa      This can be used to calculate potential temperature from temperature (and visa-versa),     since      .. math:: \Pi = \frac{T}{\theta}      Parameters     ----------     pressure : `pint.Quantity`         The total atmospheric pressure     reference_pressure : `pint.Quantity`, optional         The reference pressure against which to calculate the Exner function, defaults to P0      Returns     -------     `pint.Quantity`         The value of the Exner function at the given pressure      See Also     --------     potential_temperature     temperature_from_potential_temperature
r"""Calculate the temperature at a level assuming only dry processes.      This function lifts a parcel starting at `temperature`, conserving     potential temperature. The starting pressure can be given by `ref_pressure`.      Parameters     ----------     pressure : `pint.Quantity`         The atmospheric pressure level(s) of interest     temperature : `pint.Quantity`         The starting temperature     ref_pressure : `pint.Quantity`, optional         The reference pressure. If not given, it defaults to the first element of the         pressure array.      Returns     -------     `pint.Quantity`        The resulting parcel temperature at levels given by `pressure`      See Also     --------     moist_lapse : Calculate parcel temperature assuming liquid saturation                   processes     parcel_profile : Calculate complete parcel profile     potential_temperature
r"""Calculate the temperature at a level assuming liquid saturation processes.      This function lifts a parcel starting at `temperature`. The starting pressure can     be given by `ref_pressure`. Essentially, this function is calculating moist     pseudo-adiabats.      Parameters     ----------     pressure : `pint.Quantity`         The atmospheric pressure level(s) of interest     temperature : `pint.Quantity`         The starting temperature     ref_pressure : `pint.Quantity`, optional         The reference pressure. If not given, it defaults to the first element of the         pressure array.      Returns     -------     `pint.Quantity`        The temperature corresponding to the starting temperature and        pressure levels.      See Also     --------     dry_lapse : Calculate parcel temperature assuming dry adiabatic processes     parcel_profile : Calculate complete parcel profile      Notes     -----     This function is implemented by integrating the following differential     equation:      .. math:: \frac{dT}{dP} = \frac{1}{P} \frac{R_d T + L_v r_s}                                 {C_{pd} + \frac{L_v^2 r_s \epsilon}{R_d T^2}}      This equation comes from [Bakhshaii2013]_.
r"""Calculate the lifted condensation level (LCL) using from the starting point.      The starting state for the parcel is defined by `temperature`, `dewpt`,     and `pressure`.      Parameters     ----------     pressure : `pint.Quantity`         The starting atmospheric pressure     temperature : `pint.Quantity`         The starting temperature     dewpt : `pint.Quantity`         The starting dew point      Returns     -------     `(pint.Quantity, pint.Quantity)`         The LCL pressure and temperature      Other Parameters     ----------------     max_iters : int, optional         The maximum number of iterations to use in calculation, defaults to 50.     eps : float, optional         The desired relative error in the calculated value, defaults to 1e-5.      See Also     --------     parcel_profile      Notes     -----     This function is implemented using an iterative approach to solve for the     LCL. The basic algorithm is:      1. Find the dew point from the LCL pressure and starting mixing ratio     2. Find the LCL pressure from the starting temperature and dewpoint     3. Iterate until convergence      The function is guaranteed to finish by virtue of the `max_iters` counter.
r"""Calculate the level of free convection (LFC).      This works by finding the first intersection of the ideal parcel path and     the measured parcel temperature.      Parameters     ----------     pressure : `pint.Quantity`         The atmospheric pressure     temperature : `pint.Quantity`         The temperature at the levels given by `pressure`     dewpt : `pint.Quantity`         The dew point at the levels given by `pressure`     parcel_temperature_profile: `pint.Quantity`, optional         The parcel temperature profile from which to calculate the LFC. Defaults to the         surface parcel profile.     dewpt_start: `pint.Quantity`, optional         The dewpoint of the parcel for which to calculate the LFC. Defaults to the surface         dewpoint.      Returns     -------     `pint.Quantity`         The LFC pressure and temperature      See Also     --------     parcel_profile
r"""Calculate the equilibrium level.      This works by finding the last intersection of the ideal parcel path and     the measured environmental temperature. If there is one or fewer intersections, there is     no equilibrium level.      Parameters     ----------     pressure : `pint.Quantity`         The atmospheric pressure     temperature : `pint.Quantity`         The temperature at the levels given by `pressure`     dewpt : `pint.Quantity`         The dew point at the levels given by `pressure`     parcel_temperature_profile: `pint.Quantity`, optional         The parcel temperature profile from which to calculate the EL. Defaults to the         surface parcel profile.      Returns     -------     `pint.Quantity, pint.Quantity`         The EL pressure and temperature      See Also     --------     parcel_profile
r"""Calculate the profile a parcel takes through the atmosphere.      The parcel starts at `temperature`, and `dewpt`, lifted up     dry adiabatically to the LCL, and then moist adiabatically from there.     `pressure` specifies the pressure levels for the profile.      Parameters     ----------     pressure : `pint.Quantity`         The atmospheric pressure level(s) of interest. The first entry should be the starting         point pressure.     temperature : `pint.Quantity`         The starting temperature     dewpt : `pint.Quantity`         The starting dew point      Returns     -------     `pint.Quantity`         The parcel temperatures at the specified pressure levels.      See Also     --------     lcl, moist_lapse, dry_lapse
r"""Calculate the profile a parcel takes through the atmosphere.      The parcel starts at `temperature`, and `dewpt`, lifted up     dry adiabatically to the LCL, and then moist adiabatically from there.     `pressure` specifies the pressure levels for the profile. This function returns     a profile that includes the LCL.      Parameters     ----------     pressure : `pint.Quantity`         The atmospheric pressure level(s) of interest. The first entry should be the starting         point pressure.     temperature : `pint.Quantity`         The atmospheric temperature at the levels in `pressure`. The first entry should be the         starting point temperature.     dewpt : `pint.Quantity`         The atmospheric dew point at the levels in `pressure`. The first entry should be the         starting dew point.      Returns     -------     pressure : `pint.Quantity`         The parcel profile pressures, which includes the specified levels and the LCL     ambient_temperature : `pint.Quantity`         The atmospheric temperature values, including the value interpolated to the LCL level     ambient_dew_point : `pint.Quantity`         The atmospheric dew point values, including the value interpolated to the LCL level     profile_temperature : `pint.Quantity`         The parcel profile temperatures at all of the levels in the returned pressures array,         including the LCL.      See Also     --------     lcl, moist_lapse, dry_lapse, parcel_profile
Help calculate parcel profiles.      Returns the temperature and pressure, above, below, and including the LCL. The     other calculation functions decide what to do with the pieces.
Insert the LCL pressure into the profile.
r"""Calculate the saturation water vapor (partial) pressure.      Parameters     ----------     temperature : `pint.Quantity`         The temperature      Returns     -------     `pint.Quantity`         The saturation water vapor (partial) pressure      See Also     --------     vapor_pressure, dewpoint      Notes     -----     Instead of temperature, dewpoint may be used in order to calculate     the actual (ambient) water vapor (partial) pressure.      The formula used is that from [Bolton1980]_ for T in degrees Celsius:      .. math:: 6.112 e^\frac{17.67T}{T + 243.5}
r"""Calculate the ambient dewpoint given air temperature and relative humidity.      Parameters     ----------     temperature : `pint.Quantity`         Air temperature     rh : `pint.Quantity`         Relative humidity expressed as a ratio in the range 0 < rh <= 1      Returns     -------     `pint.Quantity`         The dew point temperature      See Also     --------     dewpoint, saturation_vapor_pressure
r"""Calculate the ambient dewpoint given the vapor pressure.      Parameters     ----------     e : `pint.Quantity`         Water vapor partial pressure      Returns     -------     `pint.Quantity`         Dew point temperature      See Also     --------     dewpoint_rh, saturation_vapor_pressure, vapor_pressure      Notes     -----     This function inverts the [Bolton1980]_ formula for saturation vapor     pressure to instead calculate the temperature. This yield the following     formula for dewpoint in degrees Celsius:      .. math:: T = \frac{243.5 log(e / 6.112)}{17.67 - log(e / 6.112)}
r"""Calculate the mixing ratio of a gas.      This calculates mixing ratio given its partial pressure and the total pressure of     the air. There are no required units for the input arrays, other than that     they have the same units.      Parameters     ----------     part_press : `pint.Quantity`         Partial pressure of the constituent gas     tot_press : `pint.Quantity`         Total air pressure     molecular_weight_ratio : `pint.Quantity` or float, optional         The ratio of the molecular weight of the constituent gas to that assumed         for air. Defaults to the ratio for water vapor to dry air         (:math:`\epsilon\approx0.622`).      Returns     -------     `pint.Quantity`         The (mass) mixing ratio, dimensionless (e.g. Kg/Kg or g/g)      Notes     -----     This function is a straightforward implementation of the equation given in many places,     such as [Hobbs1977]_ pg.73:      .. math:: r = \epsilon \frac{e}{p - e}      See Also     --------     saturation_mixing_ratio, vapor_pressure
r"""Calculate equivalent potential temperature.      This calculation must be given an air parcel's pressure, temperature, and dewpoint.     The implementation uses the formula outlined in [Bolton1980]_:      First, the LCL temperature is calculated:      .. math:: T_{L}=\frac{1}{\frac{1}{T_{D}-56}+\frac{ln(T_{K}/T_{D})}{800}}+56      Which is then used to calculate the potential temperature at the LCL:      .. math:: \theta_{DL}=T_{K}\left(\frac{1000}{p-e}\right)^k               \left(\frac{T_{K}}{T_{L}}\right)^{.28r}      Both of these are used to calculate the final equivalent potential temperature:      .. math:: \theta_{E}=\theta_{DL}\exp\left[\left(\frac{3036.}{T_{L}}                                               -1.78\right)*r(1+.448r)\right]      Parameters     ----------     pressure: `pint.Quantity`         Total atmospheric pressure     temperature: `pint.Quantity`         Temperature of parcel     dewpoint: `pint.Quantity`         Dewpoint of parcel      Returns     -------     `pint.Quantity`         The equivalent potential temperature of the parcel      Notes     -----     [Bolton1980]_ formula for Theta-e is used, since according to     [DaviesJones2009]_ it is the most accurate non-iterative formulation     available.
r"""Calculate saturation equivalent potential temperature.      This calculation must be given an air parcel's pressure and temperature.     The implementation uses the formula outlined in [Bolton1980]_ for the     equivalent potential temperature, and assumes a saturated process.      First, because we assume a saturated process, the temperature at the LCL is     equivalent to the current temperature. Therefore the following equation      .. math:: T_{L}=\frac{1}{\frac{1}{T_{D}-56}+\frac{ln(T_{K}/T_{D})}{800}}+56      reduces to      .. math:: T_{L} = T_{K}      Then the potential temperature at the temperature/LCL is calculated:      .. math:: \theta_{DL}=T_{K}\left(\frac{1000}{p-e}\right)^k               \left(\frac{T_{K}}{T_{L}}\right)^{.28r}      However, because      .. math:: T_{L} = T_{K}      it follows that      .. math:: \theta_{DL}=T_{K}\left(\frac{1000}{p-e}\right)^k      Both of these are used to calculate the final equivalent potential temperature:      .. math:: \theta_{E}=\theta_{DL}\exp\left[\left(\frac{3036.}{T_{K}}                                               -1.78\right)*r(1+.448r)\right]      Parameters     ----------     pressure: `pint.Quantity`         Total atmospheric pressure     temperature: `pint.Quantity`         Temperature of parcel      Returns     -------     `pint.Quantity`         The saturation equivalent potential temperature of the parcel      Notes     -----     [Bolton1980]_ formula for Theta-e is used (for saturated case), since according to     [DaviesJones2009]_ it is the most accurate non-iterative formulation     available.
r"""Calculate virtual temperature.      This calculation must be given an air parcel's temperature and mixing ratio.     The implementation uses the formula outlined in [Hobbs2006]_ pg.80.      Parameters     ----------     temperature: `pint.Quantity`         The temperature     mixing : `pint.Quantity`         dimensionless mass mixing ratio     molecular_weight_ratio : `pint.Quantity` or float, optional         The ratio of the molecular weight of the constituent gas to that assumed         for air. Defaults to the ratio for water vapor to dry air.         (:math:`\epsilon\approx0.622`).      Returns     -------     `pint.Quantity`         The corresponding virtual temperature of the parcel      Notes     -----     .. math:: T_v = T \frac{\text{w} + \epsilon}{\epsilon\,(1 + \text{w})}
r"""Calculate virtual potential temperature.      This calculation must be given an air parcel's pressure, temperature, and mixing ratio.     The implementation uses the formula outlined in [Markowski2010]_ pg.13.      Parameters     ----------     pressure: `pint.Quantity`         Total atmospheric pressure     temperature: `pint.Quantity`         The temperature     mixing : `pint.Quantity`         dimensionless mass mixing ratio     molecular_weight_ratio : `pint.Quantity` or float, optional         The ratio of the molecular weight of the constituent gas to that assumed         for air. Defaults to the ratio for water vapor to dry air.         (:math:`\epsilon\approx0.622`).      Returns     -------     `pint.Quantity`         The corresponding virtual potential temperature of the parcel      Notes     -----     .. math:: \Theta_v = \Theta \frac{\text{w} + \epsilon}{\epsilon\,(1 + \text{w})}
r"""Calculate density.      This calculation must be given an air parcel's pressure, temperature, and mixing ratio.     The implementation uses the formula outlined in [Hobbs2006]_ pg.67.      Parameters     ----------     temperature: `pint.Quantity`         The temperature     pressure: `pint.Quantity`         Total atmospheric pressure     mixing : `pint.Quantity`         dimensionless mass mixing ratio     molecular_weight_ratio : `pint.Quantity` or float, optional         The ratio of the molecular weight of the constituent gas to that assumed         for air. Defaults to the ratio for water vapor to dry air.         (:math:`\epsilon\approx0.622`).      Returns     -------     `pint.Quantity`         The corresponding density of the parcel      Notes     -----     .. math:: \rho = \frac{p}{R_dT_v}
r"""Calculate the relative humidity with wet bulb and dry bulb temperatures.      This uses a psychrometric relationship as outlined in [WMO8-2014]_, with     coefficients from [Fan1987]_.      Parameters     ----------     dry_bulb_temperature: `pint.Quantity`         Dry bulb temperature     web_bulb_temperature: `pint.Quantity`         Wet bulb temperature     pressure: `pint.Quantity`         Total atmospheric pressure      Returns     -------     `pint.Quantity`         Relative humidity      Notes     -----     .. math:: RH = \frac{e}{e_s}      * :math:`RH` is relative humidity as a unitless ratio     * :math:`e` is vapor pressure from the wet psychrometric calculation     * :math:`e_s` is the saturation vapor pressure      See Also     --------     psychrometric_vapor_pressure_wet, saturation_vapor_pressure
r"""Calculate the vapor pressure with wet bulb and dry bulb temperatures.      This uses a psychrometric relationship as outlined in [WMO8-2014]_, with     coefficients from [Fan1987]_.      Parameters     ----------     dry_bulb_temperature: `pint.Quantity`         Dry bulb temperature     wet_bulb_temperature: `pint.Quantity`         Wet bulb temperature     pressure: `pint.Quantity`         Total atmospheric pressure     psychrometer_coefficient: `pint.Quantity`, optional         Psychrometer coefficient. Defaults to 6.21e-4 K^-1.      Returns     -------     `pint.Quantity`         Vapor pressure      Notes     -----     .. math:: e' = e'_w(T_w) - A p (T - T_w)      * :math:`e'` is vapor pressure     * :math:`e'_w(T_w)` is the saturation vapor pressure with respect to water at temperature       :math:`T_w`     * :math:`p` is the pressure of the wet bulb     * :math:`T` is the temperature of the dry bulb     * :math:`T_w` is the temperature of the wet bulb     * :math:`A` is the psychrometer coefficient      Psychrometer coefficient depends on the specific instrument being used and the ventilation     of the instrument.      See Also     --------     saturation_vapor_pressure
r"""Calculate CAPE and CIN.      Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)     of a given upper air profile and parcel path. CIN is integrated between the surface and     LFC, CAPE is integrated between the LFC and EL (or top of sounding). Intersection points of     the measured temperature profile and parcel profile are linearly interpolated.      Parameters     ----------     pressure : `pint.Quantity`         The atmospheric pressure level(s) of interest. The first entry should be the starting         point pressure.     temperature : `pint.Quantity`         The atmospheric temperature corresponding to pressure.     dewpt : `pint.Quantity`         The atmospheric dew point corresponding to pressure.     parcel_profile : `pint.Quantity`         The temperature profile of the parcel      Returns     -------     `pint.Quantity`         Convective available potential energy (CAPE).     `pint.Quantity`         Convective inhibition (CIN).      Notes     -----     Formula adopted from [Hobbs1977]_.      .. math:: \text{CAPE} = -R_d \int_{LFC}^{EL} (T_{parcel} - T_{env}) d\text{ln}(p)      .. math:: \text{CIN} = -R_d \int_{SFC}^{LFC} (T_{parcel} - T_{env}) d\text{ln}(p)       * :math:`CAPE` Convective available potential energy     * :math:`CIN` Convective inhibition     * :math:`LFC` Pressure of the level of free convection     * :math:`EL` Pressure of the equilibrium level     * :math:`SFC` Level of the surface or beginning of parcel path     * :math:`R_d` Gas constant     * :math:`g` Gravitational acceleration     * :math:`T_{parcel}` Parcel temperature     * :math:`T_{env}` Environment temperature     * :math:`p` Atmospheric pressure      See Also     --------     lfc, el
r"""     Find and interpolate zero crossings.      Estimate the zero crossings of an x,y series and add estimated crossings to series,     returning a sorted array with no duplicate values.      Parameters     ----------     x : `pint.Quantity`         x values of data     y : `pint.Quantity`         y values of data      Returns     -------     x : `pint.Quantity`         x values of data     y : `pint.Quantity`         y values of data
Determine the most unstable parcel in a layer.      Determines the most unstable parcel of air by calculating the equivalent     potential temperature and finding its maximum in the specified layer.      Parameters     ----------     pressure: `pint.Quantity`         Atmospheric pressure profile     temperature: `pint.Quantity`         Atmospheric temperature profile     dewpoint: `pint.Quantity`         Atmospheric dewpoint profile     heights: `pint.Quantity`, optional         Atmospheric height profile. Standard atmosphere assumed when None (the default).     bottom: `pint.Quantity`, optional         Bottom of the layer to consider for the calculation in pressure or height.         Defaults to using the bottom pressure or height.     depth: `pint.Quantity`, optional         Depth of the layer to consider for the calculation in pressure or height. Defaults         to 300 hPa.      Returns     -------     `pint.Quantity`         Pressure, temperature, and dew point of most unstable parcel in the profile.     integer         Index of the most unstable parcel in the given profile      See Also     --------     get_layer
r"""Interpolate data in isobaric coordinates to isentropic coordinates.      Parameters     ----------     theta_levels : array         One-dimensional array of desired theta surfaces     pressure : array         One-dimensional array of pressure levels     temperature : array         Array of temperature     args : array, optional         Any additional variables will be interpolated to each isentropic level.      Returns     -------     list         List with pressure at each isentropic level, followed by each additional         argument interpolated to isentropic coordinates.      Other Parameters     ----------------     axis : int, optional         The axis corresponding to the vertical in the temperature array, defaults to 0.     tmpk_out : bool, optional         If true, will calculate temperature and output as the last item in the output list.         Defaults to False.     max_iters : int, optional         The maximum number of iterations to use in calculation, defaults to 50.     eps : float, optional         The desired absolute error in the calculated value, defaults to 1e-6.     bottom_up_search : bool, optional         Controls whether to search for theta levels bottom-up, or top-down. Defaults to         True, which is bottom-up search.      Notes     -----     Input variable arrays must have the same number of vertical levels as the pressure levels     array. Pressure is calculated on isentropic surfaces by assuming that temperature varies     linearly with the natural log of pressure. Linear interpolation is then used in the     vertical to find the pressure at each isentropic level. Interpolation method from     [Ziv1994]_. Any additional arguments are assumed to vary linearly with temperature and will     be linearly interpolated to the new isentropic levels.      See Also     --------     potential_temperature
r"""Calculate surface-based CAPE and CIN.      Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)     of a given upper air profile for a surface-based parcel. CIN is integrated     between the surface and LFC, CAPE is integrated between the LFC and EL (or top of     sounding). Intersection points of the measured temperature profile and parcel profile are     linearly interpolated.      Parameters     ----------     pressure : `pint.Quantity`         Atmospheric pressure profile. The first entry should be the starting         (surface) observation.     temperature : `pint.Quantity`         Temperature profile     dewpoint : `pint.Quantity`         Dewpoint profile      Returns     -------     `pint.Quantity`         Surface based Convective Available Potential Energy (CAPE).     `pint.Quantity`         Surface based Convective INhibition (CIN).      See Also     --------     cape_cin, parcel_profile
r"""Calculate most unstable CAPE/CIN.      Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)     of a given upper air profile and most unstable parcel path. CIN is integrated between the     surface and LFC, CAPE is integrated between the LFC and EL (or top of sounding).     Intersection points of the measured temperature profile and parcel profile are linearly     interpolated.      Parameters     ----------     pressure : `pint.Quantity`         Pressure profile     temperature : `pint.Quantity`         Temperature profile     dewpoint : `pint.Quantity`         Dewpoint profile      Returns     -------     `pint.Quantity`         Most unstable Convective Available Potential Energy (CAPE).     `pint.Quantity`         Most unstable Convective INhibition (CIN).      See Also     --------     cape_cin, most_unstable_parcel, parcel_profile
r"""Calculate the properties of a parcel mixed from a layer.      Determines the properties of an air parcel that is the result of complete mixing of a     given atmospheric layer.      Parameters     ----------     p : `pint.Quantity`         Atmospheric pressure profile     temperature : `pint.Quantity`         Atmospheric temperature profile     dewpt : `pint.Quantity`         Atmospheric dewpoint profile     parcel_start_pressure : `pint.Quantity`, optional         Pressure at which the mixed parcel should begin (default None)     heights: `pint.Quantity`, optional         Atmospheric heights corresponding to the given pressures (default None)     bottom : `pint.Quantity`, optional         The bottom of the layer as a pressure or height above the surface pressure         (default None)     depth : `pint.Quantity`, optional         The thickness of the layer as a pressure or height above the bottom of the layer         (default 100 hPa)     interpolate : bool, optional         Interpolate the top and bottom points if they are not in the given data      Returns     -------     `pint.Quantity, pint.Quantity, pint.Quantity`         The pressure, temperature, and dewpoint of the mixed parcel.
r"""Mix variable(s) over a layer, yielding a mass-weighted average.      This function will integrate a data variable with respect to pressure and determine the     average value using the mean value theorem.      Parameters     ----------     p : array-like         Atmospheric pressure profile     datavar : array-like         Atmospheric variable measured at the given pressures     heights: array-like, optional         Atmospheric heights corresponding to the given pressures (default None)     bottom : `pint.Quantity`, optional         The bottom of the layer as a pressure or height above the surface pressure         (default None)     depth : `pint.Quantity`, optional         The thickness of the layer as a pressure or height above the bottom of the layer         (default 100 hPa)     interpolate : bool, optional         Interpolate the top and bottom points if they are not in the given data      Returns     -------     `pint.Quantity`         The mixed value of the data variable.
r"""Calculate the dry static energy of parcels.      This function will calculate the dry static energy following the first two terms of     equation 3.72 in [Hobbs2006]_.      Notes     -----     .. math::\text{dry static energy} = c_{pd} * T + gz      * :math:`T` is temperature     * :math:`z` is height      Parameters     ----------     heights : array-like         Atmospheric height     temperature : array-like         Atmospheric temperature      Returns     -------     `pint.Quantity`         The dry static energy
r"""Calculate the moist static energy of parcels.      This function will calculate the moist static energy following     equation 3.72 in [Hobbs2006]_.     Notes     -----     .. math::\text{moist static energy} = c_{pd} * T + gz + L_v q      * :math:`T` is temperature     * :math:`z` is height     * :math:`q` is specific humidity      Parameters     ----------     heights : array-like         Atmospheric height     temperature : array-like         Atmospheric temperature     specific_humidity : array-like         Atmospheric specific humidity      Returns     -------     `pint.Quantity`         The moist static energy
r"""Calculate the thickness of a layer via the hypsometric equation.      This thickness calculation uses the pressure and temperature profiles (and optionally     mixing ratio) via the hypsometric equation with virtual temperature adjustment      .. math:: Z_2 - Z_1 = -\frac{R_d}{g} \int_{p_1}^{p_2} T_v d\ln p,      which is based off of Equation 3.24 in [Hobbs2006]_.      This assumes a hydrostatic atmosphere.      Layer bottom and depth specified in pressure.      Parameters     ----------     pressure : `pint.Quantity`         Atmospheric pressure profile     temperature : `pint.Quantity`         Atmospheric temperature profile     mixing : `pint.Quantity`, optional         Profile of dimensionless mass mixing ratio. If none is given, virtual temperature         is simply set to be the given temperature.     molecular_weight_ratio : `pint.Quantity` or float, optional         The ratio of the molecular weight of the constituent gas to that assumed         for air. Defaults to the ratio for water vapor to dry air.         (:math:`\epsilon\approx0.622`).     bottom : `pint.Quantity`, optional         The bottom of the layer in pressure. Defaults to the first observation.     depth : `pint.Quantity`, optional         The depth of the layer in hPa. Defaults to the full profile if bottom is not given,         and 100 hPa if bottom is given.      Returns     -------     `pint.Quantity`         The thickness of the layer in meters.      See Also     --------     thickness_hydrostatic_from_relative_humidity, pressure_to_height_std, virtual_temperature
r"""Calculate the thickness of a layer given pressure, temperature and relative humidity.      Similar to ``thickness_hydrostatic``, this thickness calculation uses the pressure,     temperature, and relative humidity profiles via the hypsometric equation with virtual     temperature adjustment.      .. math:: Z_2 - Z_1 = -\frac{R_d}{g} \int_{p_1}^{p_2} T_v d\ln p,      which is based off of Equation 3.24 in [Hobbs2006]_. Virtual temperature is calculated     from the profiles of temperature and relative humidity.      This assumes a hydrostatic atmosphere.      Layer bottom and depth specified in pressure.      Parameters     ----------     pressure : `pint.Quantity`         Atmospheric pressure profile     temperature : `pint.Quantity`         Atmospheric temperature profile     relative_humidity : `pint.Quantity`         Atmospheric relative humidity profile. The relative humidity is expressed as a         unitless ratio in the range [0, 1]. Can also pass a percentage if proper units are         attached.     bottom : `pint.Quantity`, optional         The bottom of the layer in pressure. Defaults to the first observation.     depth : `pint.Quantity`, optional         The depth of the layer in hPa. Defaults to the full profile if bottom is not given,         and 100 hPa if bottom is given.      Returns     -------     `pint.Quantity`         The thickness of the layer in meters.      See Also     --------     thickness_hydrostatic, pressure_to_height_std, virtual_temperature,     mixing_ratio_from_relative_humidity
r"""Calculate the square of the Brunt-Vaisala frequency.      Brunt-Vaisala frequency squared (a measure of atmospheric stability) is given by the     formula:      .. math:: N^2 = \frac{g}{\theta} \frac{d\theta}{dz}      This formula is based off of Equations 3.75 and 3.77 in [Hobbs2006]_.      Parameters     ----------     heights : array-like         One-dimensional profile of atmospheric height     potential_temperature : array-like         Atmospheric potential temperature     axis : int, optional         The axis corresponding to vertical in the potential temperature array, defaults to 0.      Returns     -------     array-like         The square of the Brunt-Vaisala frequency.      See Also     --------     brunt_vaisala_frequency, brunt_vaisala_period, potential_temperature
r"""Calculate the Brunt-Vaisala frequency.      This function will calculate the Brunt-Vaisala frequency as follows:      .. math:: N = \left( \frac{g}{\theta} \frac{d\theta}{dz} \right)^\frac{1}{2}      This formula based off of Equations 3.75 and 3.77 in [Hobbs2006]_.      This function is a wrapper for `brunt_vaisala_frequency_squared` that filters out negative     (unstable) quanties and takes the square root.      Parameters     ----------     heights : array-like         One-dimensional profile of atmospheric height     potential_temperature : array-like         Atmospheric potential temperature     axis : int, optional         The axis corresponding to vertical in the potential temperature array, defaults to 0.      Returns     -------     array-like         Brunt-Vaisala frequency.      See Also     --------     brunt_vaisala_frequency_squared, brunt_vaisala_period, potential_temperature
r"""Calculate the Brunt-Vaisala period.      This function is a helper function for `brunt_vaisala_frequency` that calculates the     period of oscilation as in Exercise 3.13 of [Hobbs2006]_:      .. math:: \tau = \frac{2\pi}{N}      Returns `NaN` when :math:`N^2 > 0`.      Parameters     ----------     heights : array-like         One-dimensional profile of atmospheric height     potential_temperature : array-like         Atmospheric potential temperature     axis : int, optional         The axis corresponding to vertical in the potential temperature array, defaults to 0.      Returns     -------     array-like         Brunt-Vaisala period.      See Also     --------     brunt_vaisala_frequency, brunt_vaisala_frequency_squared, potential_temperature
Calculate the wet-bulb temperature using Normand's rule.      This function calculates the wet-bulb temperature using the Normand method. The LCL is     computed, and that parcel brought down to the starting pressure along a moist adiabat.     The Normand method (and others) are described and compared by [Knox2017]_.      Parameters     ----------     pressure : `pint.Quantity`         Initial atmospheric pressure     temperature : `pint.Quantity`         Initial atmospheric temperature     dewpoint : `pint.Quantity`         Initial atmospheric dewpoint      Returns     -------     array-like         Wet-bulb temperature      See Also     --------     lcl, moist_lapse
r"""Calculate the static stability within a vertical profile.      .. math:: \sigma = -\frac{RT}{p} \frac{\partial \ln \theta}{\partial p}      This formuala is based on equation 4.3.6 in [Bluestein1992]_.      Parameters     ----------     pressure : array-like         Profile of atmospheric pressure     temperature : array-like         Profile of temperature     axis : int, optional         The axis corresponding to vertical in the pressure and temperature arrays, defaults         to 0.      Returns     -------     array-like         The profile of static stability.
r"""Calculate the dewpoint from specific humidity, temperature, and pressure.      Parameters     ----------     specific_humidity: `pint.Quantity`         Specific humidity of air     temperature: `pint.Quantity`         Air temperature     pressure: `pint.Quantity`         Total atmospheric pressure      Returns     -------     `pint.Quantity`         Dewpoint temperature      See Also     --------     relative_humidity_from_mixing_ratio, dewpoint_rh
r"""Calculate omega from w assuming hydrostatic conditions.      This function converts vertical velocity with respect to height     :math:`\left(w = \frac{Dz}{Dt}\right)` to that     with respect to pressure :math:`\left(\omega = \frac{Dp}{Dt}\right)`     assuming hydrostatic conditions on the synoptic scale.     By Equation 7.33 in [Hobbs2006]_,      .. math: \omega \simeq -\rho g w      Density (:math:`\rho`) is calculated using the :func:`density` function,     from the given pressure and temperature. If `mixing` is given, the virtual     temperature correction is used, otherwise, dry air is assumed.      Parameters     ----------     w: `pint.Quantity`         Vertical velocity in terms of height     pressure: `pint.Quantity`         Total atmospheric pressure     temperature: `pint.Quantity`         Air temperature     mixing: `pint.Quantity`, optional         Mixing ratio of air      Returns     -------     `pint.Quantity`         Vertical velocity in terms of pressure (in Pascals / second)      See Also     --------     density, vertical_velocity
r"""Calculate w from omega assuming hydrostatic conditions.      This function converts vertical velocity with respect to pressure     :math:`\left(\omega = \frac{Dp}{Dt}\right)` to that with respect to height     :math:`\left(w = \frac{Dz}{Dt}\right)` assuming hydrostatic conditions on     the synoptic scale. By Equation 7.33 in [Hobbs2006]_,      .. math: \omega \simeq -\rho g w      so that      .. math w \simeq \frac{- \omega}{\rho g}      Density (:math:`\rho`) is calculated using the :func:`density` function,     from the given pressure and temperature. If `mixing` is given, the virtual     temperature correction is used, otherwise, dry air is assumed.      Parameters     ----------     omega: `pint.Quantity`         Vertical velocity in terms of pressure     pressure: `pint.Quantity`         Total atmospheric pressure     temperature: `pint.Quantity`         Air temperature     mixing: `pint.Quantity`, optional         Mixing ratio of air      Returns     -------     `pint.Quantity`         Vertical velocity in terms of height (in meters / second)      See Also     --------     density, vertical_velocity_pressure
Return an iterator of (shapely) geometries for this feature.
Required input:             ws: Wind speeds (knots)             wd: Wind direction (degrees)             wsmax: Wind gust (knots)         Optional Input:             plot_range: Data range for making figure (list of (min,max,step))
Required input:             T: Temperature (deg F)             TD: Dewpoint (deg F)         Optional Input:             plot_range: Data range for making figure (list of (min,max,step))
Required input:             RH: Relative humidity (%)         Optional Input:             plot_range: Data range for making figure (list of (min,max,step))
Required input:             P: Mean Sea Level Pressure (hPa)         Optional Input:             plot_range: Data range for making figure (list of (min,max,step))
Handle geostationary projection.
Handle Lambert conformal conic projection.
Handle Mercator projection.
Handle generic stereographic projection.
Handle mapping a dictionary of metadata to keyword arguments.
Map one set of keys to another.
Initialize a `cartopy.crs.Globe` from the metadata.
Convert to a CartoPy projection.
Add a timestamp to a plot.      Adds a timestamp to a plot, defaulting to the time of plot creation in ISO format.      Parameters     ----------     ax : `matplotlib.axes.Axes`         The `Axes` instance used for plotting     time : `datetime.datetime`         Specific time to be plotted - datetime.utcnow will be use if not specified     x : float         Relative x position on the axes of the timestamp     y : float         Relative y position on the axes of the timestamp     ha : str         Horizontal alignment of the time stamp string     high_contrast : bool         Outline text for increased contrast     pretext : str         Text to appear before the timestamp, optional. Defaults to 'Created: '     time_format : str         Display format of time, optional. Defaults to ISO format.      Returns     -------     `matplotlib.text.Text`         The `matplotlib.text.Text` instance created
Add the MetPy or Unidata logo to a figure.      Adds an image to the figure.      Parameters     ----------     fig : `matplotlib.figure`        The `figure` instance used for plotting     x : int        x position padding in pixels     y : float        y position padding in pixels     zorder : int        The zorder of the logo     which : str        Which logo to plot 'metpy' or 'unidata'     size : str        Size of logo to be used. Can be 'small' for 75 px square or 'large' for        150 px square.      Returns     -------     `matplotlib.image.FigureImage`        The `matplotlib.image.FigureImage` instance created
Add the MetPy logo to a figure.      Adds an image of the MetPy logo to the figure.      Parameters     ----------     fig : `matplotlib.figure`        The `figure` instance used for plotting     x : int        x position padding in pixels     y : float        y position padding in pixels     zorder : int        The zorder of the logo     size : str        Size of logo to be used. Can be 'small' for 75 px square or 'large' for        150 px square.      Returns     -------     `matplotlib.image.FigureImage`        The `matplotlib.image.FigureImage` instance created
Create a multi-colored line.      Takes a set of points and turns them into a collection of lines colored by another array.      Parameters     ----------     x : array-like         x-axis coordinates     y : array-like         y-axis coordinates     c : array-like         values used for color-mapping     kwargs : dict         Other keyword arguments passed to :class:`matplotlib.collections.LineCollection`      Returns     -------         The created :class:`matplotlib.collections.LineCollection` instance.
Convert GEMPAK color numbers into corresponding Matplotlib colors.      Takes a sequence of GEMPAK color numbers and turns them into     equivalent Matplotlib colors. Various GEMPAK quirks are respected,     such as treating negative values as equivalent to 0.      Parameters     ----------     c : int or sequence of ints         GEMPAK color number(s)     style : str, optional         The GEMPAK 'device' to use to interpret color numbers. May be 'psc'         (the default; best for a white background) or 'xw' (best for a black background).      Returns     -------         List of strings of Matplotlib colors, or a single string if only one color requested.
Handle information for message type 3.
Handle information for message type 18.
Fix up creating the appropriate struct type based on the information in the column.
Clean up and apply standard formatting to variable names.
Clean up description column.
Write out the generated Python code.
Attach units to data in pandas dataframes and return united arrays.      Parameters     ----------     df : `pandas.DataFrame`         Data in pandas dataframe.      column_units : dict         Dictionary of units to attach to columns of the dataframe. Overrides         the units attribute if it is attached to the dataframe.      Returns     -------         Dictionary containing united arrays with keys corresponding to the dataframe         column names.
r"""Concatenate multiple values into a new unitized object.      This is essentially a unit-aware version of `numpy.concatenate`. All items     must be able to be converted to the same units. If an item has no units, it will be given     those of the rest of the collection, without conversion. The first units found in the     arguments is used as the final output units.      Parameters     ----------     arrs : Sequence of arrays         The items to be joined together      axis : integer, optional         The array axis along which to join the arrays. Defaults to 0 (the first dimension)      Returns     -------     `pint.Quantity`         New container with the value passed in and units corresponding to the first item.
Calculate the n-th discrete difference along given axis.      Wraps :func:`numpy.diff` to handle units.      Parameters     ----------     x : array-like         Input data     n : int, optional         The number of times values are differenced.     axis : int, optional         The axis along which the difference is taken, default is the last axis.      Returns     -------     diff : ndarray         The n-th differences. The shape of the output is the same as `a`         except along `axis` where the dimension is smaller by `n`. The         type of the output is the same as that of the input.      See Also     --------     numpy.diff
r"""Convert inputs to arrays with at least one dimension.      Scalars are converted to 1-dimensional arrays, whilst other     higher-dimensional inputs are preserved. This is a thin wrapper     around `numpy.atleast_1d` to preserve units.      Parameters     ----------     arrs : arbitrary positional arguments         Input arrays to be converted if necessary      Returns     -------     `pint.Quantity`         A single quantity or a list of quantities, matching the number of inputs.
Create a :class:`numpy.ma.MaskedArray` with units attached.      This is a thin wrapper around :func:`numpy.ma.masked_array` that ensures that     units are properly attached to the result (otherwise units are silently lost). Units     are taken from the ``units`` argument, or if this is ``None``, the units on ``data``     are used.      Parameters     ----------     data : array_like         The source data. If ``units`` is `None`, this should be a `pint.Quantity` with         the desired units.     data_units : str or `pint.Unit`         The units for the resulting `pint.Quantity`     **kwargs : Arbitrary keyword arguments passed to `numpy.ma.masked_array`      Returns     -------     `pint.Quantity`
Yield arguments with improper dimensionality.
Create a decorator to check units of function arguments.
r"""Obtain an interpolated slice through data using xarray.      Utilizing the interpolation functionality in `xarray`, this function takes a slice the     given data (currently only regular grids are supported), which is given as an     `xarray.DataArray` so that we can utilize its coordinate metadata.      Parameters     ----------     data: `xarray.DataArray` or `xarray.Dataset`         Three- (or higher) dimensional field(s) to interpolate. The DataArray (or each         DataArray in the Dataset) must have been parsed by MetPy and include both an x and         y coordinate dimension.     points: (N, 2) array_like         A list of x, y points in the data projection at which to interpolate the data     interp_type: str, optional         The interpolation method, either 'linear' or 'nearest' (see         `xarray.DataArray.interp()` for details). Defaults to 'linear'.      Returns     -------     `xarray.DataArray` or `xarray.Dataset`         The interpolated slice of data, with new index dimension of size N.      See Also     --------     cross_section
r"""Construct a geodesic path between two points.      This function acts as a wrapper for the geodesic construction available in `pyproj`.      Parameters     ----------     crs: `cartopy.crs`         Cartopy Coordinate Reference System to use for the output     start: (2, ) array_like         A latitude-longitude pair designating the start point of the geodesic (units are         degrees north and degrees east).     end: (2, ) array_like         A latitude-longitude pair designating the end point of the geodesic (units are degrees         north and degrees east).     steps: int, optional         The number of points along the geodesic between the start and the end point         (including the end points).      Returns     -------     `numpy.ndarray`         The list of x, y points in the given CRS of length `steps` along the geodesic.      See Also     --------     cross_section
r"""Obtain an interpolated cross-sectional slice through gridded data.      Utilizing the interpolation functionality in `xarray`, this function takes a vertical     cross-sectional slice along a geodesic through the given data on a regular grid, which is     given as an `xarray.DataArray` so that we can utilize its coordinate and projection     metadata.      Parameters     ----------     data: `xarray.DataArray` or `xarray.Dataset`         Three- (or higher) dimensional field(s) to interpolate. The DataArray (or each         DataArray in the Dataset) must have been parsed by MetPy and include both an x and         y coordinate dimension and the added `crs` coordinate.     start: (2, ) array_like         A latitude-longitude pair designating the start point of the cross section (units are         degrees north and degrees east).     end: (2, ) array_like         A latitude-longitude pair designating the end point of the cross section (units are         degrees north and degrees east).     steps: int, optional         The number of points along the geodesic between the start and the end point         (including the end points) to use in the cross section. Defaults to 100.     interp_type: str, optional         The interpolation method, either 'linear' or 'nearest' (see         `xarray.DataArray.interp()` for details). Defaults to 'linear'.      Returns     -------     `xarray.DataArray` or `xarray.Dataset`         The interpolated cross section, with new index dimension along the cross-section.      See Also     --------     interpolate_to_slice, geodesic
Decorate a function to convert all DataArray arguments to pint.Quantities.      This uses the metpy xarray accessors to do the actual conversion.
Decorate a function to make sure all given DataArrays have matching coordinates.
Reassign a units.Quantity indexer to units of relevant coordinate.
Return one-dimensional nearest-neighbor indexes based on user-specified centers.      Parameters     ----------     a : array-like         1-dimensional array of numeric values from which to         extract indexes of nearest-neighbors     centers : array-like         1-dimensional array of numeric values representing a subset of values to approximate      Returns     -------         An array of indexes representing values closest to given array values
Determine the index of the point just before two lines with common x values.      Parameters     ----------     a : array-like         1-dimensional array of y-values for line 1     b : array-like         1-dimensional array of y-values for line 2      Returns     -------         An array of indexes representing the index of the values         just before the intersection(s) of the two lines.
Calculate the best estimate of intersection.      Calculates the best estimates of the intersection of two y-value     data sets that share a common x-value set.      Parameters     ----------     x : array-like         1-dimensional array of numeric x-values     a : array-like         1-dimensional array of y-values for line 1     b : array-like         1-dimensional array of y-values for line 2     direction : string, optional         specifies direction of crossing. 'all', 'increasing' (a becoming greater than b),         or 'decreasing' (b becoming greater than a). Defaults to 'all'.      Returns     -------         A tuple (x, y) of array-like with the x and y coordinates of the         intersections of the lines.
Return the next non masked element of a masked array.      If an array is masked, return the next non-masked element (if the given index is masked).     If no other unmasked points are after the given masked point, returns none.      Parameters     ----------     a : array-like         1-dimensional array of numeric values     idx : integer         index of requested element      Returns     -------         Index of next non-masked element and next non-masked element
Delete masked points from arrays.      Takes arrays and removes masked points to help with calculations and plotting.      Parameters     ----------     arrs : one or more array-like         source arrays      Returns     -------     arrs : one or more array-like         arrays with masked elements removed
r"""Return a mask to reduce the density of points in irregularly-spaced data.      This function is used to down-sample a collection of scattered points (e.g. surface     data), returning a mask that can be used to select the points from one or more arrays     (e.g. arrays of temperature and dew point). The points selected can be controlled by     providing an array of ``priority`` values (e.g. rainfall totals to ensure that     stations with higher precipitation remain in the mask).      Parameters     ----------     points : (N, K) array-like         N locations of the points in K dimensional space     radius : float         minimum radius allowed between points     priority : (N, K) array-like, optional         If given, this should have the same shape as ``points``; these values will         be used to control selection priority for points.      Returns     -------         (N,) array-like of boolean values indicating whether points should be kept. This         can be used directly to index numpy arrays to return only the desired points.      Examples     --------     >>> metpy.calc.reduce_point_density(np.array([1, 2, 3]), 1.)     array([ True, False,  True])     >>> metpy.calc.reduce_point_density(np.array([1, 2, 3]), 1.,     ... priority=np.array([0.1, 0.9, 0.3]))     array([False,  True, False])
Calculate the bounding pressure and height in a layer.      Given pressure, optional heights, and a bound, return either the closest pressure/height     or interpolated pressure/height. If no heights are provided, a standard atmosphere is     assumed.      Parameters     ----------     pressure : `pint.Quantity`         Atmospheric pressures     bound : `pint.Quantity`         Bound to retrieve (in pressure or height)     heights : `pint.Quantity`, optional         Atmospheric heights associated with the pressure levels. Defaults to using         heights calculated from ``pressure`` assuming a standard atmosphere.     interpolate : boolean, optional         Interpolate the bound or return the nearest. Defaults to True.      Returns     -------     `pint.Quantity`         The bound pressure and height.
Return an atmospheric layer from upper air data with the requested bottom and depth.      This function will subset an upper air dataset to contain only the specified layer using     the heights only.      Parameters     ----------     heights : array-like         Atmospheric heights     depth : `pint.Quantity`         The thickness of the layer     *args : array-like         Atmospheric variable(s) measured at the given pressures     bottom : `pint.Quantity`, optional         The bottom of the layer     interpolate : bool, optional         Interpolate the top and bottom points if they are not in the given data. Defaults         to True.     with_agl : bool, optional         Returns the heights as above ground level by subtracting the minimum height in the         provided heights. Defaults to False.      Returns     -------     `pint.Quantity, pint.Quantity`         The height and data variables of the layer
r"""Return an atmospheric layer from upper air data with the requested bottom and depth.      This function will subset an upper air dataset to contain only the specified layer. The     bottom of the layer can be specified with a pressure or height above the surface     pressure. The bottom defaults to the surface pressure. The depth of the layer can be     specified in terms of pressure or height above the bottom of the layer. If the top and     bottom of the layer are not in the data, they are interpolated by default.      Parameters     ----------     pressure : array-like         Atmospheric pressure profile     *args : array-like         Atmospheric variable(s) measured at the given pressures     heights: array-like, optional         Atmospheric heights corresponding to the given pressures. Defaults to using         heights calculated from ``p`` assuming a standard atmosphere.     bottom : `pint.Quantity`, optional         The bottom of the layer as a pressure or height above the surface pressure. Defaults         to the highest pressure or lowest height given.     depth : `pint.Quantity`, optional         The thickness of the layer as a pressure or height above the bottom of the layer.         Defaults to 100 hPa.     interpolate : bool, optional         Interpolate the top and bottom points if they are not in the given data. Defaults         to True.      Returns     -------     `pint.Quantity, pint.Quantity`         The pressure and data variables of the layer
Wrap interpolate_1d for deprecated interp.
Find the indices surrounding the values within arr along axis.      Returns a set of above, below, good. Above and below are lists of arrays of indices.     These lists are formulated such that they can be used directly to index into a numpy     array and get the expected results (no extra slices or ellipsis necessary). `good` is     a boolean array indicating the "columns" that actually had values to bound the desired     value(s).      Parameters     ----------     arr : array-like         Array to search for values      values: array-like         One or more values to search for in `arr`      axis : int         The dimension of `arr` along which to search.      from_below : bool, optional         Whether to search from "below" (i.e. low indices to high indices). If `False`,         the search will instead proceed from high indices to low indices. Defaults to `True`.      Returns     -------     above : list of arrays         List of broadcasted indices to the location above the desired value      below : list of arrays         List of broadcasted indices to the location below the desired value      good : array         Boolean array indicating where the search found proper bounds for the desired value
Wrap log_interpolate_1d for deprecated log_interp.
r"""Compare values for greater or close to boolean masks.      Returns a boolean mask for values greater than or equal to a target within a specified     absolute or relative tolerance (as in :func:`numpy.isclose`).      Parameters     ----------     a : array-like         Array of values to be compared     value : float         Comparison value      Returns     -------     array-like         Boolean array where values are greater than or nearly equal to value.
r"""Compare values for less or close to boolean masks.      Returns a boolean mask for values less than or equal to a target within a specified     absolute or relative tolerance (as in :func:`numpy.isclose`).      Parameters     ----------     a : array-like         Array of values to be compared     value : float         Comparison value      Returns     -------     array-like         Boolean array where values are less than or nearly equal to value.
r"""Calculate the distance between grid points that are in a latitude/longitude format.      Calculate the distance between grid points when the grid spacing is defined by     delta lat/lon rather than delta x/y      Parameters     ----------     longitude : array_like         array of longitudes defining the grid     latitude : array_like         array of latitudes defining the grid     kwargs         Other keyword arguments to pass to :class:`~pyproj.Geod`      Returns     -------      dx, dy: 2D arrays of distances between grid points in the x and y direction      Notes     -----     Accepts, 1D or 2D arrays for latitude and longitude     Assumes [Y, X] for 2D arrays      .. deprecated:: 0.8.0         Function has been replaced with the signed delta distance calculation         `lat_lon_grid_deltas` and will be removed from MetPy in 0.11.0.
r"""Calculate the delta between grid points that are in a latitude/longitude format.      Calculate the signed delta distance between grid points when the grid spacing is defined by     delta lat/lon rather than delta x/y      Parameters     ----------     longitude : array_like         array of longitudes defining the grid     latitude : array_like         array of latitudes defining the grid     kwargs         Other keyword arguments to pass to :class:`~pyproj.Geod`      Returns     -------     dx, dy:         at least two dimensional arrays of signed deltas between grid points in the x and y         direction      Notes     -----     Accepts 1D, 2D, or higher arrays for latitude and longitude     Assumes [..., Y, X] for >=2 dimensional arrays
Calculate the horizontal deltas between grid points of a DataArray.      Calculate the signed delta distance between grid points of a DataArray in the horizontal     directions, whether the grid is lat/lon or x/y.      Parameters     ----------     f : `xarray.DataArray`         Parsed DataArray on a latitude/longitude grid, in (..., lat, lon) or (..., y, x)         dimension order      Returns     -------     dx, dy:         arrays of signed deltas between grid points in the x and y directions with dimensions         matching those of `f`.      See Also     --------     lat_lon_grid_deltas
Decorate the derivative functions to make them work nicely with DataArrays.      This will automatically determine if the coordinates can be pulled directly from the     DataArray, or if a call to lat_lon_grid_deltas is needed.
Calculate the first derivative of a grid of values.      Works for both regularly-spaced data and grids with varying spacing.      Either `x` or `delta` must be specified, or `f` must be given as an `xarray.DataArray` with     attached coordinate and projection information. If `f` is an `xarray.DataArray`, and `x` or     `delta` are given, `f` will be converted to a `pint.Quantity` and the derivative returned     as a `pint.Quantity`, otherwise, if neither `x` nor `delta` are given, the attached     coordinate information belonging to `axis` will be used and the derivative will be returned     as an `xarray.DataArray`.      This uses 3 points to calculate the derivative, using forward or backward at the edges of     the grid as appropriate, and centered elsewhere. The irregular spacing is handled     explicitly, using the formulation as specified by [Bowen2005]_.      Parameters     ----------     f : array-like         Array of values of which to calculate the derivative     axis : int or str, optional         The array axis along which to take the derivative. If `f` is ndarray-like, must be an         integer. If `f` is a `DataArray`, can be a string (referring to either the coordinate         dimension name or the axis type) or integer (referring to axis number), unless using         implicit conversion to `pint.Quantity`, in which case it must be an integer. Defaults         to 0.     x : array-like, optional         The coordinate values corresponding to the grid points in `f`.     delta : array-like, optional         Spacing between the grid points in `f`. Should be one item less than the size         of `f` along `axis`.      Returns     -------     array-like         The first derivative calculated along the selected axis.      See Also     --------     second_derivative
Calculate the gradient of a grid of values.      Works for both regularly-spaced data, and grids with varying spacing.      Either `coordinates` or `deltas` must be specified, or `f` must be given as an     `xarray.DataArray` with  attached coordinate and projection information. If `f` is an     `xarray.DataArray`, and `coordinates` or `deltas` are given, `f` will be converted to a     `pint.Quantity` and the gradient returned as a tuple of `pint.Quantity`, otherwise, if     neither `coordinates` nor `deltas` are given, the attached coordinate information belonging     to `axis` will be used and the gradient will be returned as a tuple of `xarray.DataArray`.      Parameters     ----------     f : array-like         Array of values of which to calculate the derivative     coordinates : array-like, optional         Sequence of arrays containing the coordinate values corresponding to the         grid points in `f` in axis order.     deltas : array-like, optional         Sequence of arrays or scalars that specify the spacing between the grid points in `f`         in axis order. There should be one item less than the size of `f` along the applicable         axis.     axes : sequence, optional         Sequence of strings (if `f` is a `xarray.DataArray` and implicit conversion to         `pint.Quantity` is not used) or integers that specify the array axes along which to         take the derivatives. Defaults to all axes of `f`. If given, and used with         `coordinates` or `deltas`, its length must be less than or equal to that of the         `coordinates` or `deltas` given.      Returns     -------     tuple of array-like         The first derivative calculated along each specified axis of the original array      See Also     --------     laplacian, first_derivative      Notes     -----     `gradient` previously accepted `x` as a parameter for coordinate values. This has been     deprecated in 0.9 in favor of `coordinates`.      If this function is used without the `axes` parameter, the length of `coordinates` or     `deltas` (as applicable) should match the number of dimensions of `f`.
Calculate the laplacian of a grid of values.      Works for both regularly-spaced data, and grids with varying spacing.      Either `coordinates` or `deltas` must be specified, or `f` must be given as an     `xarray.DataArray` with  attached coordinate and projection information. If `f` is an     `xarray.DataArray`, and `coordinates` or `deltas` are given, `f` will be converted to a     `pint.Quantity` and the gradient returned as a tuple of `pint.Quantity`, otherwise, if     neither `coordinates` nor `deltas` are given, the attached coordinate information belonging     to `axis` will be used and the gradient will be returned as a tuple of `xarray.DataArray`.      Parameters     ----------     f : array-like         Array of values of which to calculate the derivative     coordinates : array-like, optional         The coordinate values corresponding to the grid points in `f`     deltas : array-like, optional         Spacing between the grid points in `f`. There should be one item less than the size         of `f` along the applicable axis.     axes : sequence, optional         Sequence of strings (if `f` is a `xarray.DataArray` and implicit conversion to         `pint.Quantity` is not used) or integers that specify the array axes along which to         take the derivatives. Defaults to all axes of `f`. If given, and used with         `coordinates` or `deltas`, its length must be less than or equal to that of the         `coordinates` or `deltas` given.      Returns     -------     array-like         The laplacian      See Also     --------     gradient, second_derivative      Notes     -----     `laplacian` previously accepted `x` as a parameter for coordinate values. This has been     deprecated in 0.9 in favor of `coordinates`.      If this function is used without the `axes` parameter, the length of `coordinates` or     `deltas` (as applicable) should match the number of dimensions of `f`.
Handle reshaping coordinate array to have proper dimensionality.      This puts the values along the specified axis.
Handle common processing of arguments for gradient and gradient-like functions.
Handle common processing of arguments for derivative functions.
Calculate the meteorological angle from directional text.      Works for abbrieviations or whole words (E -> 90 | South -> 180)     and also is able to parse 22.5 degreee angles such as ESE/East South East      Parameters     ----------     input_dir : string or array-like strings         Directional text such as west, [south-west, ne], etc      Returns     -------     angle         The angle in degrees
Convert extended (non-abbrievated) directions to abbrieviation.
Returns the underlying data as a Python dict.
Given a text string, returns a list of lists; that is, a list of         "sentences," each of which is a list of words. Before splitting into         words, the sentences are filtered through `self.test_sentence_input`
Attempts `tries` (default: 10) times to generate a valid sentence,         based on the model and `test_sentence_output`. Passes `max_overlap_ratio`         and `max_overlap_total` to `test_sentence_output`.          If successful, returns the sentence as a string. If not, returns None.          If `init_state` (a tuple of `self.chain.state_size` words) is not specified,         this method chooses a sentence-start at random, in accordance with         the model.          If `test_output` is set as False then the `test_sentence_output` check         will be skipped.          If `max_words` is specified, the word count for the sentence will be         evaluated against the provided limit.
Tries making a sentence of no more than `max_chars` characters and optionally         no less than `min_chars` charcaters, passing **kwargs to `self.make_sentence`.
Tries making a sentence that begins with `beginning` string,         which should be a string of one to `self.state` words known         to exist in the corpus.                  If strict == True, then markovify will draw its initial inspiration         only from sentences that start with the specified word/phrase.          If strict == False, then markovify will draw its initial inspiration         from any sentence containing the specified word/phrase.          **kwargs are passed to `self.make_sentence`
Init a Text class based on an existing chain JSON string or object         If corpus is None, overlap checking won't work.
Cumulative calculations. (Summation, by default.)     Via: https://docs.python.org/3/library/itertools.html#itertools.accumulate
Build a Python representation of the Markov model. Returns a dict         of dicts where the keys of the outer dict represent all possible states,         and point to the inner dicts. The inner dicts represent all possibilities         for the "next" item in the chain, along with the count of times it         appears.
Caches the summation calculation and available choices for BEGIN * state_size.         Significantly speeds up chain generation on large corpuses. Thanks, @schollz!
Given a state, choose the next item at random.
Starting either with a naive BEGIN state, or the provided `init_state`         (as a tuple), return a generator that will yield successive items         until the chain reaches the END state.
Given a JSON object or JSON string that was created by `self.to_json`,         return the corresponding markovify.Chain.
Registers a new Algorithm for use when creating and verifying tokens.
Unregisters an Algorithm for use when creating and verifying tokens         Throws KeyError if algorithm is not registered.
Returns back the JWT header parameters as a dict()          Note: The signature is not verified so the header parameters         should not be fully trusted until signature verification is complete
Returns the algorithms that are implemented by the library.
Generate information for a bug report.     Based on the requests package help utility module.
Authenticate with netcup server. Must be called first.
Create record. If it already exists, do nothing.
List all records. Return an empty list if no records found.         ``rtype``, ``name`` and ``content`` are used to filter records.
Create or update a record.
Delete an existing record. If record does not exist, do nothing.
Return list of record dicts in the netcup API convention.
Insert or update a list of DNS records, specified in the netcup API         convention.          The fields ``hostname``, ``type``, and ``destination`` are mandatory         and must be provided either in the record dict or through ``data``!
Call an API method and return response data. For more info, see:         https://ccp.netcup.net/run/webservice/servers/endpoint
Configure provider parser for Rackspace
Specify arguments for AWS Route 53 Lexicon Provider.
Get base kwargs for API call.
Generator to loop through current record set.          Call next page if it exists.
Check if a zone is private
Determine the hosted zone id for the domain.
Create a record in the hosted zone.
Update a record from the hosted zone.
Delete a record from the hosted zone.
List all records for the hosted zone.
Authenticates against Easyname website and try to find out the domain         id.         Easyname uses a CSRF token in its login form, so two requests are         neccessary to actually login.          Returns:           bool: True if domain id was found.          Raises:           AssertionError: When a request returns unexpected or unknown data.           ValueError: When login data is wrong or the domain does not exist.
Create a new DNS entry in the domain zone if it does not already exist.          Args:           rtype (str): The DNS type (e.g. A, TXT, MX, etc) of the new entry.           name (str): The name of the new DNS entry, e.g the domain for which a                       MX entry shall be valid.           content (str): The content of the new DNS entry, e.g. the mail server                          hostname for a MX entry.           [identifier] (str): The easyname id of a DNS entry. Use to overwrite an                     existing entry.          Returns:           bool: True if the record was created successfully, False otherwise.
Delete one or more DNS entries in the domain zone that match the given         criteria.          Args:           [identifier] (str): An ID to match against DNS entry easyname IDs.           [rtype] (str): A DNS rtype (e.g. A, TXT, MX, etc) to match against DNS                       entry types.           [name] (str): A name to match against DNS entry names.           [content] (str): A content to match against a DNS entry contents.          Returns:           bool: True if the record(s) were deleted successfully, False                 otherwise.
Update a DNS entry identified by identifier or name in the domain zone.         Any non given argument will leave the current value of the DNS entry.          Args:           identifier (str): The easyname id of the DNS entry to update.           [rtype] (str): The DNS rtype (e.g. A, TXT, MX, etc) of the new entry.           [name] (str): The name of the new DNS entry, e.g the domain for which                         a MX entry shall be valid.           [content] (str): The content of the new DNS entry, e.g. the mail                            server hostname for a MX entry.          Returns:           bool: True if the record was updated successfully, False otherwise.          Raises:           AssertionError: When a request returns unexpected or unknown data.
Filter and list DNS entries of domain zone on Easyname.         Easyname shows each entry in a HTML table row and each attribute on a         table column.          Args:           [rtype] (str): Filter by DNS rtype (e.g. A, TXT, MX, etc)           [name] (str): Filter by the name of the DNS entry, e.g the domain for                       which a MX entry shall be valid.           [content] (str): Filter by the content of the DNS entry, e.g. the                            mail server hostname for a MX entry.           [identifier] (str): Filter by the easyname id of the DNS entry.          Returns:           list: A list of DNS entries. A DNS entry is an object with DNS                 attribute names as keys (e.g. name, content, priority, etc)                 and additionally an id.          Raises:           AssertionError: When a request returns unexpected or unknown data.
Build and return the post date that is needed to create a DNS entry.
Check if DNS entry already exists.
Return a list of DNS entries that match the given criteria.
Return the TR elements holding the DNS entries.
Filter dns entries based on type, name or content.
Return the CSRF Token of easyname login form.
Attempt to login session on easyname.
Get the authoritative name zone.
Return the easyname id of the domain.
Log Response and Tag elements. Do nothing if elements is none of them.
Find all providers registered in Lexicon, and their availability
Configure a provider parser for Hetzner
Connects to Hetzner account, adds a new record to the zone and returns a         boolean, if creation was successful or not. Needed record rtype, name and         content for record to create.
Connects to Hetzner account and returns a list of records filtered by record         rtype, name and content. The list is empty if no records found.
Connects to Hetzner account, changes an existing record and returns a boolean,         if update was successful or not. Needed identifier or rtype & name to lookup         over all records of the zone for exactly one record to update.
Connects to Hetzner account, removes an existing record from the zone and returns a         boolean, if deletion was successful or not. Uses identifier or rtype, name & content to         lookup over all records of the zone for one or more records to delete.
Creates hashed identifier based on full qualified record type, name & content         and returns hash.
Parses the record identifier and returns type, name & content of the associated record         as tuple. The tuple is empty if no associated record found.
Converts type dependent record content into well formed and fully qualified         content for domain zone and returns content.
Iterates over all records of the zone and returns a list of records filtered         by record type, name and content. The list is empty if no records found.
Requests to Hetzner by current session and returns the response.
Looks on specified or default system domain nameservers to resolve record type         & name and returns record set. The record set is empty if no propagated         record found.
Looks for domain nameservers and returns the IPs of the nameservers as a list.         The list is empty, if no nameservers were found. Needed associated domain zone         name for lookup.
Looks for associated domain zone, nameservers and linked record name until no         more linked record name was found for the given fully qualified record name or         the CNAME lookup was disabled, and then returns the parameters as a tuple.
Checks restrictions for use of CNAME lookup and returns a tuple of the         fully qualified record name to lookup and a boolean, if a CNAME lookup         should be done or not. The fully qualified record name is empty if no         record name is specified by this provider.
If the publicly propagation check should be done, waits until the domain nameservers         responses with the propagated record type, name & content and returns a boolean,         if the publicly propagation was successful or not.
If not exists, creates an DOM from a given session response, then filters the DOM         via given API filters and returns the filtered DOM. The DOM is empty if the filters         have no match.
Extracts hidden input data from DOM and returns the data as dictionary.
Extracts domain ID from given string and returns the domain ID.
Generates, authenticates and exits session to Hetzner account, and         provides tuple of additional needed domain data (domain nameservers,         zone and linked record name) to public methods. The tuple parameters         are empty if not existent or specified. Exits session and raises error         if provider fails during session.
Creates session to Hetzner account, authenticates with given credentials and         returns the session, if authentication was successful. Otherwise raises error.
Exits session to Hetzner account and returns.
Pulls all domains managed by authenticated Hetzner account, extracts their IDs         and returns the ID for the current domain, if exists. Otherwise raises error.
Pulls the zone for the current domain from authenticated Hetzner account and         returns it as an zone object.
Pushes updated zone for current domain to authenticated Hetzner account and         returns a boolean, if update was successful or not. Furthermore, waits until         the zone has been taken over, if it is a Hetzner Robot account.
validate an api server response          :param dict response: server response to check         :param str message: error message to raise         :param int exclude_code: error codes to exclude from errorhandling         :return:         ":raises Exception: on error
run any request against the API just to make sure the credentials         are valid          :return bool: success status         :raises Exception: on error
create a record         does nothing if the record already exists          :param str rtype: type of record         :param str name: name of record         :param mixed content: value of record         :return bool: success status         :raises Exception: on error
list all records          :param str rtype: type of record         :param str name: name of record         :param mixed content: value of record         :return list: list of found records         :raises Exception: on error
update a record          :param int identifier: identifier of record to update         :param str rtype: type of record         :param str name: name of record         :param mixed content: value of record         :return bool: success status         :raises Exception: on error
delete a record         filter selection to delete by identifier or rtype/name/content          :param int identifier: identifier of record to update         :param str rtype: rtype of record         :param str name: name of record         :param mixed content: value of record         :return bool: success status         :raises Exception: on error
Create record. If record already exists with the same content, do nothing.
List all records. Return an empty list if no records found         type, name and content are used to filter records.         If possible filter during the query, otherwise filter after response is received.
Update a record. Identifier must be specified.
Delete an existing record.         If record does not exist, do nothing.         If an identifier is specified, use it, otherwise do a lookup using type, name and content.
Checks to see if slaves should be notified, and notifies them if needed
Get zone data
List all record for the domain in the active Gandi zone.
Updates the specified record in a new Gandi zone          'content' should be a string or a list of strings
Determine the current domain and zone IDs for the domain.
Creates a record for the domain in a new Gandi zone.
List all record for the domain in the active Gandi zone.
Updates the specified record in a new Gandi zone.
Removes the specified records in a new Gandi zone.
Resolve the value of the given config parameter key. Key must be correctly scoped for         Lexicon, and optionally for the DNS provider for which the parameter is consumed.         For instance:             * config.resolve('lexicon:delegated') will get the delegated parameter for Lexicon             * config.resolve('lexicon:cloudflare:auth_token') will get the auth_token parameter               consumed by cloudflare DNS provider.          Value is resolved against each configured source, and value from the highest priority source         is returned. None will be returned if the given config parameter key could not be resolved         from any source.
Add a config source to the current ConfigResolver instance.         If position is not set, this source will be inserted with the lowest priority.
Configure current resolver to use every valid YAML configuration files available in the         given directory path. To be taken into account, a configuration file must conform to the         following naming convention:             * 'lexicon.yml' for a global Lexicon config file (see with_config_file doc)             * 'lexicon_[provider].yml' for a DNS provider specific configuration file, with             [provider] equals to the DNS provider name (see with_provider_config_file doc)          Example:             $ ls /etc/lexicon             lexicon.yml # global Lexicon configuration file             lexicon_cloudflare.yml # specific configuration file for clouflare DNS provder
Configure a source that consumes the dict that where used on Lexicon 2.x
Configure provider parser for CloudNS
search for a record on NS1 across zones. returns None if not found.
Convert returned data from list actions into a nice table for command line usage
Convert returned data from non-list actions into a nice table for command line usage
Print the relevant output for given output_type
Main function of Lexicon.
Execute provided configuration in class constructor to the DNS records
Logs-in the user and checks the domain name
Creates a new unique record
Updates a record. Name changes are allowed, but the record identifier will change
Updates existing record with no sub-domain name changes
Updates existing record and changes it's sub-domain name
Deletes an existing record
Creates record for Subreg API calls
Creates record for lexicon API calls
Returns full domain name of a sub-domain name
Returns sub-domain of a domain name
Lists all records by the specified criteria
Tries to find existing unique record by type, name and content
Sends Login request
Sends Add_DNS_Record request
Sends Modify_DNS_Record request
Sends Delete_DNS_Record request
Make request parse response
The Namecheap API is a little difficult to work with.         Originally this method called PyNamecheap's `domains_getList`, which is         connected to an API endpoint that only lists domains registered under         the authenticating account. However, an authenticating Namecheap user         may be permissioned to manage the DNS of additional domains. Namecheap's         API does not offer a way to list these domains.          This approach to detecting permissioned relies on some implementation         details of the Namecheap API and the PyNamecheap module:          * If the user does not own the domain, or is not permissioned to manage           it in any way, Namecheap will return an error status, which           PyNamecheap will instantly catch and raise.         * If a non-error repsonse is returned, the XML payload is analyzed.           If the user owns the domain it immediately returns valid. Otherwise           we look for "All" Modification rights, or the hosts-edit permission.          This is not feature complete and most-likely misses multiple scenarios         where:         * a user is privileged to manage the domain DNS, but via another "right"         * a user is privileged to manage the domain, but DNS is not configured          Important Note:         * the Namecheap API has inconsistent use of capitalization with strings           and a case-insensitive match should be made. e.g. the following appear           in a payload: `False` and 'false', 'OK' and 'Ok'.          TODO:         * check payload for PremiumDNS           <PremiumDnsSubscription>             <IsActive>false</IsActive>           </PremiumDnsSubscription>         * check payload for other types of DNS           <DnsDetails ProviderType="FREE" IsUsingOurDNS="true" HostCount="5"                 EmailType="No Email Service" DynamicDNSStatus="false" IsFailover="false">             <Nameserver>dns1.registrar-servers.com</Nameserver>             <Nameserver>dns2.registrar-servers.com</Nameserver>           </DnsDetails>
converts from lexicon format record to namecheap format record,         suitable to sending through the api to namecheap
converts from namecheap raw record format to lexicon format record
An innocent call to check that the credentials are okay.
Create record if doesnt already exist with same content
List all records.          record_type, name and content are used to filter the records.         If possible it filters during the query, otherwise afterwards.         An empty list is returned if no records are found.
Create or update a record.
Delete an existing record.          If the record doesn't exist, does nothing.
Configure provider parser for auto provider
Launch the authentication process: for 'auto' provider, it means first to find the relevant         provider, then call its authenticate() method. Almost every subsequent operation will then         be delegated to that provider.
Function that generates the base provider to be used by all dns providers.
Using all providers available, generate a parser that will be used by Lexicon CLI
Create a resource record. If a record already exists with the same         content, do nothing.
Return a list of records matching the supplied params. If no params are         provided, then return all zone records. If no records are found, return         an empty list.
Update a record. Returns `False` if no matching record is found.
Delete record(s) matching the provided params. If there is no match, do         nothing.
Force-set the state of factory.fuzzy's random generator.
Reseed factory.fuzzy's random generator.
Wrapper around django's get_model.
Lazy loading of get_model.      get_model loads django.conf.settings, which may fail if     the settings haven't been configured yet.
Create an instance of the model through objects.get_or_create.
Create an instance of the model, and save it to the database.
Fill in the field.
Import an object from its absolute path.      Example:         >>> import_object('datetime', 'datetime')         <type 'datetime.datetime'>
Sort an iterable of OrderedBase instances.      Args:         items (iterable): the objects to sort         getter (callable or None): a function to extract the OrderedBase instance from an object.      Examples:         >>> sort_ordered_objects([x, y, z])         >>> sort_ordered_objects(v.items(), getter=lambda e: e[1])
Find the first definition of an attribute according to MRO order.
Force the use of a different strategy.      This is an alternative to setting default_strategy in the class definition.
Provide the default value for all allowed fields.          Custom FactoryOptions classes should override this method         to update() its return value.
Identify which factory should be used for a shared counter.
Initialize our counter pointer.          If we're the top-level factory, instantiate a new counter         Otherwise, point to the top-level factory's counter.
Convert an attributes dict to a (args, kwargs) tuple.
Determines if a class attribute is a field value declaration.          Based on the name and value of the class attribute, return ``True`` if         it looks like a declaration of a default field value, ``False`` if it         is private (name starts with '_') or a classmethod or staticmethod.
Find out in what order parameters should be called.
Reset the sequence counter.          Args:             value (int or None): the new 'next' sequence value; if None,                 recompute the next value from _setup_next_sequence().             force (bool): whether to force-reset parent sequence counters                 in a factory inheritance chain.
Build a dict of attribute values, respecting declaration order.          The process is:         - Handle 'orderless' attributes, overriding defaults with provided             kwargs when applicable         - Handle ordered attributes, overriding them with provided kwargs when             applicable; the current list of computed attributes is available             to the currently processed object.
Retrieve a copy of the declared attributes.          Args:             extra_defs (dict): additional definitions to insert into the                 retrieved DeclarationDict.
generate the object.          Args:             params (dict): attributes to use for generating the object             strategy: the strategy to use
Build a batch of instances of the given class, with overriden attrs.          Args:             size (int): the number of instances to build          Returns:             object list: the built instances
Create a batch of instances of the given class, with overriden attrs.          Args:             size (int): the number of instances to create          Returns:             object list: the created instances
Stub a batch of instances of the given class, with overriden attrs.          Args:             size (int): the number of instances to stub          Returns:             object list: the stubbed instances
Generate a new instance.          The instance will be created with the given strategy (one of         BUILD_STRATEGY, CREATE_STRATEGY, STUB_STRATEGY).          Args:             strategy (str): the strategy to use for generating the instance.          Returns:             object: the generated instance
Generate a batch of instances.          The instances will be created with the given strategy (one of         BUILD_STRATEGY, CREATE_STRATEGY, STUB_STRATEGY).          Args:             strategy (str): the strategy to use for generating the instance.             size (int): the number of instances to generate          Returns:             object list: the generated instances
Generate a new instance.          The instance will be either 'built' or 'created'.          Args:             create (bool): whether to 'build' or 'create' the instance.          Returns:             object: the generated instance
Generate a batch of instances.          These instances will be either 'built' or 'created'.          Args:             size (int): the number of instances to generate             create (bool): whether to 'build' or 'create' the instances.          Returns:             object list: the generated instances
Create a new, simple factory for the given class.
Create a factory for the given class, and generate instances.
Create a factory for the given class, and simple_generate instances.
Create an instance of the model, and save it to the database.
Try to retrieve the given attribute of an object, digging on '.'.      This is an extended getattr, digging deeper if '.' is found.      Args:         obj (object): the object of which an attribute should be read         name (str): the name of an attribute to look up.         default (object): the default value to use if the attribute wasn't found      Returns:         the attribute pointed to by 'name', splitting on '.'.      Raises:         AttributeError: if obj has no 'name' attribute.
Evaluate the current ContainerAttribute.          Args:             obj (LazyStub): a lazy stub of the object being constructed, if                 needed.             containers (list of LazyStub): a list of lazy stubs of factories                 being evaluated in a chain, each item being a future field of                 next one.
Evaluate the current definition and fill its attributes.          Uses attributes definition in the following order:         - values defined when defining the ParameteredAttribute         - additional values defined when instantiating the containing factory          Args:             instance (builder.Resolver): The object holding currently computed                 attributes             step: a factory.builder.BuildStep             extra (dict): additional, call-time added kwargs                 for the step.
Evaluate the current definition and fill its attributes.          Args:             step: a factory.builder.BuildStep             params (dict): additional, call-time added kwargs                 for the step.
Split a declaration name into a (declaration, subpath) tuple.          Examples:         >>> DeclarationSet.split('foo__bar')         ('foo', 'bar')         >>> DeclarationSet.split('foo')         ('foo', None)         >>> DeclarationSet.split('foo__bar__baz')         ('foo', 'bar__baz')
Rebuild a full declaration name from its components.          for every string x, we have `join(split(x)) == x`.
Add new declarations to this set/          Args:             values (dict(name, declaration)): the declarations to ingest.
Filter a set of declarations: keep only those related to this object.          This will keep:         - Declarations that 'override' the current ones         - Declarations that are parameters to current ones
Extract a list of (key, value) pairs, suitable for our __init__.
Build a factory instance.
Recurse into a sub-factory call.
Parse an expression into AST
Parse a string expression into a set of tokens that can be used as a path     into a Python datastructure.
Get, set, delete values in a JSON structure. `expr` is a JSONpath-like     expression pointing to the desired value. `action` determines the action to     perform. See the module-level `ACTION_*` constants. `value` should be given     if action is `ACTION_SET`. If `default` is set and `expr` isn't found,     return `default` instead. This will override all exceptions.
Generate module source code given a parsetree node,       uri, and optional source filename
converts a for loop into a context manager wrapped around a for loop     when access to the `loop` variable has been detected in the for loop body
Traverse a template structure for module-level directives and         generate the start of module-level code.
write a top-level render callable.          this could be the main render() method or that of a top-level def.
write module-level template code, i.e. that which         is enclosed in <%! %> tags in the template.
write the module-level namespace-generating callable.
write variable declarations at the top of a function.          the variable declarations are in the form of callable         definitions for defs and/or name lookup within the         function's context argument. the names declared are based         on the names that are referenced in the function body,         which don't otherwise have any explicit assignment         operation. names that are assigned within the body are         assumed to be locally-scoped variables and are not         separately declared.          for def callable definitions, if the def is a top-level         callable then a 'stub' callable is generated which wraps         the current Context into a closure. if the def is not         top-level, it is fully rendered as a local closure.
write a locally-available callable referencing a top-level def
write a locally-available def callable inside an enclosing def.
write the end section of a rendering function, either outermost or         inline.          this takes into account if the rendering function was filtered,         buffered, etc.  and closes the corresponding try: block if any, and         writes code to retrieve captured content, apply filters, send proper         return value.
write a post-function decorator to replace a rendering             callable with a cached version of itself.
write a filter-applying expression based on the filters         present in the given filter names, adjusting for the global         'default' filter aliases as needed.
create a new Identifiers for a new Node, with           this Identifiers as the parent.
update the state of this Identifiers with the undeclared             and declared identifiers of the given node.
Parse the inventory contents. This returns a list of sections found in         the inventory, which can then be used to figure out which hosts belong         to which groups and such. Each section has a name, a type ('hosts',         'children', 'vars') and a list of entries for that section. Entries         consist of a hostname and the variables. For 'vars' sections, the         hostname is None.          For example:             [production:children]             frontend  purpose="web"             db        purpose="db"         Returns:             {                 'name': 'production',                 'type': 'children',                 'entries': [                     {'name': 'frontend', 'hostvars': {'purpose': 'web'}},                     {'name': 'db', 'hostvars': {'purpose': 'db'}},                 ]             }
Parse a line containing a group definition. Returns a tuple:         (group_type, group_name), where group_type is in the set ('hosts',         'children', 'vars').          For example:             [prod]         Returns:             ('hosts', 'prod')          For example:             [prod:children]         Returns:             ('children', 'prod')
Parse a section entry line into its components. In case of a 'vars'         section, the first field will be None. Otherwise, the first field will         be the unexpanded host or group name the variables apply to.          For example:             [production:children]             frontend  purpose="web"    # The line we process         Returns:             ('frontend', {'purpose': 'web'})          For example:             [production:vars]             purpose="web"              # The line we process         Returns:             (None, {'purpose': 'web'})          Undocumented feature:             [prod:vars]             json_like_vars=[{'name': 'htpasswd_auth'}]         Returns:             (None, {'name': 'htpasswd_auth'})
Parse a line in a [XXXXX:vars] section.
Given an iterable of tokens, returns variables and their values as a         dictionary.          For example:             ['dtap=prod', 'comment=some comment']         Returns:             {'dtap': 'prod', 'comment': 'some comment'}
Return a set of distinct hostnames found in the entire inventory.
Recursively find all the hosts that belong in or under a section and         add the section's group name and variables to every host.
Add the variables for each entry in a 'hosts' section to the hosts         belonging to that entry.
Add the variables for each entry in a 'children' section to the hosts         belonging to that entry.
Recursively fetch a list of each unique hostname that belongs in or         under the group. This includes hosts in children groups.
Find and return a section with `name` and `type`
Expand a host definition (e.g. "foo[001:010].bar.com") into seperate         hostnames. Supports zero-padding, numbered ranges and alphabetical         ranges. Multiple patterns in a host defnition are also supported.         Returns a list of the fully expanded hostnames. Ports are also removed         from hostnames as a bonus (e.g. "foo.bar.com:8022" -> "foo.bar.com")
Get an existing host or otherwise initialize a new empty one.
Parse a group definition from a dynamic inventory. These are top-level         elements which are not '_meta(data)'.
Parse the _meta element from a dynamic host inventory output.
create and/or verify a filesystem directory.
Deduce the encoding of a Python source file (binary mode) from magic     comment.      It does this in the same way as the `Python interpreter`__      .. __: http://docs.python.org/ref/encodings.html      The ``fp`` argument should be a seekable file object in binary mode.
repr() a dictionary with the keys in order.      Used by the lexer unit test to compare parse trees based on strings.
Attempt to restore the required classes to the _ast module if it     appears to be missing them
produce a 'union' of this dict and another (at the key level).          values in the second dict take precedence over that of the first
This function can convert a node tree back into python sourcecode.  This     is useful for debugging purposes, especially if you're dealing with custom     asts not generated by python itself.      It could be that the sourcecode is evaluable when the AST itself is not     compilable / evaluable.  The reason for this is that the AST contains some     more data than regular sourcecode does, which is dropped during     conversion.      Each level of indentation is replaced with `indent_with`.  Per default this     parameter is equal to four spaces as suggested by PEP 8, but it might be     adjusted to match the application's styleguide.
A very verbose representation of the node passed.  This is useful for     debugging purposes.
Copy the source location hint (`lineno` and `col_offset`) from the     old to the new node if possible and return the new one.
Some nodes require a line number and the column offset.  Without that     information the compiler will abort the compilation.  Because it can be     a dull task to add appropriate line numbers and column offsets when     adding new nodes this function can help.  It copies the line number and     column offset of the parent node to the child nodes without this     information.      Unlike `copy_location` this works recursive and won't touch nodes that     already have a location information.
Increment the line numbers of all nodes by `n` if they have line number     attributes.  This is useful to "move code" to a different location in a     file.
Iterate over all fields of a node, only yielding existing fields.
Iterate over all child nodes or a node.
Get the mode for `compile` of a given node.  If the node is not a `mod`     node (`Expression`, `Module` etc.) a `TypeError` is thrown.
Return the docstring for the given node or `None` if no docstring can be     found.  If the node provided does not accept docstrings a `TypeError`     will be raised.
Iterate over all nodes.  This is useful if you only want to modify nodes in     place and don't care about the context or the order the nodes are returned.
Visit a node.
compile the given regexp, cache the reg, and call match_reg().
match the given regular expression object to the current text         position.          if a match occurs, update the current text and line position.
given string/unicode or bytes/string, determine encoding            from magic encoding comment, return body as unicode            or raw if decode_raw=False
matches the multiline version of a comment
Extract messages from Mako templates.      :param fileobj: the file-like object the messages should be extracted from     :param keywords: a list of keywords (i.e. function names) that should be                      recognized as translation functions     :param comment_tags: a list of translator tags to search for and include                          in the results     :param options: a dictionary of additional options (optional)     :return: an iterator over ``(lineno, funcname, message, comments)`` tuples     :rtype: ``iterator``
return true if the given keyword is a ternary keyword         for this ControlLine
Retrieve a value from the cache, using the given creation function         to generate a new value.
Retrieve a value from the cache, using the given creation function         to generate a new value.
Place a value in the cache.          :param key: the value's key.         :param value: the value.         :param \**kw: cache configuration arguments.
Retrieve a value from the cache.          :param key: the value's key.         :param \**kw: cache configuration arguments.  The          backend is configured using these arguments upon first request.          Subsequent requests that use the same series of configuration          values will use that same backend.
Invalidate a value in the cache.          :param key: the value's key.         :param \**kw: cache configuration arguments.  The          backend is configured using these arguments upon first request.          Subsequent requests that use the same series of configuration          values will use that same backend.
Apply a caller_stack compatibility decorator to a plain     Python function.      See the example in :ref:`namespaces_python_modules`.
Execute the given template def, capturing the output into     a buffer.      See the example in :ref:`namespaces_python_modules`.
locate the template from the given uri and include it in     the current output.
called by the _inherit method in template modules to set     up the inheritance chain at the start of a template's     execution.
create a Context and return the string     output of the given template and template callable.
execute a rendering callable given the callable, a     Context, and optional explicit arguments      the contextual Template will be located if it exists, and     the error handling options specified on that Template will     be interpreted here.
push a capturing buffer onto this Context and return         the new writer function.
pop the most recent capturing buffer from this Context         and return the current writer after the pop.
Return a value from this :class:`.Context`.
Create a new :class:`.Context` with a copy of this         :class:`.Context`'s current state,         updated with the given dictionary.          The :attr:`.Context.kwargs` collection remains         unaffected.
create a new copy of this :class:`.Context`. with         tokens related to inheritance state removed.
Cycle through values as the loop progresses.
Return a :class:`.Namespace` corresponding to the given ``uri``.          If the given ``uri`` is a relative URI (i.e. it does not         contain a leading slash ``/``), the ``uri`` is adjusted to         be relative to the ``uri`` of the namespace itself. This         method is therefore mostly useful off of the built-in         ``local`` namespace, described in :ref:`namespace_local`.          In         most cases, a template wouldn't need this function, and         should instead use the ``<%namespace>`` tag to load         namespaces. However, since all ``<%namespace>`` tags are         evaluated before the body of a template ever runs,         this method can be used to locate namespaces using         expressions that were generated within the body code of         the template, or to conditionally use a particular         namespace.
Include a file at the given ``uri``.
Deep update target list, dict or set or other iterable with src     For each k,v in src: if k doesn't exist in target, it is deep copied from     src to target. Otherwise, if v is a list, target[k] is extended with     src[k]. If v is a set, target[k] is updated with v, If v is a dict,     recursively deep-update it. If `overwrite` is False, existing values in     target will not be overwritten.      Examples:     >>> t = {'name': 'Ferry', 'hobbies': ['programming', 'sci-fi']}     >>> deepupdate(t, {'hobbies': ['gaming']})     >>> print t     {'name': 'Ferry', 'hobbies': ['programming', 'sci-fi', 'gaming']}
Go through a bunch of dirs and see if dir+path_to_find exists there.     Returns the first dir that matches. Otherwise, return None.
Convert string `s` into a boolean. `s` can be 'true', 'True', 1, 'false',     'False', 0.      Examples:      >>> to_bool("true")     True     >>> to_bool("0")     False     >>> to_bool(True)     True
Render the output of this template as a string.          If the template specifies an output encoding, the string         will be encoded accordingly, else the output is raw (raw         output uses `cStringIO` and can't handle multibyte         characters). A :class:`.Context` object is created corresponding         to the given data. Arguments that are explicitly declared         by this template's internal rendering method are also         pulled from the given ``*args``, ``**data`` members.
Render the output of this template as a unicode object.
Render this :class:`.Template` with the given context.          The data is written to the context's buffer.
Given a string and an interable of extensions, strip the extenion off the     string if the string ends with one of the extensions.
Parse a host / group limit in the form of a string (e.g.         'all:!cust.acme') into a dict of things to be included and things to be         excluded.
Scan inventory. As Ansible is a big mess without any kind of         preconceived notion of design, there are several (and I use that word         lightly) different ways inventory_path can be handled:            - a non-executable file: handled as a Ansible 'hosts' file.           - an executable file: handled as a dynamic inventory file.           - a directory: scanned for Ansible 'hosts' and dynamic inventory             files.
Read all the available hosts inventory information into one big list         and parse it.
Parse host_vars dir, if it exists.
Parse a host var file and apply it to host `hostname`.
Parse group_vars dir, if it exists. Encrypted vault files are skipped.
Walk through a directory of JSON files and extract information from         them. This is used for both the Ansible fact gathering (setup module)         output and custom variables.
Execute a dynamic inventory script and parse the results.
Update a hosts information. This is called by various collectors such         as the ansible setup module output and the hosts parser to add         informatio to a host. It does some deep inspection to make sure nested         information can be updated.
Return a list of hostnames that are in a group.
Return a list of parsed hosts info, with the limit applied if required.
Instantiate a logger.
Find out our installation prefix and data directory. These can be in     different places depending on how ansible-cmdb was installed.
Find out the location of the `hosts` file. This looks in multiple places     such as the `-i` option, current dir and ansible configuration files. The     first match is returned as a list.
Load custom column definitions.
Parse the user params (-p/--params) and them as a dict.
Construct a list of possible paths to templates.
Find a template in the list of possible paths.
Render a mako or .py file.
Hook called on a filename to be sourced.
Loads a template from a file or a string
Return the argument declarations of this FunctionDecl as a printable         list.          By default the return value is appropriate for writing in a ``def``;         set `as_call` to true to build arguments to be passed to the function         instead (assuming locals with the same names as the arguments exist).
legacy HTML escape for non-unicode mode.
An encoding error handler.      This python `codecs`_ error handler replaces unencodable     characters with HTML entities, or, if no HTML entity exists for     the character, XML character references.      >>> u'The cost was \u20ac12.'.encode('latin1', 'htmlentityreplace')     'The cost was &euro;12.'
Replace characters with their character references.          Replace characters by their named entity references.         Non-ASCII characters, if they do not have a named entity reference,         are replaced by numerical character references.          The return value is guaranteed to be ASCII.
Return the multiline comment at lineno split into a list of         comment line numbers and the accompanying comment line
remove the left-whitespace margin of a block of Python code.
print a line or lines of python which already contain indentation.          The indentation of the total block of lines will be adjusted to that of         the current indent level.
print a line of python, indenting it according to the current         indent level.          this also adjusts the indentation counter according to the         content of the line.
return true if the given line is an 'unindentor',         relative to the last 'indent' event received.
indent the given line according to the current indent level.          stripspace is a string of space that will be truncated from the         start of the line before indenting.
return true if the given line is part of a multi-line block,         via backslash or triple-quote.
Provides a template that renders a stack trace in an HTML format,     providing an excerpt of code as well as substituting source template     filenames, line numbers and code for that of the originating source     template, as applicable.      The template's default ``encoding_errors`` value is     ``'htmlentityreplace'``. The template has two options. With the     ``full`` option disabled, only a section of an HTML document is     returned. With the ``css`` option disabled, the default stylesheet     won't be included.
Find a unicode representation of self.error
format a traceback from sys.exc_info() into 7-item tuples,         containing the regular four traceback tuple items, plus the original         template filename, the line number adjusted relative to the template         source, and code line from that line number of the template.
Return ``True`` if this :class:`.TemplateLookup` is         capable of returning a :class:`.Template` object for the         given ``uri``.          :param uri: String URI of the template to be resolved.
Return a :class:`.Template` object corresponding to the given         ``uri``.          .. note:: The ``relativeto`` argument is not supported here at the moment.
Adjust the given ``uri`` based on the given relative URI.
Convert the given ``filename`` to a URI relative to            this :class:`.TemplateCollection`.
Return the portion of a filename that is 'relative'            to the directories in this lookup.
Place a new :class:`.Template` object into this         :class:`.TemplateLookup`, based on the given string of         ``text``.
Gets the specified resource and parses all style URLs and their         assets in the form of the specified patterns.
Gets the URLs of the cached styles.
Fetches the given URLs and caches their contents         and their assets in the given directory.
Get style URLs from the source HTML page and specified cached         asset base URL.
Checks whether a server is currently listening on the specified     host and port.
Blocks until a local server is listening on the specified     host and port. Set cancel_event to cancel the wait.      This is intended to be used in conjunction with running     the Flask server.
Waits for the server to run and then opens the specified address in     the browser. Set cancel_event to cancel the wait.
Starts a thread that waits for the server then opens the specified     address in the browser. Set cancel_event to cancel the wait. The     started thread object is returned.
Renders the specified markdown content and embedded styles.          Raises TypeError if text is not a Unicode string.         Raises requests.HTTPError if the request fails.
Renders the specified markdown content and embedded styles.
Renders the specified cache file.
Renders the rate limit page.
Gets the content of the given list of style URLs and         inlines assets.
Downloads the assets from the style URL list, clears it, and adds         each style with its embedded asset to the literal style list.
Retrieves the style URLs from the source and caches them. This         is called before the first request is dispatched.
Returns the default asset manager using the current config.          This is only used if asset_manager is set to None in the constructor.
Renders the application and returns the HTML unicode that would         normally appear when visiting in the browser.
Starts a server to render the README.
Creates a Grip application with the specified overrides.
Starts a server to render the specified file or directory containing     a README.
Renders the specified markup text to an HTML page and returns it.
Renders the specified markup and returns the result.
Exports the rendered HTML to a file.
Processes the HTML rendered by the GitHub API, patching     any inconsistencies from the main site.
The entry point of the application.
Gets the full path and extension, or None if a README file could not         be found at the specified path.
Returns the path if it's a file; otherwise, looks for a compatible         README file in the directory specified by path.          If path is None, the current working directory is used.          If silent is set, the default relative filename will be returned         if path is a directory or None if it does not exist.          Raises ReadmeNotFoundError if no compatible README file can be         found and silent is False.
Helper that reads the UTF-8 content of the specified file, or         None if the file doesn't exist. This returns a unicode string.
Normalizes the specified subpath, or None if subpath is None.          This allows Readme files to be inferred from directories while         still allowing relative paths to work properly.          Raises werkzeug.exceptions.NotFound if the resulting path         would fall out of the root directory.
Returns the full path for the README file for the specified         subpath, or the root filename if subpath is None.          Raises ReadmeNotFoundError if a README for the specified subpath         does not exist.          Raises werkzeug.exceptions.NotFound if the resulting path         would fall out of the root directory.
Returns the relative filename for the specified subpath, or the         root filename if subpath is None.          Raises werkzeug.exceptions.NotFound if the resulting path         would fall out of the root directory.
Gets whether the specified subpath is a supported binary file.
Returns the time of the last modification of the Readme or         specified subpath, or None if the file does not exist.          The return value is a number giving the number of seconds since         the epoch (see the time module).          Raises werkzeug.exceptions.NotFound if the resulting path         would fall out of the root directory.
Returns the UTF-8 content of the specified subpath.          subpath is expected to already have been normalized.          Raises ReadmeNotFoundError if a README for the specified subpath         does not exist.          Raises werkzeug.exceptions.NotFound if the resulting path         would fall out of the root directory.
Returns the UTF-8 Readme content.          Raises ReadmeNotFoundError if subpath is specified since         subpaths are not supported for text readers.
Reads STDIN until the end of input and returns a unicode string.
Pareto/NBD model fitter.          Parameters         ----------         frequency: array_like             the frequency vector of customers' purchases             (denoted x in literature).         recency: array_like             the recency vector of customers' purchases             (denoted t_x in literature).         T: array_like             customers' age (time units since first purchase)         weights: None or array_like             Number of customers with given frequency/recency/T,             defaults to 1 if not specified. Fader and             Hardie condense the individual RFM matrix into all             observed combinations of frequency/recency/T. This             parameter represents the count of customers with a given             purchase pattern. Instead of calculating individual             log-likelihood, the log-likelihood is calculated for each             pattern and multiplied by the number of customers with             that pattern.         iterative_fitting: int, optional             perform iterative_fitting fits over random/warm-started initial params         initial_params: array_like, optional             set the initial parameters for the fitter.         verbose : bool, optional             set to true to print out convergence diagnostics.         tol : float, optional             tolerance for termination of the function minimization process.         index: array_like, optional             index for resulted DataFrame which is accessible via self.data         fit_method : string, optional             fit_method to passing to scipy.optimize.minimize         maxiter : int, optional             max iterations for optimizer in scipy.optimize.minimize will be             overwritten if set in kwargs.         kwargs:             key word arguments to pass to the scipy.optimize.minimize             function as options dict          Returns         -------         ParetoNBDFitter             with additional properties like ``params_`` and methods like ``predict``
log_A_0.
Conditional expected number of purchases up to time.          Calculate the expected number of repeat purchases up to time t for a         randomly choose individual from the population, given they have         purchase history (frequency, recency, T)          Parameters         ----------         t: array_like             times to calculate the expectation for.         frequency: array_like             historical frequency of customer.         recency: array_like             historical recency of customer.         T: array_like             age of the customer.          Returns         -------         array_like
Conditional probability alive.          Compute the probability that a customer with history         (frequency, recency, T) is currently alive.         From paper:         http://brucehardie.com/notes/009/pareto_nbd_derivations_2005-11-05.pdf          Parameters         ----------         frequency: float             historical frequency of customer.         recency: float             historical recency of customer.         T: float             age of the customer.          Returns         -------         float             value representing a probability
Compute the probability alive matrix.          Parameters         ----------         max_frequency: float, optional             the maximum frequency to plot. Default is max observed frequency.         max_recency: float, optional             the maximum recency to plot. This also determines the age of the             customer. Default to max observed age.          Returns         -------         matrix:             A matrix of the form [t_x: historical recency, x: historical frequency]
Return expected number of repeat purchases up to time t.          Calculate the expected number of repeat purchases up to time t for a         randomly choose individual from the population.          Parameters         ----------         t: array_like             times to calculate the expectation for.          Returns         -------         array_like
Return conditional probability of n purchases up to time t.          Calculate the probability of n purchases up to time t for an individual         with history frequency, recency and T (age).          From paper:         http://www.brucehardie.com/notes/028/pareto_nbd_conditional_pmf.pdf          Parameters         ----------         n: int             number of purchases.         t: a scalar             time up to which probability should be calculated.         frequency: float             historical frequency of customer.         recency: float             historical recency of customer.         T: float             age of the customer.          Returns         -------         array_like
Fit function for fitters.
Save model with dill package.          Parameters         ----------         path: str             Path where to save model.         save_data: bool, optional             Whether to save data from fitter.data to pickle object         save_generate_data_method: bool, optional             Whether to save generate_new_data method (if it exists) from             fitter.generate_new_data to pickle object.         values_to_save: list, optional             Placeholders for original attributes for saving object. If None             will be extended to attr_list length like [None] * len(attr_list)
Load model with dill package.          Parameters         ----------         path: str             From what path load model.
Summary statistics describing the fit.          Returns         -------         df : pd.DataFrame             Contains columns coef, se(coef), lower, upper          See Also         --------         ``print_summary``
Fit the data to the MBG/NBD model.          Parameters         ----------         frequency: array_like             the frequency vector of customers' purchases             (denoted x in literature).         recency: array_like             the recency vector of customers' purchases             (denoted t_x in literature).         T: array_like             customers' age (time units since first purchase)         weights: None or array_like             Number of customers with given frequency/recency/T,             defaults to 1 if not specified. Fader and             Hardie condense the individual RFM matrix into all             observed combinations of frequency/recency/T. This             parameter represents the count of customers with a given             purchase pattern. Instead of calculating individual             log-likelihood, the log-likelihood is calculated for each             pattern and multiplied by the number of customers with             that pattern.         verbose : bool, optional             set to true to print out convergence diagnostics.         tol : float, optional             tolerance for termination of the function minimization process.         index: array_like, optional             index for resulted DataFrame which is accessible via self.data         kwargs:             key word arguments to pass to the scipy.optimize.minimize             function as options dict          Returns         -------         ModifiedBetaGeoFitter:             With additional properties and methods like ``params_`` and ``predict``
Return expected number of repeat purchases up to time t.          Calculate the expected number of repeat purchases up to time t for a         randomly choose individual from the population.          Parameters         ----------         t: array_like             times to calculate the expectation for          Returns         -------         array_like
Conditional expected number of repeat purchases up to time t.          Calculate the expected number of repeat purchases up to time t for a         randomly choose individual from the population, given they have         purchase history (frequency, recency, T)         See Wagner, U. and Hoppe D. (2008).          Parameters         ----------         t: array_like             times to calculate the expectation for.         frequency: array_like             historical frequency of customer.         recency: array_like             historical recency of customer.         T: array_like             age of the customer.          Returns         -------         array_like
Conditional probability alive.          Compute the probability that a customer with history (frequency,         recency, T) is currently alive.         From https://www.researchgate.net/publication/247219660_Empirical_validation_and_comparison_of_models_for_customer_base_analysis         Appendix A, eq. (5)          Parameters         ----------         frequency: array or float             historical frequency of customer.         recency: array or float             historical recency of customer.         T: array or float             age of the customer.          Returns         -------         array:             value representing probability of being alive
Fit a dataset to the BG/NBD model.          Parameters         ----------         frequency: array_like             the frequency vector of customers' purchases             (denoted x in literature).         recency: array_like             the recency vector of customers' purchases             (denoted t_x in literature).         T: array_like             customers' age (time units since first purchase)         weights: None or array_like             Number of customers with given frequency/recency/T,             defaults to 1 if not specified. Fader and             Hardie condense the individual RFM matrix into all             observed combinations of frequency/recency/T. This             parameter represents the count of customers with a given             purchase pattern. Instead of calculating individual             loglikelihood, the loglikelihood is calculated for each             pattern and multiplied by the number of customers with             that pattern.         initial_params: array_like, optional             set the initial parameters for the fitter.         verbose : bool, optional             set to true to print out convergence diagnostics.         tol : float, optional             tolerance for termination of the function minimization process.         index: array_like, optional             index for resulted DataFrame which is accessible via self.data         kwargs:             key word arguments to pass to the scipy.optimize.minimize             function as options dict           Returns         -------         BetaGeoFitter             with additional properties like ``params_`` and methods like ``predict``
Conditional expected number of purchases up to time.          Calculate the expected number of repeat purchases up to time t for a         randomly choose individual from the population, given they have         purchase history (frequency, recency, T)          Parameters         ----------         t: array_like             times to calculate the expectation for.         frequency: array_like             historical frequency of customer.         recency: array_like             historical recency of customer.         T: array_like             age of the customer.          Returns         -------         array_like
Compute conditional probability alive.          Compute the probability that a customer with history         (frequency, recency, T) is currently alive.          From http://www.brucehardie.com/notes/021/palive_for_BGNBD.pdf          Parameters         ----------         frequency: array or scalar             historical frequency of customer.         recency: array or scalar             historical recency of customer.         T: array or scalar             age of the customer.          Returns         -------         array             value representing a probability
Compute the probability alive matrix.          Parameters         ----------         max_frequency: float, optional             the maximum frequency to plot. Default is max observed frequency.         max_recency: float, optional             the maximum recency to plot. This also determines the age of the             customer. Default to max observed age.          Returns         -------         matrix:             A matrix of the form [t_x: historical recency, x: historical frequency]
r"""         Compute the probability of n purchases.           .. math::  P( N(t) = n | \text{model} )          where N(t) is the number of repeat purchases a customer makes in t         units of time.          Parameters         ----------         t: float             number units of time         n: int             number of purchases          Returns         -------         float:             Probability to have n purchases up to t units of time
Create a summary of each customer over a calibration and holdout period.      This function creates a summary of each customer over a calibration and     holdout period (training and testing, respectively).     It accepts transaction data, and returns a DataFrame of sufficient statistics.      Parameters     ----------     transactions: :obj: DataFrame         a Pandas DataFrame that contains the customer_id col and the datetime col.     customer_id_col: string         the column in transactions DataFrame that denotes the customer_id     datetime_col:  string         the column in transactions that denotes the datetime the purchase was made.     calibration_period_end: :obj: datetime         a period to limit the calibration to, inclusive.     observation_period_end: :obj: datetime, optional          a string or datetime to denote the final date of the study.          Events after this date are truncated. If not given, defaults to the max 'datetime_col'.     freq: string, optional         Default 'D' for days. Other examples: 'W' for weekly.     datetime_format: string, optional         a string that represents the timestamp format. Useful if Pandas can't understand         the provided format.     monetary_value_col: string, optional         the column in transactions that denotes the monetary value of the transaction.         Optional, only needed for customer lifetime value estimation models.      Returns     -------     :obj: DataFrame         A dataframe with columns frequency_cal, recency_cal, T_cal, frequency_holdout, duration_holdout         If monetary_value_col isn't None, the dataframe will also have the columns monetary_value_cal and         monetary_value_holdout.
Return dataframe with first transactions.      This takes a DataFrame of transaction data of the form:         customer_id, datetime [, monetary_value]     and appends a column named 'repeated' to the transaction log which indicates which rows     are repeated transactions for that customer_id.      Parameters     ----------     transactions: :obj: DataFrame         a Pandas DataFrame that contains the customer_id col and the datetime col.     customer_id_col: string         the column in transactions DataFrame that denotes the customer_id     datetime_col:  string         the column in transactions that denotes the datetime the purchase was made.     monetary_value_col: string, optional         the column in transactions that denotes the monetary value of the transaction.         Optional, only needed for customer lifetime value estimation models.     observation_period_end: :obj: datetime         a string or datetime to denote the final date of the study.         Events after this date are truncated. If not given, defaults to the max 'datetime_col'.     datetime_format: string, optional         a string that represents the timestamp format. Useful if Pandas can't understand         the provided format.     freq: string, optional         Default 'D' for days, 'W' for weeks, 'M' for months... etc. Full list here:         http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects
Return summary data from transactions.      This transforms a DataFrame of transaction data of the form:         customer_id, datetime [, monetary_value]     to a DataFrame of the form:         customer_id, frequency, recency, T [, monetary_value]      Parameters     ----------     transactions: :obj: DataFrame         a Pandas DataFrame that contains the customer_id col and the datetime col.     customer_id_col: string         the column in transactions DataFrame that denotes the customer_id     datetime_col:  string         the column in transactions that denotes the datetime the purchase was made.     monetary_value_col: string, optional         the columns in the transactions that denotes the monetary value of the transaction.         Optional, only needed for customer lifetime value estimation models.     observation_period_end: datetime, optional          a string or datetime to denote the final date of the study.          Events after this date are truncated. If not given, defaults to the max 'datetime_col'.     datetime_format: string, optional         a string that represents the timestamp format. Useful if Pandas can't understand         the provided format.     freq: string, optional         Default 'D' for days, 'W' for weeks, 'M' for months... etc. Full list here:         http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects     freq_multiplier: int, optional         Default 1, could be use to get exact recency and T, i.e. with freq='W'         row for user id_sample=1 will be recency=30 and T=39 while data in         CDNOW summary are different. Exact values could be obtained with         freq='D' and freq_multiplier=7 which will lead to recency=30.43         and T=38.86      Returns     -------     :obj: DataFrame:         customer_id, frequency, recency, T [, monetary_value]
Calculate alive path for plotting alive history of user.      Parameters     ----------     model:         A fitted lifetimes model     transactions: DataFrame         a Pandas DataFrame containing the transactions history of the customer_id     datetime_col: string         the column in the transactions that denotes the datetime the purchase was made     t: array_like         the number of time units since the birth for which we want to draw the p_alive     freq: string         Default 'D' for days. Other examples= 'W' for weekly      Returns     -------     :obj: Series         A pandas Series containing the p_alive as a function of T (age of the customer)
Check validity of inputs.      Raises ValueError when checks failed.      Parameters     ----------     frequency: array_like         the frequency vector of customers' purchases (denoted x in literature).     recency: array_like, optional         the recency vector of customers' purchases (denoted t_x in literature).     T: array_like, optional         the vector of customers' age (time since first purchase)     monetary_value: array_like, optional         the monetary value vector of customer's purchases (denoted m in literature).
Compute the average lifetime value for a group of one or more customers.      This method computes the average lifetime value for a group of one or more customers.      Parameters     ----------     transaction_prediction_model:         the model to predict future transactions     frequency: array_like         the frequency vector of customers' purchases (denoted x in literature).     recency: array_like         the recency vector of customers' purchases (denoted t_x in literature).     T: array_like         the vector of customers' age (time since first purchase)     monetary_value: array_like         the monetary value vector of customer's purchases (denoted m in literature).     time: int, optional         the lifetime expected for the user in months. Default: 12     discount_rate: float, optional         the monthly adjusted discount rate. Default: 1      Returns     -------     :obj: Series         series with customer ids as index and the estimated customer lifetime values as values
Get expected and actual repeated cumulative transactions.      Parameters     ----------     model:         A fitted lifetimes model     transactions: :obj: DataFrame         a Pandas DataFrame containing the transactions history of the customer_id     datetime_col: string         the column in transactions that denotes the datetime the purchase was made.     customer_id_col: string         the column in transactions that denotes the customer_id     t: int         the number of time units since the begining of         data for which we want to calculate cumulative transactions     datetime_format: string, optional         a string that represents the timestamp format. Useful if Pandas can't         understand the provided format.     freq: string, optional         Default 'D' for days, 'W' for weeks, 'M' for months... etc. Full list here:         http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects     set_index_date: bool, optional         when True set date as Pandas DataFrame index, default False - number of time units     freq_multiplier: int, optional         Default 1, could be use to get exact cumulative transactions predicted         by model, i.e. model trained with freq='W', passed freq to         expected_cumulative_transactions is freq='D', and freq_multiplier=7.      Returns     -------     :obj: DataFrame         A dataframe with columns actual, predicted
Save object with attributes from attr_list.      Parameters     ----------     obj: obj         Object of class with __dict__ attribute.     attr_list: list         List with attributes to exclude from saving to dill object. If empty         list all attributes will be saved.     path: str         Where to save dill object.     values_to_save: list, optional         Placeholders for original attributes for saving object. If None will be         extended to attr_list length like [None] * len(attr_list)
Log likelihood for optimizer.
Fit the BG/BB model.          Parameters         ----------         frequency: array_like             Total periods with observed transactions         recency: array_like             Period of most recent transaction         n_periods: array_like             Number of transaction opportunities. Previously called `n`.         weights: None or array_like             Number of customers with given frequency/recency/T,             defaults to 1 if not specified. Fader and             Hardie condense the individual RFM matrix into all             observed combinations of frequency/recency/T. This             parameter represents the count of customers with a given             purchase pattern. Instead of calculating individual             log-likelihood, the log-likelihood is calculated for each             pattern and multiplied by the number of customers with             that pattern.  Previously called `n_custs`.         verbose: boolean, optional             Set to true to print out convergence diagnostics.         tol: float, optional             Tolerance for termination of the function minimization process.         index: array_like, optional             Index for resulted DataFrame which is accessible via self.data         kwargs:             Key word arguments to pass to the scipy.optimize.minimize             function as options dict          Returns         -------         BetaGeoBetaBinomFitter             fitted and with parameters estimated
r"""         Conditional expected purchases in future time period.          The  expected  number  of  future  transactions across the next m_periods_in_future         transaction opportunities by a customer with purchase history         (x, tx, n).          .. math:: E(X(n_{periods}, n_{periods}+m_{periods_in_future})| \alpha, \beta, \gamma, \delta, frequency, recency, n_{periods})          See (13) in Fader & Hardie 2010.          Parameters         ----------         t: array_like             time n_periods (n+t)          Returns         -------         array_like             predicted transactions
Conditional probability alive.          Conditional probability customer is alive at transaction opportunity         n_periods + m_periods_in_future.          .. math:: P(alive at n_periods + m_periods_in_future|alpha, beta, gamma, delta, frequency, recency, n_periods)          See (A10) in Fader and Hardie 2010.          Parameters         ----------         m: array_like             transaction opportunities          Returns         -------         array_like             alive probabilities
r"""         Return expected number of transactions in first n n_periods.          Expected number of transactions occurring across first n transaction         opportunities.         Used by Fader and Hardie to assess in-sample fit.          .. math:: Pr(X(n) = x| \alpha, \beta, \gamma, \delta)          See (7) in Fader & Hardie 2010.          Parameters         ----------         n: float             number of transaction opportunities          Returns         -------         DataFrame:             Predicted values, indexed by x
Plot a figure with period actual and predicted transactions.      Parameters     ----------     model: lifetimes model         A fitted lifetimes model.     max_frequency: int, optional         The maximum frequency to plot.     title: str, optional         Figure title     xlabel: str, optional         Figure xlabel     ylabel: str, optional         Figure ylabel     kwargs         Passed into the matplotlib.pyplot.plot command.      Returns     -------     axes: matplotlib.AxesSubplot
Plot calibration purchases vs holdout.      This currently relies too much on the lifetimes.util calibration_and_holdout_data function.      Parameters     ----------     model: lifetimes model         A fitted lifetimes model.     calibration_holdout_matrix: pandas DataFrame         DataFrame from calibration_and_holdout_data function.     kind: str, optional         x-axis :"frequency_cal". Purchases in calibration period,                  "recency_cal". Age of customer at last purchase,                  "T_cal". Age of customer at the end of calibration period,                  "time_since_last_purchase". Time since user made last purchase     n: int, optional         Number of ticks on the x axis     Returns     -------     axes: matplotlib.AxesSubplot
Plot recency frequecy matrix as heatmap.      Plot a figure of expected transactions in T next units of time by a customer's frequency and recency.      Parameters     ----------     model: lifetimes model         A fitted lifetimes model.     T: fload, optional         Next units of time to make predictions for     max_frequency: int, optional         The maximum frequency to plot. Default is max observed frequency.     max_recency: int, optional         The maximum recency to plot. This also determines the age of the customer.         Default to max observed age.     title: str, optional         Figure title     xlabel: str, optional         Figure xlabel     ylabel: str, optional         Figure ylabel     kwargs         Passed into the matplotlib.imshow command.      Returns     -------     axes: matplotlib.AxesSubplot
Plot probability alive matrix as heatmap.      Plot a figure of the probability a customer is alive based on their     frequency and recency.      Parameters     ----------     model: lifetimes model         A fitted lifetimes model.     max_frequency: int, optional         The maximum frequency to plot. Default is max observed frequency.     max_recency: int, optional         The maximum recency to plot. This also determines the age of the customer.         Default to max observed age.     title: str, optional         Figure title     xlabel: str, optional         Figure xlabel     ylabel: str, optional         Figure ylabel     kwargs         Passed into the matplotlib.imshow command.      Returns     -------     axes: matplotlib.AxesSubplot
Plot expected repeat purchases on calibration period .      Parameters     ----------     model: lifetimes model         A fitted lifetimes model.     max_frequency: int, optional         The maximum frequency to plot.     title: str, optional         Figure title     xlabel: str, optional         Figure xlabel     ax: matplotlib.AxesSubplot, optional         Using user axes     label: str, optional         Label for plot.     kwargs         Passed into the matplotlib.pyplot.plot command.      Returns     -------     axes: matplotlib.AxesSubplot
Draw a graph showing the probability of being alive for a customer in time.      Parameters     ----------     model: lifetimes model         A fitted lifetimes model.     t: int         the number of time units since the birth we want to draw the p_alive     transactions: pandas DataFrame         DataFrame containing the transactions history of the customer_id     datetime_col: str         The column in the transactions that denotes the datetime the purchase was made     freq: str, optional         Default 'D' for days. Other examples= 'W' for weekly     start_date: datetime, optional         Limit xaxis to start date     ax: matplotlib.AxesSubplot, optional         Using user axes     kwargs         Passed into the matplotlib.pyplot.plot command.      Returns     -------     axes: matplotlib.AxesSubplot
Plot a figure of the predicted and actual cumulative transactions of users.      Parameters     ----------     model: lifetimes model         A fitted lifetimes model     transactions: pandas DataFrame         DataFrame containing the transactions history of the customer_id     datetime_col: str         The column in transactions that denotes the datetime the purchase was made.     customer_id_col: str         The column in transactions that denotes the customer_id     t: float         The number of time units since the begining of         data for which we want to calculate cumulative transactions     t_cal: float         A marker used to indicate where the vertical line for plotting should be.     datetime_format: str, optional         A string that represents the timestamp format. Useful if Pandas         can't understand the provided format.     freq: str, optional         Default 'D' for days, 'W' for weeks, 'M' for months... etc.         Full list here:         http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects     set_index_date: bool, optional         When True set date as Pandas DataFrame index, default False - number of time units     title: str, optional         Figure title     xlabel: str, optional         Figure xlabel     ylabel: str, optional         Figure ylabel     ax: matplotlib.AxesSubplot, optional         Using user axes     kwargs         Passed into the pandas.DataFrame.plot command.      Returns     -------     axes: matplotlib.AxesSubplot
Plot the estimated gamma distribution of lambda (customers' propensities to purchase).      Parameters     ----------     model: lifetimes model         A fitted lifetimes model, for now only for BG/NBD     suptitle: str, optional         Figure suptitle     xlabel: str, optional         Figure xlabel     ylabel: str, optional         Figure ylabel     kwargs         Passed into the matplotlib.pyplot.plot command.      Returns     -------     axes: matplotlib.AxesSubplot
Plot the estimated gamma distribution of p.      p - (customers' probability of dropping out immediately after a transaction).      Parameters     ----------     model: lifetimes model         A fitted lifetimes model, for now only for BG/NBD     suptitle: str, optional         Figure suptitle     xlabel: str, optional         Figure xlabel     ylabel: str, optional         Figure ylabel     kwargs         Passed into the matplotlib.pyplot.plot command.      Returns     -------     axes: matplotlib.AxesSubplot
Generate artificial data according to the BG/NBD model.      See [1] for model details      Parameters     ----------     T: array_like         The length of time observing new customers.     r, alpha, a, b: float         Parameters in the model. See [1]_     size: int, optional         The number of customers to generate      Returns     -------     DataFrame         With index as customer_ids and the following columns:         'frequency', 'recency', 'T', 'lambda', 'p', 'alive', 'customer_id'      References     ----------     .. [1]: '"Counting Your Customers" the Easy Way: An Alternative to the Pareto/NBD Model'        (http://brucehardie.com/papers/bgnbd_2004-04-20.pdf)
Generate artificial transactional data according to the BG/NBD model.      See [1] for model details      Parameters     ----------     T: int, float or array_like         The length of time observing new customers.     r, alpha, a, b: float         Parameters in the model. See [1]_     observation_period_end: date_like         The date observation ends     freq: string, optional         Default 'D' for days, 'W' for weeks, 'h' for hours     size: int, optional         The number of customers to generate      Returns     -------     DataFrame         The following columns:         'customer_id', 'date'      References     ----------     .. [1]: '"Counting Your Customers" the Easy Way: An Alternative to the Pareto/NBD Model'        (http://brucehardie.com/papers/bgnbd_2004-04-20.pdf)
Generate artificial data according to the Pareto/NBD model.      See [2]_ for model details.      Parameters     ----------     T: array_like         The length of time observing new customers.     r, alpha, s, beta: float         Parameters in the model. See [1]_     size: int, optional         The number of customers to generate      Returns     -------     :obj: DataFrame         with index as customer_ids and the following columns:         'frequency', 'recency', 'T', 'lambda', 'mu', 'alive', 'customer_id'      References     ----------     .. [2]: Fader, Peter S. and Bruce G. S. Hardie (2005), "A Note on Deriving the Pareto/NBD Model        and Related Expressions," <http://brucehardie.com/notes/009/>.
Generate artificial data according to the Beta-Geometric/Beta-Binomial     Model.      You may wonder why we can have frequency = n_periods, when frequency excludes their     first order. When a customer purchases something, they are born, _and in the next     period_ we start asking questions about their alive-ness. So really they customer has     bought frequency + 1, and been observed for n_periods + 1      Parameters     ----------     N: array_like         Number of transaction opportunities for new customers.     alpha, beta, gamma, delta: float         Parameters in the model. See [1]_     size: int, optional         The number of customers to generate      Returns     -------     DataFrame         with index as customer_ids and the following columns:         'frequency', 'recency', 'n_periods', 'lambda', 'p', 'alive', 'customer_id'      References     ----------     .. [1] Fader, Peter S., Bruce G.S. Hardie, and Jen Shang (2010),        "Customer-Base Analysis in a Discrete-Time Noncontractual Setting,"        Marketing Science, 29 (6), 1086-1108.
Load cdnow customers summary with monetary value as pandas DataFrame.
Conditional expectation of the average profit.          This method computes the conditional expectation of the average profit         per transaction for a group of one or more customers.          Parameters         ----------         frequency: array_like, optional             a vector containing the customers' frequencies.             Defaults to the whole set of frequencies used for fitting the model.         monetary_value: array_like, optional             a vector containing the customers' monetary values.             Defaults to the whole set of monetary values used for             fitting the model.          Returns         -------         array_like:             The conditional expectation of the average profit per transaction
Fit the data to the Gamma/Gamma model.          Parameters         ----------         frequency: array_like             the frequency vector of customers' purchases             (denoted x in literature).         monetary_value: array_like             the monetary value vector of customer's purchases             (denoted m in literature).         weights: None or array_like             Number of customers with given frequency/monetary_value,             defaults to 1 if not specified. Fader and             Hardie condense the individual RFM matrix into all             observed combinations of frequency/monetary_value. This             parameter represents the count of customers with a given             purchase pattern. Instead of calculating individual             loglikelihood, the loglikelihood is calculated for each             pattern and multiplied by the number of customers with             that pattern.         initial_params: array_like, optional             set the initial parameters for the fitter.         verbose : bool, optional             set to true to print out convergence diagnostics.         tol : float, optional             tolerance for termination of the function minimization process.         index: array_like, optional             index for resulted DataFrame which is accessible via self.data         q_constraint: bool, optional             when q < 1, population mean will result in a negative value             leading to negative CLV outputs. If True, we penalize negative values of q to avoid this issue.         kwargs:             key word arguments to pass to the scipy.optimize.minimize             function as options dict          Returns         -------         GammaGammaFitter             fitted and with parameters estimated
Return customer lifetime value.          This method computes the average lifetime value for a group of one         or more customers.          Parameters         ----------         transaction_prediction_model: model             the model to predict future transactions, literature uses             pareto/ndb models but we can also use a different model like beta-geo models         frequency: array_like             the frequency vector of customers' purchases             (denoted x in literature).         recency: the recency vector of customers' purchases                  (denoted t_x in literature).         T: array_like             customers' age (time units since first purchase)         monetary_value: array_like             the monetary value vector of customer's purchases             (denoted m in literature).         time: float, optional             the lifetime expected for the user in months. Default: 12         discount_rate: float, optional             the monthly adjusted discount rate. Default: 0.01         freq: string, optional             {"D", "H", "M", "W"} for day, hour, month, week. This represents what unit of time your T is measure in.          Returns         -------         Series:             Series object with customer ids as index and the estimated customer             lifetime values as values
Looks for different TLDs as well as different sub-domains in SAN list
Detect CMS using whatcms.org.         Has a re-try mechanism because false negatives may occur         :param tries: Count of tries for CMS discovery
Set the proxies to any of the following:         Proxy List - a list of proxies to choose randomly from for each request. Read from file.         TOR - a dict of socks5 and the TOR service default 9050 that will be used         Else, No proxies - an empty dict will be used.
Send a GET/POST/HEAD request using the object's proxies and headers         :param method: Method to send request in. GET/POST/HEAD
Returns a new session using the object's proxies and headers
Try to extract domain (full, naked, sub-domain), IP and port.
Query DNS records for host.         :param domains: Iterable of domains to get DNS Records for         :param records: Iterable of DNS records to get from domain.
Validate that both certificates exist.         :returns: True if they are identical, False otherwise
Test for version support (SNI/non-SNI), get all SANs, get certificate details         :param sni: True will call cause _exec_openssl to call openssl with -servername flag
Will first normalize the img src and then check if this bucket was discovered before         If it is in storage_urls_found, the function returns         Else, it send a GET for the original URL (normalized image src) and will look for "AmazonS3" in         the "Server" response header.         If found, will add to URL with the resource stripped          :param storage_url: img src scraped from page
Send a HEAD request to URL and print response code if it's not in ignored_error_codes         :param uri: URI to fuzz         :param sub_domain: If True, build destination URL with {URL}.{HOST} else {HOST}/{URL}
Create a pool of threads and exhaust self.wordlist on self._fetch         Should be run in an event loop.         :param sub_domain: Indicate if this is subdomain enumeration or URL busting         :param log_file_path: Log subdomain enum results to this path.
Validate port range for Nmap scan
No more than 1 of the following can be specified: tor_routing, proxy, proxy_list
Tries to create base output directory
Define the tags used for a private raw data field.          :param length_tag: tag number of length field.         :param value_tag: tag number of value field.          Data fields are not terminated by the SOH character as is usual for         FIX, but instead have a second, preceding field that specifies the         length of the value in bytes.  The parser is initialised with all the         data fields defined in FIX.5.0, but if your application uses private         data fields, you can add them here, and the parser will process them         correctly.
Remove the tags for a data type field.          :param length_tag: tag number of the length field.         :param value_tag: tag number of the value field.          You can remove either private or standard data field definitions in         case a particular application uses them for a field of a different         type.
Process the accumulated buffer and return the first message.          If the buffer starts with FIX fields other than BeginString         (8), these are discarded until the start of a message is         found.          If no BeginString (8) field is found, this function returns         None.  Similarly, if (after a BeginString) no Checksum (10)         field is found, the function returns None.          Otherwise, it returns a simplefix.FixMessage instance         initialised with the fields from the first complete message         found in the buffer.
Make a FIX value from a string, bytes, or number.
Make a FIX tag value from string, bytes, or integer.
Append a tag=value pair to this message.          :param tag: Integer or string FIX tag number.         :param value: FIX tag value.         :param header: Append to header if True; default to body.          Both parameters are explicitly converted to strings before         storage, so it's ok to pass integers if that's easier for         your program logic.          Note: a Python 'None' value will be silently ignored, and         no field is appended.
Append a time field to this message.          :param tag: Integer or string FIX tag number.         :param timestamp: Time (see below) value to append, or None for now.         :param precision: Number of decimal digits.  Zero for seconds only,         three for milliseconds, 6 for microseconds.  Defaults to milliseconds.         :param utc: Use UTC if True, local time if False.         :param header: Append to header if True; default to body.          THIS METHOD IS DEPRECATED!         USE append_utc_timestamp() OR append_tz_timestamp() INSTEAD.          Append a timestamp in FIX format from a Python time.time or         datetime.datetime value.          Note that prior to FIX 5.0, precision must be zero or three to be         compliant with the standard.
(Internal) Append formatted datetime.
Append a field with a UTCTimestamp value.          :param tag: Integer or string FIX tag number.         :param timestamp: Time value, see below.         :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).         :param header: Append to FIX header if True; default to body.          The `timestamp` value should be a datetime, such as created by         datetime.datetime.utcnow(); a float, being the number of seconds         since midnight 1 Jan 1970 UTC, such as returned by time.time();         or, None, in which case datetime.datetime.utcnow() is used to         get the current UTC time.          Precision values other than zero (seconds), 3 (milliseconds),         or 6 (microseconds) will raise an exception.  Note that prior         to FIX 5.0, only values of 0 or 3 comply with the standard.
Append a field with a UTCTimeOnly value.          :param tag: Integer or string FIX tag number.         :param timestamp: Time value, see below.         :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).         :param header: Append to FIX header if True; default to body.          The `timestamp` value should be a datetime, such as created by         datetime.datetime.utcnow(); a float, being the number of seconds         since midnight 1 Jan 1970 UTC, such as returned by time.time();         or, None, in which case datetime.datetime.utcnow() is used to         get the current UTC time.          Precision values other than zero (seconds), 3 (milliseconds),         or 6 (microseconds) will raise an exception.  Note that prior         to FIX 5.0, only values of 0 or 3 comply with the standard.
Append a field with a TZTimestamp value, derived from local time.          :param tag: Integer or string FIX tag number.         :param timestamp: Time value, see below.         :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).         :param header: Append to FIX header if True; default to body.          The `timestamp` value should be a local datetime, such as created         by datetime.datetime.now(); a float, being the number of seconds         since midnight 1 Jan 1970 UTC, such as returned by time.time();         or, None, in which case datetime.datetime.now() is used to get         the current local time.          Precision values other than zero (seconds), 3 (milliseconds),         or 6 (microseconds) will raise an exception.  Note that prior         to FIX 5.0, only values of 0 or 3 comply with the standard.
Append a field with a TZTimeOnly value.          :param tag: Integer or string FIX tag number.         :param timestamp: Time value, see below.         :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).         :param header: Append to FIX header if True; default to body.          The `timestamp` value should be a local datetime, such as created         by datetime.datetime.now(); a float, being the number of seconds         since midnight 1 Jan 1970 UTC, such as returned by time.time();         or, None, in which case datetime.datetime.now() is used to         get the current UTC time.          Precision values other than None (minutes), zero (seconds),         3 (milliseconds), or 6 (microseconds) will raise an exception.         Note that prior to FIX 5.0, only values of 0 or 3 comply with the         standard.
Append a field with a TZTimeOnly value from components.          :param tag: Integer or string FIX tag number.         :param h: Hours, in range 0 to 23.         :param m: Minutes, in range 0 to 59.         :param s: Optional seconds, in range 0 to 59 (60 for leap second).         :param ms: Optional milliseconds, in range 0 to 999.         :param us: Optional microseconds, in range 0 to 999.         :param offset: Minutes east of UTC, in range -1439 to +1439.         :param header: Append to FIX header if True; default to body.          Formats the TZTimeOnly value from its components.          If `s`, `ms` or `us` are None, the precision is truncated at         that point.
Append a tag=value pair in string format.          :param field: String "tag=value" to be appended to this message.         :param header: Append to header if True; default to body.          The string is split at the first '=' character, and the resulting         tag and value strings are appended to the message.
Append tag=pairs for each supplied string.          :param string_list: List of "tag=value" strings.         :param header: Append to header if True; default to body.          Each string is split, and the resulting tag and value strings         are appended to the message.
Append raw data, possibly including a embedded SOH.          :param len_tag: Tag number for length field.         :param val_tag: Tag number for value field.         :param data: Raw data byte string.         :param header: Append to header if True; default to body.          Appends two pairs: a length pair, followed by a data pair,         containing the raw data supplied.  Example fields that should         use this method include: 95/96, 212/213, 354/355, etc.
Return n-th value for tag.          :param tag: FIX field tag number.         :param nth: Index of tag if repeating, first is 1.         :return: None if nothing found, otherwise value matching tag.          Defaults to returning the first matching value of 'tag', but if         the 'nth' parameter is overridden, can get repeated fields.
Remove the n-th occurrence of tag in this message.          :param tag: FIX field tag number to be removed.         :param nth: Index of tag if repeating, first is 1.         :returns: Value of the field if removed, None otherwise.
Convert message to on-the-wire FIX format.          :param raw: If True, encode pairs exactly as provided.          Unless 'raw' is set, this function will calculate and         correctly set the BodyLength (9) and Checksum (10) fields, and         ensure that the BeginString (8), Body Length (9), Message Type         (35) and Checksum (10) fields are in the right positions.          This function does no further validation of the message content.
(Internal) Convert TZ offset in minutes east to string.
unusual locations of import tables     non recognized section names     presence of long ASCII strings
Returns True is there is a high likelihood that a file is packed or contains compressed data.      The sections of the PE file will be analyzed, if enough sections     look like containing compressed data and the data makes     up for more than 20% of the total file size, the function will     return True.
Generates signatures for all the sections in a PE file.          If the section contains any data a signature will be created         for it. The signature name will be a combination of the         parameter 'name' and the section number and its name.
Generate signatures for the entry point of a PE file.          Creates a signature whose name will be the parameter 'name'         and the section number and its name.
Matches and returns the exact match(es).          If ep_only is True the result will be a string with         the packer name. Otherwise it will be a list of the         form (file_offset, packer_name) specifying where         in the file the signature was found.
Recursive function to find matches along the signature tree.          signature_tree  is the part of the tree left to walk         data    is the data being checked against the signature tree         depth   keeps track of how far we have gone down the tree
Load a PEiD signature file.          Invoking this method on different files combines the signatures.
Parse docstring from file 'pefile.py' and avoid importing     this module directly.
Parse attribute from file 'pefile.py' and avoid importing     this module directly.      __version__, __author__, __contact__,
Lookup a name for the given ordinal if it's in our     database.
Read the flags from a dictionary and return them in a usable form.      Will return a list of (flag, value) for all flags in "flag_dict"     matching the filter "flag_filter".
The next RVA is taken to be the one immediately following this one.          Such RVA could indicate the natural end of the string and will be checked         to see if there's a Unicode NULL character there.
Adds a list of lines.          The list can be indented with the optional argument 'indent'.
Adds some text, no newline will be appended.          The text can be indented with the optional argument 'indent'.
Get the text in its current state.
Returns a string representation of the structure.
Returns a dictionary representation of the structure.
Get data chunk from a section.          Allows to query data from the section by passing the         addresses where the PE file would be loaded by default.         It is then possible to retrieve code and data by their real         addresses as they would be if loaded.          Returns bytes() under Python 3.x and set() under Python 2.7
Check whether the section contains the address provided.
Calculate the entropy of a chunk of data.
Parses the rich header         see http://www.ntcore.com/files/richsign.htm for more information          Structure:         00 DanS ^ checksum, checksum, checksum, checksum         10 Symbol RVA ^ checksum, Symbol size ^ checksum...         ...         XX Rich, checksum, 0, 0,...
Process the data directories.          This method will load the data directories which might not have         been loaded if the "fast_load" option was used.
Write the PE file.          This function will process all headers and components         of the PE file and include all changes made (by just         assigning to attributes in the PE objects) and write         the changes back to a file whose name is provided as         an argument. The filename is optional, if not         provided the data will be returned as a 'str' object.
Fetch the PE file sections.          The sections will be readily available in the "sections" attribute.         Its attributes will contain all the section information plus "data"         a buffer containing the section's data.          The "Characteristics" member will be processed and attributes         representing the section characteristics (with the 'IMAGE_SCN_'         string trimmed from the constant's names) will be added to the         section instance.          Refer to the SectionStructure class for additional info.
Parse and process the PE file's data directories.          If the optional argument 'directories' is given, only         the directories at the specified indexes will be parsed.         Such functionality allows parsing of areas of interest         without the burden of having to parse all others.         The directories can then be specified as:          For export / import only:            directories = [ 0, 1 ]          or (more verbosely):            directories = [ DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT'],             DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT'] ]          If 'directories' is a list, the ones that are processed will be removed,         leaving only the ones that are not present in the image.          If `forwarded_exports_only` is True, the IMAGE_DIRECTORY_ENTRY_EXPORT         attribute will only contain exports that are forwarded to another DLL.          If `import_dllnames_only` is True, symbols will not be parsed from         the import table and the entries in the IMAGE_DIRECTORY_ENTRY_IMPORT         attribute will not have a `symbols` attribute.
Parse a data entry from the resources directory.
Parse version information structure.          The date will be made available in three attributes of the PE object.          VS_VERSIONINFO     will contain the first three fields of the main structure:             'Length', 'ValueLength', and 'Type'          VS_FIXEDFILEINFO    will hold the rest of the fields, accessible as sub-attributes:             'Signature', 'StrucVersion', 'FileVersionMS', 'FileVersionLS',             'ProductVersionMS', 'ProductVersionLS', 'FileFlagsMask', 'FileFlags',             'FileOS', 'FileType', 'FileSubtype', 'FileDateMS', 'FileDateLS'          FileInfo    is a list of all StringFileInfo and VarFileInfo structures.          StringFileInfo structures will have a list as an attribute named 'StringTable'         containing all the StringTable structures. Each of those structures contains a         dictionary 'entries' with all the key / value version information string pairs.          VarFileInfo structures will have a list as an attribute named 'Var' containing         all Var structures. Each Var structure will have a dictionary as an attribute         named 'entry' which will contain the name and value of the Var.
Parse the export directory.          Given the RVA of the export directory, it will process all         its entries.          The exports will be made available as a list of ExportData         instances in the 'IMAGE_DIRECTORY_ENTRY_EXPORT' PE attribute.
Walk and parse the delay import directory.
Walk and parse the import directory.
Returns the data corresponding to the memory layout of the PE file.          The data includes the PE header and the sections loaded at offsets         corresponding to their relative virtual addresses. (the VirtualAddress         section header member).         Any offset in this data corresponds to the absolute memory address         ImageBase+offset.          The optional argument 'max_virtual_address' provides with means of limiting         which sections are processed.         Any section with their VirtualAddress beyond this value will be skipped.         Normally, sections with values beyond this range are just there to confuse         tools. It's a common trick to see in packed executables.          If the 'ImageBase' optional argument is supplied, the file's relocations         will be applied to the image by calling the 'relocate_image()' method. Beware         that the relocation information is applied permanently.
.
Get an ASCII string from data.
Get an Unicode string located at the given address.
Get the section containing the given file offset.
Get the section containing the given address.
Dump all the PE header information into human readable string.
Dump all the PE header information into a dictionary.
Convert four bytes of data to a double word (little endian)          'offset' is assumed to index into a dword array. So setting it to         N will return a dword out of the data starting at offset N*4.          Returns None if the data can't be turned into a double word.
Return the double word value at the given file offset. (little endian)
Set the double word value at the file offset corresponding to the given RVA.
Set the double word value at the given file offset.
Convert two bytes of data to a word (little endian)          'offset' is assumed to index into a word array. So setting it to         N will return a dword out of the data starting at offset N*2.          Returns None if the data can't be turned into a word.
Return the word value at the given file offset. (little endian)
Set the word value at the file offset corresponding to the given RVA.
Set the word value at the given file offset.
Convert eight bytes of data to a word (little endian)          'offset' is assumed to index into a word array. So setting it to         N will return a dword out of the data starting at offset N*8.          Returns None if the data can't be turned into a quad word.
Return the quad-word value at the given file offset. (little endian)
Set the quad-word value at the file offset corresponding to the given RVA.
Set the quad-word value at the given file offset.
Overwrite, with the given string, the bytes at the file offset corresponding to the given RVA.          Return True if successful, False otherwise. It can fail if the         offset is outside the file's boundaries.
Overwrite the bytes at the given file offset with the given string.          Return True if successful, False otherwise. It can fail if the         offset is outside the file's boundaries.
Apply the relocation information to the image using the provided new image base.          This method will apply the relocation information to the image. Given the new base,         all the relocations will be processed and both the raw data and the section's data         will be fixed accordingly.         The resulting image can be retrieved as well through the method:              get_memory_mapped_image()          In order to get something that would more closely match what could be found in memory         once the Windows loader finished its work.
Check whether the file is a standard executable.          This will return true only if the file has the IMAGE_FILE_EXECUTABLE_IMAGE flag set         and the IMAGE_FILE_DLL not set and the file does not appear to be a driver either.
Check whether the file is a standard DLL.          This will return true only if the image has the IMAGE_FILE_DLL flag set.
Check whether the file is a Windows driver.          This will return true only if there are reliable indicators of the image         being a driver.
Get the offset of data appended to the file and not contained within         the area described in the headers.
Return the just data defined by the PE headers, removing any overlayed data.
\
\         although slow the best way to determine the best image is to download         them and check the actual dimensions of the image when on disk         so we'll go through a phased approach...         1. get a list of ALL images from the parent node         2. filter out any bad image names that we know of (gifs, ads, etc..)         3. do a head request on each file to make sure it meets            our bare requirements         4. any images left over let's do a full GET request,            download em to disk and check their dimensions         5. Score images based on different factors like height/width            and possibly things like color density
\         takes a list of image elements         and filters out the ones with bad names
\         returns the bytes of the image file on disk
\         in here we check for known image contains from sites         we've checked out like yahoo, techcrunch, etc... that have         * known  places to look for good images.         * TODO: enable this to use a series of settings files           so people can define what the image ids/classes           are on specific sites
\         This method will take an image path and build         out the absolute path to that image         * using the initial url we crawled           so we can find a link to the image           if they use relative urls like ../myimage.jpg
Returns a unicode object representing 's'. Treats bytestrings using the     'encoding' codec.      If strings_only is True, don't convert (some) non-string-like objects.
Determine if the object instance is of a protected type.      Objects of protected types are preserved as-is when passed to     force_unicode(strings_only=True).
Similar to smart_unicode, except that lazy instances are resolved to     strings, rather than kept as lazy objects.      If strings_only is True, don't convert (some) non-string-like objects.
Returns a bytestring version of 's', encoded as specified in 'encoding'.      If strings_only is True, don't convert (some) non-string-like objects.
\         Returns the language is by the article or         the configuration language
\         we could have long articles that have tons of paragraphs         so if we tried to calculate the base score against         the total text score of those paragraphs it would be unfair.         So we need to normalize the score based on the average scoring         of the paragraphs within the top node.         For example if our total score of 10 paragraphs was 1000         but each had an average value of 100 then 100 should be our base.
\         adds a score to the gravityScore Attribute we put on divs         we'll get the current score then add the score         we're passing in to the current
\         stores how many decent nodes are under a parent node
\         checks the density of links within a node,         is there not much text and most of it contains linky shit?         if so it's no good
\         remove any divs that looks like non-content,         clusters of links, or paras with no gusto
Create a video object from a video embed
\         Writes an image src http string to disk as a temporary file         and returns the LocallyStoredImage object         that has the info you should need on the image
\         if there are elements inside our top node         that have a negative gravity score,         let's give em the boot
\         remove paragraphs that have less than x number of words,         would indicate that it's some sort of link
Clean title with the use of og:site_name         in this case try to get rid of site name         and use TITLE_SPLITTERS to reformat title
\         Main method to extract an article object from a URL,         pass in a url and get back a Article
\         Extract content language from meta
\         Extract a given meta content form document
Private function used to build a batch of programs within a job.
A report of the progress of the evolution process.          Parameters         ----------         run_details : dict             Information about the evolution.
Fit the Genetic Program according to X, y.          Parameters         ----------         X : array-like, shape = [n_samples, n_features]             Training vectors, where n_samples is the number of samples and             n_features is the number of features.          y : array-like, shape = [n_samples]             Target values.          sample_weight : array-like, shape = [n_samples], optional             Weights applied to individual samples.          Returns         -------         self : object             Returns self.
Perform regression on test vectors X.          Parameters         ----------         X : array-like, shape = [n_samples, n_features]             Input vectors, where n_samples is the number of samples             and n_features is the number of features.          Returns         -------         y : array, shape = [n_samples]             Predicted values for X.
Predict probabilities on test vectors X.          Parameters         ----------         X : array-like, shape = [n_samples, n_features]             Input vectors, where n_samples is the number of samples             and n_features is the number of features.          Returns         -------         proba : array, shape = [n_samples, n_classes]             The class probabilities of the input samples. The order of the             classes corresponds to that in the attribute `classes_`.
Predict classes on test vectors X.          Parameters         ----------         X : array-like, shape = [n_samples, n_features]             Input vectors, where n_samples is the number of samples             and n_features is the number of features.          Returns         -------         y : array, shape = [n_samples,]             The predicted classes of the input samples.
Transform X according to the fitted transformer.          Parameters         ----------         X : array-like, shape = [n_samples, n_features]             Input vectors, where n_samples is the number of samples             and n_features is the number of features.          Returns         -------         X_new : array-like, shape = [n_samples, n_components]             Transformed array.
Make a fitness measure, a metric scoring the quality of a program's fit.      This factory function creates a fitness measure object which measures the     quality of a program's fit and thus its likelihood to undergo genetic     operations into the next generation. The resulting object is able to be     called with NumPy vectorized arguments and return a resulting floating     point score quantifying the quality of the program's representation of the     true relationship.      Parameters     ----------     function : callable         A function with signature function(y, y_pred, sample_weight) that         returns a floating point number. Where `y` is the input target y         vector, `y_pred` is the predicted values from the genetic program, and         sample_weight is the sample_weight vector.      greater_is_better : bool         Whether a higher value from `function` indicates a better fit. In         general this would be False for metrics indicating the magnitude of         the error, and True for metrics indicating the quality of fit.
Calculate the weighted Pearson correlation coefficient.
Calculate the weighted Spearman correlation coefficient.
Calculate the mean absolute error.
Calculate the mean square error.
Calculate the root mean square error.
Calculate the log loss.
Build a naive random program.          Parameters         ----------         random_state : RandomState instance             The random number generator.          Returns         -------         program : list             The flattened tree representation of the program.
Rough check that the embedded program in the object is valid.
Returns a string, Graphviz script for visualizing the program.          Parameters         ----------         fade_nodes : list, optional             A list of node indices to fade out for showing which were removed             during evolution.          Returns         -------         output : string             The Graphviz script to plot the tree representation of the program.
Calculates the maximum depth of the program tree.
Execute the program according to X.          Parameters         ----------         X : {array-like}, shape = [n_samples, n_features]             Training vectors, where n_samples is the number of samples and             n_features is the number of features.          Returns         -------         y_hats : array-like, shape = [n_samples]             The result of executing the program on X.
Get the indices on which to evaluate the fitness of a program.          Parameters         ----------         n_samples : int             The number of samples.          max_samples : int             The maximum number of samples to use.          random_state : RandomState instance             The random number generator.          Returns         -------         indices : array-like, shape = [n_samples]             The in-sample indices.          not_indices : array-like, shape = [n_samples]             The out-of-sample indices.
Evaluate the raw fitness of the program according to X, y.          Parameters         ----------         X : {array-like}, shape = [n_samples, n_features]             Training vectors, where n_samples is the number of samples and             n_features is the number of features.          y : array-like, shape = [n_samples]             Target values.          sample_weight : array-like, shape = [n_samples]             Weights applied to individual samples.          Returns         -------         raw_fitness : float             The raw fitness of the program.
Evaluate the penalized fitness of the program according to X, y.          Parameters         ----------         parsimony_coefficient : float, optional             If automatic parsimony is being used, the computed value according             to the population. Otherwise the initialized value is used.          Returns         -------         fitness : float             The penalized fitness of the program.
Get a random subtree from the program.          Parameters         ----------         random_state : RandomState instance             The random number generator.          program : list, optional (default=None)             The flattened tree representation of the program. If None, the             embedded tree in the object will be used.          Returns         -------         start, end : tuple of two ints             The indices of the start and end of the random subtree.
Perform the crossover genetic operation on the program.          Crossover selects a random subtree from the embedded program to be         replaced. A donor also has a subtree selected at random and this is         inserted into the original parent to form an offspring.          Parameters         ----------         donor : list             The flattened tree representation of the donor program.          random_state : RandomState instance             The random number generator.          Returns         -------         program : list             The flattened tree representation of the program.
Perform the subtree mutation operation on the program.          Subtree mutation selects a random subtree from the embedded program to         be replaced. A donor subtree is generated at random and this is         inserted into the original parent to form an offspring. This         implementation uses the "headless chicken" method where the donor         subtree is grown using the initialization methods and a subtree of it         is selected to be donated to the parent.          Parameters         ----------         random_state : RandomState instance             The random number generator.          Returns         -------         program : list             The flattened tree representation of the program.
Perform the hoist mutation operation on the program.          Hoist mutation selects a random subtree from the embedded program to         be replaced. A random subtree of that subtree is then selected and this         is 'hoisted' into the original subtrees location to form an offspring.         This method helps to control bloat.          Parameters         ----------         random_state : RandomState instance             The random number generator.          Returns         -------         program : list             The flattened tree representation of the program.
Perform the point mutation operation on the program.          Point mutation selects random nodes from the embedded program to be         replaced. Terminals are replaced by other terminals and functions are         replaced by other functions that require the same number of arguments         as the original node. The resulting tree forms an offspring.          Parameters         ----------         random_state : RandomState instance             The random number generator.          Returns         -------         program : list             The flattened tree representation of the program.
Make a function node, a representation of a mathematical relationship.      This factory function creates a function node, one of the core nodes in any     program. The resulting object is able to be called with NumPy vectorized     arguments and return a resulting vector based on a mathematical     relationship.      Parameters     ----------     function : callable         A function with signature `function(x1, *args)` that returns a Numpy         array of the same shape as its arguments.      name : str         The name for the function as it should be represented in the program         and its visualizations.      arity : int         The number of arguments that the `function` takes.
Closure of division (x1/x2) for zero denominator.
Closure of log for zero arguments.
Closure of log for zero arguments.
segment smallest regions by the algorithm of Felzenswalb and         Huttenlocher
calculate the sum of histogram intersection of colour
calculate the sum of histogram intersection of texture
calculate the fill similarity over the image
calculate colour histogram for each region          the size of output histogram will be BINS * COLOUR_CHANNELS(3)          number of bins is 25 as same as [uijlings_ijcv2013_draft.pdf]          extract HSV
calculate texture gradient for entire image          The original SelectiveSearch algorithm proposed Gaussian derivative         for 8 orientations, but we use LBP instead.          output will be [height(*)][width(*)]
calculate texture histogram for each region          calculate the histogram of gradient for each colours         the size of output histogram will be             BINS * ORIENTATIONS * COLOUR_CHANNELS(3)
Selective Search      Parameters     ----------         im_orig : ndarray             Input image         scale : int             Free parameter. Higher means larger clusters in felzenszwalb segmentation.         sigma : float             Width of Gaussian kernel for felzenszwalb segmentation.         min_size : int             Minimum component size for felzenszwalb segmentation.     Returns     -------         img : ndarray             image with region label             region label is stored in the 4th value of each pixel [r,g,b,(region)]         regions : array of dict             [                 {                     'rect': (left, top, width, height),                     'labels': [...],                     'size': component_size                 },                 ...             ]
Reset the scene ready for playing.          :param old_scene: The previous version of this Scene that was running before the             application reset - e.g. due to a screen resize.         :param screen: New screen to use if old_scene is not None.
Add an effect to the Scene.          This method can be called at any time - even when playing the Scene.  The default logic         assumes that the Effect needs to be reset before being displayed.  This can be overridden         using the `reset` parameter.          :param effect: The Effect to be added.         :param reset: Whether to reset the Effect that has just been added.
Process a new input event.          This method will pass the event on to any Effects in reverse Z order so that the         top-most Effect has priority.          :param event: The Event that has been triggered.         :returns: None if the Scene processed the event, else the original event.
Catmull-Rom cubic spline to interpolate 4 given points.      :param t: Time index through the spline (must be 0-1).     :param p0: The previous point in the curve (for continuity).     :param p1: The first point to interpolate.     :param p2: The second point to interpolate.     :param p3: The last point to interpolate.
:return: The next position tuple (x, y) for the Sprite on this path.
Add a step to the end of the current recorded path.          :param pos: The position tuple (x, y) to add to the list.
Wait at the current location for the specified number of iterations.          :param delay: The time to wait (in animation frames).
Move straight to the newly specified location - i.e. create a straight         line Path from the current location to the specified point.          :param x:  X coord for the end position.         :param y: Y coord for the end position.         :param steps: How many steps to take for the move.
Follow a path pre-defined by a set of at least 4 points.  This Path will         interpolate the points into a curve and follow that curve.          :param points: The list of points that defines the path.         :param steps: The number of steps to take to follow the path.
Reset the Path for use next time.
Enforce a displayed piece of text to be a certain number of cells wide.  This takes into     account double-width characters used in CJK languages.      :param text: The text to be truncated     :param width: The screen cell width to enforce     :return: The resulting truncated text
Find the starting point in the string that will reduce it to be less than or equal to the     specified width when displayed on screen.      :param text: The text to analyze.     :param max_width: The required maximum width     :param at_end: At the end of the editable line, so allow spaced for cursor.      :return: The offset within `text` to start at to reduce it to the required length.
Find the character offset within some text for a given visible offset (taking into account the     fact that some character glyphs are double width).      :param text: The text to analyze     :param visible_width: The required location within that text (as seen on screen).     :return: The offset within text (as a character offset within the string).
Split text to required dimensions.      This will first try to split the text into multiple lines, then put a "..." on the last     3 characters of the last line if this still doesn't fit.      :param text: The text to split.     :param width: The maximum width for any line.     :param height: The maximum height for the resulting text.     :return: A list of strings of the broken up text.
Get current position for scroll bar.
Set current position for scroll bar.
Add a Layout to the Frame.          :param layout: The Layout to be added.
Add an Effect to the Frame.          :param effect: The Effect to be added.
Fix the layouts and calculate the locations of all the widgets.          This function should be called once all Layouts have been added to the Frame and all         widgets added to the Layouts.
Clear the current canvas.
Pick a palette from the list of supported THEMES.          :param theme: The name of the theme to set.
The widget that currently has the focus within this Frame.
The number of frames before this Effect should be updated.
Look for a widget with a specified name.          :param name: The name to search for.          :returns: The widget that matches or None if one couldn't be found.
Create a clone of this Frame into a new Screen.          :param _: ignored.         :param scene: The new Scene object to clone into.
Save the current values in all the widgets back to the persistent data storage.          :param validate: Whether to validate the data before saving.          Calling this while setting the `data` field (e.g. in a widget callback) will have no         effect.          When validating data, it can throw an Exception for any
Switch focus to the specified widget.          :param layout: The layout that owns the widget.         :param column: The column the widget is in.         :param widget: The index of the widget to take the focus.
Make the specified location visible.  This is typically used by a widget to scroll the         canvas such that it is visible.          :param x: The x location to make visible.         :param y: The y location to make visible.         :param h: The height of the location to make visible.
Rebase the coordinates of the passed event to frame-relative coordinates.          :param event: The event to be rebased.         :returns: A new event object appropriately re-based.
The number of frames before this Layout should be updated.
Register the Frame that owns this Widget.          :param frame: The owning Frame.
Add a widget to this Layout.          If you are adding this Widget to the Layout dynamically after starting to play the Scene,         don't forget to ensure that the value is explicitly set before the next update.          :param widget: The widget to be added.         :param column: The column within the widget for this widget.  Defaults to zero.
Call this to give this Layout the input focus.          :param force_first: Optional parameter to force focus to first widget.         :param force_last: Optional parameter to force focus to last widget.         :param force_column: Optional parameter to mandate the new column index.         :param force_widget: Optional parameter to mandate the new widget index.          The force_column and force_widget parameters must both be set together or they will         otherwise be ignored.          :raises IndexError: if a force option specifies a bad column or widget, or if the whole             Layout is readonly.
Call this to take the input focus from this Layout.
Fix the location and size of all the Widgets in this Layout.          :param start_x: The start column for the Layout.         :param start_y: The start line for the Layout.         :param max_width: Max width to allow this layout.         :param max_height: Max height to allow this layout.         :returns: The next line to be used for any further Layouts.
Find the next widget to get the focus, stopping at the start/end of the list if hit.          :param direction: The direction to move through the widgets.         :param stay_in_col: Whether to limit search to current column.         :param start_at: Optional starting point in current column.         :param wrap: Whether to wrap around columns when at the end.
Process any input event.          :param event: The event that was triggered.         :param hover_focus: Whether to trigger focus change on mouse moves.         :returns: None if the Effect processed the event, else the original event.
Redraw the widgets inside this Layout.          :param frame_no: The current frame to be drawn.
Save the current values in all the widgets back to the persistent data storage.          :param validate: whether to validate the saved data or not.         :raises: InvalidFields if any invalid data is found.
Look for a widget with a specified name.          :param name: The name to search for.          :returns: The widget that matches or None if one couldn't be found.
Reset the values for any Widgets in this Layout based on the current Frame data store.          :param new_frame: optional old Frame - used when cloning scenes.
Reset this Layout and the Widgets it contains.
Register the Frame that owns this Widget.          :param frame: The owning Frame.
Set the size and position of the Widget.          This should not be called directly.  It is used by the :py:obj:`.Layout` class to arrange         all widgets within the Frame.          :param x: The x position of the widget.         :param y: The y position of the widget.         :param offset: The allowed label size for the widget.         :param w: The width of the widget.         :param h: The height of the widget.
Return the absolute location of this widget on the Screen, taking into account the         current state of the Frame that is displaying it and any label offsets of the Widget.          :returns: A tuple of the form (<X coordinate>, <Y coordinate>).
Call this to give this Widget the input focus.
Check if the specified mouse event is over this widget.          :param event: The MouseEvent to check.         :param include_label: Include space reserved for the label when checking.         :param width_modifier: Adjustement to width (e.g. for scroll bars).         :returns: True if the mouse is over the active parts of the widget.
Draw the label for this widget if needed.
Draw a flashing cursor for this widget.          :param char: The character to use for the cursor (when not a block)         :param frame_no: The current frame number.         :param x: The x coordinate for the cursor.         :param y: The y coordinate for the cursor.
Pick the rendering colour for a widget based on the current state.          :param palette_name: The stem name for the widget - e.g. "button".         :param selected: Whether this item is selected or not.         :param allow_input_state: Whether to allow input state (e.g. focus) to affect result.         :returns: A colour palette key to be used.
Pick the rendering colour for a widget based on the current state.          :param palette_name: The stem name for the widget - e.g. "button".         :param selected: Whether this item is selected or not.         :returns: A colour tuple (fg, attr, bg) to be used.
Move the cursor up/down the specified number of lines.          :param delta: The number of lines to move (-ve is up, +ve is down).
The text as should be formatted on the screen.          This is an array of tuples of the form (text, value line, value column offset) where         the line and column offsets are indeces into the value (not displayed glyph coordinates).
Add or remove a scrollbar from this listbox based on height and available options.          :param width: Width of the Listbox         :param height: Height of the Listbox.         :param dy: Vertical offset from top of widget.
Get current position for scroll bar.
Set current position for scroll bar.
Helper function to figure out the actual column width from the various options.          :param width: The size of column requested         :param max_width: The maximum width allowed for this widget.         :return: the integer width of the column in characters
Internal function to handle directory traversal or bubble notifications up to user of the         Widget as needed.
Populate the current multi-column list with the contents of the selected directory.          :param value: The new value to use.
Create a clone of this Dialog into a new Screen.          :param screen: The new Screen object to clone into.         :param scene: The new Scene object to clone into.
Close this temporary pop-up.          :param cancelled: Whether the pop-up was cancelled (e.g. by pressing Esc).
Draw the scroll bar.
Check whether a MouseEvent is over thus scroll bar.          :param event: The MouseEvent to check.          :returns: True if the mouse event is over the scroll bar.
Handle input on the scroll bar.          :param event: the event to be processed.          :returns: True if the scroll bar handled the event.
:param mem: An integer number of bytes to convert to human-readable form.     :return: A human-readable string representation of the number.
:param stamp: A floating point number representing the POSIX file timestamp.     :return: A short human-readable string representation of the timestamp.
Convert any images into a more Screen-friendly format.
:return: An iterator of all the images in the Renderer.
:return: The next image and colour map in the sequence as a tuple.
:return: The max height of the rendered text (across all images if an             animated renderer).
:return: The max width of the rendered text (across all images if an             animated renderer).
Clear the current image.
Write some text to the specified location in the current image.          :param text: The text to be added.         :param x: The X coordinate in the image.         :param y: The Y coordinate in the image.         :param colour: The colour of the text to add.         :param attr: The attribute of the image.         :param bg: The background colour of the text to add.
Process the animation effect for the specified frame number.          :param frame_no: The index of the frame being generated.
Pick a random location for the star making sure it does         not overwrite an existing piece of text.
Draw the star.
Randomly create a new column once this one is finished.
Update that trail!          :param reseed: Whether we are in the normal reseed cycle or not.
Returns the last position of this Sprite as a tuple         (x, y, width, height).
Check whether this Sprite overlaps another.          :param other: The other Sprite to check for an overlap.         :param use_new_pos: Whether to use latest position (due to recent             update).  Defaults to False.         :returns: True if the two Sprites overlap.
Randomly create a new snowflake once this one is finished.
Update that snowflake!          :param reseed: Whether we are in the normal reseed cycle or not.
Clear the double-buffer.          This does not clear the screen buffer and so the next call to deltas will still show all changes.          :param fg: The foreground colour to use for the new buffer.         :param attr: The attribute value to use for the new buffer.         :param bg: The background colour to use for the new buffer.
Set the cell value from the specified location          :param x: The column (x coord) of the character.         :param y: The row (y coord) of the character.         :param value: A 5-tuple of (unicode, foreground, attributes, background, width).
Return a list-like (i.e. iterable) object of (y, x) tuples
Scroll the window up or down.          :param lines: Number of lines to scroll.  Negative numbers move the buffer up.
Copy a buffer entirely to this double buffer.          :param buffer: The double buffer to copy         :param x: The X origin for where to place it in this buffer         :param y: The Y origin for where to place it in this buffer
Provide a slice of data from the buffer at the specified location          :param x: The X origin         :param y: The Y origin         :param width: The width of slice required         :return: The slice of tuples from the current double-buffer
Clear the current double-buffer used by this object.          :param fg: The foreground colour to use for the new buffer.         :param attr: The attribute value to use for the new buffer.         :param bg: The background colour to use for the new buffer.
Reset the internal buffers for the abstract canvas.
Scroll the abstract canvas to make a specific line.          :param line: The line to scroll to.
Get the character at the specified location.          :param x: The column (x coord) of the character.         :param y: The row (y coord) of the character.          :return: A 4-tuple of (ascii code, foreground, attributes, background)                  for the character at the location.
Print the text at the specified location using the specified colour and attributes.          :param text: The (single line) text to be printed.         :param x: The column (x coord) for the start of the text.         :param y: The line (y coord) for the start of the text.         :param colour: The colour of the text to be displayed.         :param attr: The cell attribute of the text to be displayed.         :param bg: The background colour of the text to be displayed.         :param transparent: Whether to print spaces or not, thus giving a             transparent effect.          The colours and attributes are the COLOUR_xxx and A_yyy constants         defined in the Screen class.
Copy a buffer to the screen double buffer at a specified location.          :param buffer: The double buffer to copy         :param x: The X origin for where to place it in the Screen         :param y: The Y origin for where to place it in the Screen
Centre the text on the specified line (y) using the optional colour and attributes.          :param text: The (single line) text to be printed.         :param y: The line (y coord) for the start of the text.         :param colour: The colour of the text to be displayed.         :param attr: The cell attribute of the text to be displayed.         :param colour_map: Colour/attribute list for multi-colour text.          The colours and attributes are the COLOUR_xxx and A_yyy constants         defined in the Screen class.
Paint multi-colour text at the defined location.          :param text: The (single line) text to be printed.         :param x: The column (x coord) for the start of the text.         :param y: The line (y coord) for the start of the text.         :param colour: The default colour of the text to be displayed.         :param attr: The default cell attribute of the text to be displayed.         :param bg: The default background colour of the text to be displayed.         :param transparent: Whether to print spaces or not, thus giving a             transparent effect.         :param colour_map: Colour/attribute list for multi-colour text.          The colours and attributes are the COLOUR_xxx and A_yyy constants         defined in the Screen class.         colour_map is a list of tuples (foreground, attribute, background) that         must be the same length as the passed in text (or None if no mapping is         required).
Blend the new colour with the old according to the ratio.          :param new: The new colour (or None if not required).         :param old: The old colour.         :param ratio: The ratio to blend new and old         :returns: the new colour index to use for the required blend.
Highlight a specified section of the screen.          :param x: The column (x coord) for the start of the highlight.         :param y: The line (y coord) for the start of the highlight.         :param w: The width of the highlight (in characters).         :param h: The height of the highlight (in characters).         :param fg: The foreground colour of the highlight.         :param bg: The background colour of the highlight.         :param blend: How much (as a percentage) to take of the new colour             when blending.          The colours and attributes are the COLOUR_xxx and A_yyy constants         defined in the Screen class.  If fg or bg are None that means don't         change the foreground/background as appropriate.
Return whether the specified location is on the visible screen.          :param x: The column (x coord) for the location to check.         :param y: The line (y coord) for the location to check.
Move the drawing cursor to the specified position.          :param x: The column (x coord) for the location to check.         :param y: The line (y coord) for the location to check.
Draw a line from drawing cursor to the specified position.          This uses a modified Bressenham algorithm, interpolating twice as many points to         render down to anti-aliased characters when no character is specified,         or uses standard algorithm plotting with the specified character.          :param x: The column (x coord) for the location to check.         :param y: The line (y coord) for the location to check.         :param char: Optional character to use to draw the line.         :param colour: Optional colour for plotting the line.         :param bg: Optional background colour for plotting the line.         :param thin: Optional width of anti-aliased line.
Draw a filled polygon.          This function uses the scan line algorithm to create the polygon.  See         https://www.cs.uic.edu/~jbell/CourseNotes/ComputerGraphics/PolygonFilling.html for details.          :param polygons: A list of polygons (which are each a list of (x,y) coordinates for the             points of the polygon) - i.e. nested list of 2-tuples.         :param colour: The foreground colour to use for the polygon         :param bg: The background colour to use for the polygon
Flush the canvas content to the underlying screen.
Construct a new Screen for any platform.  This will just create the         correct Screen object for your environment.  See :py:meth:`.wrapper` for         a function to create and tidy up once you've finished with the Screen.          :param height: The buffer height for this window (for testing only).         :param catch_interrupt: Whether to catch and prevent keyboard             interrupts.  Defaults to False to maintain backwards compatibility.         :param unicode_aware: Whether the application can use unicode or not.             If None, try to detect from the environment if UTF-8 is enabled.
Construct a new Screen for any platform.  This will initialize the         Screen, call the specified function and then tidy up the system as         required when the function exits.          :param func: The function to call once the Screen has been created.         :param height: The buffer height for this Screen (only for test purposes).         :param catch_interrupt: Whether to catch and prevent keyboard             interrupts.  Defaults to False to maintain backwards compatibility.         :param arguments: Optional arguments list to pass to func (after the             Screen object).         :param unicode_aware: Whether the application can use unicode or not.             If None, try to detect from the environment if UTF-8 is enabled.
Reset the Screen.
Refresh the screen.
Clear the Screen of all content.          Note that this will instantly clear the Screen and reset all buffers to the default state,         without waiting for you to call :py:meth:`~.Screen.refresh`.
Check for a key without waiting.  This method is deprecated.  Use         :py:meth:`.get_event` instead.
Calculate the control code for a given key.  For example, this converts         "a" to 1 (which is the code for ctrl-a).          :param char: The key to convert to a control code.         :return: The control code as an integer or None if unknown.
Print text at the specified location.  This method is deprecated.  Use         :py:meth:`.print_at` instead.          :param text: The (single line) text to be printed.         :param x: The column (x coord) for the start of the text.         :param y: The line (y coord) for the start of the text.         :param colour: The colour of the text to be displayed.         :param attr: The cell attribute of the text to be displayed.         :param bg: The background colour of the text to be displayed.         :param transparent: Whether to print spaces or not, thus giving a             transparent effect.
Default unhandled event handler for handling simple scene navigation.
Play a set of scenes.          This is effectively a helper function to wrap :py:meth:`.set_scenes` and         :py:meth:`.draw_next_frame` to simplify animation for most applications.          :param scenes: a list of :py:obj:`.Scene` objects to play.         :param stop_on_resize: Whether to stop when the screen is resized.             Default is to carry on regardless - which will typically result             in an error. This is largely done for back-compatibility.         :param unhandled_input: Function to call for any input not handled             by the Scenes/Effects being played.  Defaults to a function that             closes the application on "Q" or "X" being pressed.         :param start_scene: The old Scene to start from.  This must have name             that matches the name of one of the Scenes passed in.         :param repeat: Whether to repeat the Scenes once it has reached the end.             Defaults to True.         :param allow_int: Allow input to interrupt frame rate delay.          :raises ResizeScreenError: if the screen is resized (and allowed by             stop_on_resize).          The unhandled input function just takes one parameter - the input         event that was not handled.
Remember a set of scenes to be played.  This must be called before         using :py:meth:`.draw_next_frame`.          :param scenes: a list of :py:obj:`.Scene` objects to play.         :param unhandled_input: Function to call for any input not handled             by the Scenes/Effects being played.  Defaults to a function that             closes the application on "Q" or "X" being pressed.         :param start_scene: The old Scene to start from.  This must have name             that matches the name of one of the Scenes passed in.          :raises ResizeScreenError: if the screen is resized (and allowed by             stop_on_resize).          The unhandled input function just takes one parameter - the input         event that was not handled.
Draw the next frame in the currently configured Scenes. You must call         :py:meth:`.set_scenes` before using this for the first time.          :param repeat: Whether to repeat the Scenes once it has reached the end.             Defaults to True.          :raises StopApplication: if the application should be terminated.
Default next character implementation - linear progression through         each character.
Default next colour implementation - linear progression through         each colour tuple.
The set of attributes for this particle for the next frame to be         rendered.          :returns: A tuple of (character, x, y, fg, attribute, bg)
Helper function to find an existing colour in the particle palette.
The function to draw a new frame for the particle system.
Convert from tile coordinates to "pixels" - i.e. text characters.
Convert from latitude to the y position in overall map.
Shift the latitude by the required number of pixels (i.e. text lines).
Load up a single satellite image tile.
Load up a single vector tile.
Background thread to download map tiles as required.
Decide which layers to render based on current zoom level and view type.
Helper to draw lines connecting a set of nodes that are scaled for the Screen.
Draw a set of polygons from a vector tile.
Draw a set of lines from a vector tile.
Draw a single feature from a layer in a vector tile.
Draw the visible geometry in the specified map tile.
Draw a satellite image tile to screen.
Render all visible tiles a layer at a time.
Animate the zoom in/out as appropriate for the displayed map tile.
Animate movement to desired location on map.
Draw the latest set of tiles to the Screen.
User input for the main map view.
Set a new desired location entered in the pop-up form.
Sign a request URL with a Crypto Key.         Usage:         from urlsigner import sign_url         signed_url = sign_url(base_url=my_url,                               params=url_params,                               client_secret=CLIENT_SECRET)         Args:         base_url - The trunk of the URL to sign. E.g. https://maps.googleapis.com/maps/api/geocode/json         params - List of tuples of URL parameters INCLUDING YOUR CLIENT ID ('client','gme-...')         client_secret - Your Crypto Key from Google for Work         Returns:         The signature as a dictionary #signed request URL
Geocode an arbitrary number of strings from Command Line.
Will be overridden according to the targetted web service
Changed: removed check on number of elements:             - totalResultsCount not sytematically returned (e.g in hierarchy)             - done in base.py
Signs a request url with a security key.
Parse the raw JSON with all attributes/methods defined in the class, except for the             ones defined starting with '_' or flagged in cls._TO_EXCLUDE.              The final result is stored in self.json
Helper function to validate that URLs are well formed, i.e that it contains a valid             protocol and a valid domain. It does not actually check if the URL exists
- Query self.url (validated cls._URL)             - Analyse reponse and set status, errors accordingly             - On success:                   returns the content of the response as a JSON object                  This object will be passed to self._parse_json_response
Check if specific URL has not been provided, otherwise, use cls._URL
Calculate the great-circle distance bewteen two points on the Earth surface.      :input: two 2-tuples, containing the latitude and longitude of each point     in decimal degrees.      Example: haversine((45.7597, 4.8422), (48.8567, 2.3508))      :output: Returns the distance bewteen the two points.     The default unit is kilometers. Miles can be returned     if the ``miles`` parameter is set to True.
Get Geocode      :param ``location``: Your search location you want geocoded.     :param ``provider``: The geocoding engine you want to use.      :param ``method``: Define the method (geocode, method).
Reverse Geocoding      :param ``location``: Your search location you want to reverse geocode.     :param ``key``: (optional) use your own API Key from Bing.     :param ``provider``: (default=google) Use the following:         > google         > bing
Length in Feet (f)
Square Foot Area (sqft)
Length in Feet (f)
Returns dict with gif informations from the API.
Returns complete html tag string.
Doing the regex parsing and running the create_html function.
Update modification and creation times from git
Add sha metadata to content
Convert a str to object supporting buffer API and update a hash with it.
Add git based permalink id to content metadata
Sets user specified MathJax settings (see README for more details)
Ensures summaries are not cut off. Also inserts     mathjax script so that math will be rendered
Instructs Typogrify to ignore math tags - which allows Typogrify     to play nicely with math related content
Load the mathjax script template from file, and render with the settings
Instantiates a customized markdown extension for handling mathjax     related content
Setup math for RST
Loads the mathjax script according to the settings.     Instantiate the Python markdown extension, passing in the mathjax     script as config parameter.
Adds mathjax script for reStructuredText
Ensure mathjax script is applied to RST and summaries are     corrected if specified in user settings.      Handles content attached to ArticleGenerator and PageGenerator objects,     since the plugin doesn't know how to handle other Generator types.      For reStructuredText content, examine both articles and pages.     If article or page is reStructuredText and there is math present,     append the mathjax script.      Also process summaries if present (only applies to articles)     and user wants summaries processed (via user settings)
Method to get permalink ids from content. To be bound to the class last     thing.
Get just path component of permalink.
Add permalink methods to object
Setup context
Generate redirect files
Get all the text associated with this node.        With recursive == True, all text from child nodes is retrieved.
Helper function to add timezone information to datetime,     so that datetime is comparable to other datetime objects in recent versions     that now also have timezone information.
Get photo informations from flickr api.
Url for direct jpg use.
Returns html code.
Create a gzip cache file for every file that a webserver would     reasonably want to cache (e.g., text type files).      :param pelican: The Pelican instance
Create a gzipped file in the same directory with a filepath.gz name.      :param filepath: A file to compress     :param overwrite: Whether the original file should be overwritten
Separates out <div class="math"> from the parent tag <p>. Anything         in between is put into its own parent tag of <p>
Searches for <div class="math"> that are children in <p> tags and corrects         the invalid HTML that results
A function to read through each page and post as it comes through from Pelican, find all instances of triple-backtick (```...```) code blocks, and add an HTML wrapper to each line of each of those code blocks
Get all commits involving this filename         :returns: List of commits newest to oldest
Get oldest commit involving this file          :returns: Oldest commit
Get oldest commit involving this file          :returns: Newest commit
Get the original filename of this content. Implies follow
Get datetime of oldest commit involving this file          :returns: Datetime of oldest commit
Get datetime of newest commit involving this file          :returns: Datetime of newest commit
From a given Org text, return the header separate from the content.         The given text must be separate line by line and be a list.         The return is a list of two items: header and content.         Theses two items are text separate line by line in format of a list         Keyword Arguments:         text_lines -- A list, each item is a line of the texte         Return:         [           header   -- A list, each item is a line of the texte           content  -- A list, each item is a line of the texte         ]
From a given Org text, return the metadatas          Keyword Arguments:         text_lines -- A list, each item is a line of the texte         Return:         A dict containing metadatas
Parse content and metadata of Org files         Keyword Arguments:         source_path -- Path to the Org file to parse
This function creates an inline console input block as defined in the twitter bootstrap documentation         overrides the default behaviour of the kbd role          *usage:*             :kbd:`<your code>`          *Example:*              :kbd:`<section>`          This code is not highlighted
This function defines a glyph inline role that show a glyph icon from the          twitter bootstrap framework          *Usage:*              :glyph:`<glyph_name>`          *Example:*              Love this music :glyph:`music` :)          Can be subclassed to include a target          *Example:*              .. role:: story_time_glyph(glyph)                 :target: http://www.youtube.com/watch?v=5g8ykQLYnX0                 :class: small text-info              Love this music :story_time_glyph:`music` :)
Process the metadata dict, lowercasing the keys and textilizing the value of the 'summary' key (if present).  Keys that share the same lowercased form will be overridden in some arbitrary order.
Parse content and metadata of textile files.
Optimized jpg and png images      :param pelican: The Pelican instance
Check if the name is a type of file that should be optimized.     And optimizes it if required.      :param dirpath: Path of the file to be optimzed     :param name: A file name to be optimized
Article generator connector for the Libravatar plugin
Generate diagramm and return data
Blockdiag parser
registered handler for the github activity plugin         it puts in generator.context the html needed to be displayed on a         template
Plugin registration
returns a list of html snippets fetched from github actitivy feed
Parse content and metadata of an rdf file
Adds a function/method to an object. Uses the name of the first argument as a hint about whether it is a method (``self``), class method (``cls`` or ``klass``), or static method (anything else). Works on both instances and classes.  >>> class color: ... def __init__(self, r, g, b): ... self.r, self.g, self.b = r, g, b >>> c = color(0, 1, 0) >>> c # doctest: +ELLIPSIS <__main__.color instance at ...> >>> @magic_set(color) ... def __repr__(self): ... return '<color %s %s %s>' % (self.r, self.g, self.b) >>> c <color 0 1 0> >>> @magic_set(color) ... def red(cls): ... return cls(1, 0, 0) >>> color.red() <color 1 0 0> >>> c.red() <color 1 0 0> >>> @magic_set(color) ... def name(): ... return 'color' >>> color.name() 'color' >>> @magic_set(c) ... def name(self): ... return 'red' >>> c.name() 'red' >>> @magic_set(c) ... def name(cls): ... return cls.__name__ >>> c.name() 'color' >>> @magic_set(c) ... def pr(obj): ... print obj >>> c.pr(1) 1
Return content from local or remote file.
This hook is invoked before the generator's .categories property is        filled in. Each article has already been assigned a category        object, but these objects are _not_ unique per category and so are        not safe to tack metadata onto (as is).         The category metadata we're looking for is represented as an        Article object, one per directory, whose filename is 'index.ext'.
Prepare configurations for the MD plugin
Minify CSS and JS with YUI Compressor       :param pelican: The Pelican instance
function to insert an html element into another html fragment     example:         html = '<p>paragraph1</p><p>paragraph2...</p>'         element = '<a href="/read-more/">read more</a>'         ---> '<p>paragraph1</p><p>paragraph2...<a href="/read-more/">read more</a></p>'
Insert an inline "read more" link into the last element of the summary     :param instance:     :return:
Validate a generated HTML file     :param pelican: pelican object
Use W3C validator service: https://bitbucket.org/nmb10/py_w3c/ .     :param filename: the filename to validate
Decorator to register a new include tag
Get appropriate wrapper factory and cache instance for path
:param path: Path to check         :returns: True if path is managed by git
Get all commits including a given path following renames
Get all commits including path          :param path: Path which we will find commits for         :param bool follow: If True we will follow path through renames          :returns: Sequence of commit objects. Newest to oldest
Get datetime of commit comitted_date
Enable code to run in a context with a temporary locale      Resets the locale back when exiting context.     Can set a temporary locale if provided
Initialize internal DBs using the Pelican settings dict      This clears the DBs for e.g. autoreload mode to work
Prepare overrides and create _SITE_DB      _SITE_DB.keys() need to be ready for filter_translations
Get relative path from siteurl of lang to siteurl of base_lang      the output is cached in _SITES_RELPATH_DB
Transform an Article to Draft
Filter the content and translations lists of a generator      Filters out         1) translations which will be generated in a different site         2) content that is not in the language of the currently         generated site but in that of a different site, content in a         language which has no site is generated always. The filtering         method bay be modified by the respective untranslated policy
Install gettext translations in the jinja2.Environment      Only if the 'jinja2.ext.i18n' jinja2 extension is enabled     the translations for the current DEFAULT_LANG are installed.
Adds useful iterable variables to template context
Link content to translations in their main language      so the URL (including localized month names) of the different subsites     will be honored
Make translations link to the native locations      for generators that may contain translated content
For all contents removed from generation queue update interlinks      link to the native location
Add links to static files in the main site if necessary
Update the context of all generators      Ads useful variables and translations into the template context     and interlink translations
Get the Pelican class requested in settings
Create the next subsite using the lang-specific config      If there are no more subsites in the generation queue, update all     the generators (interlink translations and removed content, add     variables and translations to template context). Otherwise get the     language and overrides for next the subsite in the queue and apply     overrides.  Then generate the subsite using a PELICAN_CLASS     instance and its run method. Finally, restore the previous locale.
Register the plugin only if required signals are available
Iterator over lists of content translations
Iterator over pairs of normal and hidden contents
Get the policy for untranslated content
Iterator over all contents
Include a file as part of the content of this reST file.
Parse content and metadata of markdown files
Parse a timestamp string in format "YYYY-MM-DD HH:MM"      :returns: datetime
Parse a timedelta string in format [<num><multiplier> ]*     e.g. 2h 30m      :returns: timedelta
Collect articles metadata to be used for building the event calendar      :returns: None
Generate an iCalendar file
Generates localized events dict if i18n_subsites plugin is active
Populate the event_list variable to be used in jinja templates
Plugin registration.
Processes each article/page object and formulates copy from and copy     to destinations, as well as adding a source file URL as an attribute.
A very rough and ready copy from / to function.
Calls the shots, based on signals
Parse content and metadata of creole files
Thumbnail URL generator for Vimeo videos.
render identicon to PIL.Image          @param size identicon patchsize. (image size is 3 * [size])         @return PIL.Image
Make posts on reddit if it's not a draft, on whatever subs are specified
this is a hack to make sure the reddit object keeps track of a session     trough article scanning, speeding up networking as the connection can be      kept alive.
create a url and call make posts (which has less information)
Reduces Opacity.      Returns an image with reduced opacity.     Taken from http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/362879
Runs generator on both pages and articles.
Uses the new style of registration based on GitHub Pelican issue #314.
Add Webassets to Jinja2 extensions in Pelican settings.
Define the assets environment and pass it to the generator.
Plugin registration.
Runs pygal programs and returns image data
Simple pygal parser
Resize a directory tree full of images into thumbnails      :param pelican: The pelican instance     :return: None
Expand a gallery tag to include all of the files in a specific directory under IMAGE_PATH      :param pelican: The pelican instance     :return: None
Given a filename, resize and save the image per the specification into out_path          :param in_path: path to image file to save.  Must be supported by PIL         :param out_path: path to the directory root for the outputted thumbnails to be stored         :return: None
Runs graphviz programs and returns image data          Copied from https://github.com/tkf/ipython-hierarchymagic/blob/master/hierarchymagic.py
Simple Graphviz parser
Sets user specified settings (see README for more details)
Instantiates a customized Markdown extension
Assembles articles and pages into lists based on each     article or page's content. These lists are available     through the global context passed to the template engine.      When multiple categories are present, splits category names     based on commas and trims whitespace surrounding a     category's name. Thus, commas may not appear within a category     but they can be used to delimit categories and may be surrounded by     arbitrary amounts of whitespace.      For each category, substitutes '_' for all whitespace and '-'     characters, then creates a list named `SUBSTITUTED_CATEGORY_NAME`_articles     or `SUBSTITUTED_CATEGORY_NAME`_pages for Articles or Pages,     respectively.      Note that the *original* category name must appear in the     `CATEGORIES_TO_COLLATE` when using this plugin with category     filtering enabled.
Returns whether a request has import permission.
Prepares/returns kwargs used when initializing Resource
Perform the actual import action (after the user has confirmed the import)
Prepare kwargs for import_data.
Perform a dry_run of the import to make sure the import will not         result in errors.  If there where no error, save the user         uploaded file to a local temp file that will be used by         'process_import' for the actual import.
Returns whether a request has export permission.
Returns export queryset.          Default implementation respects applied search and filters.
Returns file_format representation for given queryset.
Exports the selected rows using file_format.
Factory for creating ``ModelResource`` class for given Django model.
Returns the field name for a given field.
Either fetches an already existing instance or initializes a new one.
Takes any validation errors that were raised by         :meth:`~import_export.resources.Resource.import_obj`, and combines them         with validation errors raised by the instance's ``full_clean()``         method. The combined errors are then re-raised as single, multi-field         ValidationError.          If the ``clean_model_instances`` option is False, the instances's         ``full_clean()`` method is not called, and only the errors raised by         ``import_obj()`` are re-raised.
Takes care of saving the object to the database.          Keep in mind that this is done by calling ``instance.save()``, so         objects are not created in bulk!
Calls :meth:`instance.delete` as long as ``dry_run`` is not set.
Calls :meth:`import_export.fields.Field.save` if ``Field.attribute``         and ``Field.column_name`` are found in ``data``.
Traverses every field in this Resource and calls         :meth:`~import_export.resources.Resource.import_field`. If         ``import_field()`` results in a ``ValueError`` being raised for         one of more fields, those errors are captured and reraised as a single,         multi-field ValidationError.
Saves m2m fields.          Model instance need to have a primary key value before         a many-to-many relationship can be used.
Returns ``True`` if ``row`` importing should be skipped.          Default implementation returns ``False`` unless skip_unchanged == True.         Override this method to handle skipping rows meeting certain         conditions.          Use ``super`` if you want to preserve default handling while overriding         ::             class YourResource(ModelResource):                 def skip_row(self, instance, original):                     # Add code here                     return super(YourResource, self).skip_row(instance, original)
Imports data from ``tablib.Dataset``. Refer to :doc:`import_workflow`         for a more complete description of the whole import process.          :param row: A ``dict`` of the row to import          :param instance_loader: The instance loader to be used to load the row          :param using_transactions: If ``using_transactions`` is set, a transaction             is being used to wrap the import          :param dry_run: If ``dry_run`` is set, or error occurs, transaction             will be rolled back.
Imports data from ``tablib.Dataset``. Refer to :doc:`import_workflow`         for a more complete description of the whole import process.          :param dataset: A ``tablib.Dataset``          :param raise_errors: Whether errors should be printed to the end user             or raised regularly.          :param use_transactions: If ``True`` the import process will be processed             inside a transaction.          :param collect_failed_rows: If ``True`` the import process will collect             failed rows.          :param dry_run: If ``dry_run`` is set, or an error occurs, if a transaction             is being used, it will be rolled back.
Exports a resource.
Prepare widget for m2m field
Prepare widget for fk and o2o fields
Returns the widget that would likely be associated with each         Django type.          Includes mapping of Postgres Array and JSON fields. In the case that         psycopg2 is not installed, we consume the error and process the field         regardless.
Returns widget kwargs for given field_name.
Returns a Resource Field instance for the given Django model field.
Reset the SQL sequences after new objects are imported
Translates the value stored in the imported datasource to an         appropriate Python object and returns it.
Returns the value of the object's attribute.
If this field is not declared readonly, the object's attribute will         be set to the value returned by :meth:`~import_export.fields.Field.clean`.
Returns value from the provided object converted to export         representation.
Create dataset from first sheet.
Create dataset from first sheet.
Returns a dictionary of field-specific validation errors for this row.
Returns the total number of validation errors for this row.
Returns an ActionForm subclass containing a ChoiceField populated with     the given formats.
Returns a queryset of all objects for this Model.          Overwrite this method if you want to limit the pool of objects from         which the related object is retrieved.          :param value: The field's value in the datasource.         :param row: The datasource's current row.          As an example; if you'd like to have ForeignKeyWidget look up a Person         by their pre- **and** lastname column, you could subclass the widget         like so::              class FullNameForeignKeyWidget(ForeignKeyWidget):                 def get_queryset(self, value, row):                     return self.model.objects.filter(                         first_name__iexact=row["first_name"],                         last_name__iexact=row["last_name"]                     )
Return tmuxp configuration directory.      ``TMUXP_CONFIGDIR`` environmental variable has precedence if set. We also      evaluate XDG default directory from XDG_CONFIG_HOME environmental variable      if set or its default. Then the old default ~/.tmuxp is returned for      compatibility.      Returns     -------     str :         absolute path to tmuxp config directory
Return tmuxinator configuration directory.      Checks for ``TMUXINATOR_CONFIG`` environmental variable.      Returns     -------     str :         absolute path to tmuxinator config directory      See Also     --------     :meth:`tmuxp.config.import_tmuxinator`
Callback wrapper for validating click.prompt input.      Parameters     ----------     options : list         List of allowed choices      Returns     -------     :func:`callable`         callback function for value_proc in :func:`click.prompt`.      Raises     ------     :class:`click.BadParameter`
Set layout hooks to normalize layout.      References:          - tmuxp issue: https://github.com/tmux-python/tmuxp/issues/309         - tmux issue: https://github.com/tmux/tmux/issues/1106      tmux 2.6+ requires that the window be viewed with the client before     select-layout adjustments can take effect.      To handle this, this function creates temporary hook for this session to     iterate through all windows and select the layout.      In order for layout changes to take effect, a client must at the very     least be attached to the window (not just the session).      hook_name is provided to allow this to set to multiple scenarios, such     as 'client-attached' (which the user attaches the session). You may     also want 'after-switch-client' for cases where the user loads tmuxp     sessions inside tmux since tmuxp offers to switch for them.      Also, the hooks are set immediately unbind after they're invoked via -u.      Parameters     ----------     session : :class:`libtmux.session.Session`         session to bind hook to     hook_name : str         hook name to bind to, e.g. 'client-attached'
Return True if path is a name and not a file path.      Parameters     ----------     path : str         Path (can be absolute, relative, etc.)      Returns     -------     bool         True if path is a name of config in config dir, not file path.
Validate / translate config name/path values for click config arg.      Wrapper on top of :func:`cli.scan_config`.
Return the real config path or raise an exception.      If config is directory, scan for .tmuxp.{yaml,yml,json} in directory. If     one or more found, it will warn and pick the first.      If config is ".", "./" or None, it will scan current directory.      If config is has no path and only a filename, e.g. "myconfig.yaml" it will     search config dir.      If config has no path and only a name with no extension, e.g. "myconfig",     it will scan for file name with yaml, yml and json. If multiple exist, it     will warn and pick the first.      Parameters     ----------     config : str         config file, valid examples:          - a file name, myconfig.yaml         - relative path, ../config.yaml or ../project         - a period, .      Raises     ------     :class:`click.exceptions.FileError`
Load a tmux "workspace" session via tmuxp file.      Parameters     ----------     config_file : str         absolute path to config file     socket_name : str, optional         ``tmux -L <socket-name>``     socket_path: str, optional         ``tmux -S <socket-path>``     colors : str, optional         '-2'             Force tmux to support 256 colors     detached : bool         Force detached state. default False.     answer_yes : bool         Assume yes when given prompt. default False.      Notes     -----      tmuxp will check and load a configuration file. The file will use kaptan     to load a JSON/YAML into a :py:obj:`dict`. Then :func:`config.expand` and     :func:`config.trickle` will be used to expand any shorthands, template     variables, or file paths relative to where the config/script is executed     from.      :func:`config.expand` accepts the directory of the config file, so the     user's configuration can resolve absolute paths relative to where the     config file is. In otherwords, if a config file at */var/moo/hi.yaml*     has *./* in its configs, we want to be sure any file path with *./* is     relative to */var/moo*, not the user's PWD.      A :class:`libtmux.Server` object is created. No tmux server is started yet,     just the object.      The prepared configuration and the server object is passed into an instance     of :class:`~tmuxp.workspacebuilder.WorkspaceBuilder`.      A sanity check against :meth:`libtmux.common.which` is ran. It will raise     an exception if tmux isn't found.      If a tmux session under the same name as ``session_name`` in the tmuxp     configuration exists, tmuxp offers to attach the session. Currently, tmuxp     does not allow appending a workspace / incremental building on top of a     current session (pull requests are welcome!).      :meth:`~tmuxp.workspacebuilder.WorkspaceBuilder.build` will build the session in     the background via using tmux's detached state (``-d``).      After the session (workspace) is built, unless the user decided to load     the session in the background via ``tmuxp -d`` (which is in the spirit     of tmux's ``-d``), we need to prompt the user to attach the session.      If the user is already inside a tmux client, which we detect via the     ``TMUX`` environment variable bring present, we will prompt the user to     switch their current client to it.      If they're outside of tmux client - in a plain-old PTY - we will     automatically ``attach``.      If an exception is raised during the building of the workspace, tmuxp will     prompt to cleanup (``$ tmux kill-session``) the session on the user's     behalf. An exception raised during this process means it's not easy to     predict how broken the session is.      .. versionchanged:: tmux 2.6+          In tmux 2.6, the way layout and proportion's work when interfacing         with tmux in a detached state (outside of a client) changed. Since         tmuxp builds workspaces in a detached state, the WorkspaceBuilder isn't         able to rely on functionality requiring awarness of session geometry,         e.g. ``set-layout``.          Thankfully, tmux is able to defer commands to run after the user         performs certain actions, such as loading a client via         ``attach-session`` or ``switch-client``.          Upon client switch, ``client-session-changed`` is triggered [1]_.      References     ----------     .. [1] cmd-switch-client.c hook. GitHub repo for tmux.        https://github.com/tmux/tmux/blob/2.6/cmd-switch-client.c#L132.        Accessed April 8th, 2018.
Manage tmux sessions.      Pass the "--help" argument to any command to see detailed help.     See detailed documentation and examples at:     http://tmuxp.readthedocs.io/en/latest/
Setup logging for CLI use.      Tries to do some conditionals to prevent handlers from being added twice.     Just to be safe.      Parameters     ----------     logger : :py:class:`Logger`         logger instance for tmuxp
Snapshot a session into a config.      If SESSION_NAME is provided, snapshot that session. Otherwise, use the     current session.
Load a tmux workspace from each CONFIG.      CONFIG is a specifier for a configuration file.      If CONFIG is a path to a directory, tmuxp will search it for     ".tmuxp.{yaml,yml,json}".      If CONFIG is has no directory component and only a filename, e.g.     "myconfig.yaml", tmuxp will search the users's config directory for that     file.      If CONFIG has no directory component, and only a name with no extension,     e.g. "myconfig", tmuxp will search the users's config directory for any     file with the extension ".yaml", ".yml", or ".json" that matches that name.      If multiple configuration files that match a given CONFIG are found, tmuxp     will warn and pick the first one found.      If multiple CONFIGs are provided, workspaces will be created for all of     them. The last one provided will be attached. The others will be created in     detached mode.
Convert a tmuxp config between JSON and YAML.
Freeze live tmux session and Return session config :py:obj:`dict`.      Parameters     ----------     session : :class:`libtmux.Session`         session object      Returns     -------     dict         tmuxp compatible workspace config
Build tmux workspace in session.          Optionally accepts ``session`` to build with only session object.          Without ``session``, it will use :class:`libmtux.Server` at         ``self.server`` passed in on initialization to create a new Session         object.          Parameters         ----------         session : :class:`libtmux.Session`             session to build workspace in
Return :class:`libtmux.Window` iterating through session config dict.          Generator yielding :class:`libtmux.Window` by iterating through         ``sconf['windows']``.          Applies ``window_options`` to window.          Parameters         ----------         session : :class:`libtmux.Session`             session to create windows in          Returns         -------         tuple of (:class:`libtmux.Window`, ``wconf``)             Newly created window, and the section from the tmuxp configuration             that was used to create the window.
Return :class:`libtmux.Pane` iterating through window config dict.          Run ``shell_command`` with ``$ tmux send-keys``.          Parameters         ----------         w : :class:`libtmux.Window`             window to create panes for         wconf : dict             config section for window          Returns         -------         tuple of (:class:`libtmux.Pane`, ``pconf``)             Newly created pane, and the section from the tmuxp configuration             that was used to create the pane.
Actions to apply to window after window and pane finished.          When building a tmux session, sometimes its easier to postpone things         like setting options until after things are already structurally         prepared.          Parameters         ----------         w : :class:`libtmux.Window`             window to create panes for         wconf : dict             config section for window
Return the prefix for the log message. Template for Formatter.      Parameters     ----------     record :  :py:class:`logging.LogRecord`         This is passed in from inside the :py:meth:`logging.Formatter.format`         record.      Returns     -------     str         Log template.
Return True if config schema is correct.      Parameters     ----------     sconf : dict         session configuration      Returns     -------     bool
Return True if file has a valid config file type.      Parameters     ----------     filename : str         filename to check (e.g. ``mysession.json``).     extensions : str or list         filetypes to check (e.g. ``['.yaml', '.json']``).      Returns     -------     bool
Return a list of configs in ``config_dir``.      Parameters     ----------     config_dir : str         directory to search     extensions : list         filetypes to check (e.g. ``['.yaml', '.json']``).      Returns     -------     list
Return list of configs in current working directory.      If filename is ``.tmuxp.py``, ``.tmuxp.json``, ``.tmuxp.yaml``.      Returns     -------     list         configs in current working directory
Return config in inline form, opposite of :meth:`config.expand`.      Parameters     ----------     sconf : dict      Returns     -------     dict         configuration with optional inlined configs.
Return config with shorthand and inline properties expanded.      This is necessary to keep the code in the :class:`WorkspaceBuilder` clean     and also allow for neat, short-hand configurations.      As a simple example, internally, tmuxp expects that config options     like ``shell_command`` are a list (array)::          'shell_command': ['htop']      tmuxp configs allow for it to be simply a string::          'shell_command': 'htop'      Kaptan will load JSON/YAML files into python dicts for you.      Parameters     ----------     sconf : dict         the configuration for the session     cwd : str         directory to expand relative paths against. should be the dir of the         config directory.     parent : str         (used on recursive entries) start_directory of parent window or session         object.      Returns     -------     dict
Return a dict with "trickled down" / inherited config values.      This will only work if config has been expanded to full form with     :meth:`config.expand`.      tmuxp allows certain commands to be default at the session, window     level. shell_command_before trickles down and prepends the     ``shell_command`` for the pane.      Parameters     ----------     sconf : dict         the session configuration.      Returns     -------     dict
Return tmuxp config from a `tmuxinator`_ yaml config.      .. _tmuxinator: https://github.com/aziz/tmuxinator      Parameters     ----------     sconf : dict         python dict for session configuration.      Returns     -------     dict
Return tmuxp config from a `teamocil`_ yaml config.      .. _teamocil: https://github.com/remiprev/teamocil      Parameters     ----------     sconf : dict         python dict for session configuration      Notes     -----      Todos:      - change  'root' to a cd or start_directory     - width in pane -> main-pain-width     - with_env_var     - clear     - cmd_separator
Function to wrap try/except for subprocess.check_call().
Give warning and offer to fix ``DISABLE_AUTO_TITLE``.      see: https://github.com/robbyrussell/oh-my-zsh/pull/257
Install handlers for Mitogen loggers to redirect them into the Ansible     display framework. Ansible installs its own logging framework handlers when     C.DEFAULT_LOG_PATH is set, therefore disable propagation for our handlers.
Return a dict representing the differences between the dicts `old` and     `new`. Deleted keys appear as a key with the value :data:`None`, added and     changed keys appear as a key with the new value.
Vanilla Ansible local commands execute with an environment inherited         from WorkerProcess, we must emulate that.
Add a :class:`mitogen.core.Receiver`, :class:`Select` or         :class:`mitogen.core.Latch` to the select.          :raises mitogen.select.Error:             An attempt was made to add a :class:`Select` to which this select             is indirectly a member of.
Remove an object from from the select. Note that if the receiver has         notified prior to :meth:`remove`, it will still be returned by a         subsequent :meth:`get`. This may change in a future version.
Remove the select's notifier function from each registered receiver,         mark the associated latch as closed, and cause any thread currently         sleeping in :meth:`get` to be woken with         :class:`mitogen.core.LatchError`.          This is necessary to prevent memory leaks in long-running receivers. It         is called automatically when the Python :keyword:`with` statement is         used.
Call `get_event(timeout, block)` returning :attr:`Event.data` of the         first available event.
Fetch the next available :class:`Event` from any source, or raise         :class:`mitogen.core.TimeoutError` if no value is available within         `timeout` seconds.          On success, the message's :attr:`receiver         <mitogen.core.Message.receiver>` attribute is set to the receiver.          :param float timeout:             Timeout in seconds.         :param bool block:             If :data:`False`, immediately raise             :class:`mitogen.core.TimeoutError` if the select is empty.         :return:             :class:`Event`.         :raises mitogen.core.TimeoutError:             Timeout was reached.         :raises mitogen.core.LatchError:             :meth:`close` has been called, and the underlying latch is no             longer valid.
Return a set of paths from which Python imports the standard library.
Return :data:`True` if `modname` appears to come from the standard     library.
Return the suffixes of submodules directly neated beneath of the package     directory at `path`.      :param str path:         Path to the module's source code on disk, or some PEP-302-recognized         equivalent. Usually this is the module's ``__file__`` attribute, but         is specified explicitly to avoid loading the module.      :return:         List of submodule name suffixes.
Master version of parent.get_core_source().
Given a code object `co`, scan its bytecode yielding any ``IMPORT_NAME``     and associated prior ``LOAD_CONST`` instructions representing an `Import`     statement or `ImportFrom` statement.      :return:         Generator producing `(level, modname, namelist)` tuples, where:          * `level`: -1 for normal import, 0, for absolute import, and >0 for           relative import.         * `modname`: Name of module to import, or from where `namelist` names           are imported.         * `namelist`: for `ImportFrom`, the list of names to be imported from           `modname`.
If we have forked since the watch dictionaries were initialized, all         that has is garbage, so clear it.
Return :data:`True` if the (possibly extensionless) file at `path`         resembles a Python script. For now we simply verify the file contains         ASCII text.
Recent versions of Python 3.x introduced an incomplete notion of         importer specs, and in doing so created permanent asymmetry in the         :mod:`pkgutil` interface handling for the `__main__` module. Therefore         we must handle `__main__` specially.
Attempt to fetch source code via pkgutil. In an ideal world, this would         be the only required implementation of get_module().
Attempt to fetch source code via sys.modules. This is specifically to         support __main__, but it may catch a few more cases.
Attempt to fetch source code by examining the module's (hopefully less         insane) parent package. Required for older versions of         ansible.compat.six and plumbum.colors.
Explicitly install a source cache entry, preventing usual lookup         methods from being used.          Beware the value of `path` is critical when `is_pkg` is specified,         since it directs where submodules are searched for.          :param str fullname:             Name of the module to override.         :param str path:             Module's path as it will appear in the cache.         :param bytes source:             Module source code as a bytestring.         :param bool is_pkg:             :data:`True` if the module is a package.
Given the name of a loaded module `fullname`, attempt to find its         source code.          :returns:             Tuple of `(module path, source text, is package?)`, or :data:`None`             if the source cannot be found.
Given an ImportFrom AST node, guess the prefix that should be tacked         on to an alias name to produce a canonical name. `fullname` is the name         of the module in which the ImportFrom appears.
Return a list of non-stdlib modules that are directly imported by         `fullname`, plus their parents.          The list is determined by retrieving the source code of         `fullname`, compiling it, and examining all IMPORT_NAME ops.          :param fullname: Fully qualified name of an _already imported_ module             for which source code can be retrieved         :type fullname: str
Return a list of non-stdlib modules that are imported directly or         indirectly by `fullname`, plus their parents.          This method is like :py:meth:`find_related_imports`, but also         recursively searches any modules which are imported by `fullname`.          :param fullname: Fully qualified name of an _already imported_ module             for which source code can be retrieved         :type fullname: str
See :meth:`ModuleFinder.add_source_override.
Given the source for the __main__ module, try to find where it         begins conditional execution based on a "if __name__ == '__main__'"         guard, and remove any code after that point.
Return performance data for the module responder.          :returns:              Dict containing keys:              * `get_module_count`: Integer count of               :data:`mitogen.core.GET_MODULE` messages received.             * `get_module_secs`: Floating point total seconds spent servicing               :data:`mitogen.core.GET_MODULE` requests.             * `good_load_module_count`: Integer count of successful               :data:`mitogen.core.LOAD_MODULE` messages sent.             * `good_load_module_size`: Integer total bytes sent in               :data:`mitogen.core.LOAD_MODULE` message payloads.             * `bad_load_module_count`: Integer count of negative               :data:`mitogen.core.LOAD_MODULE` messages sent.             * `minify_secs`: CPU seconds spent minifying modules marked                minify-safe.
Arrange for a unique context ID to be allocated and associated with a         route leading to the active context. In masters, the ID is generated         directly, in children it is forwarded to the master via a         :data:`mitogen.core.ALLOCATE_ID` message.
Used by :mod:`mitogen.core` and :mod:`mitogen.service` to automatically     register every broker and pool on Python 2.4/2.5.
Construct a socketpair, saving one side of it, and passing the other to         `obj` to be written to by one of its threads.
Pause until the socket `rsock` indicates readability, due to         :meth:`_do_cork` triggering a blocking write on another thread.
Arrange for any associated brokers and pools to be paused with no locks         held. This will not return until each thread acknowledges it has ceased         execution.
Reconstruct a Module's canonical path by recursing through its parents.
Compile and return a Module's code object.
Return a Module instance describing the first matching module found on the     search path.      :param str name:         Module name.     :param list path:         List of directory names to search for the module.     :param Module parent:         Optional module parent.
Executed on the main thread of the Python interpreter running on each     target machine, Context.call() from the master. It simply sends the output     of the UNIX 'ps' command at regular intervals toward a Receiver on master.          :param mitogen.core.Sender sender:         The Sender to use for delivering our result. This could target         anywhere, but the sender supplied by the master simply causes results         to be delivered to the master's associated per-host Receiver.
Loop until CTRL+C is pressed, waiting for the next result delivered by the     Select. Use parse_output() to turn that result ('ps' command output) into     rich data, and finally repaint the screen if the repaint delay has passed.
Main program entry point. @mitogen.main() is just a helper to handle     reliable setup/destruction of Broker, Router and the logging package.
Return a unique string representation of a dict as quickly as possible.     Used to generated deduplication keys from a request.
Return a reference, forcing close and discard of the underlying         connection. Used for 'meta: reset_connection' or when some other error         is detected.
Return a reference, making it eligable for recycling once its reference         count reaches zero.
Reply to every waiting request matching a configuration key with a         response dictionary, deleting the list of waiters when done.          :param str key:             Result of :meth:`key_from_dict`         :param dict response:             Response dictionary         :returns:             Number of waiters that were replied to.
Arrange for `context` to be shut down, and optionally add `new_context`         to the LRU list while holding the lock.
Update the LRU ("MRU"?) list associated with the connection described         by `kwargs`, destroying the most recently created context if the list         is full. Finally add `new_context` to the list.
For testing, return a list of dicts describing every currently         connected context.
For testing use, arrange for all connections to be shut down.
Respond to Context disconnect event by deleting any record of the no         longer reachable context.  This method runs in the Broker thread and         must not to block.
Actual connect implementation. Arranges for the Mitogen connection to         be created and enqueues an asynchronous call to start the forked task         parent in the remote context.          :param key:             Deduplication key representing the connection configuration.         :param spec:             Connection specification.         :returns:             Dict like::                  {                     'context': mitogen.core.Context or None,                     'via': mitogen.core.Context or None,                     'init_child_result': {                         'fork_context': mitogen.core.Context,                         'home_dir': str or None,                     },                     'msg': str or None                 }              Where `context` is a reference to the newly constructed context,             `init_child_result` is the result of executing             :func:`ansible_mitogen.target.init_child` in that context, `msg` is             an error message and the remaining fields are :data:`None`, or             `msg` is :data:`None` and the remaining fields are set.
Return a Context referring to an established connection with the given         configuration, establishing new connections as necessary.          :param list stack:             Connection descriptions. Each element is a dict containing 'method'             and 'kwargs' keys describing the Router method and arguments.             Subsequent elements are proxied via the previous.          :returns dict:             * context: mitogen.parent.Context or None.             * init_child_result: Result of :func:`init_child`.             * msg: StreamError exception text or None.             * method_name: string failing method name.
(str|unicode).(partition|rpartition) for Python 2.4/2.5.
Policy function for use with :class:`Receiver` and     :meth:`Router.add_handler` that requires incoming messages to originate     from a parent context, or on a :class:`Stream` whose :attr:`auth_id     <Stream.auth_id>` has been set to that of a parent context or the current     context.
Arrange for `func(*args, **kwargs)` to be invoked when the named signal is     fired by `obj`.
Arrange for `func(*args, **kwargs)` to be invoked for every function     registered for the named signal on `obj`.
Return :data:`True` if `fullname` is part of a blacklisted package, or if     any packages have been whitelisted and `fullname` is not part of one.      NB:       - If a package is on both lists, then it is treated as blacklisted.       - If any package is whitelisted, then all non-whitelisted packages are         treated as blacklisted.
Set the file descriptor `fd` to automatically close on     :func:`os.execve`. This has no effect on file descriptors inherited across     :func:`os.fork`, they must be explicitly closed through some other means,     such as :func:`mitogen.fork.on_fork`.
Inverse of :func:`set_nonblock`, i.e. cause `fd` to block the thread     when the underlying kernel buffer is exhausted.
Wrap `func(*args)` that may raise :class:`select.error`,     :class:`IOError`, or :class:`OSError`, trapping UNIX error codes relating     to disconnection and retry events in various subsystems:      * When a signal is delivered to the process on Python 2, system call retry       is signalled through :data:`errno.EINTR`. The invocation is automatically       restarted.     * When performing IO against a TTY, disconnection of the remote end is       signalled by :data:`errno.EIO`.     * When performing IO against a socket, disconnection of the remote end is       signalled by :data:`errno.ECONNRESET`.     * When performing IO against a pipe, disconnection of the remote end is       signalled by :data:`errno.EPIPE`.      :returns:         Tuple of `(return_value, disconnect_reason)`, where `return_value` is         the return value of `func(*args)`, and `disconnected` is an exception         instance when disconnection was detected, otherwise :data:`None`.
Return the class implementing `module_name.class_name` or raise         `StreamError` if the module is not whitelisted.
Syntax helper to construct a dead message.
Construct a pickled message, setting :attr:`data` to the serialization         of `obj`, and setting remaining fields using `kwargs`.          :returns:             The new message.
Compose a reply to this message and send it using :attr:`router`, or         `router` is :attr:`router` is :data:`None`.          :param obj:             Either a :class:`Message`, or an object to be serialized in order             to construct a new message.         :param router:             Optional router to use if :attr:`router` is :data:`None`.         :param kwargs:             Optional keyword parameters overriding message fields in the reply.
Unpickle :attr:`data`, optionally raising any exceptions present.          :param bool throw_dead:             If :data:`True`, raise exceptions, otherwise it is the caller's             responsibility.          :raises CallError:             The serialized data contained CallError exception.         :raises ChannelError:             The `is_dead` field was set.
Send `data` to the remote end.
Send a dead message to the remote, causing :meth:`ChannelError` to be         raised in any waiting thread.
Callback registered for the handle with :class:`Router`; appends data         to the internal queue.
Unregister the receiver's handle from its associated router, and cause         :class:`ChannelError` to be raised in any thread waiting in :meth:`get`         on this receiver.
Sleep waiting for a message to arrive on this receiver.          :param float timeout:             If not :data:`None`, specifies a timeout in seconds.          :raises mitogen.core.ChannelError:             The remote end indicated the channel should be closed,             communication with it was lost, or :meth:`close` was called in the             local process.          :raises mitogen.core.TimeoutError:             Timeout was reached.          :returns:             :class:`Message` that was received.
The Python 2.4 linecache module, used to fetch source code for         tracebacks and :func:`inspect.getsource`, does not support PEP-302,         meaning it needs extra help to for Mitogen-loaded modules. Directly         populate its cache if a loaded module belongs to the Mitogen package.
#305: during startup :class:`LogHandler` may be installed before it is         possible to route messages, therefore messages are buffered until         :meth:`uncork` is called by :class:`ExternalContext`.
Call :func:`os.close` on :attr:`fd` if it is not :data:`None`,         then set it to :data:`None`.
Read up to `n` bytes from the file descriptor, wrapping the underlying         :func:`os.read` call with :func:`io_op` to trap common disconnection         conditions.          :meth:`read` always behaves as if it is reading from a regular UNIX         file; socket, pipe, and TTY disconnection errors are masked and result         in a 0-sized read like a regular file.          :returns:             Bytes read, or the empty to string to indicate disconnection was             detected.
Write as much of the bytes from `s` as possible to the file descriptor,         wrapping the underlying :func:`os.write` call with :func:`io_op` to         trap common disconnection conditions.          :returns:             Number of bytes written, or :data:`None` if disconnection was             detected.
Called by :meth:`Broker.shutdown` to allow the stream time to         gracefully shutdown. The base implementation simply called         :meth:`on_disconnect`.
Called by :class:`Broker` to force disconnect the stream. The base         implementation simply closes :attr:`receive_side` and         :attr:`transmit_side` and unregisters the stream from the broker.
Handle the next complete message on the stream. Raise         :class:`StreamError` on failure.
Transmit buffered messages.
Send `data` to `handle`, and tell the broker we have output. May         be called from any thread.
Arrange for `msg` to be delivered to this context, with replies         directed to a newly constructed receiver. :attr:`dst_id         <Message.dst_id>` is set to the target context ID, and :attr:`reply_to         <Message.reply_to>` is set to the newly constructed receiver's handle.          :param bool persist:             If :data:`False`, the handler will be unregistered after a single             message has been received.          :param mitogen.core.Message msg:             The message.          :returns:             :class:`Receiver` configured to receive any replies sent to the             message's `reply_to` handle.
Arrange for `msg` to be delivered to this context. :attr:`dst_id         <Message.dst_id>` is set to the target context ID.          :param Message msg:             Message.
Like :meth:`send_async`, but expect a single reply (`persist=False`)         delivered within `deadline` seconds.          :param mitogen.core.Message msg:             The message.         :param float deadline:             If not :data:`None`, seconds before timing out waiting for a reply.         :returns:             Deserialized reply.         :raises TimeoutError:             No message was received and `deadline` passed.
Return a list of `(fd, data)` tuples for every FD registered for         receive readiness.
Return a list of `(fd, data)` tuples for every FD registered for         transmit readiness.
Cause :meth:`poll` to yield `data` when `fd` is readable.
Stop yielding readability events for `fd`.          Redundant calls to :meth:`stop_receive` are silently ignored, this may         change in future.
Cause :meth:`poll` to yield `data` when `fd` is writeable.
Stop yielding writeability events for `fd`.          Redundant calls to :meth:`stop_transmit` are silently ignored, this may         change in future.
Block the calling thread until one or more FDs are ready for IO.          :param float timeout:             If not :data:`None`, seconds to wait without an event before             returning an empty iterable.         :returns:             Iterable of `data` elements associated with ready FDs.
Clean up any files belonging to the parent process after a fork.
Mark the latch as closed, and cause every sleeping thread to be woken,         with :class:`mitogen.core.LatchError` raised in each thread.
Return :data:`True` if calling :meth:`get` would block.          As with :class:`Queue.Queue`, :data:`True` may be returned even         though a subsequent call to :meth:`get` will succeed, since a         message may be posted at any moment between :meth:`empty` and         :meth:`get`.          As with :class:`Queue.Queue`, :data:`False` may be returned even         though a subsequent call to :meth:`get` will block, since another         waiting thread may be woken at any moment between :meth:`empty` and         :meth:`get`.          :raises LatchError:             The latch has already been marked closed.
Return an unused socketpair, creating one if none exist.
Return a string encoding the ID of the process, instance and thread.         This disambiguates legitimate wake-ups, accidental writes to the FD,         and buggy internal FD sharing.
Return the next enqueued object, or sleep waiting for one.          :param float timeout:             If not :data:`None`, specifies a timeout in seconds.          :param bool block:             If :data:`False`, immediately raise             :class:`mitogen.core.TimeoutError` if the latch is empty.          :raises mitogen.core.LatchError:             :meth:`close` has been called, and the object is no longer valid.          :raises mitogen.core.TimeoutError:             Timeout was reached.          :returns:             The de-queued object.
When a result is not immediately available, sleep waiting for         :meth:`put` to write a byte to our socket pair.
Enqueue an object, waking the first thread waiting for a result, if one         exists.          :param obj:             Object to enqueue. Defaults to :data:`None` as a convenience when             using :class:`Latch` only for synchronization.         :raises mitogen.core.LatchError:             :meth:`close` has been called, and the object is no longer valid.
Prevent immediate Broker shutdown while deferred functions remain.
Drain the pipe and fire callbacks. Since :attr:`_deferred` is         synchronized, :meth:`defer` and :meth:`on_receive` can conspire to         ensure only one byte needs to be pending regardless of queue length.
Wake the multiplexer by writing a byte. If Broker is midway through         teardown, the FD may already be closed, so ignore EBADF.
Arrange for `func()` to execute on the broker thread. This function         returns immediately without waiting the result of `func()`. Use         :meth:`defer_sync` to block until a result is available.          :raises mitogen.core.Error:             :meth:`defer` was called after :class:`Broker` has begun shutdown.
Shut down the write end of the logging socket.
This is done in the :class:`Router` constructor for historical reasons.         It must be called before ExternalContext logs its first messages, but         after logging has been setup. It must also be called when any router is         constructed for a consumer app.
Stub :data:`DEL_ROUTE` handler; fires 'disconnect' events on the         corresponding :attr:`_context_by_id` member. This is replaced by         :class:`mitogen.parent.RouteMonitor` in an upgraded context.
Return a :class:`Context` referring to the current process.
Messy factory/lookup function to find a context by its ID, or construct         it. This will eventually be replaced by a more sensible interface.
Register a newly constructed context and its associated stream, and add         the stream's receive side to the I/O multiplexer. This method remains         public while the design has not yet settled.
Return the :class:`Stream` that should be used to communicate with         `dst_id`. If a specific route for `dst_id` is not known, a reference to         the parent context's stream is returned.
Remove the handle registered for `handle`          :raises KeyError:             The handle wasn't registered.
Invoke `fn(msg)` on the :class:`Broker` thread for each Message sent to         `handle` from this context. Unregister after one invocation if         `persist` is :data:`False`. If `handle` is :data:`None`, a new handle         is allocated and returned.          :param int handle:             If not :data:`None`, an explicit handle to register, usually one of             the ``mitogen.core.*`` constants. If unspecified, a new unused             handle will be allocated.          :param bool persist:             If :data:`False`, the handler will be unregistered after a single             message has been received.          :param Context respondent:             Context that messages to this handle are expected to be sent from.             If specified, arranges for a dead message to be delivered to `fn`             when disconnection of the context is detected.              In future `respondent` will likely also be used to prevent other             contexts from sending messages to the handle.          :param function policy:             Function invoked as `policy(msg, stream)` where `msg` is a             :class:`mitogen.core.Message` about to be delivered, and `stream`             is the :class:`mitogen.core.Stream` on which it was received. The             function must return :data:`True`, otherwise an error is logged and             delivery is refused.              Two built-in policy functions exist:              * :func:`has_parent_authority`: requires the message arrived from a               parent context, or a context acting with a parent context's               authority (``auth_id``).              * :func:`mitogen.parent.is_immediate_child`: requires the               message arrived from an immediately connected child, for use in               messaging patterns where either something becomes buggy or               insecure by permitting indirect upstream communication.              In case of refusal, and the message's ``reply_to`` field is             nonzero, a :class:`mitogen.core.CallError` is delivered to the             sender indicating refusal occurred.          :param bool overwrite:             If :data:`True`, allow existing handles to be silently overwritten.          :return:             `handle`, or if `handle` was :data:`None`, the newly allocated             handle.         :raises Error:             Attemp to register handle that was already registered.
Called during :meth:`Broker.shutdown`, informs callbacks registered         with :meth:`add_handle_cb` the connection is dead.
Arrange for `msg` to be forwarded towards its destination. If its         destination is the local context, then arrange for it to be dispatched         using the local handlers.          This is a lower overhead version of :meth:`route` that may only be         called from the :class:`Broker` thread.          :param Stream in_stream:             If not :data:`None`, the stream the message arrived on. Used for             performing source route verification, to ensure sensitive messages             such as ``CALL_FUNCTION`` arrive only from trusted contexts.
Python 2.4/2.5 have grave difficulties with threads/fork. We         mandatorily quiesce all running threads during fork using a         monkey-patch there.
Mark the :attr:`receive_side <Stream.receive_side>` on `stream` as         ready for reading. Safe to call from any thread. When the associated         file descriptor becomes ready for reading,         :meth:`BasicStream.on_receive` will be called.
Mark the :attr:`receive_side <Stream.receive_side>` on `stream` as not         ready for reading. Safe to call from any thread.
Mark the :attr:`transmit_side <Stream.transmit_side>` on `stream` as         ready for writing. Must only be called from the Broker thread. When the         associated file descriptor becomes ready for writing,         :meth:`BasicStream.on_transmit` will be called.
Mark the :attr:`transmit_side <Stream.receive_side>` on `stream` as not         ready for writing.
Return :data:`True` if any reader's :attr:`Side.keep_alive` attribute         is :data:`True`, or any :class:`Context` is still registered that is         not the master. Used to delay shutdown while some important work is in         progress (e.g. log draining).
Arrange for `func()` to execute on :class:`Broker` thread, blocking the         current thread until a result or exception is available.          :returns:             Return value of `func()`.
Call `func(self)`, catching any exception that might occur, logging it,         and force-disconnecting the related `stream`.
Execute a single :class:`Poller` wait, dispatching any IO events that         caused the wait to complete.          :param float timeout:             If not :data:`None`, maximum time in seconds to wait for events.
Forcefully call :meth:`Stream.on_disconnect` on any streams that failed         to shut down gracefully, then discard the :class:`Poller`.
Invoke :meth:`Stream.on_shutdown` for every active stream, then allow         up to :attr:`shutdown_timeout` seconds for the streams to unregister         themselves, logging an error if any did not unregister during the grace         period.
Broker thread main function. Dispatches IO events until         :meth:`shutdown` is called.
Request broker gracefully disconnect streams and stop. Safe to call         from any thread.
Stub service handler. Start a thread to import the mitogen.service         implementation from, and deliver the message to the newly constructed         pool. This must be done as CALL_SERVICE for e.g. PushFileService may         race with a CALL_FUNCTION blocking the main thread waiting for a result         from that service.
Open /dev/null to replace stdin, and stdout/stderr temporarily. In case         of odd startup, assume we may be allocated a standard handle.
Read line chunks from it, either yielding them directly, or building up and     logging individual lines if they look like SSH debug output.      This contains the mess of dealing with both line-oriented input, and partial     lines such as the password prompt.      Yields `(line, partial)` tuples, where `line` is the line, `partial` is     :data:`True` if no terminating newline character was present and no more     data exists in the read buffer. Consuming code can use this to unreliably     detect the presence of an interactive prompt.
Initialize the base class :attr:`create_child` and         :attr:`create_child_args` according to whether we need a PTY or not.
This runs in the target context, it is invoked by _fakessh_main running in     the fakessh context immediately after startup. It starts the slave process     (the the point where it has a stdin_handle to target but not stdout_chan to     write to), and waits for main to.
Run the command specified by `args` such that ``PATH`` searches for SSH by     the command will cause its attempt to use SSH to execute a remote program     to be redirected to use mitogen to execute that program using the context     `dest` instead.      :param list args:         Argument vector.     :param mitogen.core.Context dest:         The destination context to execute the SSH command line in.      :param mitogen.core.Router router:      :param list[str] args:         Command line arguments for local program, e.g.         ``['rsync', '/tmp', 'remote:/tmp']``      :returns:         Exit status of the child process.
issue #400: AWX loads a display callback that suffers from thread-safety     issues. Detect the presence of older AWX versions and patch the bug.
While the mitogen strategy is active, trap action_loader.get() calls,     augmenting any fetched class with ActionModuleMixin, which replaces various     helper methods inherited from ActionBase with implementations that avoid     the use of shell fragments wherever possible.      This is used instead of static subclassing as it generalizes to third party     action modules outside the Ansible tree.
While the strategy is active, rewrite connection_loader.get() calls for     some transports into requests for a compatible Mitogen transport.
While the strategy is active, rewrite connection_loader.get() calls for     some transports into requests for a compatible Mitogen transport.
Install our PluginLoader monkey patches and update global variables         with references to the real functions.
Uninstall the PluginLoader monkey patches.
Add the Mitogen plug-in directories to the ModuleLoader path, avoiding         the need for manual configuration.
Many PluginLoader caches are defective as they are only populated in         the ephemeral WorkerProcess. Touch each plug-in path before forking to         ensure all workers receive a hot cache.
Arrange for a mitogen.master.Router to be available for the duration of         the strategy's real run() method.
Annotate a method to permit access to contexts matching an authorization     policy. The annotation may be specified multiple times. Methods lacking any     authorization policy are not accessible.      ::          @mitogen.service.expose(policy=mitogen.service.AllowParents())         def unsafe_operation(self):             ...      :param mitogen.service.Policy policy:         The policy to require.
Arrange for `func(*args, **kwargs)` to be invoked in the context of a         service pool thread.
Fetch a file from the cache.
One size fits all method to ensure a target context has been preloaded         with a set of small files and Python modules.
Authorize a path for access by children. Repeat calls with the same         path has no effect.          :param str path:             File path.
Authorize a path and any subpaths for access by children. Repeat calls         with the same path has no effect.          :param str path:             File path.
Respond to shutdown by sending close() to every target, allowing their         receive loop to exit and clean up gracefully.
Consider the pending transfers for a stream, pumping new chunks while         the unacknowledged byte count is below :attr:`window_size_bytes`. Must         be called with the FileStreamState lock held.          :param FileStreamState state:             Stream to schedule chunks for.
Return the set of all possible directory prefixes for `path`.         :func:`os.path.abspath` is used to ensure the path is absolute.          :param str path:             The path.         :returns: Set of prefixes.
Start a transfer for a registered path.          :param str path:             File path.         :param mitogen.core.Sender sender:             Sender to receive file data.         :returns:             Dict containing the file metadata:              * ``size``: File size in bytes.             * ``mode``: Integer file mode.             * ``owner``: Owner account name on host machine.             * ``group``: Owner group name on host machine.             * ``mtime``: Floating point modification time.             * ``ctime``: Floating point change time.         :raises Error:             Unregistered path, or Sender did not match requestee context.
Acknowledge bytes received by a transfer target, scheduling new chunks         to keep the window full. This should be called for every chunk received         by the target.
Streamily download a file from the connection multiplexer process in         the controller.          :param mitogen.core.Context context:             Reference to the context hosting the FileService that will be used             to fetch the file.         :param bytes path:             FileService registered name of the input file.         :param bytes out_path:             Name of the output path on the local disk.         :returns:             Tuple of (`ok`, `metadata`), where `ok` is :data:`True` on success,             or :data:`False` if the transfer was interrupted and the output             should be discarded.              `metadata` is a dictionary of file metadata as documented in             :meth:`fetch`.
Rather than statically import every interesting subclass, forcing it all to     be transferred and potentially disrupting the debugged environment,     enumerate only those loaded in memory. Also returns the original class.
Make a trivial single-dispatch generic function
Yields (module_loader, name, ispkg) for all modules recursively     on path, or, if path is None, all accessible modules.      'path' should be either None or a list of paths to look for     modules in.      'prefix' is a string to output on the front of every module name     on output.      Note that this function must import all *packages* (NOT all     modules!) on the given path, in order to access the __path__     attribute to find submodules.      'onerror' is a function which gets called with one argument (the     name of the package which was being imported) if any exception     occurs while trying to import a package.  If no onerror function is     supplied, ImportErrors are caught and ignored, while all other     exceptions are propagated, terminating the search.      Examples:      # list all modules python can access     walk_packages()      # list all submodules of ctypes     walk_packages(ctypes.__path__, ctypes.__name__+'.')
Yields (module_loader, name, ispkg) for all submodules on path,     or, if path is None, all top-level modules on sys.path.      'path' should be either None or a list of paths to look for     modules in.      'prefix' is a string to output on the front of every module name     on output.
Retrieve a PEP 302 importer for the given path item      The returned importer is cached in sys.path_importer_cache     if it was newly created by a path hook.      If there is no importer, a wrapper around the basic import     machinery is returned. This wrapper is never inserted into     the importer cache (None is inserted instead).      The cache (or part of it) can be cleared manually if a     rescan of sys.path_hooks is necessary.
Yield PEP 302 importers for the given module name      If fullname contains a '.', the importers will be for the package     containing fullname, otherwise they will be importers for sys.meta_path,     sys.path, and Python's "classic" import machinery, in that order.  If     the named module is in a package, that package is imported as a side     effect of invoking this function.      Non PEP 302 mechanisms (e.g. the Windows registry) used by the     standard import machinery to find files in alternative locations     are partially supported, but are searched AFTER sys.path. Normally,     these locations are searched BEFORE sys.path, preventing sys.path     entries from shadowing them.      For this to cause a visible difference in behaviour, there must     be a module or package name that is accessible via both sys.path     and one of the non PEP 302 file system mechanisms. In this case,     the emulation will find the former version, while the builtin     import mechanism will find the latter.      Items of the following types can be affected by this discrepancy:         imp.C_EXTENSION, imp.PY_SOURCE, imp.PY_COMPILED, imp.PKG_DIRECTORY
Get a PEP 302 "loader" object for module_or_name      If the module or package is accessible via the normal import     mechanism, a wrapper around the relevant part of that machinery     is returned.  Returns None if the module cannot be found or imported.     If the named module is not already imported, its containing package     (if any) is imported, in order to establish the package __path__.      This function uses iter_importers(), and is thus subject to the same     limitations regarding platform-specific special import locations such     as the Windows registry.
Find a PEP 302 "loader" object for fullname      If fullname contains dots, path must be the containing package's __path__.     Returns None if the module cannot be found or imported. This function uses     iter_importers(), and is thus subject to the same limitations regarding     platform-specific special import locations such as the Windows registry.
Extend a package's path.      Intended use is to place the following code in a package's __init__.py:          from pkgutil import extend_path         __path__ = extend_path(__path__, __name__)      This will add to the package's __path__ all subdirectories of     directories on sys.path named after the package.  This is useful     if one wants to distribute different parts of a single logical     package as multiple directories.      It also looks for *.pkg files beginning where * matches the name     argument.  This feature is similar to *.pth files (see site.py),     except that it doesn't special-case lines starting with 'import'.     A *.pkg file is trusted at face value: apart from checking for     duplicates, all entries found in a *.pkg file are added to the     path, regardless of whether they are exist the filesystem.  (This     is a feature.)      If the input path is not a list (as is the case for frozen     packages) it is returned unchanged.  The input path is not     modified; an extended copy is returned.  Items are only appended     to the copy at the end.      It is assumed that sys.path is a sequence.  Items of sys.path that     are not (unicode or 8-bit) strings referring to existing     directories are ignored.  Unicode items of sys.path that cause     errors when used as filenames may cause this function to raise an     exception (in line with os.path.isdir() behavior).
Get a resource from a package.      This is a wrapper round the PEP 302 loader get_data API. The package     argument should be the name of a package, in standard module format     (foo.bar). The resource argument should be in the form of a relative     filename, using '/' as the path separator. The parent directory name '..'     is not allowed, and nor is a rooted name (starting with a '/').      The function returns a binary string, which is the contents of the     specified resource.      For packages located in the filesystem, which have already been imported,     this is the rough equivalent of          d = os.path.dirname(sys.modules[package].__file__)         data = open(os.path.join(d, resource), 'rb').read()      If the package cannot be located or loaded, or it uses a PEP 302 loader     which does not support get_data(), then None is returned.
Find a Planner subclass corresnding to `invocation` and use it to invoke     the module.      :param Invocation invocation:     :returns:         Module return dict.     :raises ansible.errors.AnsibleError:         Unrecognized/unsupported module type.
If :meth:`detect` returned :data:`True`, plan for the module's         execution, including granting access to or delivering any files to it         that are known to be absent, and finally return a dict::              {                 # Name of the class from runners.py that implements the                 # target-side execution of this module type.                 "runner_name": "...",                  # Remaining keys are passed to the constructor of the class                 # named by `runner_name`.             }
Given the original interpreter binary extracted from the script's         interpreter line, look up the associated `ansible_*_interpreter`         variable, render it and return it.          :param str path:             Absolute UNIX path to original interpreter.          :returns:             Shell fragment prefix used to execute the script via "/bin/sh -c".             While `ansible_*_interpreter` documentation suggests shell isn't             involved here, the vanilla implementation uses it and that use is             exploited in common playbooks.
In addition to asynchronous tasks, new-style modules should be forked         if:          * the user specifies mitogen_task_isolation=fork, or         * the new-style module has a custom module search path, or         * the module is known to leak like a sieve.
Get an integer-valued environment variable `key`, if it exists and parses     as an integer, otherwise return `default`.
When debugging and profiling, it is very annoying to poke through the     process list to discover the currently running Ansible and MuxProcess IDs,     especially when trying to catch an issue during early startup. So here, if     a magic environment variable set, stash them in hidden files in the CWD::          alias muxpid="cat .ansible-mux.pid"         alias anspid="cat .ansible-controller.pid"          gdb -p $(muxpid)         perf top -p $(anspid)
Arrange for the subprocess to be started, if it is not already running.          The parent process picks a UNIX socket path the child will use prior to         fork, creates a socketpair used essentially as a semaphore, then blocks         waiting for the child to indicate the UNIX socket is ready for use.          :param bool _init_logging:             For testing, if :data:`False`, don't initialize logging.
The main function of for the mux process: setup the Mitogen broker         thread and ansible_mitogen services, then sleep waiting for the socket         connected to the parent to be closed (indicating the parent has died).
We support serving simplejson for Python 2.4 targets on Ansible 2.3, at         least so the package's own CI Docker scripts can run without external         help, however newer versions of simplejson no longer support Python         2.4. Therefore override any installed/loaded version with a         2.4-compatible version we ship in the compat/ directory.
Configure :class:`mitogen.master.ModuleResponder` to only permit         certain packages, and to generate custom responses for certain modules.
Construct a Router, Broker, and mitogen.unix listener
Construct a ContextService and a thread to service requests for it         arriving from worker processes.
Respond to the broker thread about to exit by sending SIGTERM to         ourself. In future this should gracefully join the pool, but TERM is         fine for now.
Coerce an object to bytes if it is Unicode.
Replace the file descriptor belonging to the file object `fp` with one     open on the same file (`fp.name`), but opened with :py:data:`os.O_RDONLY`.     This enables temporary files to be executed on Linux, which usually throws     ``ETXTBUSY`` if any writeable handle exists pointing to a file passed to     `execve()`.
linux-pam-1.3.1/modules/pam_env/pam_env.c#L207
When a change is detected, remove keys that existed in the old file.
Compare the :func:`os.stat` for the pam_env style environmnt file         `path` with the previous result `old_st`, which may be :data:`None` if         the previous stat attempt failed. Reload its contents if the file has         changed or appeared since last attempt.          :returns:             New :func:`os.stat` result. The new call to :func:`reload_env` should             pass it as the value of `old_st`.
Apply changes from /etc/environment files before creating a         TemporaryEnvironment to snapshot environment state prior to module run.
Set up the process environment in preparation for running an Ansible         module. This monkey-patches the Ansible libraries in various places to         prevent it from trying to kill the process on completion, and to         prevent it from reading sys.stdin.          :returns:             Module result dictionary.
Restore the original :func:`atexit.register`.
Intercept :func:`atexit.register` calls, diverting any to         :func:`shutil.rmtree` into a private list.
Revert changes made by the module to the process environment. This must         always run, as some modules (e.g. git.py) set variables like GIT_SSH         that must be cleared out between runs.
Create a temporary file containing the program code. The code is         fetched via :meth:`_get_program`.
Fetch the module binary from the master if necessary.
Delete the temporary program file.
Create a temporary file containing the module's arguments. The         arguments are formatted via :meth:`_get_args`.
Mutate the source according to the per-task parameters.
Ensure the local importer and PushFileService has everything for the         Ansible module before setup() completes, but before detach() is called         in an asynchronous task.          The master automatically streams modules towards us concurrent to the         runner invocation, however there is no public API to synchronize on the         completion of those preloads. Instead simply reuse the importer's         synchronization mechanism by importing everything the module will need         prior to detaching.
Beginning with Ansible >2.6, some modules (file.py) install a         sys.excepthook which is a closure over AnsibleModule, redirecting the         magical exception to AnsibleModule.fail_json().          For extra special needs bonus points, the class is not defined in         module_utils, but is defined in the module itself, meaning there is no         type for isinstance() that outlasts the invocation.
Mimic the argument formatting behaviour of         ActionBase._execute_module().
Return ContextService arguments for an SSH connection.
Return ContextService arguments for a Docker connection.
Return ContextService arguments for a Kubernetes connection.
Return ContextService arguments for a FreeBSD jail connection.
Return ContextService arguments for an LXC Classic container connection.
Return ContextService arguments for an LXD container connection.
Return ContextService arguments for a mitogen_setns connection.
Return ContextService arguments for su as a become method.
Return ContextService arguments for sudo as a become method.
Return ContextService arguments for su as a first class connection.
Return ContextService arguments for sudo as a first class connection.
Return ContextService arguments for doas as a first class connection.
Like :meth:`mitogen.parent.CallChain.call`, but log timings.