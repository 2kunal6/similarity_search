Wrapper for only doing the rendering on request (drastically reduces memory)
If the next line is an include statement, inserts the contents of the included file into the pending buffer.
Takes a FortranObject and adds it to the appropriate list, if not already present.
Returns the node corresponding to obj. If does not already exist then it will create it.
Adds nodes and edges to the graph as long as the maximum number of nodes is not exceeded. All edges are expected to have a reference to an entry in nodes. If the list of nodes is not added in the first hop due to graph size limitations, they are stored in hopNodes. If the graph was extended the function returns True, otherwise the result will be False.
Adds nodes and edges for generating the graph showing the relationship between modules and submodules listed in nodes.
Adds edges showing dependencies between source files listed in the nodes.
Adds edges indicating the call-tree for the procedures listed in the nodes.
What to return when there's an exception.
Return a timestamp for the provided datestring, described by RFC 7231.
Get the ttl from headers.
Get the wrapped object.
Get the entity that corresponds to URL.
Return (expiration, obj) corresponding to provided url, exercising the cache_policy as necessary.
Return true if the provided URL is allowed to agent.
Return (expiration, Agent) for the robots.txt at the provided URL.
Time this block.
Additional command line arguments for the behave management command
Additional command line arguments extracted directly from behave
Add behave's and our command line arguments to the command
Get a list of those command line arguments specified with the management command that are meant as arguments for running behave.
Apply fixtures that are registered with the @fixtures decorator.
Integrate behave_django in behave via before/after scenario hooks
Patches the context to add utility functions Sets up the base_url, and the get_url() utility function.
Sets up fixtures
Prepare and execute a HTTP POST call to AppCommand.xml end point. Returns XML ElementTree on success and None on fail.
Get status XML via HTTP and return it as XML ElementTree.
Send command via HTTP get to receiver.
Send command via HTTP post to receiver.
Create instances of additional zones for the receiver.
Get the latest status information from device. Method executes the update method for the current receiver type.
Get the latest status information from device. Method queries device via HTTP and updates instance attributes. Returns "True" on success and "False" on fail. This method is for pre 2016 AVR(-X) devices
Get the latest status information from device. Method queries device via HTTP and updates instance attributes. Returns "True" on success and "False" on fail. This method is for AVR-X devices built in 2016 and later.
Update sources list from receiver. Internal method which updates sources list of receiver after getting sources and potential renaming information from receiver.
Get name of receiver from web interface if not set.
Get receivers zone name if not set yet.
Get if sound mode is supported from device. Method executes the method for the current receiver type.
Get if sound mode is supported from device. Method queries device via HTTP. Returns "True" if sound mode supported and "False" if not. This method is for pre 2016 AVR(-X) devices
Get renamed and deleted sources lists from receiver . Internal method which queries device via HTTP to get names of renamed input sources.
Get renamed and deleted sources lists from receiver . Internal method which queries device via HTTP to get names of renamed input sources. In this method AppCommand.xml is used.
Get sources list from receiver. Internal method which queries device via HTTP to get the receiver's input sources. This method also determines the type of the receiver (avr, avr-x, avr-x-2016).
Update media data for playing devices. Internal method which queries device via HTTP to update media information (title, artist, etc.) and URL of cover image.
Get relevant status tags from XML structure with this internal method. Status is saved to internal attributes. Return dictionary of tags not found in XML.
Set input_func of device. Valid values depend on the device and should be taken from "input_func_list". Return "True" on success and "False" on fail.
Set All Zone Stereo option on the device. Calls command to activate/deactivate the mode Return "True" when successfully sent.
Set sound_mode of device. Valid values depend on the device and should be taken from "sound_mode_list". Return "True" on success and "False" on fail.
Set the matching dictionary used to match the raw sound mode.
Construct the sm_match_dict. Reverse the key value structure. The sm_match_dict is bigger, but allows for direct matching using a dictionary key access. The sound_mode_dict is uses externally to set this dictionary because that has a nicer syntax.
Match the raw_sound_mode to its corresponding sound_mode.
Toggle play pause media player.
Send play command to receiver command via HTTP post.
Send pause command to receiver command via HTTP post.
Send previous track command to receiver command via HTTP post.
Turn off receiver via HTTP get command.
Turn off receiver via HTTP get command.
Volume up receiver via HTTP get command.
Volume down receiver via HTTP get command.
Set receiver volume via HTTP get command. Volume is send in a format like -50.0. Minimum is -80.0, maximum at 18.0
Mute receiver via HTTP get command.
Return the matched current sound mode as a string.
Identify DenonAVR using SSDP and SCPD queries. Returns a list of dictionaries which includes all discovered Denon AVR devices with keys "host", "modelName", "friendlyName", "presentationURL".
Send SSDP broadcast message to discover UPnP devices. Returns a list of dictionaries with "address" (IP, PORT) and "URL" of SCPD XML for all discovered devices.
Get and evaluate SCPD XML to identified URLs. Returns dictionary with keys "host", "modelName", "friendlyName" and "presentationURL" if a Denon AVR device was found and "False" if not.
Initialize all discovered Denon AVR receivers in LAN zone. Returns a list of created Denon AVR instances. By default SSDP broadcasts are sent up to 3 times with a 2 seconds timeout.
Parses webvtt and returns timestamps for words and lines Tested on automatically generated subtitles from YouTube
setter for the framerate attribute :param framerate: :return:
Converts the given timecode to frames
Converts frames back to timecode :returns str: the string representation of the current time code
parses timecode string frames '00:00:00:00' or '00:00:00;00' or milliseconds '00:00:00:000'
Get ngrams from a text Sourced from: https://gist.github.com/dannguyen/93c2c43f4e65328b85af
Converts an array of ordered timestamps into an EDL string
Convert an srt timespan into a start and end timestamp.
Convert an srt timestamp into seconds.
Remove damaging line breaks and numbers from srt files and return a dictionary.
Search for and remove temp log files found in the output directory.
Print out timespans to be cut followed by the line number in the srt.
Concatenate video clips together and output finished video file to the output directory.
Create & concatenate video clips in groups of size BATCH_SIZE and output finished video file to output directory.
Return True if search term is found in given line, False otherwise.
Return a list of subtitle files.
Return a list of vtt files.
Takes a list of subtitle (srt) filenames, search term and search type and, returns a list of timestamps for composing a supercut.
Takes transcripts created by audiogrep/pocketsphinx, a search and search type and returns a list of timestamps for creating a supercut
Search through and find all instances of the search term in an srt or transcript, create a supercut around that instance, and output a new video file comprised of those supercuts.
INPUT: XML file with captions OUTPUT: parsed object like: [{'texlines': [u"So, I'm going to rewrite this", 'in a more concise form as'], 'time': {'hours':'1', 'min':'2','sec':44,'msec':232} }]
INPUT: array with captions, i.e. [{'texlines': [u"So, I'm going to rewrite this", 'in a more concise form as'], 'time': {'hours':'1', 'min':'2','sec':44,'msec':232} }] OUTPUT: srtformated string
调整音量大小
屏幕上下滚动 :params incrment: 1 向下滚动 -1 向上滚动
返回总字符数(考虑英文和中文在终端所占字块) return: int
切换页面线程
接受按键, 存入queue
第一次载入时查找歌词
通过歌词生成屏幕需要显示的内容
16 colors supported
256 colors supported
Call color function base on name
解析json列表,转换成utf-8
解析json字典,转换成utf-8
提供登陆的认证 这里顺带增加了 volume, channel, theme_id , netease, run_times的默认值
记录退出时的播放状态
统计用户信息
获取配置并检查是否更改
存储历史记录和登陆信息
获取channel列表
这个貌似没啥用 :params fcid, tcid: string
这里包装了一个函数,发送post_data :param ptype: n 列表无歌曲,返回新列表 e 发送歌曲完毕 b 不再播放,返回新列表 s 下一首,返回新的列表 r 标记喜欢 u 取消标记喜欢
初始获取歌曲 :params return: json
获取歌词 如果测试频繁会发如下信息: {'msg': 'You API access rate limit has been exceeded. Contact api-master@douban.com if you want higher limit. ', 'code': 1998, 'request': 'GET /j/v2/lyric'}
运行播放器（若当前已有正在运行的，强制推出） extra_cmd: 额外的参数 (list)
监控正在运行的播放器（独立线程） 播放器退出后将会设置 _exit_event
pasue状态下如果取时间会使歌曲继续, 这里加了一个_pause状态
Send a command to MPlayer. cmd: the command string expect: expect the output starts with a certain string The result, if any, is returned as a string.
互斥锁
更新队列线程
返回当前播放歌曲歌词
设置api发送的FM频道 :params channel_num: channel_list的索引值 int
获取每日推荐歌曲
获取单个歌曲
获取歌曲, 对外统一接口
设置显示信息
生成输出行 注意: 多线程终端同时输出会有bug, 导致起始位置偏移, 需要在每行加\r
每个controller需要提供run方法, 来提供启动
标题时间显示
从queue里取出字符执行命令
发送桌面通知
发送Linux桌面通知
发送Mac桌面通知
获取专辑封面
第一次桌面通知时加入图片
需要解码一下,通知需要unicode编码
时间状态
从queue里取出字符执行命令
登陆界面
通过帐号,密码请求token,返回一个dict { "user_info": { "ck": "-VQY", "play_record": { "fav_chls_count": 4, "liked": 802, "banned": 162, "played": 28368 }, "is_new_user": 0, "uid": "taizilongxu", "third_party_info": null, "url": "http://www.douban.com/people/taizilongxu/", "is_dj": false, "id": "2053207", "is_pro": false, "name": "刘小备" }, "r": 0 }
切换歌曲时刷新
打开豆瓣网页
从queue里取出字符执行命令
根据歌曲名搜索歌曲 : params : song_title: 歌曲名 limit: 搜索数量
根据歌名获取歌曲id
根据歌名搜索320k地址
因为历史列表动态更新,需要刷新
界面执行程序
Instantiate a new Trade from a dict (generally from loading a JSON response). The data used to instantiate the Trade is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeSummary from a dict (generally from loading a JSON response). The data used to instantiate the TradeSummary is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CalculatedTradeState from a dict (generally from loading a JSON response). The data used to instantiate the CalculatedTradeState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Create, replace and cancel a Trade's dependent Orders (Take Profit, Stop Loss and Trailing Stop Loss) through the Trade itself Args: accountID: Account Identifier tradeSpecifier: Specifier for the Trade takeProfit: The specification of the Take Profit to create/modify/cancel. If takeProfit is set to null, the Take Profit Order will be cancelled if it exists. If takeProfit is not provided, the exisiting Take Profit Order will not be modified. If a sub- field of takeProfit is not specified, that field will be set to a default value on create, and be inherited by the replacing order on modify. stopLoss: The specification of the Stop Loss to create/modify/cancel. If stopLoss is set to null, the Stop Loss Order will be cancelled if it exists. If stopLoss is not provided, the exisiting Stop Loss Order will not be modified. If a sub-field of stopLoss is not specified, that field will be set to a default value on create, and be inherited by the replacing order on modify. trailingStopLoss: The specification of the Trailing Stop Loss to create/modify/cancel. If trailingStopLoss is set to null, the Trailing Stop Loss Order will be cancelled if it exists. If trailingStopLoss is not provided, the exisiting Trailing Stop Loss Order will not be modified. If a sub-field of trailngStopLoss is not specified, that field will be set to a default value on create, and be inherited by the replacing order on modify. Returns: v20.response.Response containing the results from submitting the request
Instantiate a new Transaction from a dict (generally from loading a JSON response). The data used to instantiate the Transaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new ClientConfigureTransaction from a dict (generally from loading a JSON response). The data used to instantiate the ClientConfigureTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new ClientConfigureRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the ClientConfigureRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TransferFundsTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TransferFundsTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TransferFundsRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TransferFundsRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new MarketOrderTransaction from a dict (generally from loading a JSON response). The data used to instantiate the MarketOrderTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderFillTransaction from a dict (generally from loading a JSON response). The data used to instantiate the OrderFillTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderClientExtensionsModifyTransaction from a dict (generally from loading a JSON response). The data used to instantiate the OrderClientExtensionsModifyTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderClientExtensionsModifyRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the OrderClientExtensionsModifyRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeClientExtensionsModifyTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TradeClientExtensionsModifyTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeClientExtensionsModifyRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TradeClientExtensionsModifyRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeOpen from a dict (generally from loading a JSON response). The data used to instantiate the TradeOpen is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeReduce from a dict (generally from loading a JSON response). The data used to instantiate the TradeReduce is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new LiquidityRegenerationSchedule from a dict (generally from loading a JSON response). The data used to instantiate the LiquidityRegenerationSchedule is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new LiquidityRegenerationScheduleStep from a dict (generally from loading a JSON response). The data used to instantiate the LiquidityRegenerationScheduleStep is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OpenTradeFinancing from a dict (generally from loading a JSON response). The data used to instantiate the OpenTradeFinancing is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionFinancing from a dict (generally from loading a JSON response). The data used to instantiate the PositionFinancing is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get a list of Transactions pages that satisfy a time-based Transaction query. Args: accountID: Account Identifier fromTime: The starting time (inclusive) of the time range for the Transactions being queried. toTime: The ending time (inclusive) of the time range for the Transactions being queried. pageSize: The number of Transactions to include in each page of the results. type: A filter for restricting the types of Transactions to retreive. Returns: v20.response.Response containing the results from submitting the request
Get a stream of Transactions for an Account starting from when the request is made. Args: accountID: Account Identifier Returns: v20.response.Response containing the results from submitting the request
Instantiate a new PriceBucket from a dict (generally from loading a JSON response). The data used to instantiate the PriceBucket is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new Price from a dict (generally from loading a JSON response). The data used to instantiate the Price is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new Candlestick from a dict (generally from loading a JSON response). The data used to instantiate the Candlestick is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CandlestickData from a dict (generally from loading a JSON response). The data used to instantiate the CandlestickData is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderBook from a dict (generally from loading a JSON response). The data used to instantiate the OrderBook is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderBookBucket from a dict (generally from loading a JSON response). The data used to instantiate the OrderBookBucket is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionBook from a dict (generally from loading a JSON response). The data used to instantiate the PositionBook is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionBookBucket from a dict (generally from loading a JSON response). The data used to instantiate the PositionBookBucket is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Fetch candlestick data for an instrument. Args: instrument: Name of the Instrument price: The Price component(s) to get candlestick data for. Can contain any combination of the characters "M" (midpoint candles) "B" (bid candles) and "A" (ask candles). granularity: The granularity of the candlesticks to fetch count: The number of candlesticks to return in the reponse. Count should not be specified if both the start and end parameters are provided, as the time range combined with the graularity will determine the number of candlesticks to return. fromTime: The start of the time range to fetch candlesticks for. toTime: The end of the time range to fetch candlesticks for. smooth: A flag that controls whether the candlestick is "smoothed" or not. A smoothed candlestick uses the previous candle's close price as its open price, while an unsmoothed candlestick uses the first price from its time range as its open price. includeFirst: A flag that controls whether the candlestick that is covered by the from time should be included in the results. This flag enables clients to use the timestamp of the last completed candlestick received to poll for future candlesticks but avoid receiving the previous candlestick repeatedly. dailyAlignment: The hour of the day (in the specified timezone) to use for granularities that have daily alignments. alignmentTimezone: The timezone to use for the dailyAlignment parameter. Candlesticks with daily alignment will be aligned to the dailyAlignment hour within the alignmentTimezone. Note that the returned times will still be represented in UTC. weeklyAlignment: The day of the week used for granularities that have weekly alignment. Returns: v20.response.Response containing the results from submitting the request
Fetch a price for an instrument. Accounts are not associated in any way with this endpoint. Args: instrument: Name of the Instrument time: The time at which the desired price is in effect. The current price is returned if no time is provided. Returns: v20.response.Response containing the results from submitting the request
Instantiate a new Position from a dict (generally from loading a JSON response). The data used to instantiate the Position is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionSide from a dict (generally from loading a JSON response). The data used to instantiate the PositionSide is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CalculatedPositionState from a dict (generally from loading a JSON response). The data used to instantiate the CalculatedPositionState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new DynamicOrderState from a dict (generally from loading a JSON response). The data used to instantiate the DynamicOrderState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new Order from a dict (generally from loading a JSON response). The data used to instantiate the Order is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new LimitOrder from a dict (generally from loading a JSON response). The data used to instantiate the LimitOrder is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TakeProfitOrderRequest from a dict (generally from loading a JSON response). The data used to instantiate the TakeProfitOrderRequest is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new UnitsAvailableDetails from a dict (generally from loading a JSON response). The data used to instantiate the UnitsAvailableDetails is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new UnitsAvailable from a dict (generally from loading a JSON response). The data used to instantiate the UnitsAvailable is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new GuaranteedStopLossOrderEntryData from a dict (generally from loading a JSON response). The data used to instantiate the GuaranteedStopLossOrderEntryData is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get a list of Orders for an Account Args: accountID: Account Identifier ids: List of Order IDs to retrieve state: The state to filter the requested Orders by instrument: The instrument to filter the requested orders by count: The maximum number of Orders to return beforeID: The maximum Order ID to return. If not provided the most recent Orders in the Account are returned Returns: v20.response.Response containing the results from submitting the request
Replace an Order in an Account by simultaneously cancelling it and creating a replacement Order Args: accountID: Account Identifier orderSpecifier: The Order Specifier order: Specification of the replacing Order Returns: v20.response.Response containing the results from submitting the request
Cancel a pending Order in an Account Args: accountID: Account Identifier orderSpecifier: The Order Specifier Returns: v20.response.Response containing the results from submitting the request
Update the Client Extensions for an Order in an Account. Do not set, modify, or delete clientExtensions if your account is associated with MT4. Args: accountID: Account Identifier orderSpecifier: The Order Specifier clientExtensions: The Client Extensions to update for the Order. Do not set, modify, or delete clientExtensions if your account is associated with MT4. tradeClientExtensions: The Client Extensions to update for the Trade created when the Order is filled. Do not set, modify, or delete clientExtensions if your account is associated with MT4. Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Market Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a MarketOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Limit Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a LimitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Limit Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Limit Order to replace kwargs : The arguments to create a LimitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Stop Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a StopOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Stop Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Stop Order to replace kwargs : The arguments to create a StopOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a MarketIfTouched Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a MarketIfTouchedOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending MarketIfTouched Order in an Account Args: accountID : The ID of the Account orderID : The ID of the MarketIfTouched Order to replace kwargs : The arguments to create a MarketIfTouchedOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Take Profit Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a TakeProfitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Take Profit Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Take Profit Order to replace kwargs : The arguments to create a TakeProfitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Stop Loss Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a StopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Stop Loss Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Stop Loss Order to replace kwargs : The arguments to create a StopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Trailing Stop Loss Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a TrailingStopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Trailing Stop Loss Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Take Profit Order to replace kwargs : The arguments to create a TrailingStopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Instantiate a new Instrument from a dict (generally from loading a JSON response). The data used to instantiate the Instrument is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new InstrumentCommission from a dict (generally from loading a JSON response). The data used to instantiate the InstrumentCommission is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new GuaranteedStopLossOrderLevelRestriction from a dict (generally from loading a JSON response). The data used to instantiate the GuaranteedStopLossOrderLevelRestriction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Set the token for the v20 context Args: token: The token used to access the v20 REST api
Set the Accept-Datetime-Format header to an acceptable value Args: format: UNIX or RFC3339
Perform an HTTP request through the context Args: request: A v20.request.Request object Returns: A v20.response.Response object
Instantiate a new Account from a dict (generally from loading a JSON response). The data used to instantiate the Account is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new AccountChangesState from a dict (generally from loading a JSON response). The data used to instantiate the AccountChangesState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new AccountSummary from a dict (generally from loading a JSON response). The data used to instantiate the AccountSummary is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CalculatedAccountState from a dict (generally from loading a JSON response). The data used to instantiate the CalculatedAccountState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new AccountChanges from a dict (generally from loading a JSON response). The data used to instantiate the AccountChanges is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get a list of all Accounts authorized for the provided token. Args: Returns: v20.response.Response containing the results from submitting the request
Get the list of tradeable instruments for the given Account. The list of tradeable instruments is dependent on the regulatory division that the Account is located in, thus should be the same for all Accounts owned by a single user. Args: accountID: Account Identifier instruments: List of instruments to query specifically. Returns: v20.response.Response containing the results from submitting the request
Set the client-configurable portions of an Account. Args: accountID: Account Identifier alias: Client-defined alias (name) for the Account marginRate: The string representation of a decimal number. Returns: v20.response.Response containing the results from submitting the request
Fetch the user information for the specified user. This endpoint is intended to be used by the user themself to obtain their own information. Args: userSpecifier: The User Specifier Returns: v20.response.Response containing the results from submitting the request
Instantiate a new ClientPrice from a dict (generally from loading a JSON response). The data used to instantiate the ClientPrice is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new QuoteHomeConversionFactors from a dict (generally from loading a JSON response). The data used to instantiate the QuoteHomeConversionFactors is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new HomeConversions from a dict (generally from loading a JSON response). The data used to instantiate the HomeConversions is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get pricing information for a specified list of Instruments within an Account. Args: accountID: Account Identifier instruments: List of Instruments to get pricing for. since: Date/Time filter to apply to the response. Only prices and home conversions (if requested) with a time later than this filter (i.e. the price has changed after the since time) will be provided, and are filtered independently. includeUnitsAvailable: Flag that enables the inclusion of the unitsAvailable field in the returned Price objects. includeHomeConversions: Flag that enables the inclusion of the homeConversions field in the returned response. An entry will be returned for each currency in the set of all base and quote currencies present in the requested instruments list. Returns: v20.response.Response containing the results from submitting the request
Get a stream of Account Prices starting from when the request is made. This pricing stream does not include every single price created for the Account, but instead will provide at most 4 prices per second (every 250 milliseconds) for each instrument being requested. If more than one price is created for an instrument during the 250 millisecond window, only the price in effect at the end of the window is sent. This means that during periods of rapid price movement, subscribers to this stream will not be sent every price. Pricing windows for different connections to the price stream are not all aligned in the same way (i.e. they are not all aligned to the top of the second). This means that during periods of rapid price movement, different subscribers may observe different prices depending on their alignment. Args: accountID: Account Identifier instruments: List of Instruments to stream Prices for. snapshot: Flag that enables/disables the sending of a pricing snapshot when initially connecting to the stream. Returns: v20.response.Response containing the results from submitting the request
Esse método trata a data recebida de acordo com o timezone do
usuário. O seu retorno é dividido em duas partes:
1) A data em si;
2) As horas;
:param cDateUTC: string contendo as informações da data
:param timezone: timezone do usuário do sistema
:return: data e hora convertidos para a timezone do usuário
Remove special characters and strip spaces
Format datetime
Format date
Get a list of program files by expanding a list of path patterns and interpreting it as relative to the executable. This method can be used as helper for implementing the method program_files(). Contrary to the default implementation of program_files(), this method does not explicitly add the executable to the list of returned files, it assumes that required_paths contains a path that covers the executable. @param executable: the path to the executable of the tool (typically the result of executable()) @param required_paths: a list of required path patterns @param parent_dir: whether required_paths are relative to the directory of executable or the parent directory @return a list of paths as strings, suitable for result of program_files()
Get version of a tool by executing it with argument "--version" and returning stdout. @param executable: the path to the executable of the tool (typically the result of executable()) @param arg: an argument to pass to the tool to let it print its version @param use_stderr: True if the tool prints version on stderr, False for stdout @return a (possibly empty) string of output of the tool
Mark the benchmark as erroneous, e.g., because the benchmarking tool crashed. The message is intended as explanation for the user.
This method writes information about benchmark and system into txt_file.
The method output_before_run_set() calculates the length of the first column for the output in terminal and stores information about the runSet in XML. @param runSet: current run set
This function writes a simple message to terminal and logfile, when a run set is skipped. There is no message about skipping a run set in the xml-file.
This method writes the information about a run set into the txt_file.
The method output_before_run() prints the name of a file to terminal. It returns the name of the logfile. @param run: a Run object
The method output_after_run() prints filename, result, time and status of a run to terminal and stores all data in XML
The method output_after_run_set() stores the times of a run set in XML. @params cputime, walltime: accumulated times of the run set
This function creates the XML structure for a list of runs
This function adds the result values to the XML representation of a run.
This function adds the result values to the XML representation of a runSet.
This function returns the name of the file for a run set with an extension ("txt", "xml").
Formats the file name of a program for printing on console.
Write a rough string version of the XML (for temporary files).
Writes a nicely formatted XML file with DOCTYPE, and compressed if necessary.
returns a function that extracts the value for a column.
Calculate an assignment of the available CPU cores to a number of parallel benchmark executions such that each run gets its own cores without overlapping of cores between runs. In case the machine has hyper-threading, this method tries to avoid putting two different runs on the same physical core (but it does not guarantee this if the number of parallel runs is too high to avoid it). In case the machine has multiple CPUs, this method avoids splitting a run across multiple CPUs if the number of cores per run is lower than the number of cores per CPU (splitting a run over multiple CPUs provides worse performance). It will also try to split the runs evenly across all available CPUs. A few theoretically-possible cases are not implemented, for example assigning three 10-core runs on a machine with two 16-core CPUs (this would have unfair core assignment and thus undesirable performance characteristics anyway). The list of available cores is read from the cgroup file system, such that the assigned cores are a subset of the cores that the current process is allowed to use. This script does currently not support situations where the available cores are asymmetrically split over CPUs, e.g. 3 cores on one CPU and 5 on another. @param coreLimit: the number of cores for each run @param num_of_threads: the number of parallel benchmark executions @param coreSet: the list of CPU cores identifiers provided by a user, None makes benchexec using all cores @return a list of lists, where each inner list contains the cores for one run
This method does the actual work of _get_cpu_cores_per_run without reading the machine architecture from the file system in order to be testable. For description, c.f. above. Note that this method might change the input parameters! Do not call it directly, call getCpuCoresPerRun()! @param allCpus: the list of all available cores @param cores_of_package: a mapping from package (CPU) ids to lists of cores that belong to this CPU @param siblings_of_core: a mapping from each core to a list of sibling cores including the core itself (a sibling is a core sharing the same physical core)
Get an assignment of memory banks to runs that fits to the given coreAssignment, i.e., no run is allowed to use memory that is not local (on the same NUMA node) to one of its CPU cores.
Get all memory banks the kernel lists in a given directory. Such a directory can be /sys/devices/system/node/ (contains all memory banks) or /sys/devices/system/cpu/cpu*/ (contains all memory banks on the same NUMA node as that core).
Check whether the desired amount of parallel benchmarks fits in the memory. Implemented are checks for memory limits via cgroup controller "memory" and memory bank restrictions via cgroup controller "cpuset", as well as whether the system actually has enough memory installed. @param memLimit: the memory limit in bytes per run @param num_of_threads: the number of parallel benchmark executions @param memoryAssignment: the allocation of memory banks to runs (if not present, all banks are assigned to all runs)
Get the size of a memory bank in bytes.
Returns a BenchExec result status based on the output of SMACK
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Handle the options specified by add_basic_container_args(). @return: a dict that can be used as kwargs for the ContainerExecutor constructor
Define command-line arguments for output of a container (result files). @param argument_parser: an argparse parser instance
Handle the options specified by add_container_output_args(). @return: a dict that can be used as kwargs for the ContainerExecutor.execute_run()
A simple command-line interface for the containerexecutor module of BenchExec.
Given the temp directory that is created for each run, return the path to the directory where files created by the tool are stored.
This method executes the command line and waits for the termination of it, handling all setup and cleanup. @param args: the command line to run @param rootDir: None or a root directory that contains all relevant files for starting a new process @param workingDir: None or a directory which the execution should use as working directory @param output_dir: the directory where to write result files (required if result_files_pattern) @param result_files_patterns: a list of patterns of files to retrieve as result files
Execute the given command and measure its resource usage similarly to super()._start_execution(), but inside a container implemented using Linux namespaces. The command has no network access (only loopback), a fresh directory as /tmp and no write access outside of this, and it does not see other processes except itself.
Setup the filesystem layout in the container. As first step, we create a copy of all existing mountpoints in mount_base, recursively, and as "private" mounts (i.e., changes to existing mountpoints afterwards won't propagate to our copy). Then we iterate over all mountpoints and change them according to the mode the user has specified (hidden, read-only, overlay, or full-access). This has do be done for each mountpoint because overlays are not recursive. Then we chroot into the new mount hierarchy. The new filesystem layout still has a view of the host's /proc. We do not mount a fresh /proc here because the grandchild still needs the old /proc. We do simply iterate over all existing mount points and set them to read-only/overlay them, because it is easier to create a new hierarchy and chroot into it. First, we still have access to the original mountpoints while doing so, and second, we avoid race conditions if someone else changes the existing mountpoints. @param temp_dir: The base directory under which all our directories should be created.
Setup the filesystem layout in the given root directory. Create a copy of the existing proc- and dev-mountpoints in the specified root directory. Afterwards we chroot into it. @param root_dir: The path of the root directory that is used to execute the process.
Transfer files created by the tool in the container to the output directory. @param tool_output_dir: The directory under which all tool output files are created. @param working_dir: The absolute working directory of the tool in the container. @param output_dir: the directory where to write result files @param patterns: a list of patterns of files to retrieve as result files
Read an parse the XML of a table-definition file. @return: an ElementTree object for the table definition
Load all results in files that are listed in the given table-definition file. @return: a list of RunSetResult objects
Load results from given files with column definitions taken from a table-definition file. @return: a list of RunSetResult objects
Extract all columns mentioned in the result tag of a table definition file.
Extract columns that are relevant for the diff table. @param columns_to_show: (list) A list of columns that should be shown @return: (set) Set of columns that are relevant for the diff table. If none is marked relevant, the column named "status" will be returned in the set.
Return a unique identifier for a given task. @param task: the XML element that represents a task @return a tuple with filename of task as first element
Load the module with the tool-specific code.
Version of load_result for multiple input files that will be loaded concurrently.
Completely handle loading a single result file. @param result_file the file to parse @param options additional options @param run_set_id the identifier of the run set @param columns the list of columns @param columns_relevant_for_diff a set of columns that is relevant for the diff table @return a fully ready RunSetResult instance or None
This function parses an XML file that contains the results of the execution of a run set. It returns the "result" XML tag. @param resultFile: The file name of the XML file that contains the results. @param run_set_id: An optional identifier of this set of results.
This function merges the results of all RunSetResult objects. If necessary, it can merge lists of names: [A,C] + [A,B] --> [A,B,C] and add dummy elements to the results. It also ensures the same order of tasks.
Set the filelists of all RunSetResult elements so that they contain the same files in the same order. For missing files a dummy element is inserted.
Create list of rows with all data. Each row consists of several RunResults.
Find all rows with differences in the status column.
Find out which of the entries in Row.id are equal for all given rows. @return: A list of True/False values according to whether the i-th part of the id is always equal.
Calculcate number of true/false tasks and maximum achievable score.
This function returns the numbers of the statistics. @param runResults: All the results of the execution of one run set (as list of RunResult objects)
Count the number of regressions, i.e., differences in status of the two right-most results where the new one is not "better" than the old one. Any change in status between error, unknown, and wrong result is a regression. Different kind of errors or wrong results are also a regression.
Create tables and write them to files. @return a list of futures to allow waiting for completion
Append the result for one run. Needs to be called before collect_data().
Load the actual result values from the XML file and the log files. This may take some time if many log files have to be opened and parsed.
This function extracts everything necessary for creating a RunSetResult object from the "result" XML tag of a benchmark result file. It returns a RunSetResult object, which is not yet fully initialized. To finish initializing the object, call collect_data() before using it for anything else (this is to separate the possibly costly collect_data() call from object instantiation).
This function collects the values from one run. Only columns that should be part of the table are collected.
generate output representation of rows
Find the path to the executable file that will get executed. This method always needs to be overridden, and most implementations will look similar to this one. The path returned should be relative to the current directory.
Determine whether the version is greater than some given version
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
A simple command-line interface for the runexecutor module of BenchExec.
Get the user account info from the passwd database. Only works on Linux. @param user The name of a user account or a numeric uid prefixed with '#' @return a tuple that corresponds to the members of the passwd structure @raise KeyError: If user account is unknown @raise ValueError: If uid is not a valid number
This function shrinks a file. We remove only the middle part of a file, the file-start and the file-end remain unchanged.
Segmentation faults and some memory failures reference a file with more information (hs_err_pid_*). We append this file to the log. The format that we expect is a line "# An error report file with more information is saved as:" and the file name of the dump file on the next line. @param output_filename name of log file with tool output @param base_path string that needs to be preprended to paths for lookup of files
Join a thread, but if the thread doesn't terminate for some time, ignore it instead of waiting infinitely.
This function initializes the cgroups for the limitations and measurements.
Build the final command line for executing the given command, using sudo if necessary.
Try to send signal to given process, either directly of with sudo. Because we cannot send signals to the sudo process itself, this method checks whether the target is the sudo process and redirects the signal to sudo's child in this case.
Send signal to given process, either directly or with sudo. If the target is the sudo process itself, the signal will be lost, because we do not have the rights to send signals to sudo. Use _kill_process() because of this.
Return the list of files in a directory, assuming that our user can read it.
This method creates the CGroups for the following execution. @param my_cpus: None or a list of the CPU cores to use @param memlimit: None or memory limit in bytes @param memory_nodes: None or a list of memory nodes of a NUMA system to use @param cgroup_values: dict of additional values to set @return cgroups: a map of all the necessary cgroups for the following execution. Please add the process of the following execution to all those cgroups!
Create a temporary directory for the run.
Delete given temporary directory and all its contents.
Return map with desired environment variables for run.
Open and prepare output file.
Start time-limit handler. @return None or the time-limit handler for calling cancel()
Start memory-limit handler. @return None or the memory-limit handler for calling cancel()
Setup time limit with ulimit for the current process.
Start thread that enforces any file-hiearchy limits.
This function executes a given command with resource limits, and writes the output to a file. @param args: the command line to run @param output_filename: the file where the output should be written to @param stdin: What to uses as stdin for the process (None: /dev/null, a file descriptor, or a file object) @param hardtimelimit: None or the CPU time in seconds after which the tool is forcefully killed. @param softtimelimit: None or the CPU time in seconds after which the tool is sent a kill signal. @param walltimelimit: None or the wall time in seconds after which the tool is forcefully killed (default: hardtimelimit + a few seconds) @param cores: None or a list of the CPU cores to use @param memlimit: None or memory limit in bytes @param memory_nodes: None or a list of memory nodes in a NUMA system to use @param environments: special environments for running the command @param workingDir: None or a directory which the execution should use as working directory @param maxLogfileSize: None or a number of bytes to which the output of the tool should be truncated approximately if there is too much output. @param cgroupValues: dict of additional cgroup values to set (key is tuple of subsystem and option, respective subsystem needs to be enabled in RunExecutor; cannot be used to override values set by BenchExec) @param files_count_limit: None or maximum number of files that may be written. @param files_size_limit: None or maximum size of files that may be written. @param error_filename: the file where the error output should be written to (default: same as output_filename) @param write_headers: Write informational headers to the output and the error file if separate (default: True) @param **kwargs: further arguments for ContainerExecutor.execute_run() @return: dict with result of run (measurement results and process exitcode)
This method executes the command line and waits for the termination of it, handling all setup and cleanup, but does not check whether arguments are valid.
This method calculates the exact results for time and memory measurements. It is not important to call this method as soon as possible after the run.
Check that the user account's home directory now does not contain more files than when this instance was created, and warn otherwise. Does nothing if no user account was given to RunExecutor. @return set of newly created files
Set the host name of the machine.
Basic utility to check the availability and permissions of cgroups. This will log some warnings for the user if necessary. On some systems, daemons such as cgrulesengd might interfere with the cgroups of a process soon after it was started. Thus this function starts a process, waits a configurable amount of time, and check whether the cgroups have been changed. @param wait: a non-negative int that is interpreted as seconds to wait during the check @raise SystemExit: if cgroups are not usable
Run check_cgroup_availability() in a separate thread to detect the following problem: If "cgexec --sticky" is used to tell cgrulesengd to not interfere with our child processes, the sticky flag unfortunately works only for processes spawned by the main thread, not those spawned by other threads (and this will happen if "benchexec -N" is used).
A simple command-line interface for the cgroups check of BenchExec.
This function checks, if all the words appear in the given order in the text.
This function prints the given String immediately and flushes the output.
This function returns True, if a line of the file contains bracket '{'.
This function searches for all "option"-tags and returns a list with all attributes and texts.
Get a single child tag from an XML element. Similar to "elem.find(tag)", but warns if there are multiple child tags with the given name.
This method returns a shallow copy of a XML-Element. This method is for compatibility with Python 2.6 or earlier.. In Python 2.7 you can use 'copyElem = elem.copy()' instead.
Parse a comma-separated list of strings. The list may additionally contain ranges such as "1-5", which will be expanded into "1,2,3,4,5".
Parse a string that consists of a integer number and an optional unit. @param s a non-empty string that starts with an int and is followed by some letters @return a triple of the number (as int) and the unit
Parse a string that contains a number of bytes, optionally with a unit like MB. @return the number of bytes encoded by the string
Parse a string that contains a time span, optionally with a unit like s. @return the number of seconds encoded by the string
Expand a file name pattern containing wildcards, environment variables etc. @param pattern: The pattern string to expand. @param base_dir: The directory where relative paths are based on. @return: A list of file names (possibly empty).
Replace certain keys with respective values in a string. @param template: the string in which replacements should be made @param replacements: a dict or a list of pairs of keys and values
Suited as onerror handler for (sh)util.rmtree() that logs a warning.
Same as shutil.rmtree, but supports directories without write or execute permissions.
Copy all lines from an input file object to an output file object.
Simply write some content to a file, overriding the file if necessary.
Shrink a text file to approximately maxSize bytes by removing lines from the middle of the file.
Read the full content of a file.
Read key value pairs from a file (each pair on a separate line). Key and value are separated by ' ' as often used by the kernel. @return a generator of tuples
Add and commit all files given in a list into a git repository in the base_dir directory. Nothing is done if the git repository has local changes. @param files: the files to commit @param description: the commit message
Setup the logging framework with a basic configuration
Interrupt running process, and provide a python prompt for interactive debugging. This code is based on http://stackoverflow.com/a/133384/396730
This function executes the tool with a sourcefile with options. It also calls functions for output before and after the run.
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Check whether Turbo Boost (scaling CPU frequency beyond nominal frequency) is active on this system. @return: A bool, or None if Turbo Boost is not supported.
Check whether any of the CPU cores monitored by this instance has throttled since this instance was created. @return a boolean value
Check whether any swapping occured on this system since this instance was created. @return a boolean value
Add some basic options for an executor to an argparse argument_parser.
Handle the options specified by add_basic_executor_options().
Try to send signal to given process.
Actually start the tool and the measurements. @param parent_setup_fn a function without parameters that is called in the parent process immediately before the tool is started @param child_setup_fn a function without parameters that is called in the child process before the tool is started @param parent_cleanup_fn a function that is called in the parent process immediately after the tool terminated, with three parameters: the result of parent_setup_fn, the result of the executed process as ProcessExitCode, and the base path for looking up files as parameter values @return: a tuple of PID of process and a blocking function, which waits for the process and a triple of the exit code and the resource usage of the process and the result of parent_cleanup_fn (do not use os.wait)
Wait for the given process to terminate. @return tuple of exit code and resource usage
Return a pretty-printed XML string for the Element. Also allows setting a document type.
Determine the file paths to be adopted
This method replaces special substrings from a list of string and return a new list.
Open and parse a task-definition file in YAML format.
Load the tool-info class. @param tool_name: The name of the tool-info module. Either a full Python package name or a name within the benchexec.tools package. @return: A tuple of the full name of the used tool-info module and an instance of the tool-info class.
This function builds a list of SourcefileSets (containing filename with options). The files and their options are taken from the list of sourcefilesTags.
Get the task-definition files from the XML definition. Task-definition files are files for which we create a run (typically an input file or a YAML task definition).
Create a Run from a direct definition of the main input file (without task definition)
Create a Run from a task definition in yaml format
The function expand_filename_pattern expands a filename pattern to a sorted list of filenames. The pattern can contain variables and wildcards. If base_dir is given and pattern is not absolute, base_dir and pattern are joined.
Set the result of this run. Use this method instead of manually setting the run attributes and calling after_execution(), this method handles all this by itself. @param values: a dictionary with result values as returned by RunExecutor.execute_run(), may also contain arbitrary additional values @param visible_columns: a set of keys of values that should be visible by default (i.e., not marked as hidden), apart from those that BenchExec shows by default anyway
Return status according to result and output of tool.
try to find out whether the tool terminated because of a timeout
Create a dict of property->ExpectedResult from information encoded in a filename.
Return the achieved score of a task according to the SV-COMP scoring scheme. @param category: result category as determined by get_result_category @param result: the result given by the tool
Return the possible score of task, depending on whether the result is correct or not.
Classify the given result into "true" (property holds), "false" (property does not hold), "unknown", and "error". @param result: The result given by the tool (needs to be one of the RESULT_* strings to be recognized). @return One of RESULT_CLASS_* strings
This function determines the relation between actual result and expected result for the given file and properties. @param filename: The file name of the input file. @param result: The result given by the tool (needs to be one of the RESULT_* strings to be recognized). @param properties: The list of property names to check. @return One of the CATEGORY_* strings.
Create a Property instance by attempting to parse the given property file. @param propertyfile: A file name of a property file @param allow_unknown: Whether to accept unknown properties
Create a Property instance from a list of well-known property names @param property_names: a non-empty list of property names
The function get_file_list expands a short filename to a sorted list of filenames. The short filename can contain variables and wildcards.
Open a URL and ensure that the result is seekable, copying it into a buffer if necessary.
Split a string into two parts: a prefix and a suffix. Splitting is done from the end, so the split is done around the position of the last digit in the string (that means the prefix may include any character, mixing digits and chars). The flag 'numbers_into_suffix' determines whether the suffix consists of digits or non-digits.
Helper function for formatting the content of the options line
Take a tuple (values, counts), remove consecutive values and increment their count instead.
Returns a list where sequences of post-fixed entries are shortened to their common prefix. This might be useful in cases of several similar values, where the prefix is identical for several entries. If less than 'number_of_needed_commons' are identically prefixed, they are kept unchanged. Example: ['test', 'pc1', 'pc2', 'pc3', ... , 'pc10'] -> ['test', 'pc*']
Filter out duplicate values while keeping order.
If the value is a number (or number followed by a unit), this function returns a string-representation of the number with the specified number of significant digits, optionally aligned at the decimal point.
Returns the type of the given column based on its row values on the given RunSetResult. @param column: the column to return the correct ColumnType for @param column_values: the column values to consider @return: a tuple of a type object describing the column - the concrete ColumnType is stored in the attribute 'type', the display unit of the column, which may be None, the source unit of the column, which may be None, and the scale factor to convert from the source unit to the display unit. If no scaling is necessary for conversion, this value is 1.
Returns the amount of decimal digits of the given regex match, considering the number of significant digits for the provided column. @param decimal_number_match: a regex match of a decimal number, resulting from REGEX_MEASURE.match(x). @param number_of_significant_digits: the number of significant digits required @return: the number of decimal digits of the given decimal number match's representation, after expanding the number to the required amount of significant digits
Format a value nicely for human-readable output (including rounding). @param value: the value to format @param isToAlign: if True, spaces will be added to the returned String representation to align it to all other values in this column, correctly @param format_target the target the value should be formatted for @return: a formatted String representation of the given value.
Find the path to the executable file that will get executed. This method always needs to be overridden, and most implementations will look similar to this one. The path returned should be relative to the current directory.
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
OPTIONAL, this method is only necessary for situations when the benchmark environment needs to know all files belonging to a tool (to transport them to a cloud service, for example). Returns a list of files or directories that are necessary to run the tool.
Return a Cgroup object with the cgroups of the current process. Note that it is not guaranteed that all subsystems are available in the returned object, as a subsystem may not be mounted. Check with "subsystem in <instance>" before using. A subsystem may also be present but we do not have the rights to create child cgroups, this can be checked with require_subsystem(). @param cgroup_paths: If given, use this instead of reading /proc/self/cgroup.
Return the information which subsystems are mounted where. @return a generator of tuples (subsystem, mountpoint)
For all subsystems, return the information in which (sub-)cgroup this process is in. (Each process is in exactly cgroup in each hierarchy.) @return a generator of tuples (subsystem, cgroup)
Parse a /proc/*/cgroup file into tuples of (subsystem,cgroup). @param content: An iterable over the lines of the file. @return: a generator of tuples
Tell cgrulesengd daemon to not move the given process into other cgroups, if libcgroup is available.
Check whether the given subsystem is enabled and is writable (i.e., new cgroups can be created for it). Produces a log message for the user if one of the conditions is not fulfilled. If the subsystem is enabled but not writable, it will be removed from this instance such that further checks with "in" will return "False". @return A boolean value.
Create child cgroups of the current cgroup for at least the given subsystems. @return: A Cgroup instance representing the new child cgroup(s).
Add a process to the cgroups represented by this instance.
Return a generator of all PIDs currently in this cgroup for the given subsystem.
Kill all tasks in this cgroup and all its children cgroups forcefully. Additionally, the children cgroups will be deleted.
Check whether the given value exists in the given subsystem. Does not make a difference whether the value is readable, writable, or both. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Read the given value from the given subsystem. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Read the lines of the given file from the given subsystem. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Read the lines of the given file from the given subsystem and split the lines into key-value pairs. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Write the given value for the given subsystem. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Remove all cgroups this instance represents from the system. This instance is afterwards not usable anymore!
Parse a time stamp in the "year-month-day hour-minute" format.
The main method of BenchExec for use in a command-line script. In addition to calling benchexec.start(argv), it also handles signals and keyboard interrupts. It does not return but calls sys.exit(). @param benchexec: An instance of BenchExec for executing benchmarks. @param argv: optionally the list of command-line options to use
Start BenchExec. @param argv: command-line options for BenchExec
Create a parser for the command-line options. May be overwritten for adding more configuration options. @return: an argparse.ArgumentParser instance
Configure the logging framework.
Execute a single benchmark as defined in a file. If called directly, ensure that config and executor attributes are set up. @param benchmark_file: the name of a benchmark-definition XML file @return: a result value from the executor module
Check and abort if the target directory for the benchmark results already exists in order to avoid overwriting results.
Stop the execution of a benchmark. This instance cannot be used anymore afterwards. Timely termination is not guaranteed, and this method may return before everything is terminated.
Allocate some memory that can be used as a stack. @return: a ctypes void pointer to the *top* of the stack.
Execute a function in a child process in separate namespaces. @param func: a parameter-less function returning an int (which will be the process' exit value) @return: the PID of the created child process
Write uid_map and gid_map in /proc to create a user mapping that maps our user from outside the container to the same user inside the container (and no other users are mapped). @see: http://man7.org/linux/man-pages/man7/user_namespaces.7.html @param pid: The PID of the process in the container.
Bring up the given network interface. @raise OSError: if interface does not exist or permissions are missing
Get all current mount points of the system. Changes to the mount points during iteration may be reflected in the result. @return a generator of (source, target, fstype, options), where options is a list of bytes instances, and the others are bytes instances (this avoids encoding problems with mount points with problematic characters).
Remount an existing mount point with additional flags. @param mountpoint: the mount point as bytes @param existing_options: dict with current mount existing_options as bytes @param mountflags: int with additional mount existing_options (cf. libc.MS_* constants)
Make a bind mount. @param source: the source directory as bytes @param target: the target directory as bytes @param recursive: whether to also recursively bind mount all mounts below source @param private: whether to mark the bind as private, i.e., changes to the existing mounts won't propagate and vice-versa (changes to files/dirs will still be visible).
Drop all capabilities this process has. @param keep: list of capabilities to not drop
Install all signal handler that forwards all signals to the given process.
Wait for a child to terminate and in the meantime forward all signals the current process receives to this child. @return a tuple of exit code and resource usage of the child as given by os.waitpid
Close all open file descriptors except those in a given set. @param keep_files: an iterable of file descriptors or file-like objects.
Create a minimal system configuration for use in a container. @param basedir: The directory where the configuration files should be placed as bytes. @param mountdir: If present, bind mounts to the configuration files will be added below this path (given as bytes).
Determine whether a given file is one of the files created by setup_container_system_config(). @param file: Absolute file path as string.
Compose the command line to execute from the name of the executable, the user-specified options, and the inputfile to analyze. This method can get overridden, if, for example, some options should be enabled or if the order of arguments must be changed. All paths passed to this method (executable, tasks, and propertyfile) are either absolute or have been made relative to the designated working directory. @param executable: the path to the executable of the tool (typically the result of executable()) @param options: a list of options, in the same order as given in the XML-file. @param tasks: a list of tasks, that should be analysed with the tool in one run. In most cases we we have only _one_ inputfile. @param propertyfile: contains a specification for the verifier. @param rlimits: This dictionary contains resource-limits for a run, for example: time-limit, soft-time-limit, hard-time-limit, memory-limit, cpu-core-limit. All entries in rlimits are optional, so check for existence before usage!
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
OPTIONAL, this method is only necessary for situations when the benchmark environment needs to know all files belonging to a tool (to transport them to a cloud service, for example). Returns a list of files or directories that are necessary to run the tool.
Add content to the represented file. If keep is False, the new content will be forgotten during the next call to this method.
Take the result of an energy measurement and return a flat dictionary that contains all values.
Starts the external measurement program.
Stops the external measurement program and returns the measurement result, if the measurement was running.
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Render the error document
Call Paste's FileApp (a WSGI application) to serve the file at the specified path
Show login form.
Extract prefix attributes from arbitary dict.
Extract pool attributes from arbitary dict.
List VRFs and return JSON encoded result.
Perform a smart VRF search. The "smart" search function tries extract a query from a text string. This query is then passed to the search_vrf function, which performs the search.
Add a new VRF to NIPAP and return its data.
Remove a VRF.
List pools and return JSON encoded result.
Perform a smart pool search. The "smart" search function tries extract a query from a text string. This query is then passed to the search_pool function, which performs the search.
Edit a pool.
Remove a pool.
List prefixes and return JSON encoded result.
Search prefixes. Does not yet incorporate all the functions of the search_prefix API function due to difficulties with transferring a complete 'dict-to-sql' encoded data structure. Instead, a list of prefix attributes can be given which will be matched with the 'equals' operator if notheing else is specified. If multiple attributes are given, they will be combined with the 'and' operator. Currently, it is not possible to specify different operators for different attributes.
Perform a smart search. The smart search function tries extract a query from a text string. This query is then passed to the search_prefix function, which performs the search.
Add prefix according to the specification. The following keys can be used: vrf ID of VRF to place the prefix in prefix the prefix to add if already known family address family (4 or 6) description A short description expires Expiry time of assignment comment Longer comment node Hostname of node type Type of prefix; reservation, assignment, host status Status of prefix; assigned, reserved, quarantine pool ID of pool country Country where the prefix is used order_id Order identifier customer_id Customer identifier vlan VLAN ID alarm_priority Alarm priority of prefix monitor If the prefix should be monitored or not from-prefix A prefix the prefix is to be allocated from from-pool A pool (ID) the prefix is to be allocated from prefix_length Prefix length of allocated prefix
Remove a prefix.
Add VRF to filter list session variable
Remove VRF to filter list session variable
Return VRF filter list from session variable Before returning list, make a search for all VRFs currently in the list to verify that they still exist.
List Tags and return JSON encoded result.
Read the configuration file
Display NIPAP version info
Initialize auth backends.
Returns an authentication object. Examines the auth backend given after the '@' in the username and returns a suitable instance of a subclass of the BaseAuth class. * `username` [string] Username to authenticate as. * `password` [string] Password to authenticate with. * `authoritative_source` [string] Authoritative source of the query. * `auth_options` [dict] A dict which, if authenticated as a trusted user, can override `username` and `authoritative_source`.
Verify authentication. Returns True/False dependant on whether the authentication succeeded or not.
Set up database Creates tables required for the authentication module.
Verify authentication. Returns True/False dependant on whether the authentication succeeded or not.
Fetch the user from the database The function will return None if the user is not found
Add user to SQLite database. * `username` [string] Username of new user. * `password` [string] Password of new user. * `full_name` [string] Full name of new user. * `trusted` [boolean] Whether the new user should be trusted or not. * `readonly` [boolean] Whether the new user can only read or not
Remove user from the SQLite database. * `username` [string] Username of user to remove.
Modify user in SQLite database. Since username is used as primary key and we only have a single argument for it we can't modify the username right now.
List all users.
Generate password hash.
Adds readwrite authorization This will check if the user is a readonly user and if so reject the query. Apply this decorator to readwrite functions.
Parse the 'expires' attribute, guessing what format it is in and returning a datetime
Create the INET type and an Inet adapter.
Return true if given arg is a valid IPv4 address
Return address-family (4 or 6) for IP or None if invalid address
Open database connection
Execute query, catch and log errors.
Expand a dict so it fits in a INSERT clause
Expand a dict so it fits in a INSERT clause
Get rows updated by last update query * `function` [function] Function to use for searching (one of the search_* functions). Helper function used to fetch all rows which was updated by the latest UPDATE ... RETURNING id query.
Split a query string into its parts
Get the schema version of the nipap psql db.
Install nipap database schema
Upgrade nipap database schema
Expand VRF specification to SQL. id [integer] internal database id of VRF name [string] name of VRF A VRF is referenced either by its internal database id or by its name. Both are used for exact matching and so no wildcard or regular expressions are allowed. Only one key may be used and an error will be thrown if both id and name is specified.
Add a new VRF. * `auth` [BaseAuth] AAA options. * `attr` [vrf_attr] The news VRF's attributes. Add a VRF based on the values stored in the `attr` dict. Returns a dict describing the VRF which was added. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_vrf` for full understanding.
Remove a VRF. * `auth` [BaseAuth] AAA options. * `spec` [vrf_spec] A VRF specification. Remove VRF matching the `spec` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_vrf` for full understanding.
Return a list of VRFs matching `spec`. * `auth` [BaseAuth] AAA options. * `spec` [vrf_spec] A VRF specification. If omitted, all VRFs are returned. Returns a list of dicts. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_vrf` for full understanding.
Get a VRF based on prefix spec Shorthand function to reduce code in the functions below, since more or less all of them needs to perform the actions that are specified here. The major difference to :func:`list_vrf` is that we always return results - empty results if no VRF is specified in prefix spec.
Update VRFs matching `spec` with attributes `attr`. * `auth` [BaseAuth] AAA options. * `spec` [vrf_spec] Attibutes specifying what VRF to edit. * `attr` [vrf_attr] Dict specifying fields to be updated and their new values. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_vrf` for full understanding.
Perform a smart search on VRF list. * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_vrf`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what VRF attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_vrf` function. If multiple search keys are detected, they are combined with a logical AND. It will basically just take each search term and try to match it against the name or description column with regex match or the VRF column with an exact match. See the :func:`search_vrf` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_vrf` for full understanding.
Parse a smart search query for VRFs This is a helper function to smart_search_vrf for easier unit testing of the parser.
Create a pool according to `attr`. * `auth` [BaseAuth] AAA options. * `attr` [pool_attr] A dict containing the attributes the new pool should have. Returns a dict describing the pool which was added. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_pool` for full understanding.
Remove a pool. * `auth` [BaseAuth] AAA options. * `spec` [pool_spec] Specifies what pool(s) to remove. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_pool` for full understanding.
Return a list of pools. * `auth` [BaseAuth] AAA options. * `spec` [pool_spec] Specifies what pool(s) to list. Of omitted, all will be listed. Returns a list of dicts. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_pool` for full understanding.
Check pool attributes.
Get a pool. Shorthand function to reduce code in the functions below, since more or less all of them needs to perform the actions that are specified here. The major difference to :func:`list_pool` is that an exception is raised if no pool matching the spec is found.
Update pool given by `spec` with attributes `attr`. * `auth` [BaseAuth] AAA options. * `spec` [pool_spec] Specifies what pool to edit. * `attr` [pool_attr] Attributes to update and their new values. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_pool` for full understanding.
Perform a smart search on pool list. * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_pool`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what pool attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_pool` function. If multiple search keys are detected, they are combined with a logical AND. It will basically just take each search term and try to match it against the name or description column with regex match. See the :func:`search_pool` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_pool` for full understanding.
Parse a smart search query for pools This is a helper function to smart_search_pool for easier unit testing of the parser.
Expand prefix specification to SQL.
Expand prefix query dict into a WHERE-clause. If you need to prefix each column reference with a table name, that can be supplied via the table_name argument.
Add a prefix and return its ID. * `auth` [BaseAuth] AAA options. * `attr` [prefix_attr] Prefix attributes. * `args` [add_prefix_args] Arguments explaining how the prefix should be allocated. Returns a dict describing the prefix which was added. Prefixes can be added in three ways; manually, from a pool or from a prefix. Manually All prefix data, including the prefix itself is specified in the `attr` argument. The `args` argument shall be omitted. From a pool Most prefixes are expected to be automatically assigned from a pool. In this case, the :attr:`prefix` key is omitted from the `attr` argument. Also the :attr:`type` key can be omitted and the prefix type will then be set to the pools default prefix type. The :func:`find_free_prefix` function is used to find available prefixes for this allocation method, see its documentation for a description of how the `args` argument should be formatted. From a prefix A prefix can also be selected from another prefix. Also in this case the :attr:`prefix` key is omitted from the `attr` argument. See the documentation for the :func:`find_free_prefix` for a description of how the `args` argument is to be formatted. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_prefix` for full understanding.
Update prefix matching `spec` with attributes `attr`. * `auth` [BaseAuth] AAA options. * `spec` [prefix_spec] Specifies the prefix to edit. * `attr` [prefix_attr] Prefix attributes. Note that there are restrictions on when and how a prefix's type can be changed; reservations can be changed to assignments and vice versa, but only if they contain no child prefixes. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_prefix` for full understanding.
Finds free prefixes in the sources given in `args`. * `auth` [BaseAuth] AAA options. * `vrf` [vrf] Full VRF-dict specifying in which VRF the prefix should be unique. * `args` [find_free_prefix_args] Arguments to the find free prefix function. Returns a list of dicts. Prefixes can be found in two ways: from a pool of from a prefix. From a pool The `args` argument is set to a dict with key :attr:`from-pool` set to a pool spec. This is the pool from which the prefix will be assigned. Also the key :attr:`family` needs to be set to the adress family (integer 4 or 6) of the requested prefix. Optionally, also the key :attr:`prefix_length` can be added to the `attr` argument, and will then override the default prefix length. Example:: args = { 'from-pool': { 'name': 'CUSTOMER-' }, 'family': 6, 'prefix_length': 64 } From a prefix Instead of specifying a pool, a prefix which will be searched for new prefixes can be specified. In `args`, the key :attr:`from-prefix` is set to list of prefixes you want to allocate from and the key :attr:`prefix_length` is set to the wanted prefix length. Example:: args = { 'from-prefix': ['192.0.2.0/24'], 'prefix_length': 27 } The key :attr:`count` can also be set in the `args` argument to specify how many prefixes that should be returned. If omitted, the default value is 1000. The internal backend function :func:`find_free_prefix` is used internally by the :func:`add_prefix` function to find available prefixes from the given sources. It's also exposed over XML-RPC, please see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.find_free_prefix` for full understanding.
List prefixes matching the `spec`. * `auth` [BaseAuth] AAA options. * `spec` [prefix_spec] Specifies prefixes to list. If omitted, all will be listed. Returns a list of dicts. This is a quite blunt tool for finding prefixes, mostly useful for fetching data about a single prefix. For more capable alternatives, see the :func:`search_prefix` or :func:`smart_search_prefix` functions. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_prefix` for full understanding.
Do the underlying database operations to delete a prefix
Remove prefix matching `spec`. * `auth` [BaseAuth] AAA options. * `spec` [prefix_spec] Specifies prefixe to remove. * `recursive` [bool] When set to True, also remove child prefixes. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_prefix` for full understanding.
Search prefix list for prefixes matching `query`. * `auth` [BaseAuth] AAA options. * `query` [dict_to_sql] How the search should be performed. * `search_options` [options_dict] Search options, see below. Returns a list of dicts. The `query` argument passed to this function is designed to be able to express quite advanced search filters. It is internally expanded to an SQL WHERE-clause. The `query` is a dict with three elements, where one specifies the operation to perform and the two other specifies its arguments. The arguments can themselves be `query` dicts, i.e. nested, to build more complex queries. The :attr:`operator` key specifies what operator should be used for the comparison. Currently the following operators are supported: * :data:`and` - Logical AND * :data:`or` - Logical OR * :data:`equals_any` - Equality of any element in array * :data:`equals` - Equality; = * :data:`not_equals` - Inequality; != * :data:`less` - Less than; < * :data:`less_or_equal` - Less than or equal to; <= * :data:`greater` - Greater than; > * :data:`greater_or_equal` - Greater than or equal to; >= * :data:`like` - SQL LIKE * :data:`regex_match` - Regular expression match * :data:`regex_not_match` - Regular expression not match * :data:`contains` - IP prefix contains * :data:`contains_equals` - IP prefix contains or is equal to * :data:`contained_within` - IP prefix is contained within * :data:`contained_within_equals` - IP prefix is contained within or equals The :attr:`val1` and :attr:`val2` keys specifies the values which are subjected to the comparison. :attr:`val1` can be either any prefix attribute or a query dict. :attr:`val2` can be either the value you want to compare the prefix attribute to, or a `query` dict. Example 1 - Find the prefixes which contains 192.0.2.0/24:: query = { 'operator': 'contains', 'val1': 'prefix', 'val2': '192.0.2.0/24' } This will be expanded to the pseudo-SQL query:: SELECT * FROM prefix WHERE prefix contains '192.0.2.0/24' Example 2 - Find for all assignments in prefix 192.0.2.0/24:: query = { 'operator': 'and', 'val1': { 'operator': 'equals', 'val1': 'type', 'val2': 'assignment' }, 'val2': { 'operator': 'contained_within', 'val1': 'prefix', 'val2': '192.0.2.0/24' } } This will be expanded to the pseudo-SQL query:: SELECT * FROM prefix WHERE (type == 'assignment') AND (prefix contained within '192.0.2.0/24') If you want to combine more than two expressions together with a boolean expression you need to nest them. For example, to match on three values, in this case the tag 'foobar' and a prefix-length between /10 and /24, the following could be used:: query = { 'operator': 'and', 'val1': { 'operator': 'and', 'val1': { 'operator': 'greater', 'val1': 'prefix_length', 'val2': 9 }, 'val2': { 'operator': 'less_or_equal', 'val1': 'prefix_length', 'val2': 24 } }, 'val2': { 'operator': 'equals_any', 'val1': 'tags', 'val2': 'foobar' } } The `options` argument provides a way to alter the search result to assist in client implementations. Most options regard parent and children prefixes, that is the prefixes which contain the prefix(es) matching the search terms (parents) or the prefixes which are contained by the prefix(es) matching the search terms. The search options can also be used to limit the number of rows returned. The following options are available: * :attr:`parents_depth` - How many levels of parents to return. Set to :data:`-1` to include all parents. * :attr:`children_depth` - How many levels of children to return. Set to :data:`-1` to include all children. * :attr:`include_all_parents` - Include all parents, no matter what depth is specified. * :attr:`include_all_children` - Include all children, no matter what depth is specified. * :attr:`max_result` - The maximum number of prefixes to return (default :data:`50`). * :attr:`offset` - Offset the result list this many prefixes (default :data:`0`). The options above gives the possibility to specify how many levels of parent and child prefixes to return in addition to the prefixes that actually matched the search terms. This is done by setting the :attr:`parents_depth` and :attr:`children depth` keys in the `search_options` dict to an integer value. In addition to this it is possible to get all all parents and/or children included in the result set even though they are outside the limits set with :attr:`*_depth`. The extra prefixes included will have the attribute :attr:`display` set to :data:`false` while the other ones (the actual search result togther with the ones included due to given depth) :attr:`display` set to :data:`true`. This feature is usable obtain search results with some context given around them, useful for example when displaying prefixes in a tree without the need to implement client side IP address logic. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.search_prefix` for full understanding.
Perform a smart search on prefix list. * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_prefix`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what prefix attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_prefix` function. If multiple search keys are detected, they are combined with a logical AND. It tries to automatically detect IP addresses and prefixes and put these into the `query` dict with "contains_within" operators and so forth. See the :func:`search_prefix` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_prefix` for full understanding.
Parse a smart search query for prefixes This is a helper function to smart_search_prefix for easier unit testing of the parser.
List AS numbers matching `spec`. * `auth` [BaseAuth] AAA options. * `spec` [asn_spec] An automous system number specification. If omitted, all ASNs are returned. Returns a list of dicts. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_asn` for full understanding.
Add AS number to NIPAP. * `auth` [BaseAuth] AAA options. * `attr` [asn_attr] ASN attributes. Returns a dict describing the ASN which was added. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_asn` for full understanding.
Edit AS number * `auth` [BaseAuth] AAA options. * `asn` [integer] AS number to edit. * `attr` [asn_attr] New AS attributes. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_asn` for full understanding.
Remove an AS number. * `auth` [BaseAuth] AAA options. * `spec` [asn] An ASN specification. Remove ASNs matching the `asn` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_asn` for full understanding.
Search ASNs for entries matching 'query' * `auth` [BaseAuth] AAA options. * `query` [dict_to_sql] How the search should be performed. * `search_options` [options_dict] Search options, see below. Returns a list of dicts. The `query` argument passed to this function is designed to be able to specify how quite advanced search operations should be performed in a generic format. It is internally expanded to a SQL WHERE-clause. The `query` is a dict with three elements, where one specifies the operation to perform and the two other specifies its arguments. The arguments can themselves be `query` dicts, to build more complex queries. The :attr:`operator` key specifies what operator should be used for the comparison. Currently the following operators are supported: * :data:`and` - Logical AND * :data:`or` - Logical OR * :data:`equals` - Equality; = * :data:`not_equals` - Inequality; != * :data:`like` - SQL LIKE * :data:`regex_match` - Regular expression match * :data:`regex_not_match` - Regular expression not match The :attr:`val1` and :attr:`val2` keys specifies the values which are subjected to the comparison. :attr:`val1` can be either any prefix attribute or an entire query dict. :attr:`val2` can be either the value you want to compare the prefix attribute to, or an entire `query` dict. The search options can also be used to limit the number of rows returned or set an offset for the result. The following options are available: * :attr:`max_result` - The maximum number of prefixes to return (default :data:`50`). * :attr:`offset` - Offset the result list this many prefixes (default :data:`0`). This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.search_tag` for full understanding.
Perform a smart search operation among AS numbers * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_asn`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what ASN attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_asn` function. If multiple search keys are detected, they are combined with a logical AND. See the :func:`search_asn` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_asn` for full understanding.
Parse a smart search query for ASNs This is a helper function to smart_search_asn for easier unit testing of the parser.
Expand Tag query dict into a WHERE-clause. If you need to prefix each column reference with a table name, that can be supplied via the table_name argument.
Create, configure and return the routes Mapper
Set up the global pynipap connection object
Returns pool to work with Returns a pynipap.Pool object representing the pool we are working with.
Returns VRF to work in Returns a pynipap.VRF object representing the VRF we are working in. If there is a VRF set globally, return this. If not, fetch the VRF named 'arg'. If 'arg' is None, fetch the default_vrf attribute from the config file and return this VRF.
List pools matching a search criteria
List VRFs matching a search criteria
List prefixes matching 'arg'
Return a prefix based on options passed from command line Used by add_prefix() and add_prefix_from_pool() to avoid duplicate parsing
Add prefix to NIPAP
Add prefix using from-pool to NIPAP
Add VRF to NIPAP
Add a pool.
View a single VRF
View a single pool
View a single prefix.
Remove VRF
Remove pool
Remove prefix
Modify a VRF with the options set in opts
Modify a pool with the options set in opts
Expand a pool with the ranges set in opts
Shrink a pool by removing the ranges in opts from it
Modify the prefix 'arg' with the options 'opts'
Add attributes to a prefix
Add attributes to a VRF
Remove attributes from a prefix
Add attributes to a pool
Remove attributes from a prefix
Returns valid string completions Takes the string 'key' and compares it to each of the strings in 'haystack'. The ones which beginns with 'key' are returned as result.
Complete NIPAP prefix type
Complete member prefixes of pool
Complete node hostname This function is currently a bit special as it looks in the config file for a command to use to complete a node hostname from an external system. It is configured by setting the config attribute "complete_node_cmd" to a shell command. The string "%search_string%" in the command will be replaced by the current search string.
Returns list of matching pool names
Returns list of matching VRFs
Returns list of matching VRFs Includes "virtual" VRF 'all' which is used in search operations
Parse a smart search string and return it in an AST like form
Parse matching expression in form key <op> value For example; vlan > 1 node = FOO-BAR
Do magic matching of single words or quoted string
Do magic matching of single words or quoted string
Examine the current matching key Extracts information, such as function to execute and command options, from the current key (passed to function as 'key_name' and 'key_val').
Extract command and options from string. The tree argument should contain a specifically formatted dict which describes the available commands, options, arguments and callbacks to methods for completion of arguments. TODO: document dict format The inp_cmd argument should contain a list of strings containing the complete command to parse, such as sys.argv (without the first element which specified the command itself).
Return list of valid completions Returns a list of valid completions on the current level in the tree. If an element of type 'value' is found, its complete callback function is called (if set).
Return list of valid next values
Add a pool.
Edit a pool.
Remove pool.
Remove a prefix from pool 'id'.
Add hosts from ipplan to networks object.
Gather network and host information from ipplan export files.
Put your network information in the prefix object.
Put your host information in the prefix object.
Get version of nipapd we're connected to. Maps to the function :py:func:`nipap.xmlrpc.NipapXMLRPC.version` in the XML-RPC API. Please see the documentation for the XML-RPC function for information regarding the return value.
Get schema version of database we're connected to. Maps to the function :py:func:`nipap.backend.Nipap._get_db_version` in the backend. Please see the documentation for the backend function for information regarding the return value.
Converts XML-RPC Fault objects to Pynipap-exceptions. TODO: Is this one neccesary? Can be done inline...
Create new Tag-object from dict. Suitable for creating objects from XML-RPC data. All available keys must exist.
Search tags. For more information, see the backend function :py:func:`nipap.backend.Nipap.search_tag`.
List VRFs. Maps to the function :py:func:`nipap.backend.Nipap.list_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Create new VRF-object from dict. Suitable for creating objects from XML-RPC data. All available keys must exist.
Get the VRF with id 'id'.
Search VRFs. Maps to the function :py:func:`nipap.backend.Nipap.search_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Perform a smart VRF search. Maps to the function :py:func:`nipap.backend.Nipap.smart_search_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Save changes made to object to NIPAP. If the object represents a new VRF unknown to NIPAP (attribute `id` is `None`) this function maps to the function :py:func:`nipap.backend.Nipap.add_vrf` in the backend, used to create a new VRF. Otherwise it maps to the function :py:func:`nipap.backend.Nipap.edit_vrf` in the backend, used to modify the VRF. Please see the documentation for the backend functions for information regarding input arguments and return values.
Remove VRF. Maps to the function :py:func:`nipap.backend.Nipap.remove_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Save changes made to pool to NIPAP. If the object represents a new pool unknown to NIPAP (attribute `id` is `None`) this function maps to the function :py:func:`nipap.backend.Nipap.add_pool` in the backend, used to create a new pool. Otherwise it maps to the function :py:func:`nipap.backend.Nipap.edit_pool` in the backend, used to modify the pool. Please see the documentation for the backend functions for information regarding input arguments and return values.
Get the pool with id 'id'.
Search pools. Maps to the function :py:func:`nipap.backend.Nipap.search_pool` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Perform a smart pool search. Maps to the function :py:func:`nipap.backend.Nipap.smart_search_pool` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Create new Pool-object from dict. Suitable for creating objects from XML-RPC data. All available keys must exist.
List pools. Maps to the function :py:func:`nipap.backend.Nipap.list_pool` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Get the prefix with id 'id'.
Finds a free prefix. Maps to the function :py:func:`nipap.backend.Nipap.find_free_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Search for prefixes. Maps to the function :py:func:`nipap.backend.Nipap.search_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Perform a smart prefix search. Maps to the function :py:func:`nipap.backend.Nipap.smart_search_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
List prefixes. Maps to the function :py:func:`nipap.backend.Nipap.list_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Save prefix to NIPAP. If the object represents a new prefix unknown to NIPAP (attribute `id` is `None`) this function maps to the function :py:func:`nipap.backend.Nipap.add_prefix` in the backend, used to create a new prefix. Otherwise it maps to the function :py:func:`nipap.backend.Nipap.edit_prefix` in the backend, used to modify the VRF. Please see the documentation for the backend functions for information regarding input arguments and return values.
Remove the prefix. Maps to the function :py:func:`nipap.backend.Nipap.remove_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Create a Prefix object from a dict. Suitable for creating Prefix objects from XML-RPC input.
Edit a VRF
Add a new VRF.
Removes a VRF.
Place any commands to setup nipapwww here
Create a Pylons WSGI application and return it ``global_conf`` The inherited configuration for this application. Normally from the [DEFAULT] section of the Paste ini file. ``full_stack`` Whether this application provides a full WSGI stack (by default, meaning it handles its own exceptions and errors). Disable full_stack when this application is "managed" by another WSGI middleware. ``static_files`` Whether this application serves its own static files; disable when another web server is responsible for serving them. ``app_conf`` The application's local configuration. Normally specified in the [app:<name>] section of the Paste ini file (where <name> defaults to main).
Add a prefix.
Edit a prefix.
Remove a prefix.
Configure the Pylons environment via the ``pylons.config`` object
Get prefix data from NIPAP
Write the config to file
Detach a process from the controlling terminal and run it in the background as a daemon.
Parse one line
Mangle prefix result
Class decorator for XML-RPC functions that requires auth
An echo function An API test function which simply echoes what is is passed in the 'message' element in the args-dict.. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `message` [string] String to echo. * `sleep` [integer] Number of seconds to sleep before echoing. Returns a string.
Add a new VRF. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `attr` [struct] VRF attributes. Returns the internal database ID for the VRF.
Add a prefix. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `attr` [struct] Attributes to set on the new prefix. * `args` [srgs] Arguments for addition of prefix, such as what pool or prefix it should be allocated from. Returns ID of created prefix.
Remove a prefix. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `prefix` [struct] Attributes used to select what prefix to remove. * `recursive` [boolean] When set to 1, also remove child prefixes.
Find a free prefix. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `args` [struct] Arguments for the find_free_prefix-function such as what prefix or pool to allocate from.
Read content of specified NEWS file
Parse content of DCH file
Formats a record and serializes it as a JSON str. If record message isnt already a dict, initializes a new dict and uses `default_msg_fieldname` as a key as the record msg as the value.
:type record: aiologger.loggers.json.LogRecord
:type record: aiologger.loggers.json.LogRecord
Open the current base file with the (original) mode and encoding. Return the resulting stream.
Emit a record. Output the record to the file, catering for rollover as described in `do_rollover`.
Modify the filename of a log file when rotating. This is provided so that a custom filename can be provided. :param default_name: The default name for the log file.
When rotating, rotate the current log. The default implementation calls the 'rotator' attribute of the handler, if it's callable, passing the source and dest arguments to it. If the attribute isn't callable (the default is None), the source is simply renamed to the destination. :param source: The source filename. This is normally the base filename, e.g. 'test.log' :param dest: The destination filename. This is normally what the source is rotated to, e.g. 'test.log.1'.
Work out the rollover time based on the specified time. If we are rolling over at midnight or weekly, then the interval is already known. need to figure out is WHEN the next interval is. In other words, if you are rolling over at midnight, then your base interval is 1 day, but you want to start that one day clock at midnight, not now. So, we have to fudge the `rollover_at` value in order to trigger the first rollover at the right time. After that, the regular interval will take care of the rest. Note that this code doesn't care about leap seconds. :)
Determine if rollover should occur. record is not used, as we are just comparing times, but it is needed so the method signatures are the same
Determine the files to delete when rolling over.
do a rollover; in this case, a date/time stamp is appended to the filename when the rollover happens. However, you want the file to be named for the start of the interval, not the current time. If there is a backup count, then we have to get a list of matching filenames, sort them and remove the one with the oldest suffix.
Low-level logging routine which creates a LogRecord and then calls all the handlers of this logger to handle the record. Overwritten to properly handle log methods kwargs
Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler.
Actually log the specified logging record to the stream.
Pass a record to all relevant handlers. Loop through all handlers for this logger and its parents in the logger hierarchy. If no handler was found, raises an error. Stop searching up the hierarchy whenever a logger with the "propagate" attribute set to zero is found - that will be the last logger whose handlers are called.
Call the handlers for the specified record. This method is used for unpickled records received from a socket, as well as those created locally. Logger-level filtering is applied.
Creates an asyncio.Task for a msg if logging is enabled for level. Returns a dummy task otherwise.
Log msg with severity 'DEBUG'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.debug("Houston, we have a %s", "thorny problem", exc_info=1)
Log msg with severity 'INFO'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.info("Houston, we have an interesting problem", exc_info=1)
Log msg with severity 'WARNING'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.warning("Houston, we have a bit of a problem", exc_info=1)
Log msg with severity 'ERROR'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.error("Houston, we have a major problem", exc_info=1)
Log msg with severity 'CRITICAL'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.critical("Houston, we have a major disaster", exc_info=1)
Convenience method for logging an ERROR with exception information.
Perform any cleanup actions in the logging system (e.g. flushing buffers). Should be called at application exit.
From an array-like input, infer the correct vega typecode ('ordinal', 'nominal', 'quantitative', or 'temporal') Parameters ---------- data: Numpy array or Pandas Series data for which the type will be inferred ordinal_threshold: integer (default: 0) integer data will result in a 'quantitative' type, unless the number of unique values is smaller than ordinal_threshold. Adapted from code at http://github.com/altair-viz/altair/ Licence: BSD-3
Unpivot a dataframe for use with Vega/Vega-Lite The input is a frame with any number of columns, output is a frame with three columns: x value, y values, and variable names.
Validate an aggregation for use in Vega-Lite. Translate agg to one of the following supported named aggregations: ['mean', 'sum', 'median', 'min', 'max', 'count'] Parameters ---------- agg : string or callable A string Supported reductions are ['mean', 'sum', 'median', 'min', 'max', 'count']. If agg is a numpy function, the return value is the string representation. If agg is unrecognized, raise a ValueError
Obtain the packge version from a python file e.g. pkg/__init__.py See <https://packaging.python.org/en/latest/single_source_version.html>.
Draw a matrix of scatter plots. The result is an interactive pan/zoomable plot, with linked-brushing enabled by holding the shift key. Parameters ---------- frame : DataFrame The dataframe for which to draw the scatter matrix. c : string (optional) If specified, the name of the column to be used to determine the color of each point. s : string (optional) If specified, the name of the column to be used to determine the size of each point, figsize : tuple (optional) A length-2 tuple speficying the size of the figure in inches dpi : float (default=72) The dots (i.e. pixels) per inch used to convert the figure size from inches to pixels. Returns ------- chart: alt.Chart object The alt.Chart representation of the plot. See Also -------- pandas.plotting.scatter_matrix : matplotlib version of this routine
Generates an Andrews curves visualization for visualising clusters of multivariate data. Andrews curves have the functional form: f(t) = x_1/sqrt(2) + x_2 sin(t) + x_3 cos(t) + x_4 sin(2t) + x_5 cos(2t) + ... Where x coefficients correspond to the values of each dimension and t is linearly spaced between -pi and +pi. Each row of frame then corresponds to a single curve. Parameters: ----------- data : DataFrame Data to be plotted, preferably normalized to (0.0, 1.0) class_column : string Name of the column containing class names samples : integer Number of points to plot in each curve alpha: float, optional The transparency of the lines width : int, optional the width of the plot in pixels height : int, optional the height of the plot in pixels **kwds: keywords Additional options Returns: -------- chart: alt.Chart object
Parallel coordinates plotting. Parameters ---------- frame: DataFrame class_column: str Column name containing class names cols: list, optional A list of column names to use alpha: float, optional The transparency of the lines interactive : bool, optional if True (default) then produce an interactive plot width : int, optional the width of the plot in pixels height : int, optional the height of the plot in pixels var_name : string, optional the legend title value_name : string, optional the y-axis label Returns ------- chart: alt.Chart object The altair representation of the plot. See Also -------- pandas.plotting.parallel_coordinates : matplotlib version of this routine
Lag plot for time series. Parameters ---------- data: pandas.Series the time series to plot lag: integer The lag of the scatter plot, default=1 kind: string The kind of plot to use (e.g. 'scatter', 'line') **kwds: Additional keywords passed to data.vgplot.scatter Returns ------- chart: alt.Chart object
Return a hash of the contents of a dictionary
Exec a code block & return evaluation of the last line
Import the object given by clsname. If default_module is specified, import from this module.
Strip the vega-lite extension (either vl.json or json) from filename
Utility to return (prev, this, next) tuples from an iterator
Get TurboCARTO CartoCSS based on input parameters
Create a custom scheme. Args: colors (list of str): List of hex values for styling data bins (int, optional): Number of bins to style by. If not given, the number of colors will be used. bin_method (str, optional): Classification method. One of the values in :obj:`BinMethod`. Defaults to `quantiles`, which only works with quantitative data.
Return a custom scheme based on CARTOColors. Args: name (str): Name of a CARTOColor. bins (int or iterable): If an `int`, the number of bins for classifying data. CARTOColors have 7 bins max for quantitative data, and 11 max for qualitative data. If `bins` is a `list`, it is the upper range for classifying data. E.g., `bins` can be of the form ``(10, 20, 30, 40, 50)``. bin_method (str, optional): One of methods in :obj:`BinMethod`. Defaults to ``quantiles``. If `bins` is an interable, then that is the bin method that will be used and this will be ignored. .. Warning:: Input types are particularly sensitive in this function, and little feedback is given for errors. ``name`` and ``bin_method`` arguments are case-sensitive.
Checks if credentials allow for authenticated carto access
Read a table from CARTO into a pandas DataFrames. Args: table_name (str): Name of table in user's CARTO account. limit (int, optional): Read only `limit` lines from `table_name`. Defaults to ``None``, which reads the full table. decode_geom (bool, optional): Decodes CARTO's geometries into a `Shapely <https://github.com/Toblerity/Shapely>`__ object that can be used, for example, in `GeoPandas <http://geopandas.org/>`__. shared_user (str, optional): If a table has been shared with you, specify the user name (schema) who shared it. retry_times (int, optional): If the read call is rate limited, number of retries to be made Returns: pandas.DataFrame: DataFrame representation of `table_name` from CARTO. Example: .. code:: python import cartoframes cc = cartoframes.CartoContext(BASEURL, APIKEY) df = cc.read('acadia_biodiversity')
List all tables in user's CARTO account Returns: :obj:`list` of :py:class:`Table <cartoframes.analysis.Table>`
Write a DataFrame to a CARTO table. Examples: Write a pandas DataFrame to CARTO. .. code:: python cc.write(df, 'brooklyn_poverty', overwrite=True) Scrape an HTML table from Wikipedia and send to CARTO with content guessing to create a geometry from the country column. This uses a CARTO Import API param `content_guessing` parameter. .. code:: python url = 'https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy' # retrieve first HTML table from that page df = pd.read_html(url, header=0)[0] # send to carto, let it guess polygons based on the 'country' # column. Also set privacy to 'public' cc.write(df, 'life_expectancy', content_guessing=True, privacy='public') cc.map(layers=Layer('life_expectancy', color='both_sexes_life_expectancy')) Args: df (pandas.DataFrame): DataFrame to write to ``table_name`` in user CARTO account table_name (str): Table to write ``df`` to in CARTO. temp_dir (str, optional): Directory for temporary storage of data that is sent to CARTO. Defaults are defined by `appdirs <https://github.com/ActiveState/appdirs/blob/master/README.rst>`__. overwrite (bool, optional): Behavior for overwriting ``table_name`` if it exits on CARTO. Defaults to ``False``. lnglat (tuple, optional): lng/lat pair that can be used for creating a geometry on CARTO. Defaults to ``None``. In some cases, geometry will be created without specifying this. See CARTO's `Import API <https://carto.com/developers/import-api/reference/#tag/Standard-Tables>`__ for more information. encode_geom (bool, optional): Whether to write `geom_col` to CARTO as `the_geom`. geom_col (str, optional): The name of the column where geometry information is stored. Used in conjunction with `encode_geom`. **kwargs: Keyword arguments to control write operations. Options are: - `compression` to set compression for files sent to CARTO. This will cause write speedups depending on the dataset. Options are ``None`` (no compression, default) or ``gzip``. - Some arguments from CARTO's Import API. See the `params listed in the documentation <https://carto.com/developers/import-api/reference/#tag/Standard-Tables>`__ for more information. For example, when using `content_guessing='true'`, a column named 'countries' with country names will be used to generate polygons for each country. Another use is setting the privacy of a dataset. To avoid unintended consequences, avoid `file`, `url`, and other similar arguments. Returns: :py:class:`Dataset <cartoframes.datasets.Dataset>` .. note:: DataFrame indexes are changed to ordinary columns. CARTO creates an index called `cartodb_id` for every table that runs from 1 to the length of the DataFrame.
gets current privacy of a table
Updates the privacy of a dataset
Delete a table in user's CARTO account. Args: table_name (str): Name of table to delete Returns: bool: `True` if table is removed
Pull the result from an arbitrary SELECT SQL query from a CARTO account into a pandas DataFrame. Args: query (str): SELECT query to run against CARTO user database. This data will then be converted into a pandas DataFrame. decode_geom (bool, optional): Decodes CARTO's geometries into a `Shapely <https://github.com/Toblerity/Shapely>`__ object that can be used, for example, in `GeoPandas <http://geopandas.org/>`__. Returns: pandas.DataFrame: DataFrame representation of query supplied. Pandas data types are inferred from PostgreSQL data types. In the case of PostgreSQL date types, dates are attempted to be converted, but on failure a data type 'object' is used. Examples: This query gets the 10 highest values from a table and returns a dataframe. .. code:: python topten_df = cc.query( ''' SELECT * FROM my_table ORDER BY value_column DESC LIMIT 10 ''' ) This query joins points to polygons based on intersection, and aggregates by summing the values of the points in each polygon. The query returns a dataframe, with a geometry column that contains polygons. .. code:: python points_aggregated_to_polygons = cc.query( ''' SELECT polygons.*, sum(points.values) FROM polygons JOIN points ON ST_Intersects(points.the_geom, polygons.the_geom) GROUP BY polygons.the_geom, polygons.cartodb_id ''', decode_geom=True )
Pull the result from an arbitrary SQL SELECT query from a CARTO account into a pandas DataFrame. This is the default behavior, when `is_select=True` Can also be used to perform database operations (creating/dropping tables, adding columns, updates, etc.). In this case, you have to explicitly specify `is_select=False` This method is a helper for the `CartoContext.fetch` and `CartoContext.execute` methods. We strongly encourage you to use any of those methods depending on the type of query you want to run. If you want to get the results of a `SELECT` query into a pandas DataFrame, then use `CartoContext.fetch`. For any other query that performs an operation into the CARTO database, use `CartoContext.execute` Args: query (str): Query to run against CARTO user database. This data will then be converted into a pandas DataFrame. table_name (str, optional): If set (and `is_select=True`), this will create a new table in the user's CARTO account that is the result of the SELECT query provided. Defaults to None (no table created). decode_geom (bool, optional): Decodes CARTO's geometries into a `Shapely <https://github.com/Toblerity/Shapely>`__ object that can be used, for example, in `GeoPandas <http://geopandas.org/>`__. It only works for SELECT queries when `is_select=True` is_select (bool, optional): This argument has to be set depending on the query performed. True for SELECT queries, False for any other query. For the case of a SELECT SQL query (`is_select=True`) the result will be stored into a pandas DataFrame. When an arbitrary SQL query (`is_select=False`) it will perform a database operation (UPDATE, DROP, INSERT, etc.) By default `is_select=None` that means that the method will return a dataframe if the `query` starts with a `select` clause, otherwise it will just execute the query and return `None` Returns: pandas.DataFrame: When `is_select=True` and the query is actually a SELECT query this method returns a pandas DataFrame representation of query supplied otherwise returns None. Pandas data types are inferred from PostgreSQL data types. In the case of PostgreSQL date types, dates are attempted to be converted, but on failure a data type 'object' is used. Raises: CartoException: If there's any error when executing the query Examples: Query a table in CARTO and write a new table that is result of query. This query gets the 10 highest values from a table and returns a dataframe, as well as creating a new table called 'top_ten' in the CARTO account. .. code:: python topten_df = cc.query( ''' SELECT * FROM my_table ORDER BY value_column DESC LIMIT 10 ''', table_name='top_ten' ) This query joins points to polygons based on intersection, and aggregates by summing the values of the points in each polygon. The query returns a dataframe, with a geometry column that contains polygons and also creates a new table called 'points_aggregated_to_polygons' in the CARTO account. .. code:: python points_aggregated_to_polygons = cc.query( ''' SELECT polygons.*, sum(points.values) FROM polygons JOIN points ON ST_Intersects(points.the_geom, polygons.the_geom) GROUP BY polygons.the_geom, polygons.cartodb_id ''', table_name='points_aggregated_to_polygons', decode_geom=True ) Drops `my_table` .. code:: python cc.query( ''' DROP TABLE my_table ''' ) Updates the column `my_column` in the table `my_table` .. code:: python cc.query( ''' UPDATE my_table SET my_column = 1 ''' )
Produce a CARTO map visualizing data layers. Examples: Create a map with two data :py:class:`Layer <cartoframes.layer.Layer>`\s, and one :py:class:`BaseMap <cartoframes.layer.BaseMap>` layer:: import cartoframes from cartoframes import Layer, BaseMap, styling cc = cartoframes.CartoContext(BASEURL, APIKEY) cc.map(layers=[BaseMap(), Layer('acadia_biodiversity', color={'column': 'simpson_index', 'scheme': styling.tealRose(7)}), Layer('peregrine_falcon_nest_sites', size='num_eggs', color={'column': 'bird_id', 'scheme': styling.vivid(10))], interactive=True) Create a snapshot of a map at a specific zoom and center:: cc.map(layers=Layer('acadia_biodiversity', color='simpson_index'), interactive=False, zoom=14, lng=-68.3823549, lat=44.3036906) Args: layers (list, optional): List of zero or more of the following: - :py:class:`Layer <cartoframes.layer.Layer>`: cartoframes :py:class:`Layer <cartoframes.layer.Layer>` object for visualizing data from a CARTO table. See :py:class:`Layer <cartoframes.layer.Layer>` for all styling options. - :py:class:`BaseMap <cartoframes.layer.BaseMap>`: Basemap for contextualizng data layers. See :py:class:`BaseMap <cartoframes.layer.BaseMap>` for all styling options. - :py:class:`QueryLayer <cartoframes.layer.QueryLayer>`: Layer from an arbitrary query. See :py:class:`QueryLayer <cartoframes.layer.QueryLayer>` for all styling options. interactive (bool, optional): Defaults to ``True`` to show an interactive slippy map. Setting to ``False`` creates a static map. zoom (int, optional): Zoom level of map. Acceptable values are usually in the range 0 to 19. 0 has the entire earth on a single tile (256px square). Zoom 19 is the size of a city block. Must be used in conjunction with ``lng`` and ``lat``. Defaults to a view to have all data layers in view. lat (float, optional): Latitude value for the center of the map. Must be used in conjunction with ``zoom`` and ``lng``. Defaults to a view to have all data layers in view. lng (float, optional): Longitude value for the center of the map. Must be used in conjunction with ``zoom`` and ``lat``. Defaults to a view to have all data layers in view. size (tuple, optional): List of pixel dimensions for the map. Format is ``(width, height)``. Defaults to ``(800, 400)``. ax: matplotlib axis on which to draw the image. Only used when ``interactive`` is ``False``. Returns: IPython.display.HTML or matplotlib Axes: Interactive maps are rendered as HTML in an `iframe`, while static maps are returned as matplotlib Axes objects or IPython Image.
gets geometry type(s) of specified layer
Find all boundaries available for the world or a `region`. If `boundary` is specified, get all available boundary polygons for the region specified (if any). This method is espeically useful for getting boundaries for a region and, with :py:meth:`CartoContext.data <cartoframes.context.CartoContext.data>` and :py:meth:`CartoContext.data_discovery <cartoframes.context.CartoContext.data_discovery>`, getting tables of geometries and the corresponding raw measures. For example, if you want to analyze how median income has changed in a region (see examples section for more). Examples: Find all boundaries available for Australia. The columns `geom_name` gives us the name of the boundary and `geom_id` is what we need for the `boundary` argument. .. code:: python import cartoframes cc = cartoframes.CartoContext('base url', 'api key') au_boundaries = cc.data_boundaries(region='Australia') au_boundaries[['geom_name', 'geom_id']] Get the boundaries for Australian Postal Areas and map them. .. code:: python from cartoframes import Layer au_postal_areas = cc.data_boundaries(boundary='au.geo.POA') cc.write(au_postal_areas, 'au_postal_areas') cc.map(Layer('au_postal_areas')) Get census tracts around Idaho Falls, Idaho, USA, and add median income from the US census. Without limiting the metadata, we get median income measures for each census in the Data Observatory. .. code:: python cc = cartoframes.CartoContext('base url', 'api key') # will return DataFrame with columns `the_geom` and `geom_ref` tracts = cc.data_boundaries( boundary='us.census.tiger.census_tract', region=[-112.096642,43.429932,-111.974213,43.553539]) # write geometries to a CARTO table cc.write(tracts, 'idaho_falls_tracts') # gather metadata needed to look up median income median_income_meta = cc.data_discovery( 'idaho_falls_tracts', keywords='median income', boundaries='us.census.tiger.census_tract') # get median income data and original table as new dataframe idaho_falls_income = cc.data( 'idaho_falls_tracts', median_income_meta, how='geom_refs') # overwrite existing table with newly-enriched dataframe cc.write(idaho_falls_income, 'idaho_falls_tracts', overwrite=True) Args: boundary (str, optional): Boundary identifier for the boundaries that are of interest. For example, US census tracts have a boundary ID of ``us.census.tiger.census_tract``, and Brazilian Municipios have an ID of ``br.geo.municipios``. Find IDs by running :py:meth:`CartoContext.data_boundaries <cartoframes.context.CartoContext.data_boundaries>` without any arguments, or by looking in the `Data Observatory catalog <http://cartodb.github.io/bigmetadata/>`__. region (str, optional): Region where boundary information or, if `boundary` is specified, boundary polygons are of interest. `region` can be one of the following: - table name (str): Name of a table in user's CARTO account - bounding box (list of float): List of four values (two lng/lat pairs) in the following order: western longitude, southern latitude, eastern longitude, and northern latitude. For example, Switzerland fits in ``[5.9559111595,45.8179931641,10.4920501709,47.808380127]`` timespan (str, optional): Specific timespan to get geometries from. Defaults to use the most recent. See the Data Observatory catalog for more information. decode_geom (bool, optional): Whether to return the geometries as Shapely objects or keep them encoded as EWKB strings. Defaults to False. include_nonclipped (bool, optional): Optionally include non-shoreline-clipped boundaries. These boundaries are the raw boundaries provided by, for example, US Census Tiger. Returns: pandas.DataFrame: If `boundary` is specified, then all available boundaries and accompanying `geom_refs` in `region` (or the world if `region` is ``None`` or not specified) are returned. If `boundary` is not specified, then a DataFrame of all available boundaries in `region` (or the world if `region` is ``None``)
Discover Data Observatory measures. This method returns the full Data Observatory metadata model for each measure or measures that match the conditions from the inputs. The full metadata in each row uniquely defines a measure based on the timespan, geographic resolution, and normalization (if any). Read more about the metadata response in `Data Observatory <https://carto.com/docs/carto-engine/data/measures-functions/#obs_getmetaextent-geometry-metadata-json-max_timespan_rank-max_score_rank-target_geoms>`__ documentation. Internally, this method finds all measures in `region` that match the conditions set in `keywords`, `regex`, `time`, and `boundaries` (if any of them are specified). Then, if `boundaries` is not specified, a geographical resolution for that measure will be chosen subject to the type of region specified: 1. If `region` is a table name, then a geographical resolution that is roughly equal to `region size / number of subunits`. 2. If `region` is a country name or bounding box, then a geographical resolution will be chosen roughly equal to `region size / 500`. Since different measures are in some geographic resolutions and not others, different geographical resolutions for different measures are oftentimes returned. .. tip:: To remove the guesswork in how geographical resolutions are selected, specify one or more boundaries in `boundaries`. See the boundaries section for each region in the `Data Observatory catalog <http://cartodb.github.io/bigmetadata/>`__. The metadata returned from this method can then be used to create raw tables or for augmenting an existing table from these measures using :py:meth:`CartoContext.data <cartoframes.context.CartoContext.data>`. For the full Data Observatory catalog, visit https://cartodb.github.io/bigmetadata/. When working with the metadata DataFrame returned from this method, be careful to only remove rows not columns as `CartoContext.data <cartoframes.context.CartoContext.data>` generally needs the full metadata. .. note:: Narrowing down a discovery query using the `keywords`, `regex`, and `time` filters is important for getting a manageable metadata set. Besides there being a large number of measures in the DO, a metadata response has acceptable combinations of measures with demonimators (normalization and density), and the same measure from other years. For example, setting the region to be United States counties with no filter values set will result in many thousands of measures. Examples: Get all European Union measures that mention ``freight``. .. code:: meta = cc.data_discovery('European Union', keywords='freight', time='2010') print(meta['numer_name'].values) Arguments: region (str or list of float): Information about the region of interest. `region` can be one of three types: - region name (str): Name of region of interest. Acceptable values are limited to: 'Australia', 'Brazil', 'Canada', 'European Union', 'France', 'Mexico', 'Spain', 'United Kingdom', 'United States'. - table name (str): Name of a table in user's CARTO account with geometries. The region will be the bounding box of the table. .. Note:: If a table name is also a valid Data Observatory region name, the Data Observatory name will be chosen over the table. - bounding box (list of float): List of four values (two lng/lat pairs) in the following order: western longitude, southern latitude, eastern longitude, and northern latitude. For example, Switzerland fits in ``[5.9559111595,45.8179931641,10.4920501709,47.808380127]`` .. Note:: Geometry levels are generally chosen by subdividing the region into the next smallest administrative unit. To override this behavior, specify the `boundaries` flag. For example, set `boundaries` to ``'us.census.tiger.census_tract'`` to choose US census tracts. keywords (str or list of str, optional): Keyword or list of keywords in measure description or name. Response will be matched on all keywords listed (boolean `or`). regex (str, optional): A regular expression to search the measure descriptions and names. Note that this relies on PostgreSQL's case insensitive operator ``~*``. See `PostgreSQL docs <https://www.postgresql.org/docs/9.5/static/functions-matching.html>`__ for more information. boundaries (str or list of str, optional): Boundary or list of boundaries that specify the measure resolution. See the boundaries section for each region in the `Data Observatory catalog <http://cartodb.github.io/bigmetadata/>`__. include_quantiles (bool, optional): Include quantiles calculations which are a calculation of how a measure compares to all measures in the full dataset. Defaults to ``False``. If ``True``, quantiles columns will be returned for each column which has it pre-calculated. Returns: pandas.DataFrame: A dataframe of the complete metadata model for specific measures based on the search parameters. Raises: ValueError: If `region` is a :obj:`list` and does not consist of four elements, or if `region` is not an acceptable region CartoException: If `region` is not a table in user account
Get an augmented CARTO dataset with `Data Observatory <https://carto.com/data-observatory>`__ measures. Use `CartoContext.data_discovery <#context.CartoContext.data_discovery>`__ to search for available measures, or see the full `Data Observatory catalog <https://cartodb.github.io/bigmetadata/index.html>`__. Optionally persist the data as a new table. Example: Get a DataFrame with Data Observatory measures based on the geometries in a CARTO table. .. code:: cc = cartoframes.CartoContext(BASEURL, APIKEY) median_income = cc.data_discovery('transaction_events', regex='.*median income.*', time='2011 - 2015') df = cc.data('transaction_events', median_income) Pass in cherry-picked measures from the Data Observatory catalog. The rest of the metadata will be filled in, but it's important to specify the geographic level as this will not show up in the column name. .. code:: median_income = [{'numer_id': 'us.census.acs.B19013001', 'geom_id': 'us.census.tiger.block_group', 'numer_timespan': '2011 - 2015'}] df = cc.data('transaction_events', median_income) Args: table_name (str): Name of table on CARTO account that Data Observatory measures are to be added to. metadata (pandas.DataFrame): List of all measures to add to `table_name`. See :py:meth:`CartoContext.data_discovery <cartoframes.context.CartoContext.data_discovery>` outputs for a full list of metadata columns. persist_as (str, optional): Output the results of augmenting `table_name` to `persist_as` as a persistent table on CARTO. Defaults to ``None``, which will not create a table. how (str, optional): **Not fully implemented**. Column name for identifying the geometry from which to fetch the data. Defaults to `the_geom`, which results in measures that are spatially interpolated (e.g., a neighborhood boundary's population will be calculated from underlying census tracts). Specifying a column that has the geometry identifier (for example, GEOID for US Census boundaries), results in measures directly from the Census for that GEOID but normalized how it is specified in the metadata. Returns: pandas.DataFrame: A DataFrame representation of `table_name` which has new columns for each measure in `metadata`. Raises: NameError: If the columns in `table_name` are in the ``suggested_name`` column of `metadata`. ValueError: If metadata object is invalid or empty, or if the number of requested measures exceeds 50. CartoException: If user account consumes all of Data Observatory quota
Checks if query from Layer or QueryLayer is valid
Return the bounds of all data layers involved in a cartoframes map. Args: layers (list): List of cartoframes layers. See `cartoframes.layers` for all types. Returns: dict: Dictionary of northern, southern, eastern, and western bounds of the superset of data layers. Keys are `north`, `south`, `east`, and `west`. Units are in WGS84.
CARTO VL-powered interactive map Args: layers (list of Layer-types): List of layers. One or more of :py:class:`Layer <cartoframes.contrib.vector.Layer>`, :py:class:`QueryLayer <cartoframes.contrib.vector.QueryLayer>`, or :py:class:`LocalLayer <cartoframes.contrib.vector.LocalLayer>`. context (:py:class:`CartoContext <cartoframes.context.CartoContext>`): A :py:class:`CartoContext <cartoframes.context.CartoContext>` instance size (tuple of int or str): a (width, height) pair for the size of the map. Default is None, which makes the map 100% wide and 640px tall. If specified as int, will be used as pixels, but you can also use string values for the CSS attributes. So, you could specify it as size=('75%', 250). basemap (str): - if a `str`, name of a CARTO vector basemap. One of `positron`, `voyager`, or `darkmatter` from the :obj:`BaseMaps` class - if a `dict`, Mapbox or other style as the value of the `style` key. If a Mapbox style, the access token is the value of the `token` key. bounds (dict or list): a dict with `east`,`north`,`west`,`south` properties, or a list of floats in the following order: [west, south, east, north]. If not provided the bounds will be automatically calculated to fit all features. viewport (dict): Configure where and how map will be centered. If not specified, or specified without lat / lng, automatic bounds or the bounds argument will be used to center the map. You can specify only zoom, bearing or pitch if you desire automatic bounds but want to tweak the viewport. - lng (float): Longitude to center the map on. Must specify lat as well. - lat (float): Latitude to center the map on. Must specify lng as well. - zoom (float): Zoom level. - bearing (float): A bearing, or heading, is the direction you're facing, measured clockwise as an angle from true north on a compass. (north is 0, east is 90, south is 180, and west is 270). - pitch (float): The angle towards the horizon measured in degrees, with a range between 0 and 60 degrees. Zero degrees results in a two-dimensional map, as if your line of sight forms a perpendicular angle with the earth's surface. Example: .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://your_user_name.carto.com', api_key='your api key' ) vector.vmap([vector.Layer('table in your account'), ], cc) CARTO basemap style. .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://your_user_name.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, basemap=vector.BaseMaps.darkmatter ) Custom basemap style. Here we use the Mapbox streets style, which requires an access token. .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://<username>.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, basemap={ 'style': 'mapbox://styles/mapbox/streets-v9', 'token: '<your mapbox token>' } ) Custom bounds .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://<username>.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, bounds={'west': -10, 'east': 10, 'north': -10, 'south': 10} ) Adjusting the map's viewport. .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://<username>.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, viewport={'lng': 10, 'lat': 15, 'zoom': 10, 'bearing': 90, 'pitch': 45} )
Aggregates bounding boxes of all local layers return: dict of bounding box of all bounds in layers
Takes two bounding boxes dicts and gives a new bbox that encompasses them both
Appends `prop` with `style` to layer styling
Adds interactivity syntax to the styling
normalize country name to match data obs
Creates a map named based on supplied parameters
Creates a map template based on custom parameters supplied
Setup the color scheme
Parse time inputs
Parse size inputs
Validate the options in the styles
Setups layers once geometry types and data types are known, and when a map is requested to be rendered from zero or more data layers
Choose color scheme
generates CartoCSS for time-based maps (torque)
Generate cartocss for class properties
Given an arbitrary column name, translate to a SQL-normalized column name a la CARTO's Import API will translate to Examples * 'Field: 2' -> 'field_2' * '2 Items' -> '_2_items' * 'Unnamed: 0' -> 'unnamed_0', * '201moore' -> '_201moore', * '201moore' -> '_201moore_1', * 'Acadia 1.2.3' -> 'acadia_1_2_3', * 'old_soaker' -> 'old_soaker', * '_testingTesting' -> '_testingtesting', * 1 -> '_1', * 1.0 -> '_1_0', * 'public' -> 'public', * 'SELECT' -> '_select', * 'à' -> 'a', * 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabesplittedrightnow' -> \ 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabespli', * 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabesplittedrightnow' -> \ 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabe_1', * 'all' -> '_all' Args: column_names (list): List of column names that will be SQL normalized Returns: list: List of SQL-normalized column names
Function to get CartoCSS from Python dicts
htmlify string
Temporarily ignores warnings like those emitted by the carto python sdk
Get list of cartoframes.columns.Column
Get column names and types from a query
decorator for encoding and decoding geoms
Decode encoded wkb into a shapely geometry
Checks to see if table exists
Create the read (COPY TO) query
Get column names and types from a table
Get column names and types from a table
Saves current user credentials to user directory. Args: config_loc (str, optional): Location where credentials are to be stored. If no argument is provided, it will be send to the default location. Example: .. code:: from cartoframes import Credentials creds = Credentials(username='eschbacher', key='abcdefg') creds.save() # save to default location
Retrives credentials from a file. Defaults to the user config directory
Deletes the credentials file specified in `config_file`. If no file is specified, it deletes the default user credential file. Args: config_file (str): Path to configuration file. Defaults to delete the user default location if `None`. .. Tip:: To see if there is a default user credential file stored, do the following:: >>> creds = Credentials() >>> print(creds) Credentials(username=eschbacher, key=abcdefg, base_url=https://eschbacher.carto.com/)
Update the credentials of a Credentials instance instead with new values. Args: key (str): API key of user account. Defaults to previous value if not specified. username (str): User name of account. This parameter is optional if `base_url` is not specified, but defaults to the previous value if not set. base_url (str): Base URL of user account. This parameter is optional if `username` is specified and on CARTO's cloud-based account. Generally of the form ``https://your_user_name.carto.com/`` for cloud-based accounts. If on-prem or otherwise, contact your admin. Example: .. code:: from cartoframes import Credentials # load credentials saved in previous session creds = Credentials() # set new API key creds.set(key='new_api_key') # save new creds to default user config directory creds.save() Note: If the `username` is specified but the `base_url` is not, the `base_url` will be updated to ``https://<username>.carto.com/``.
Return or set `base_url`. Args: base_url (str, optional): If set, updates the `base_url`. Otherwise returns current `base_url`. Note: This does not update the `username` attribute. Separately update the username with ``Credentials.username`` or update `base_url` and `username` at the same time with ``Credentials.set``. Example: .. code:: >>> from cartoframes import Credentials # load credentials saved in previous session >>> creds = Credentials() # returns current base_url >>> creds.base_url() 'https://eschbacher.carto.com/' # updates base_url with new value >>> creds.base_url('new_base_url')
Quick setup for a chatroom. :param str room: Roomname, if not given, a random sequence is generated and printed. :param MediaStream stream: The media stream to share, if not given a CameraStream will be created. :rtype: WebRTCRoom
Adds an object as a child in the scene graph.
Conveniience function: Adds objects as children in the scene graph.
Returns a copy of the projection matrix
Updates Camera.aspect to match the viewport's aspect ratio.
np.array: The Camera's lens-shift matrix.
np.array: The Camera's Projection Matrix. Will be an Orthographic matrix if ortho_mode is set to True.
Save Camera to a pickle file, given a filename.
Loads and Returns a Camera from a pickle file, given a filename.
Converges the two cameras to look at the specific point
Bind the FBO. Anything drawn afterward will be stored in the FBO's texture.
Unbind the FBO.
Returns int pointing to an OpenGL texture
Makes GLfloat or GLuint vector containing float or uint args. By default, newtype is 'float', but can be set to 'int' to make uint list.
Return Nx3 normal array from Nx3 vertex array.
Experimental anaglyph drawing function for VR system with red/blue glasses, used in Sirota lab. Draws a virtual scene in red and blue, from subject's (heda trackers) perspective in active scene. Note: assumes shader uses playerPos like ratcave's default shader Args: cube_fbo: texture frameBuffer object. vr_scene: virtual scene object active_scene: active scene object eye_poses: the eye positions Returns:
Draw each visible mesh in the scene from the perspective of the scene's camera and lit by its light.
Draw each visible mesh in the scene from the perspective of the scene's camera and lit by its light, and applies it to each face of cubetexture, which should be currently bound to an FBO.
Load data into a vbo
Returns a copy of the Mesh.
Loads and Returns a Mesh from a pickle file, given a filename.
Resets the uniforms to the Mesh object to the ""global"" coordinate system
Return a Mesh with (vertices, normals, texcoords) as arrays, in that order. Useful for when you want a standardized array location format across different amounts of info in each mesh.
Put array location in VAO for shader in same order as arrays given to Mesh.
Draw the Mesh if it's visible, from the perspective of the camera and lit by the light. The function sends the uniforms
Returns a 3x3 cross-product matrix from a 3-element vector.
Returns a rotation matrix to rotate from 3d vector "from_vec" to 3d vector "to_vec". Equation from https://math.stackexchange.com/questions/180418/calculate-rotation-matrix-to-align-vector-a-to-vector-b-in-3d
Generates combinations of named coordinate values, mapping them to the internal array. For Example: x, xy, xyz, y, yy, zyx, etc
The maximum number of textures available for this graphic card's fragment shader.
Applies some hard-coded texture filtering settings.
Attach the texture to a bound FBO object, for rendering to texture.
Uses Pyglet's image.load function to generate a Texture from an image file. If 'mipmap', then texture will have mipmap layers calculated.
Generate an empty texture in OpenGL
Builds Mesh from geom name in the wavefront file. Takes all keyword arguments that Mesh takes.
Sends all the key-value pairs to the graphics card. These uniform variables will be available in the currently-bound shader.
Activate this Shader, making it the currently-bound program. Any Mesh.draw() calls after bind() will have their data processed by this Shader. To unbind, call Shader.unbind(). Example:: shader.bind() mesh.draw() shader.unbind() .. note:: Shader.bind() and Shader.unbind() can be also be called implicitly by using the 'with' statement. Example of with statement with Shader:: with shader: mesh.draw()
Reads the shader programs, given the vert and frag filenames Arguments: - vert (str): The filename of the vertex shader program (ex: 'vertshader.vert') - frag (str): The filename of the fragment shader program (ex: 'fragshader.frag') Returns: - shader (Shader): The Shader using these files.
link the program, making it the active shader. .. note:: Shader.bind() is preferred here, because link() Requires the Shader to be compiled already.
Rotate so orientation is toward (x, y, z) coordinates.
Adds an object as a child in the scene graph. With modify=True, model_matrix_transform gets change from identity and prevents the changes of the coordinates of the child
Setup an example Treeview
Put widgets in the grid
Take a screenshot, crop and save
Take a screenshot for all themes available
Load the themes into the Tkinter interpreter
Append a theme dir to the Tk interpreter auto_path
Set new theme to use. Uses a direct tk call to allow usage of the themes supplied with this package. :param theme_name: name of theme to activate
Load an advanced theme that is dynamically created Applies the given modifiers to the images of the theme given and then creates a theme from these new images with the name 'advanced' and then applies this theme. Is not available without support for PNG-based themes, then raises RuntimeError.
Setup all the files required to enable an advanced theme. Copies all the files over and creates the required directories if they do not exist. :param theme_name: theme to copy the files over from :param output_dir: output directory to place the files in
Apply modifiers to the images of a theme Modifies the images using the PIL.ImageEnhance module. Using this function, theme images are modified to given them a unique look and feel. Works best with PNG-based images.
Redirect the set_theme call to also set Tk background color
Setup Toplevel.__init__ hook for background color
configure redirect to support additional options
cget redirect to support additional options
:param command: command to run on os.system :return: exit code
Build a binary distribution wheel and install it
Run the most common CI tasks
Setup Travis-CI macOS for wheel building
Set a new theme to use or return current theme name :param theme_name: name of theme to use :returns: active theme name
Like os.chdir(), but always restores the old working directory For example, code like this... old_curdir = os.getcwd() os.chdir('stuff') do_some_stuff() os.chdir(old_curdir) ...leaves the current working directory unchanged if do_some_stuff() raises an error, so it should be rewritten like this: old_curdir = os.getcwd() os.chdir('stuff') try: do_some_stuff() finally: os.chdir(old_curdir) Or equivalently, like this: with utils.temporary_chdir('stuff'): do_some_stuff()
Return an absolute path to an existing temporary directory
Return an absolute path the to /themes directory
Create directory but first delete it if it exists
Shifts the hue of an image in HSV format. :param image: PIL Image to perform operation on :param hue: value between 0 and 2.0
Turn all black pixels in an image into transparent ones
Setup platform specific network settings
Connects to WIFI
Disables any Accesspoint
Depending on the interval: returns True if its time for an update, returns False if its not yet time for an update
Return the property id from topic as integer
add a node class of HomieNode to this device
subscribe to all registered device and node topics
publish device and node properties
publish node data if node has updates
publish device and node properties, run forever
Factory method for shared memory arrays supporting all numpy dtypes.
Get configuration value from PYMP/OMP env variables.
Print synchronized.
Get the correctly distributed parallel chunks. This corresponds to using the OpenMP 'static' schedule.
Get an iterator for this threads chunk of work. This corresponds to using the OpenMP 'dynamic' schedule.
Iterate over an iterable. The iterator is executed in the host thread. The threads dynamically grab the elements. The iterator elements must hence be picklable to be transferred through the queue. If there is only one thread, no special operations are performed. Otherwise, effectively n-1 threads are used to process the iterable elements, and the host thread is used to provide them. You can specify a timeout for the clients to adhere.
Iterator implementation.
Iterator implementation.
creates (if needed) and updates the value of the key in the config with a value entered by the user Parameters ---------- config: ConfigParser object existing configuration key: string key to update instruction: string text to show in the prompt is_sensitive: bool if true, require confirmation and do not show typed characters Notes ----- sets key in config passed in
Configure information about Databricks account and default behavior. Configuration is stored in a `.apparatecfg` file. A config file must exist before this package can be used, and can be supplied either directly as a text file or generated using this configuration tool.
upload an egg to the Databricks filesystem. Parameters ---------- filename: string local location of file to upload match: FilenameMatch object match object with library_type, library_name, and version folder: string Databricks folder to upload to (e.g. '/Users/htorrence@shoprunner.com/') token: string Databricks API key host: string Databricks host (e.g. https://my-organization.cloud.databricks.com) Side Effects ------------ uploads egg to Databricks
get a list of jobs using the major version of the given library Parameters ---------- logger: logging object configured in cli_commands.py match: FilenameMatch object match object with suffix library_mapping: dict first element of get_library_mapping output token: string Databricks API key host: string Databricks host (e.g. https://my-organization.cloud.databricks.com) Returns ------- list of dictionaries containing the job id, job name, and library path for each job
returns a pair of library mappings, the first mapping library uri to a library name for all libraries in the production folder, and the second mapping library name to info for libraries in the production folder with parsable versions Parameters ---------- logger: logging object configured in cli_commands.py prod_folder: string name of folder in Databricks UI containing production libraries token: string Databricks API key host: string Databricks account url (e.g. https://fake-organization.cloud.databricks.com) Returns ------- dictionary mapping a library uri to a library name dictionary mapping library UI path to base name, major version, minor version, and id number
update libraries on jobs using same major version Parameters ---------- logger: logging object configured in cli_commands.py job_list: list of strings output of get_job_list match: FilenameMatch object match object with suffix new_library_path: string path to library in dbfs (including uri) token: string Databricks API key with admin permissions host: string Databricks account url (e.g. https://fake-organization.cloud.databricks.com) Side Effects ------------ jobs now require updated version of library
delete any other versions of the same library where: it has the same major version it has a smaller minor version it lives in prod_folder Parameters ---------- logger: logging object configured in cli_commands.py match: FilenameMatch object match object with library_name_, major_version, minor_version id_nums: dict second output of get_library_mapping token: string Databricks API key with admin permissions prod_folder: string name of folder in Databricks UI containing production libraries host: string Databricks account url (e.g. https://fake-organization.cloud.databricks.com) Side Effects ------------ delete any other versions of the same library with the same major version and smaller minor versions
upload library, update jobs using the same major version, and delete libraries with the same major and lower minor versions (depending on update_jobs and cleanup flags) Parameters ---------- logger: logging object configured in cli_commands.py path: string path with name of egg as output from setuptools (e.g. dist/new_library-1.0.0-py3.6.egg) token: string Databricks API key folder: string Databricks folder to upload to (e.g. '/Users/my_email@fake_organization.com/') update_jobs: bool if true, jobs using this library will be updated to point to the new version if false, will not touch jobs or other library versions at all cleanup: bool if true, outdated libraries will be deleted if false, nothing will be deleted Side Effects ------------ new library in Databricks if update_jobs is true, then updated jobs if update_jobs and cleanup are true, removed outdated libraries
True if self can safely replace other based on version numbers only - snapshot and branch tags are ignored
Resolve input entered as option values with config values If option values are provided (passed in as `variable`), then they are returned unchanged. If `variable` is None, then we first look for a config value to use. If no config value is found, then raise an error. Parameters ---------- variable: string or numeric value passed in as input by the user variable_name: string name of the variable, for clarity in the error message config_key: string key in the config whose value could be used to fill in the variable config: ConfigParser contains keys/values in .apparatecfg
The egg that the provided path points to will be uploaded to Databricks.
The egg that the provided path points to will be uploaded to Databricks. All jobs which use the same major version of the library will be updated to use the new version, and all version of this library in the production folder with the same major version and a lower minor version will be deleted. Unlike `upload`, `upload_and_update` does not ask for a folder because it relies on the production folder specified in the config. This is to protect against accidentally updating jobs to versions of a library still in testing/development. All egg names already in Databricks must be properly formatted with versions of the form <name>-0.0.0.
Parse a SAS token into its components. :param sas_token: The SAS token. :type sas_token: str :rtype: dict[str, str]
The offset of the event data object. :rtype: ~azure.eventhub.common.Offset
The enqueued timestamp of the event data object. :rtype: datetime.datetime
The partition key of the event data object. :rtype: bytes
Set the partition key of the event data object. :param value: The partition key to set. :type value: str or bytes
Application defined properties on the message. :param value: The application properties for the EventData. :type value: dict
The body of the event data as a string if the data is of a compatible type. :param encoding: The encoding to use for decoding message data. Default is 'UTF-8' :rtype: str or unicode
The body of the event loaded as a JSON object is the data is compatible. :param encoding: The encoding to use for decoding message data. Default is 'UTF-8' :rtype: dict
Creates a selector expression of the offset. :rtype: bytes
Returns an auth token dictionary for making calls to eventhub REST API. :rtype: str
Returns an auth token for making calls to eventhub REST API. :rtype: str
Open the Sender using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.connection.Connection
Whether the handler has completed all start up processes such as establishing the connection, session, link and authentication, and is not ready to process messages. **This function is now deprecated and will be removed in v2.0+.** :rtype: bool
Close down the handler. If the handler has already closed, this will be a no op. An optional exception can be passed in to indicate that the handler was shutdown due to error. :param exception: An optional exception if the handler is closing due to an error. :type exception: Exception
Sends an event data and blocks until acknowledgement is received or operation times out. :param event_data: The event to be sent. :type event_data: ~azure.eventhub.common.EventData :raises: ~azure.eventhub.common.EventHubError if the message fails to send. :return: The outcome of the message send. :rtype: ~uamqp.constants.MessageSendResult
Transfers an event data and notifies the callback when the operation is done. :param event_data: The event to be sent. :type event_data: ~azure.eventhub.common.EventData :param callback: Callback to be run once the message has been send. This must be a function that accepts two arguments. :type callback: callable[~uamqp.constants.MessageSendResult, ~azure.eventhub.common.EventHubError]
Wait until all transferred events have been sent.
Called when the outcome is received for a delivery. :param outcome: The outcome of the message delivery - success or failure. :type outcome: ~uamqp.constants.MessageSendResult
The EventProcessorHost can't pass itself to the AzureStorageCheckpointLeaseManager constructor because it is still being constructed. Do other initialization here also because it might throw and hence we don't want it in the constructor.
Get the checkpoint data associated with the given partition. Could return null if no checkpoint has been created for that partition. :param partition_id: The partition ID. :type partition_id: str :return: Given partition checkpoint info, or `None` if none has been previously stored. :rtype: ~azure.eventprocessorhost.checkpoint.Checkpoint
Create the given partition checkpoint if it doesn't exist.Do nothing if it does exist. The offset/sequenceNumber for a freshly-created checkpoint should be set to StartOfStream/0. :param partition_id: The partition ID. :type partition_id: str :return: The checkpoint for the given partition, whether newly created or already existing. :rtype: ~azure.eventprocessorhost.checkpoint.Checkpoint
Update the checkpoint in the store with the offset/sequenceNumber in the provided checkpoint checkpoint:offset/sequeceNumber to update the store with. :param lease: The stored lease to be updated. :type lease: ~azure.eventprocessorhost.lease.Lease :param checkpoint: The checkpoint to update the lease with. :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint
Create the lease store if it does not exist, do nothing if it does exist. :return: `True` if the lease store already exists or was created successfully, `False` if not. :rtype: bool
Return the lease info for the specified partition. Can return null if no lease has been created in the store for the specified partition. :param partition_id: The partition ID. :type partition_id: str :return: lease info for the partition, or `None`. :rtype: ~azure.eventprocessorhost.lease.Lease
Return the lease info for all partitions. A typical implementation could just call get_lease_async() on all partitions. :return: A list of lease info. :rtype: list[~azure.eventprocessorhost.lease.Lease]
Create in the store the lease info for the given partition, if it does not exist. Do nothing if it does exist in the store already. :param partition_id: The ID of a given parition. :type partition_id: str :return: the existing or newly-created lease info for the partition. :rtype: ~azure.eventprocessorhost.lease.Lease
Delete the lease info for the given partition from the store. If there is no stored lease for the given partition, that is treated as success. :param lease: The stored lease to be deleted. :type lease: ~azure.eventprocessorhost.lease.Lease
Acquire the lease on the desired partition for this EventProcessorHost. Note that it is legal to acquire a lease that is already owned by another host. Lease-stealing is how partitions are redistributed when additional hosts are started. :param lease: The stored lease to be acquired. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the lease was acquired successfully, `False` if not. :rtype: bool
Renew a lease currently held by this host. If the lease has been stolen, or expired, or released, it is not possible to renew it. You will have to call getLease() and then acquireLease() again. :param lease: The stored lease to be renewed. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the lease was renewed successfully, `False` if not. :rtype: bool
Give up a lease currently held by this host. If the lease has been stolen, or expired, releasing it is unnecessary, and will fail if attempted. :param lease: The stored lease to be released. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the lease was released successfully, `False` if not. :rtype: bool
Update the store with the information in the provided lease. It is necessary to currently hold a lease in order to update it. If the lease has been stolen, or expired, or released, it cannot be updated. Updating should renew the lease before performing the update to avoid lease expiration during the process. :param lease: The stored lease to be updated. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the updated was performed successfully, `False` if not. :rtype: bool
Open the Receiver using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.async_ops.connection_async.ConnectionAsync
Whether the handler has completed all start up processes such as establishing the connection, session, link and authentication, and is not ready to process messages. **This function is now deprecated and will be removed in v2.0+.** :rtype: bool
Create a shared access signiture token as a string literal. :returns: SAS token as string literal. :rtype: str
Create an EventHubClient from an existing auth token or token generator. :param address: The Event Hub address URL :type address: str :param sas_token: A SAS token or function that returns a SAS token. If a function is supplied, it will be used to retrieve subsequent tokens in the case of token expiry. The function should take no arguments. :type sas_token: str or callable :param eventhub: The name of the EventHub, if not already included in the address URL. :type eventhub: str :param debug: Whether to output network trace logs to the logger. Default is `False`. :type debug: bool :param http_proxy: HTTP proxy settings. This must be a dictionary with the following keys: 'proxy_hostname' (str value) and 'proxy_port' (int value). Additionally the following keys may also be present: 'username', 'password'. :type http_proxy: dict[str, Any] :param auth_timeout: The time in seconds to wait for a token to be authorized by the service. The default value is 60 seconds. If set to 0, no timeout will be enforced from the client. :type auth_timeout: int
Create an EventHubClient from a connection string. :param conn_str: The connection string. :type conn_str: str :param eventhub: The name of the EventHub, if the EntityName is not included in the connection string. :type eventhub: str :param debug: Whether to output network trace logs to the logger. Default is `False`. :type debug: bool :param http_proxy: HTTP proxy settings. This must be a dictionary with the following keys: 'proxy_hostname' (str value) and 'proxy_port' (int value). Additionally the following keys may also be present: 'username', 'password'. :type http_proxy: dict[str, Any] :param auth_timeout: The time in seconds to wait for a token to be authorized by the service. The default value is 60 seconds. If set to 0, no timeout will be enforced from the client. :type auth_timeout: int
Create an EventHubClient from an IoTHub connection string. :param conn_str: The connection string. :type conn_str: str :param debug: Whether to output network trace logs to the logger. Default is `False`. :type debug: bool :param http_proxy: HTTP proxy settings. This must be a dictionary with the following keys: 'proxy_hostname' (str value) and 'proxy_port' (int value). Additionally the following keys may also be present: 'username', 'password'. :type http_proxy: dict[str, Any] :param auth_timeout: The time in seconds to wait for a token to be authorized by the service. The default value is 60 seconds. If set to 0, no timeout will be enforced from the client. :type auth_timeout: int
Format the properties with which to instantiate the connection. This acts like a user agent over HTTP. :rtype: dict
Run the EventHubClient in blocking mode. Opens the connection and starts running all Sender/Receiver clients. Returns a list of the start up results. For a succcesful client start the result will be `None`, otherwise the exception raised. If all clients failed to start, then run will fail, shut down the connection and raise an exception. If at least one client starts up successfully the run command will succeed. :rtype: list[~azure.eventhub.common.EventHubError]
Stop the EventHubClient and all its Sender/Receiver clients.
Get details on the specified EventHub. Keys in the details dictionary include: -'name' -'type' -'created_at' -'partition_count' -'partition_ids' :rtype: dict
Add a receiver to the client for a particular consumer group and partition. :param consumer_group: The name of the consumer group. :type consumer_group: str :param partition: The ID of the partition. :type partition: str :param offset: The offset from which to start receiving. :type offset: ~azure.eventhub.common.Offset :param prefetch: The message prefetch count of the receiver. Default is 300. :type prefetch: int :operation: An optional operation to be appended to the hostname in the source URL. The value must start with `/` character. :type operation: str :rtype: ~azure.eventhub.receiver.Receiver
Add a sender to the client to EventData object to an EventHub. :param partition: Optionally specify a particular partition to send to. If omitted, the events will be distributed to available partitions via round-robin. :type parition: str :operation: An optional operation to be appended to the hostname in the target URL. The value must start with `/` character. :type operation: str :param send_timeout: The timeout in seconds for an individual event to be sent from the time that it is queued. Default value is 60 seconds. If set to 0, there will be no timeout. :type send_timeout: int :param keep_alive: The time interval in seconds between pinging the connection to keep it alive during periods of inactivity. The default value is 30 seconds. If set to `None`, the connection will not be pinged. :type keep_alive: int :param auto_reconnect: Whether to automatically reconnect the sender if a retryable error occurs. Default value is `True`. :rtype: ~azure.eventhub.sender.Sender
Create an ~uamqp.authentication.cbs_auth_async.SASTokenAuthAsync instance to authenticate the session. :param username: The name of the shared access policy. :type username: str :param password: The shared access key. :type password: str
Run the EventHubClient asynchronously. Opens the connection and starts running all AsyncSender/AsyncReceiver clients. Returns a list of the start up results. For a succcesful client start the result will be `None`, otherwise the exception raised. If all clients failed to start, then run will fail, shut down the connection and raise an exception. If at least one client starts up successfully the run command will succeed. :rtype: list[~azure.eventhub.common.EventHubError]
Stop the EventHubClient and all its Sender/Receiver clients.
Get details on the specified EventHub async. :rtype: dict
Add an async receiver to the client for a particular consumer group and partition. :param consumer_group: The name of the consumer group. :type consumer_group: str :param partition: The ID of the partition. :type partition: str :param offset: The offset from which to start receiving. :type offset: ~azure.eventhub.common.Offset :param prefetch: The message prefetch count of the receiver. Default is 300. :type prefetch: int :operation: An optional operation to be appended to the hostname in the source URL. The value must start with `/` character. :type operation: str :rtype: ~azure.eventhub.async_ops.receiver_async.ReceiverAsync
Add an async sender to the client to send ~azure.eventhub.common.EventData object to an EventHub. :param partition: Optionally specify a particular partition to send to. If omitted, the events will be distributed to available partitions via round-robin. :type partition: str :operation: An optional operation to be appended to the hostname in the target URL. The value must start with `/` character. :type operation: str :param send_timeout: The timeout in seconds for an individual event to be sent from the time that it is queued. Default value is 60 seconds. If set to 0, there will be no timeout. :type send_timeout: int :param keep_alive: The time interval in seconds between pinging the connection to keep it alive during periods of inactivity. The default value is 30 seconds. If set to `None`, the connection will not be pinged. :type keep_alive: int :param auto_reconnect: Whether to automatically reconnect the sender if a retryable error occurs. Default value is `True`. :type auto_reconnect: bool :rtype: ~azure.eventhub.async_ops.sender_async.SenderAsync
Creates a new Checkpoint from an existing checkpoint. :param checkpoint: Existing checkpoint. :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint
Init Azure Blob Lease with existing blob.
Init Azure Blob Lease from existing.
Check and return Azure Blob Lease state using Storage API.
Makes pump sync so that it can be run in a thread.
Updates pump status and logs update to console.
Sets a new partition lease to be processed by the pump. :param lease: The lease to set. :type lease: ~azure.eventprocessorhost.lease.Lease
Opens partition pump.
Safely closes the pump. :param reason: The reason for the shutdown. :type reason: str
Process pump events. :param events: List of events to be processed. :type events: list[~azure.eventhub.common.EventData]
Eventhub Override for on_open_async.
Responsible for establishing connection to event hub client throws EventHubsException, IOException, InterruptedException, ExecutionException.
Resets the pump swallows all exceptions.
Overides partition pump on closing. :param reason: The reason for the shutdown. :type reason: str
Runs the async partion reciever event loop to retrive messages from the event queue.
Handles processing errors this is never called since python recieve client doesn't have error handling implemented (TBD add fault pump handling). :param error: An error the occurred. :type error: Exception
Init with partition Id. :param partition_id: ID of a given partition. :type partition_id: str
Init with existing lease. :param lease: An existing Lease. :type lease: ~azure.eventprocessorhost.lease.Lease
Starts the host.
Updates offset based on event. :param event_data: A received EventData with valid offset and sequenceNumber. :type event_data: ~azure.eventhub.common.EventData
Gets the initial offset for processing the partition. :rtype: str
Generates a checkpoint for the partition using the curren offset and sequenceNumber for and persists to the checkpoint manager. :param event_processor_context An optional custom state value for the Event Processor. This data must be in a JSON serializable format. :type event_processor_context: str or dict
Stores the offset and sequenceNumber from the provided received EventData instance, then writes those values to the checkpoint store via the checkpoint manager. Optionally stores the state of the Event Processor along the checkpoint. :param event_data: A received EventData with valid offset and sequenceNumber. :type event_data: ~azure.eventhub.common.EventData :param event_processor_context An optional custom state value for the Event Processor. This data must be in a JSON serializable format. :type event_processor_context: str or dict :raises: ValueError if suplied event_data is None. :raises: ValueError if the sequenceNumber is less than the last checkpointed value.
Returns the parition context in the following format: "PartitionContext({EventHubPath}{ConsumerGroupName}{PartitionId}{SequenceNumber})" :rtype: str
Persists the checkpoint, and - optionally - the state of the Event Processor. :param checkpoint: The checkpoint to persist. :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint :param event_processor_context An optional custom state value for the Event Processor. This data must be in a JSON serializable format. :type event_processor_context: str or dict
Called by processor host to indicate that the event processor is being stopped. :param context: Information about the partition :type context: ~azure.eventprocessorhost.PartitionContext
Called by the processor host when a batch of events has arrived. This is where the real work of the event processor is done. :param context: Information about the partition :type context: ~azure.eventprocessorhost.PartitionContext :param messages: The events to be processed. :type messages: list[~azure.eventhub.common.EventData]
Open the Receiver using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.connection.Connection
Receive events from the EventHub. :param max_batch_size: Receive a batch of events. Batch size will be up to the maximum specified, but will return as soon as service returns no new events. If combined with a timeout and no events are retrieve before the time, the result will be empty. If no batch size is supplied, the prefetch size will be the maximum. :type max_batch_size: int :rtype: list[~azure.eventhub.common.EventData]
Returns a list of all the event hub partition IDs. :rtype: list[str]
Intializes the partition checkpoint and lease store and then calls run async.
Terminiates the partition manger.
Starts the run loop and manages exceptions and cleanup.
Intializes the partition checkpoint and lease store ensures that a checkpoint exists for all partitions. Note in this case checkpoint and lease stores are the same storage manager construct. :return: Returns the number of partitions. :rtype: int
Make attempt_renew_lease async call sync.
Throws if it runs out of retries. If it returns, action succeeded.
This is the main execution loop for allocating and manging pumps.
Updates the lease on an exisiting pump. :param partition_id: The partition ID. :type partition_id: str :param lease: The lease to be used. :type lease: ~azure.eventprocessorhost.lease.Lease
Create a new pump thread with a given lease. :param partition_id: The partition ID. :type partition_id: str :param lease: The lease to be used. :type lease: ~azure.eventprocessorhost.lease.Lease
Stops a single partiton pump. :param partition_id: The partition ID. :type partition_id: str :param reason: A reason for closing. :type reason: str
Stops all partition pumps (Note this might be wrong and need to await all tasks before returning done). :param reason: A reason for closing. :type reason: str :rtype: bool
Determines and return which lease to steal If the number of leases is a multiple of the number of hosts, then the desired configuration is that all hosts own the name number of leases, and the difference between the "biggest" owner and any other is 0. If the number of leases is not a multiple of the number of hosts, then the most even configurationpossible is for some hosts to have (self, leases/hosts) leases and others to have (self, (self, leases/hosts) + 1). For example, for 16 partitions distributed over five hosts, the distribution would be 4, 3, 3, 3, 3, or any of the possible reorderings. In either case, if the difference between this host and the biggest owner is 2 or more, then thesystem is not in the most evenly-distributed configuration, so steal one lease from the biggest. If there is a tie for biggest, we pick whichever appears first in the list because it doesn't really matter which "biggest" is trimmed down. Stealing one at a time prevents flapping because it reduces the difference between the biggest and this host by two at a time. If the starting difference is two or greater, then the difference cannot end up below 0. This host may become tied for biggest, but it cannot become larger than the host that it is stealing from. :param stealable_leases: List of leases to determine which can be stolen. :type stealable_leases: list[~azure.eventprocessorhost.lease.Lease] :param have_lease_count: Lease count. :type have_lease_count: int :rtype: ~azure.eventprocessorhost.lease.Lease
Returns a dictionary of leases by current owner.
Make attempt_renew_lease async call sync.
Attempts to renew a potential lease if possible and marks in the queue as none adds to adds to the queue.
Open the Sender using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.async_ops.connection_async.ConnectionAsync
Close down the handler. If the handler has already closed, this will be a no op. An optional exception can be passed in to indicate that the handler was shutdown due to error. :param exception: An optional exception if the handler is closing due to an error. :type exception: Exception
Wait until all transferred events have been sent.
This method opens an IP connection on the IP device :return: None
This method writes sends data to the IP device :param data: :return: None
Setup pins
Function that gets called again as soon as it finishes (forever).
Retrieve the last data update for the specified digital pin. It is intended for a polling application. :param pin: Digital pin number :returns: Last value reported for the digital pin
This command enables the rotary encoder (2 pin + ground) and will enable encoder reporting. This is a FirmataPlus feature. Encoder data is retrieved by performing a digital_read from pin a (encoder pin 1) :param pin_a: Encoder pin 1. :param pin_b: Encoder pin 2. :param cb: callback function to report encoder changes :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param hall_encoder: wheel hall_encoder - set to True to select hall encoder support support. :returns: No return value
This method retrieves the latest encoder data value. It is a FirmataPlus feature. :param pin: Encoder Pin :returns: encoder data value
Enables digital reporting. By turning reporting on for all 8 bits in the "port". This is part of Firmata's protocol specification. :param pin: Pin and all pins for this port :returns: No return value
This method will send an extended-data analog write command to the selected pin.. :param pin: 0 - 127 :param data: 0 - 0-0x4000 (14 bits) :returns: No return value
A list is returned containing the latch state for the pin, the latched value, and the time stamp [pin_num, latch_state, latched_value, time_stamp] If the the latch state is LATCH_LATCHED, the table is reset (data and timestamp set to zero) It is intended to be used for a polling application. :param pin: Pin number. :returns: [latched_state, threshold_type, threshold_value, latched_data, time_stamp]
This method requests and returns an analog map. :param cb: Optional callback reference :returns: An analog map response or None if a timeout occurs
This method retrieves the Firmata capability report :param raw: If True, it either stores or provides the callback with a report as list. If False, prints a formatted report to the console :param cb: Optional callback reference to receive a raw report :returns: capability report
This method retrieves the Firmata firmware version :param cb: Reference to a callback function :returns:If no callback is specified, the firmware version
This method retrieves a pin state report for the specified pin :param pin: Pin of interest :param cb: optional callback reference :returns: pin state report
This method retrieves the PyMata version number :returns: PyMata version number.
This method configures Arduino i2c with an optional read delay time. :param read_delay_time: firmata i2c delay time :returns: No return value
Retrieve result of last data read from i2c device. i2c_read_request should be called before trying to retrieve data. It is intended for use by a polling application. :param address: i2c :returns: last data read or None if no data is present.
This method issues an i2c read request for a single read,continuous read or a stop, specified by the read_type. Because different i2c devices return data at different rates, if a callback is not specified, the user must first call this method and then call i2c_read_data after waiting for sufficient time for the i2c device to respond. Some devices require that transmission be restarted (e.g. MMA8452Q accelerometer). Use I2C_READ | I2C_RESTART_TX for those cases. :param address: i2c device :param register: i2c register number :param number_of_bytes: number of bytes to be returned :param read_type: Constants.I2C_READ, Constants.I2C_READ_CONTINUOUSLY or Constants.I2C_STOP_READING. Constants.I2C_RESTART_TX may be OR'ed when required :param cb: optional callback reference :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value
Write data to an i2c device. :param address: i2c device address :param args: A variable number of bytes to be sent to the device passed in as a list. :returns: No return value
Periodically send a keep alive message to the Arduino. Frequency of keep alive transmission is calculated as follows: keep_alive_sent = period - (period * margin) :param period: Time period between keepalives. Range is 0-10 seconds. 0 disables the keepalive mechanism. :param margin: Safety margin to assure keepalives are sent before period expires. Range is 0.1 to 0.9 :returns: No return value
This method will call the Tone library for the selected pin. It requires FirmataPlus to be loaded onto the arduino If the tone command is set to TONE_TONE, then the specified tone will be played. Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled. :param pin: Pin number :param tone_command: Either TONE_TONE, or TONE_NO_TONE :param frequency: Frequency of tone :param duration: Duration of tone in milliseconds :returns: No return value
Send a Firmata reset command :returns: No return value
This method configures the Arduino for servo operation. :param pin: Servo control pin :param min_pulse: Minimum pulse width :param max_pulse: Maximum pulse width :returns: No return value
This method "arms" an analog pin for its data to be latched and saved in the latching table. If a callback method is provided, when latching criteria is achieved, the callback function is called with latching data notification. :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5 :param threshold_type: Constants.LATCH_GT | Constants.LATCH_LT | Constants.LATCH_GTE | Constants.LATCH_LTE :param threshold_value: numerical value - between 0 and 1023 :param cb: callback method :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: True if successful, False if parameter data is invalid
This method sets the pin mode for the specified pin. :param pin_number: Arduino Pin Number :param pin_state: INPUT/OUTPUT/ANALOG/PWM/PULLUP - for SERVO use servo_config() :param callback: Optional: A reference to a call back function to be called when pin data value changes :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value
This method sets the sampling interval for the Firmata loop method :param interval: time in milliseconds :returns: No return value
Perform an asyncio sleep for the time specified in seconds. T his method should be used in place of time.sleep() :param time: time in seconds :returns: No return value
Shutdown the application and exit :returns: No return value
Retrieve Ping (HC-SR04 type) data. The data is presented as a dictionary. The 'key' is the trigger pin specified in sonar_config() and the 'data' is the current measured distance (in centimeters) for that pin. If there is no data, the value is set to None. This is a FirmataPlus feature. :param trigger_pin: trigger pin specified in sonar_config :returns: active_sonar_map
Configure the pins,ping interval and maximum distance for an HC-SR04 type device. Single pin configuration may be used. To do so, set both the trigger and echo pins to the same value. Up to a maximum of 6 SONAR devices is supported If the maximum is exceeded a message is sent to the console and the request is ignored. NOTE: data is measured in centimeters This is FirmataPlus feature. :param trigger_pin: The pin number of for the trigger (transmitter). :param echo_pin: The pin number for the received echo. :param cb: optional callback function to report sonar data changes :param ping_interval: Minimum interval between pings. Lowest number to use is 33 ms.Max is 127 :param max_distance: Maximum distance in cm. Max is 200. :param cb_type: direct call or asyncio yield from :returns: No return value
Configure stepper motor prior to operation. This is a FirmataPlus feature. :param steps_per_revolution: number of steps per motor revolution :param stepper_pins: a list of control pin numbers - either 4 or 2 :returns: No return value
Move a stepper motor for the number of steps at the specified speed This is a FirmataPlus feature. :param motor_speed: 21 bits of data to set motor speed :param number_of_steps: 14 bits for number of steps & direction positive is forward, negative is reverse
Initialize Pixy and will enable Pixy block reporting. This is a FirmataPlusRB feature. :param cb: callback function to report Pixy blocks :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param max_blocks: Maximum number of Pixy blocks to report when many signatures are found. :returns: No return value.
Sends the setServos Pixy command. This method sets the pan/tilt servos that are plugged into Pixy's two servo ports. :param s0: value 0 to 1000 :param s1: value 0 to 1000 :returns: No return value.
Sends the setBrightness Pixy command. This method sets the brightness (exposure) of Pixy's camera. :param brightness: range between 0 and 255 with 255 being the brightest setting :returns: No return value.
Sends the setLed Pixy command. This method sets the RGB LED on front of Pixy. :param r: red range between 0 and 255 :param g: green range between 0 and 255 :param b: blue range between 0 and 255 :returns: No return value.
This is an asyncio adapted version of pyserial write. It provides a non-blocking write and returns the number of bytes written upon completion :param data: Data to be written :return: Number of bytes written
This is an asyncio adapted version of pyserial read. It provides a non-blocking read and returns a line of data read. :return: A line of data
Prints the Pixy blocks data.
Prints the Pixy blocks data.
This method checks verifies the device ID. @return: True if valid, False if not
Put the device into standby mode so that the registers can be set. @return: No return value
Set the device scale register. Device must be in standby before calling this function @param scale: scale factor @return: No return value
Set the device output data rate. Device must be in standby before calling this function @param output_data_rate: Desired data rate @return: No return value.
This method checks to see if new xyz data is available @return: Returns 0 if not available. 1 if it is available
The device returns an MSB and LSB (in that order) for each axis. These are 12 bit values - that is only the upper 4 bits of the LSB are used. To make things more confusing, firmata returns each axis as 4 bytes, and reverses the order because it looks at the world as lsb, msb order. :param callback: Callback function :returns: callback data is set with x,y,z raw (integers) followed by x,y,z corrected ( floating point) Call available() first to make sure new data is really available.
This is a utility function to wait for return data call back @return: Returns resultant data from callback
Basically the same as drive(), but omitting the right motor.
Basically the same as drive(), but omitting the left motor.
allows left motor to coast to a stop
allows left motor to coast to a stop
allows left motor to coast to a stop
allows left motor to coast to a stop
pivot() controls the pivot speed of the RedBot. The values of the pivot function inputs range from -255:255, with -255 indicating a full speed counter-clockwise rotation. 255 indicates a full speed clockwise rotation
This function reads the portrait/landscape status register of the MMA8452Q. It will return either PORTRAIT_U, PORTRAIT_D, LANDSCAPE_R, LANDSCAPE_L, or LOCKOUT. LOCKOUT indicates that the sensor is in neither p or ls. :param callback: Callback function :returns: See above.
This method sets the tap thresholds. Device must be in standby before calling this function. Set up single and double tap - 5 steps: for more info check out this app note: http://cache.freescale.com/files/sensors/doc/app_note/AN4072.pdf Set the threshold - minimum required acceleration to cause a tap. @param x_ths: x tap threshold @param y_ths: y tap threshold @param z_ths: z tap threshold @return: No return value.
This function returns any taps read by the MMA8452Q. If the function returns 0, no new taps were detected. Otherwise the function will return the lower 7 bits of the PULSE_SRC register. :param callback: Callback function :returns: 0 or lower 7 bits of the PULSE_SRC register.
This is a utility function to wait for return data call back @return: Returns resultant data from callback
Prints the Pixy blocks data.
Set digital pin 6 as a PWM output and set its output value to 128 @param my_board: A PymataCore instance @return: No Return Value
Blink LED 13 @return: No Return Value
turns RedBot to the Right
turns RedBot to the Left
This method must be called immediately after the class is instantiated. It instantiates the serial interface and then performs auto pin discovery. It is intended for use by pymata3 applications that do not use asyncio coroutines directly. :returns: No return value.
Set the selected pin to the specified value. :param pin: PWM pin number :param value: Pin value (0 - 0x4000) :returns: No return value
Set the specified pin to the specified value directly without port manipulation. :param pin: pin number :param value: pin value :returns: No return value
Set the specified pin to the specified value. :param pin: pin number :param value: pin value :returns: No return value
Disables analog reporting for a single analog pin. :param pin: Analog pin number. For example for A0, the number is 0. :returns: No return value
Disables digital reporting. By turning reporting off for this pin, Reporting is disabled for all 8 bits in the "port" :param pin: Pin and all pins for this port :returns: No return value
This command enables the rotary encoder support and will enable encoder reporting. This command is not part of StandardFirmata. For 2 pin + ground encoders, FirmataPlus is required to be used for 2 pin rotary encoder, and for hell effect wheel encoder support, FirmataPlusRB is required. Encoder data is retrieved by performing a digital_read from pin a (encoder pin_a). When using 2 hall effect sensors (e.g. 2 wheel robot) specify pin_a for 1st encoder and pin_b for 2nd encoder. :param pin_a: Encoder pin 1. :param pin_b: Encoder pin 2. :param cb: callback function to report encoder changes :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param hall_encoder: wheel hall_encoder - set to True to select hall encoder support support. :returns: No return value
Enables analog reporting. By turning reporting on for a single pin, :param pin: Analog pin number. For example for A0, the number is 0. :returns: No return value
Enables digital reporting. By turning reporting on for all 8 bits in the "port" - this is part of Firmata's protocol specification. :param pin: Pin and all pins for this port :returns: No return value
This method will send an extended-data analog write command to the selected pin. :param pin: 0 - 127 :param data: 0 - 0xfffff :returns: No return value
A list is returned containing the latch state for the pin, the latched value, and the time stamp [latched_state, threshold_type, threshold_value, latched_data, time_stamp] :param pin: Pin number. :returns: [latched_state, threshold_type, threshold_value, latched_data, time_stamp]
This method requests a Firmata analog map query and returns the results. :returns: An analog map response or None if a timeout occurs
This method requests and returns a Firmata capability query report :returns: A capability report in the form of a list
A list is returned containing the latch state for the pin, the latched value, and the time stamp [pin_num, latch_state, latched_value, time_stamp] :param pin: Pin number. :returns: [latched_state, threshold_type, threshold_value, latched_data, time_stamp]
This method retrieves the Firmata firmware version :returns: Firmata firmware version
This method returns the major and minor values for the protocol version, i.e. 2.4 :returns: Firmata protocol version
This method retrieves a pin state report for the specified pin :param pin: Pin of interest :returns: pin state report
NOTE: THIS METHOD MUST BE CALLED BEFORE ANY I2C REQUEST IS MADE This method initializes Firmata for I2c operations. :param read_delay_time (in microseconds): an optional parameter, default is 0 :returns: No Return Value
This method retrieves cached i2c data to support a polling mode. :param address: I2C device address :returns: Last cached value read
This method requests the read of an i2c device. Results are retrieved by a call to i2c_get_read_data(). or by callback. If a callback method is provided, when data is received from the device it will be sent to the callback method. Some devices require that transmission be restarted (e.g. MMA8452Q accelerometer). Use Constants.I2C_READ | Constants.I2C_END_TX_MASK for those cases. :param address: i2c device address :param register: register number (can be set to zero) :param number_of_bytes: number of bytes expected to be returned :param read_type: I2C_READ or I2C_READ_CONTINUOUSLY. I2C_END_TX_MASK may be OR'ed when required :param cb: Optional callback function to report i2c data as a result of read command :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value.
Write data to an i2c device. :param address: i2c device address :param args: A variable number of bytes to be sent to the device passed in as a list :returns: No return value.
Periodically send a keep alive message to the Arduino. Frequency of keep alive transmission is calculated as follows: keep_alive_sent = period - (period * margin) :param period: Time period between keepalives. Range is 0-10 seconds. 0 disables the keepalive mechanism. :param margin: Safety margin to assure keepalives are sent before period expires. Range is 0.1 to 0.9 :returns: No return value
This method will call the Tone library for the selected pin. It requires FirmataPlus to be loaded onto the arduino If the tone command is set to TONE_TONE, then the specified tone will be played. Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled. :param pin: Pin number :param tone_command: Either TONE_TONE, or TONE_NO_TONE :param frequency: Frequency of tone :param duration: Duration of tone in milliseconds :returns: No return value
Configure a pin as a servo pin. Set pulse min, max in ms. Use this method (not set_pin_mode) to configure a pin for servo operation. :param pin: Servo Pin. :param min_pulse: Min pulse width in ms. :param max_pulse: Max pulse width in ms. :returns: No return value
This method "arms" an analog pin for its data to be latched and saved in the latching table If a callback method is provided, when latching criteria is achieved, the callback function is called with latching data notification. Data returned in the callback list has the pin number as the first element, :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5 :param threshold_type: ANALOG_LATCH_GT | ANALOG_LATCH_LT | ANALOG_LATCH_GTE | ANALOG_LATCH_LTE :param threshold_value: numerical value - between 0 and 1023 :param cb: callback method :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: True if successful, False if parameter data is invalid
This method "arms" a digital pin for its data to be latched and saved in the latching table If a callback method is provided, when latching criteria is achieved, the callback function is called with latching data notification. Data returned in the callback list has the pin number as the first element, :param pin: Digital pin number :param threshold_value: 0 or 1 :param cb: callback function :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: True if successful, False if parameter data is invalid
This method sets the pin mode for the specified pin. For Servo, use servo_config() instead. :param pin_number: Arduino Pin Number :param pin_state: INPUT/OUTPUT/ANALOG/PWM/PULLUP - for SERVO use servo_config() :param callback: Optional: A reference to a call back function to be called when pin data value changes :param callback_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value
This method sends the desired sampling interval to Firmata. Note: Standard Firmata will ignore any interval less than 10 milliseconds :param interval: Integer value for desired sampling interval in milliseconds :returns: No return value.
This method attempts an orderly shutdown If any exceptions are thrown, just ignore them. :returns: No return value
This method is a proxy method for asyncio.sleep :param sleep_time: Sleep interval in seconds :returns: No return value.
Configure the pins,ping interval and maximum distance for an HC-SR04 type device. Single pin configuration may be used. To do so, set both the trigger and echo pins to the same value. Up to a maximum of 6 SONAR devices is supported If the maximum is exceeded a message is sent to the console and the request is ignored. NOTE: data is measured in centimeters :param trigger_pin: The pin number of for the trigger (transmitter). :param echo_pin: The pin number for the received echo. :param cb: optional callback function to report sonar data changes :param ping_interval: Minimum interval between pings. Lowest number to use is 33 ms. Max is 127ms. :param max_distance: Maximum distance in cm. Max is 200. :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value.
Retrieve Ping (HC-SR04 type) data. The data is presented as a dictionary. The 'key' is the trigger pin specified in sonar_config() and the 'data' is the current measured distance (in centimeters) for that pin. If there is no data, the value is set to None. :param trigger_pin: key into sonar data map :returns: active_sonar_map
Configure stepper motor prior to operation. This is a FirmataPlus feature. :param steps_per_revolution: number of steps per motor revolution :param stepper_pins: a list of control pin numbers - either 4 or 2 :returns: No return value.
Move a stepper motor for the number of steps at the specified speed This is a FirmataPlus feature. :param motor_speed: 21 bits of data to set motor speed :param number_of_steps: 14 bits for number of steps & direction positive is forward, negative is reverse :returns: No return value.
Initialize Pixy and enable Pixy block reporting. This is a FirmataPlusRB feature. :param cb: callback function to report Pixy blocks :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param max_blocks: Maximum number of Pixy blocks to report when many signatures are found. :returns: No return value.
Sends the setServos Pixy command. This method sets the pan/tilt servos that are plugged into Pixy's two servo ports. :param s0: value 0 to 1000 :param s1: value 0 to 1000 :returns: No return value.
Sends the setBrightness Pixy command. This method sets the brightness (exposure) of Pixy's camera. :param brightness: range between 0 and 255 with 255 being the brightest setting :returns: No return value.
Sends the setLed Pixy command. This method sets the RGB LED on front of Pixy. :param r: red range between 0 and 255 :param g: green range between 0 and 255 :param b: blue range between 0 and 255 :returns: No return value.
This is a private method. It continually accepts and interprets data coming from Firmata,and then dispatches the correct handler to process the data. :returns: This method never returns
This is a private message handler method. It is a message handler for analog messages. :param data: message data :returns: None - but saves the data in the pins structure
