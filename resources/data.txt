Wrapper for only doing the rendering on request (drastically reduces memory)
If the next line is an include statement, inserts the contents of the included file into the pending buffer.
Takes a FortranObject and adds it to the appropriate list, if not already present.
Returns the node corresponding to obj. If does not already exist then it will create it.
Adds nodes and edges to the graph as long as the maximum number of nodes is not exceeded. All edges are expected to have a reference to an entry in nodes. If the list of nodes is not added in the first hop due to graph size limitations, they are stored in hopNodes. If the graph was extended the function returns True, otherwise the result will be False.
Adds nodes and edges for generating the graph showing the relationship between modules and submodules listed in nodes.
Adds edges showing dependencies between source files listed in the nodes.
Adds edges indicating the call-tree for the procedures listed in the nodes.
What to return when there's an exception.
Return a timestamp for the provided datestring, described by RFC 7231.
Get the ttl from headers.
Get the wrapped object.
Get the entity that corresponds to URL.
Return (expiration, obj) corresponding to provided url, exercising the cache_policy as necessary.
Return true if the provided URL is allowed to agent.
Return (expiration, Agent) for the robots.txt at the provided URL.
Time this block.
Additional command line arguments for the behave management command
Additional command line arguments extracted directly from behave
Add behave's and our command line arguments to the command
Get a list of those command line arguments specified with the management command that are meant as arguments for running behave.
Apply fixtures that are registered with the @fixtures decorator.
Integrate behave_django in behave via before/after scenario hooks
Patches the context to add utility functions Sets up the base_url, and the get_url() utility function.
Sets up fixtures
Prepare and execute a HTTP POST call to AppCommand.xml end point. Returns XML ElementTree on success and None on fail.
Get status XML via HTTP and return it as XML ElementTree.
Send command via HTTP get to receiver.
Send command via HTTP post to receiver.
Create instances of additional zones for the receiver.
Get the latest status information from device. Method executes the update method for the current receiver type.
Get the latest status information from device. Method queries device via HTTP and updates instance attributes. Returns "True" on success and "False" on fail. This method is for pre 2016 AVR(-X) devices
Get the latest status information from device. Method queries device via HTTP and updates instance attributes. Returns "True" on success and "False" on fail. This method is for AVR-X devices built in 2016 and later.
Update sources list from receiver. Internal method which updates sources list of receiver after getting sources and potential renaming information from receiver.
Get name of receiver from web interface if not set.
Get receivers zone name if not set yet.
Get if sound mode is supported from device. Method executes the method for the current receiver type.
Get if sound mode is supported from device. Method queries device via HTTP. Returns "True" if sound mode supported and "False" if not. This method is for pre 2016 AVR(-X) devices
Get renamed and deleted sources lists from receiver . Internal method which queries device via HTTP to get names of renamed input sources.
Get renamed and deleted sources lists from receiver . Internal method which queries device via HTTP to get names of renamed input sources. In this method AppCommand.xml is used.
Get sources list from receiver. Internal method which queries device via HTTP to get the receiver's input sources. This method also determines the type of the receiver (avr, avr-x, avr-x-2016).
Update media data for playing devices. Internal method which queries device via HTTP to update media information (title, artist, etc.) and URL of cover image.
Get relevant status tags from XML structure with this internal method. Status is saved to internal attributes. Return dictionary of tags not found in XML.
Set input_func of device. Valid values depend on the device and should be taken from "input_func_list". Return "True" on success and "False" on fail.
Set All Zone Stereo option on the device. Calls command to activate/deactivate the mode Return "True" when successfully sent.
Set sound_mode of device. Valid values depend on the device and should be taken from "sound_mode_list". Return "True" on success and "False" on fail.
Set the matching dictionary used to match the raw sound mode.
Construct the sm_match_dict. Reverse the key value structure. The sm_match_dict is bigger, but allows for direct matching using a dictionary key access. The sound_mode_dict is uses externally to set this dictionary because that has a nicer syntax.
Match the raw_sound_mode to its corresponding sound_mode.
Toggle play pause media player.
Send play command to receiver command via HTTP post.
Send pause command to receiver command via HTTP post.
Send previous track command to receiver command via HTTP post.
Turn off receiver via HTTP get command.
Turn off receiver via HTTP get command.
Volume up receiver via HTTP get command.
Volume down receiver via HTTP get command.
Set receiver volume via HTTP get command. Volume is send in a format like -50.0. Minimum is -80.0, maximum at 18.0
Mute receiver via HTTP get command.
Return the matched current sound mode as a string.
Identify DenonAVR using SSDP and SCPD queries. Returns a list of dictionaries which includes all discovered Denon AVR devices with keys "host", "modelName", "friendlyName", "presentationURL".
Send SSDP broadcast message to discover UPnP devices. Returns a list of dictionaries with "address" (IP, PORT) and "URL" of SCPD XML for all discovered devices.
Get and evaluate SCPD XML to identified URLs. Returns dictionary with keys "host", "modelName", "friendlyName" and "presentationURL" if a Denon AVR device was found and "False" if not.
Initialize all discovered Denon AVR receivers in LAN zone. Returns a list of created Denon AVR instances. By default SSDP broadcasts are sent up to 3 times with a 2 seconds timeout.
Parses webvtt and returns timestamps for words and lines Tested on automatically generated subtitles from YouTube
setter for the framerate attribute :param framerate: :return:
Converts the given timecode to frames
Converts frames back to timecode :returns str: the string representation of the current time code
parses timecode string frames '00:00:00:00' or '00:00:00;00' or milliseconds '00:00:00:000'
Get ngrams from a text Sourced from: https://gist.github.com/dannguyen/93c2c43f4e65328b85af
Converts an array of ordered timestamps into an EDL string
Convert an srt timespan into a start and end timestamp.
Convert an srt timestamp into seconds.
Remove damaging line breaks and numbers from srt files and return a dictionary.
Search for and remove temp log files found in the output directory.
Print out timespans to be cut followed by the line number in the srt.
Concatenate video clips together and output finished video file to the output directory.
Create & concatenate video clips in groups of size BATCH_SIZE and output finished video file to output directory.
Return True if search term is found in given line, False otherwise.
Return a list of subtitle files.
Return a list of vtt files.
Takes a list of subtitle (srt) filenames, search term and search type and, returns a list of timestamps for composing a supercut.
Takes transcripts created by audiogrep/pocketsphinx, a search and search type and returns a list of timestamps for creating a supercut
Search through and find all instances of the search term in an srt or transcript, create a supercut around that instance, and output a new video file comprised of those supercuts.
INPUT: XML file with captions OUTPUT: parsed object like: [{'texlines': [u"So, I'm going to rewrite this", 'in a more concise form as'], 'time': {'hours':'1', 'min':'2','sec':44,'msec':232} }]
INPUT: array with captions, i.e. [{'texlines': [u"So, I'm going to rewrite this", 'in a more concise form as'], 'time': {'hours':'1', 'min':'2','sec':44,'msec':232} }] OUTPUT: srtformated string
调整音量大小
屏幕上下滚动 :params incrment: 1 向下滚动 -1 向上滚动
返回总字符数(考虑英文和中文在终端所占字块) return: int
切换页面线程
接受按键, 存入queue
第一次载入时查找歌词
通过歌词生成屏幕需要显示的内容
16 colors supported
256 colors supported
Call color function base on name
解析json列表,转换成utf-8
解析json字典,转换成utf-8
提供登陆的认证 这里顺带增加了 volume, channel, theme_id , netease, run_times的默认值
记录退出时的播放状态
统计用户信息
获取配置并检查是否更改
存储历史记录和登陆信息
获取channel列表
这个貌似没啥用 :params fcid, tcid: string
这里包装了一个函数,发送post_data :param ptype: n 列表无歌曲,返回新列表 e 发送歌曲完毕 b 不再播放,返回新列表 s 下一首,返回新的列表 r 标记喜欢 u 取消标记喜欢
初始获取歌曲 :params return: json
获取歌词 如果测试频繁会发如下信息: {'msg': 'You API access rate limit has been exceeded. Contact api-master@douban.com if you want higher limit. ', 'code': 1998, 'request': 'GET /j/v2/lyric'}
运行播放器（若当前已有正在运行的，强制推出） extra_cmd: 额外的参数 (list)
监控正在运行的播放器（独立线程） 播放器退出后将会设置 _exit_event
pasue状态下如果取时间会使歌曲继续, 这里加了一个_pause状态
Send a command to MPlayer. cmd: the command string expect: expect the output starts with a certain string The result, if any, is returned as a string.
互斥锁
更新队列线程
返回当前播放歌曲歌词
设置api发送的FM频道 :params channel_num: channel_list的索引值 int
获取每日推荐歌曲
获取单个歌曲
获取歌曲, 对外统一接口
设置显示信息
生成输出行 注意: 多线程终端同时输出会有bug, 导致起始位置偏移, 需要在每行加\r
每个controller需要提供run方法, 来提供启动
标题时间显示
从queue里取出字符执行命令
发送桌面通知
发送Linux桌面通知
发送Mac桌面通知
获取专辑封面
第一次桌面通知时加入图片
需要解码一下,通知需要unicode编码
时间状态
从queue里取出字符执行命令
登陆界面
通过帐号,密码请求token,返回一个dict { "user_info": { "ck": "-VQY", "play_record": { "fav_chls_count": 4, "liked": 802, "banned": 162, "played": 28368 }, "is_new_user": 0, "uid": "taizilongxu", "third_party_info": null, "url": "http://www.douban.com/people/taizilongxu/", "is_dj": false, "id": "2053207", "is_pro": false, "name": "刘小备" }, "r": 0 }
切换歌曲时刷新
打开豆瓣网页
从queue里取出字符执行命令
根据歌曲名搜索歌曲 : params : song_title: 歌曲名 limit: 搜索数量
根据歌名获取歌曲id
根据歌名搜索320k地址
因为历史列表动态更新,需要刷新
界面执行程序
Instantiate a new Trade from a dict (generally from loading a JSON response). The data used to instantiate the Trade is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeSummary from a dict (generally from loading a JSON response). The data used to instantiate the TradeSummary is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CalculatedTradeState from a dict (generally from loading a JSON response). The data used to instantiate the CalculatedTradeState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Create, replace and cancel a Trade's dependent Orders (Take Profit, Stop Loss and Trailing Stop Loss) through the Trade itself Args: accountID: Account Identifier tradeSpecifier: Specifier for the Trade takeProfit: The specification of the Take Profit to create/modify/cancel. If takeProfit is set to null, the Take Profit Order will be cancelled if it exists. If takeProfit is not provided, the exisiting Take Profit Order will not be modified. If a sub- field of takeProfit is not specified, that field will be set to a default value on create, and be inherited by the replacing order on modify. stopLoss: The specification of the Stop Loss to create/modify/cancel. If stopLoss is set to null, the Stop Loss Order will be cancelled if it exists. If stopLoss is not provided, the exisiting Stop Loss Order will not be modified. If a sub-field of stopLoss is not specified, that field will be set to a default value on create, and be inherited by the replacing order on modify. trailingStopLoss: The specification of the Trailing Stop Loss to create/modify/cancel. If trailingStopLoss is set to null, the Trailing Stop Loss Order will be cancelled if it exists. If trailingStopLoss is not provided, the exisiting Trailing Stop Loss Order will not be modified. If a sub-field of trailngStopLoss is not specified, that field will be set to a default value on create, and be inherited by the replacing order on modify. Returns: v20.response.Response containing the results from submitting the request
Instantiate a new Transaction from a dict (generally from loading a JSON response). The data used to instantiate the Transaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new ClientConfigureTransaction from a dict (generally from loading a JSON response). The data used to instantiate the ClientConfigureTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new ClientConfigureRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the ClientConfigureRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TransferFundsTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TransferFundsTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TransferFundsRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TransferFundsRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new MarketOrderTransaction from a dict (generally from loading a JSON response). The data used to instantiate the MarketOrderTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderFillTransaction from a dict (generally from loading a JSON response). The data used to instantiate the OrderFillTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderClientExtensionsModifyTransaction from a dict (generally from loading a JSON response). The data used to instantiate the OrderClientExtensionsModifyTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderClientExtensionsModifyRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the OrderClientExtensionsModifyRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeClientExtensionsModifyTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TradeClientExtensionsModifyTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeClientExtensionsModifyRejectTransaction from a dict (generally from loading a JSON response). The data used to instantiate the TradeClientExtensionsModifyRejectTransaction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeOpen from a dict (generally from loading a JSON response). The data used to instantiate the TradeOpen is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TradeReduce from a dict (generally from loading a JSON response). The data used to instantiate the TradeReduce is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new LiquidityRegenerationSchedule from a dict (generally from loading a JSON response). The data used to instantiate the LiquidityRegenerationSchedule is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new LiquidityRegenerationScheduleStep from a dict (generally from loading a JSON response). The data used to instantiate the LiquidityRegenerationScheduleStep is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OpenTradeFinancing from a dict (generally from loading a JSON response). The data used to instantiate the OpenTradeFinancing is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionFinancing from a dict (generally from loading a JSON response). The data used to instantiate the PositionFinancing is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get a list of Transactions pages that satisfy a time-based Transaction query. Args: accountID: Account Identifier fromTime: The starting time (inclusive) of the time range for the Transactions being queried. toTime: The ending time (inclusive) of the time range for the Transactions being queried. pageSize: The number of Transactions to include in each page of the results. type: A filter for restricting the types of Transactions to retreive. Returns: v20.response.Response containing the results from submitting the request
Get a stream of Transactions for an Account starting from when the request is made. Args: accountID: Account Identifier Returns: v20.response.Response containing the results from submitting the request
Instantiate a new PriceBucket from a dict (generally from loading a JSON response). The data used to instantiate the PriceBucket is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new Price from a dict (generally from loading a JSON response). The data used to instantiate the Price is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new Candlestick from a dict (generally from loading a JSON response). The data used to instantiate the Candlestick is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CandlestickData from a dict (generally from loading a JSON response). The data used to instantiate the CandlestickData is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderBook from a dict (generally from loading a JSON response). The data used to instantiate the OrderBook is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new OrderBookBucket from a dict (generally from loading a JSON response). The data used to instantiate the OrderBookBucket is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionBook from a dict (generally from loading a JSON response). The data used to instantiate the PositionBook is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionBookBucket from a dict (generally from loading a JSON response). The data used to instantiate the PositionBookBucket is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Fetch candlestick data for an instrument. Args: instrument: Name of the Instrument price: The Price component(s) to get candlestick data for. Can contain any combination of the characters "M" (midpoint candles) "B" (bid candles) and "A" (ask candles). granularity: The granularity of the candlesticks to fetch count: The number of candlesticks to return in the reponse. Count should not be specified if both the start and end parameters are provided, as the time range combined with the graularity will determine the number of candlesticks to return. fromTime: The start of the time range to fetch candlesticks for. toTime: The end of the time range to fetch candlesticks for. smooth: A flag that controls whether the candlestick is "smoothed" or not. A smoothed candlestick uses the previous candle's close price as its open price, while an unsmoothed candlestick uses the first price from its time range as its open price. includeFirst: A flag that controls whether the candlestick that is covered by the from time should be included in the results. This flag enables clients to use the timestamp of the last completed candlestick received to poll for future candlesticks but avoid receiving the previous candlestick repeatedly. dailyAlignment: The hour of the day (in the specified timezone) to use for granularities that have daily alignments. alignmentTimezone: The timezone to use for the dailyAlignment parameter. Candlesticks with daily alignment will be aligned to the dailyAlignment hour within the alignmentTimezone. Note that the returned times will still be represented in UTC. weeklyAlignment: The day of the week used for granularities that have weekly alignment. Returns: v20.response.Response containing the results from submitting the request
Fetch a price for an instrument. Accounts are not associated in any way with this endpoint. Args: instrument: Name of the Instrument time: The time at which the desired price is in effect. The current price is returned if no time is provided. Returns: v20.response.Response containing the results from submitting the request
Instantiate a new Position from a dict (generally from loading a JSON response). The data used to instantiate the Position is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new PositionSide from a dict (generally from loading a JSON response). The data used to instantiate the PositionSide is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CalculatedPositionState from a dict (generally from loading a JSON response). The data used to instantiate the CalculatedPositionState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new DynamicOrderState from a dict (generally from loading a JSON response). The data used to instantiate the DynamicOrderState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new Order from a dict (generally from loading a JSON response). The data used to instantiate the Order is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new LimitOrder from a dict (generally from loading a JSON response). The data used to instantiate the LimitOrder is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new TakeProfitOrderRequest from a dict (generally from loading a JSON response). The data used to instantiate the TakeProfitOrderRequest is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new UnitsAvailableDetails from a dict (generally from loading a JSON response). The data used to instantiate the UnitsAvailableDetails is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new UnitsAvailable from a dict (generally from loading a JSON response). The data used to instantiate the UnitsAvailable is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new GuaranteedStopLossOrderEntryData from a dict (generally from loading a JSON response). The data used to instantiate the GuaranteedStopLossOrderEntryData is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get a list of Orders for an Account Args: accountID: Account Identifier ids: List of Order IDs to retrieve state: The state to filter the requested Orders by instrument: The instrument to filter the requested orders by count: The maximum number of Orders to return beforeID: The maximum Order ID to return. If not provided the most recent Orders in the Account are returned Returns: v20.response.Response containing the results from submitting the request
Replace an Order in an Account by simultaneously cancelling it and creating a replacement Order Args: accountID: Account Identifier orderSpecifier: The Order Specifier order: Specification of the replacing Order Returns: v20.response.Response containing the results from submitting the request
Cancel a pending Order in an Account Args: accountID: Account Identifier orderSpecifier: The Order Specifier Returns: v20.response.Response containing the results from submitting the request
Update the Client Extensions for an Order in an Account. Do not set, modify, or delete clientExtensions if your account is associated with MT4. Args: accountID: Account Identifier orderSpecifier: The Order Specifier clientExtensions: The Client Extensions to update for the Order. Do not set, modify, or delete clientExtensions if your account is associated with MT4. tradeClientExtensions: The Client Extensions to update for the Trade created when the Order is filled. Do not set, modify, or delete clientExtensions if your account is associated with MT4. Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Market Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a MarketOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Limit Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a LimitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Limit Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Limit Order to replace kwargs : The arguments to create a LimitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Stop Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a StopOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Stop Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Stop Order to replace kwargs : The arguments to create a StopOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a MarketIfTouched Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a MarketIfTouchedOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending MarketIfTouched Order in an Account Args: accountID : The ID of the Account orderID : The ID of the MarketIfTouched Order to replace kwargs : The arguments to create a MarketIfTouchedOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Take Profit Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a TakeProfitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Take Profit Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Take Profit Order to replace kwargs : The arguments to create a TakeProfitOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Stop Loss Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a StopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Stop Loss Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Stop Loss Order to replace kwargs : The arguments to create a StopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to create a Trailing Stop Loss Order in an Account Args: accountID : The ID of the Account kwargs : The arguments to create a TrailingStopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Shortcut to replace a pending Trailing Stop Loss Order in an Account Args: accountID : The ID of the Account orderID : The ID of the Take Profit Order to replace kwargs : The arguments to create a TrailingStopLossOrderRequest Returns: v20.response.Response containing the results from submitting the request
Instantiate a new Instrument from a dict (generally from loading a JSON response). The data used to instantiate the Instrument is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new InstrumentCommission from a dict (generally from loading a JSON response). The data used to instantiate the InstrumentCommission is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new GuaranteedStopLossOrderLevelRestriction from a dict (generally from loading a JSON response). The data used to instantiate the GuaranteedStopLossOrderLevelRestriction is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Set the token for the v20 context Args: token: The token used to access the v20 REST api
Set the Accept-Datetime-Format header to an acceptable value Args: format: UNIX or RFC3339
Perform an HTTP request through the context Args: request: A v20.request.Request object Returns: A v20.response.Response object
Instantiate a new Account from a dict (generally from loading a JSON response). The data used to instantiate the Account is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new AccountChangesState from a dict (generally from loading a JSON response). The data used to instantiate the AccountChangesState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new AccountSummary from a dict (generally from loading a JSON response). The data used to instantiate the AccountSummary is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new CalculatedAccountState from a dict (generally from loading a JSON response). The data used to instantiate the CalculatedAccountState is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new AccountChanges from a dict (generally from loading a JSON response). The data used to instantiate the AccountChanges is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get a list of all Accounts authorized for the provided token. Args: Returns: v20.response.Response containing the results from submitting the request
Get the list of tradeable instruments for the given Account. The list of tradeable instruments is dependent on the regulatory division that the Account is located in, thus should be the same for all Accounts owned by a single user. Args: accountID: Account Identifier instruments: List of instruments to query specifically. Returns: v20.response.Response containing the results from submitting the request
Set the client-configurable portions of an Account. Args: accountID: Account Identifier alias: Client-defined alias (name) for the Account marginRate: The string representation of a decimal number. Returns: v20.response.Response containing the results from submitting the request
Fetch the user information for the specified user. This endpoint is intended to be used by the user themself to obtain their own information. Args: userSpecifier: The User Specifier Returns: v20.response.Response containing the results from submitting the request
Instantiate a new ClientPrice from a dict (generally from loading a JSON response). The data used to instantiate the ClientPrice is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new QuoteHomeConversionFactors from a dict (generally from loading a JSON response). The data used to instantiate the QuoteHomeConversionFactors is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Instantiate a new HomeConversions from a dict (generally from loading a JSON response). The data used to instantiate the HomeConversions is a shallow copy of the dict passed in, with any complex child types instantiated appropriately.
Get pricing information for a specified list of Instruments within an Account. Args: accountID: Account Identifier instruments: List of Instruments to get pricing for. since: Date/Time filter to apply to the response. Only prices and home conversions (if requested) with a time later than this filter (i.e. the price has changed after the since time) will be provided, and are filtered independently. includeUnitsAvailable: Flag that enables the inclusion of the unitsAvailable field in the returned Price objects. includeHomeConversions: Flag that enables the inclusion of the homeConversions field in the returned response. An entry will be returned for each currency in the set of all base and quote currencies present in the requested instruments list. Returns: v20.response.Response containing the results from submitting the request
Get a stream of Account Prices starting from when the request is made. This pricing stream does not include every single price created for the Account, but instead will provide at most 4 prices per second (every 250 milliseconds) for each instrument being requested. If more than one price is created for an instrument during the 250 millisecond window, only the price in effect at the end of the window is sent. This means that during periods of rapid price movement, subscribers to this stream will not be sent every price. Pricing windows for different connections to the price stream are not all aligned in the same way (i.e. they are not all aligned to the top of the second). This means that during periods of rapid price movement, different subscribers may observe different prices depending on their alignment. Args: accountID: Account Identifier instruments: List of Instruments to stream Prices for. snapshot: Flag that enables/disables the sending of a pricing snapshot when initially connecting to the stream. Returns: v20.response.Response containing the results from submitting the request
Esse método trata a data recebida de acordo com o timezone do usuário. O seu retorno é dividido em duas partes: 1) A data em si; 2) As horas; :param cDateUTC: string contendo as informações da data :param timezone: timezone do usuário do sistema :return: data e hora convertidos para a timezone do usuário
Remove special characters and strip spaces
Format datetime
Format date
Get a list of program files by expanding a list of path patterns and interpreting it as relative to the executable. This method can be used as helper for implementing the method program_files(). Contrary to the default implementation of program_files(), this method does not explicitly add the executable to the list of returned files, it assumes that required_paths contains a path that covers the executable. @param executable: the path to the executable of the tool (typically the result of executable()) @param required_paths: a list of required path patterns @param parent_dir: whether required_paths are relative to the directory of executable or the parent directory @return a list of paths as strings, suitable for result of program_files()
Get version of a tool by executing it with argument "--version" and returning stdout. @param executable: the path to the executable of the tool (typically the result of executable()) @param arg: an argument to pass to the tool to let it print its version @param use_stderr: True if the tool prints version on stderr, False for stdout @return a (possibly empty) string of output of the tool
Mark the benchmark as erroneous, e.g., because the benchmarking tool crashed. The message is intended as explanation for the user.
This method writes information about benchmark and system into txt_file.
The method output_before_run_set() calculates the length of the first column for the output in terminal and stores information about the runSet in XML. @param runSet: current run set
This function writes a simple message to terminal and logfile, when a run set is skipped. There is no message about skipping a run set in the xml-file.
This method writes the information about a run set into the txt_file.
The method output_before_run() prints the name of a file to terminal. It returns the name of the logfile. @param run: a Run object
The method output_after_run() prints filename, result, time and status of a run to terminal and stores all data in XML
The method output_after_run_set() stores the times of a run set in XML. @params cputime, walltime: accumulated times of the run set
This function creates the XML structure for a list of runs
This function adds the result values to the XML representation of a run.
This function adds the result values to the XML representation of a runSet.
This function returns the name of the file for a run set with an extension ("txt", "xml").
Formats the file name of a program for printing on console.
Write a rough string version of the XML (for temporary files).
Writes a nicely formatted XML file with DOCTYPE, and compressed if necessary.
returns a function that extracts the value for a column.
Calculate an assignment of the available CPU cores to a number of parallel benchmark executions such that each run gets its own cores without overlapping of cores between runs. In case the machine has hyper-threading, this method tries to avoid putting two different runs on the same physical core (but it does not guarantee this if the number of parallel runs is too high to avoid it). In case the machine has multiple CPUs, this method avoids splitting a run across multiple CPUs if the number of cores per run is lower than the number of cores per CPU (splitting a run over multiple CPUs provides worse performance). It will also try to split the runs evenly across all available CPUs. A few theoretically-possible cases are not implemented, for example assigning three 10-core runs on a machine with two 16-core CPUs (this would have unfair core assignment and thus undesirable performance characteristics anyway). The list of available cores is read from the cgroup file system, such that the assigned cores are a subset of the cores that the current process is allowed to use. This script does currently not support situations where the available cores are asymmetrically split over CPUs, e.g. 3 cores on one CPU and 5 on another. @param coreLimit: the number of cores for each run @param num_of_threads: the number of parallel benchmark executions @param coreSet: the list of CPU cores identifiers provided by a user, None makes benchexec using all cores @return a list of lists, where each inner list contains the cores for one run
This method does the actual work of _get_cpu_cores_per_run without reading the machine architecture from the file system in order to be testable. For description, c.f. above. Note that this method might change the input parameters! Do not call it directly, call getCpuCoresPerRun()! @param allCpus: the list of all available cores @param cores_of_package: a mapping from package (CPU) ids to lists of cores that belong to this CPU @param siblings_of_core: a mapping from each core to a list of sibling cores including the core itself (a sibling is a core sharing the same physical core)
Get an assignment of memory banks to runs that fits to the given coreAssignment, i.e., no run is allowed to use memory that is not local (on the same NUMA node) to one of its CPU cores.
Get all memory banks the kernel lists in a given directory. Such a directory can be /sys/devices/system/node/ (contains all memory banks) or /sys/devices/system/cpu/cpu*/ (contains all memory banks on the same NUMA node as that core).
Check whether the desired amount of parallel benchmarks fits in the memory. Implemented are checks for memory limits via cgroup controller "memory" and memory bank restrictions via cgroup controller "cpuset", as well as whether the system actually has enough memory installed. @param memLimit: the memory limit in bytes per run @param num_of_threads: the number of parallel benchmark executions @param memoryAssignment: the allocation of memory banks to runs (if not present, all banks are assigned to all runs)
Get the size of a memory bank in bytes.
Returns a BenchExec result status based on the output of SMACK
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Handle the options specified by add_basic_container_args(). @return: a dict that can be used as kwargs for the ContainerExecutor constructor
Define command-line arguments for output of a container (result files). @param argument_parser: an argparse parser instance
Handle the options specified by add_container_output_args(). @return: a dict that can be used as kwargs for the ContainerExecutor.execute_run()
A simple command-line interface for the containerexecutor module of BenchExec.
Given the temp directory that is created for each run, return the path to the directory where files created by the tool are stored.
This method executes the command line and waits for the termination of it, handling all setup and cleanup. @param args: the command line to run @param rootDir: None or a root directory that contains all relevant files for starting a new process @param workingDir: None or a directory which the execution should use as working directory @param output_dir: the directory where to write result files (required if result_files_pattern) @param result_files_patterns: a list of patterns of files to retrieve as result files
Execute the given command and measure its resource usage similarly to super()._start_execution(), but inside a container implemented using Linux namespaces. The command has no network access (only loopback), a fresh directory as /tmp and no write access outside of this, and it does not see other processes except itself.
Setup the filesystem layout in the container. As first step, we create a copy of all existing mountpoints in mount_base, recursively, and as "private" mounts (i.e., changes to existing mountpoints afterwards won't propagate to our copy). Then we iterate over all mountpoints and change them according to the mode the user has specified (hidden, read-only, overlay, or full-access). This has do be done for each mountpoint because overlays are not recursive. Then we chroot into the new mount hierarchy. The new filesystem layout still has a view of the host's /proc. We do not mount a fresh /proc here because the grandchild still needs the old /proc. We do simply iterate over all existing mount points and set them to read-only/overlay them, because it is easier to create a new hierarchy and chroot into it. First, we still have access to the original mountpoints while doing so, and second, we avoid race conditions if someone else changes the existing mountpoints. @param temp_dir: The base directory under which all our directories should be created.
Setup the filesystem layout in the given root directory. Create a copy of the existing proc- and dev-mountpoints in the specified root directory. Afterwards we chroot into it. @param root_dir: The path of the root directory that is used to execute the process.
Transfer files created by the tool in the container to the output directory. @param tool_output_dir: The directory under which all tool output files are created. @param working_dir: The absolute working directory of the tool in the container. @param output_dir: the directory where to write result files @param patterns: a list of patterns of files to retrieve as result files
Read an parse the XML of a table-definition file. @return: an ElementTree object for the table definition
Load all results in files that are listed in the given table-definition file. @return: a list of RunSetResult objects
Load results from given files with column definitions taken from a table-definition file. @return: a list of RunSetResult objects
Extract all columns mentioned in the result tag of a table definition file.
Extract columns that are relevant for the diff table. @param columns_to_show: (list) A list of columns that should be shown @return: (set) Set of columns that are relevant for the diff table. If none is marked relevant, the column named "status" will be returned in the set.
Return a unique identifier for a given task. @param task: the XML element that represents a task @return a tuple with filename of task as first element
Load the module with the tool-specific code.
Version of load_result for multiple input files that will be loaded concurrently.
Completely handle loading a single result file. @param result_file the file to parse @param options additional options @param run_set_id the identifier of the run set @param columns the list of columns @param columns_relevant_for_diff a set of columns that is relevant for the diff table @return a fully ready RunSetResult instance or None
This function parses an XML file that contains the results of the execution of a run set. It returns the "result" XML tag. @param resultFile: The file name of the XML file that contains the results. @param run_set_id: An optional identifier of this set of results.
This function merges the results of all RunSetResult objects. If necessary, it can merge lists of names: [A,C] + [A,B] --> [A,B,C] and add dummy elements to the results. It also ensures the same order of tasks.
Set the filelists of all RunSetResult elements so that they contain the same files in the same order. For missing files a dummy element is inserted.
Create list of rows with all data. Each row consists of several RunResults.
Find all rows with differences in the status column.
Find out which of the entries in Row.id are equal for all given rows. @return: A list of True/False values according to whether the i-th part of the id is always equal.
Calculcate number of true/false tasks and maximum achievable score.
This function returns the numbers of the statistics. @param runResults: All the results of the execution of one run set (as list of RunResult objects)
Count the number of regressions, i.e., differences in status of the two right-most results where the new one is not "better" than the old one. Any change in status between error, unknown, and wrong result is a regression. Different kind of errors or wrong results are also a regression.
Create tables and write them to files. @return a list of futures to allow waiting for completion
Append the result for one run. Needs to be called before collect_data().
Load the actual result values from the XML file and the log files. This may take some time if many log files have to be opened and parsed.
This function extracts everything necessary for creating a RunSetResult object from the "result" XML tag of a benchmark result file. It returns a RunSetResult object, which is not yet fully initialized. To finish initializing the object, call collect_data() before using it for anything else (this is to separate the possibly costly collect_data() call from object instantiation).
This function collects the values from one run. Only columns that should be part of the table are collected.
generate output representation of rows
Find the path to the executable file that will get executed. This method always needs to be overridden, and most implementations will look similar to this one. The path returned should be relative to the current directory.
Determine whether the version is greater than some given version
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
A simple command-line interface for the runexecutor module of BenchExec.
Get the user account info from the passwd database. Only works on Linux. @param user The name of a user account or a numeric uid prefixed with '#' @return a tuple that corresponds to the members of the passwd structure @raise KeyError: If user account is unknown @raise ValueError: If uid is not a valid number
This function shrinks a file. We remove only the middle part of a file, the file-start and the file-end remain unchanged.
Segmentation faults and some memory failures reference a file with more information (hs_err_pid_*). We append this file to the log. The format that we expect is a line "# An error report file with more information is saved as:" and the file name of the dump file on the next line. @param output_filename name of log file with tool output @param base_path string that needs to be preprended to paths for lookup of files
Join a thread, but if the thread doesn't terminate for some time, ignore it instead of waiting infinitely.
This function initializes the cgroups for the limitations and measurements.
Build the final command line for executing the given command, using sudo if necessary.
Try to send signal to given process, either directly of with sudo. Because we cannot send signals to the sudo process itself, this method checks whether the target is the sudo process and redirects the signal to sudo's child in this case.
Send signal to given process, either directly or with sudo. If the target is the sudo process itself, the signal will be lost, because we do not have the rights to send signals to sudo. Use _kill_process() because of this.
Return the list of files in a directory, assuming that our user can read it.
This method creates the CGroups for the following execution. @param my_cpus: None or a list of the CPU cores to use @param memlimit: None or memory limit in bytes @param memory_nodes: None or a list of memory nodes of a NUMA system to use @param cgroup_values: dict of additional values to set @return cgroups: a map of all the necessary cgroups for the following execution. Please add the process of the following execution to all those cgroups!
Create a temporary directory for the run.
Delete given temporary directory and all its contents.
Return map with desired environment variables for run.
Open and prepare output file.
Start time-limit handler. @return None or the time-limit handler for calling cancel()
Start memory-limit handler. @return None or the memory-limit handler for calling cancel()
Setup time limit with ulimit for the current process.
Start thread that enforces any file-hiearchy limits.
This function executes a given command with resource limits, and writes the output to a file. @param args: the command line to run @param output_filename: the file where the output should be written to @param stdin: What to uses as stdin for the process (None: /dev/null, a file descriptor, or a file object) @param hardtimelimit: None or the CPU time in seconds after which the tool is forcefully killed. @param softtimelimit: None or the CPU time in seconds after which the tool is sent a kill signal. @param walltimelimit: None or the wall time in seconds after which the tool is forcefully killed (default: hardtimelimit + a few seconds) @param cores: None or a list of the CPU cores to use @param memlimit: None or memory limit in bytes @param memory_nodes: None or a list of memory nodes in a NUMA system to use @param environments: special environments for running the command @param workingDir: None or a directory which the execution should use as working directory @param maxLogfileSize: None or a number of bytes to which the output of the tool should be truncated approximately if there is too much output. @param cgroupValues: dict of additional cgroup values to set (key is tuple of subsystem and option, respective subsystem needs to be enabled in RunExecutor; cannot be used to override values set by BenchExec) @param files_count_limit: None or maximum number of files that may be written. @param files_size_limit: None or maximum size of files that may be written. @param error_filename: the file where the error output should be written to (default: same as output_filename) @param write_headers: Write informational headers to the output and the error file if separate (default: True) @param **kwargs: further arguments for ContainerExecutor.execute_run() @return: dict with result of run (measurement results and process exitcode)
This method executes the command line and waits for the termination of it, handling all setup and cleanup, but does not check whether arguments are valid.
This method calculates the exact results for time and memory measurements. It is not important to call this method as soon as possible after the run.
Check that the user account's home directory now does not contain more files than when this instance was created, and warn otherwise. Does nothing if no user account was given to RunExecutor. @return set of newly created files
Set the host name of the machine.
Basic utility to check the availability and permissions of cgroups. This will log some warnings for the user if necessary. On some systems, daemons such as cgrulesengd might interfere with the cgroups of a process soon after it was started. Thus this function starts a process, waits a configurable amount of time, and check whether the cgroups have been changed. @param wait: a non-negative int that is interpreted as seconds to wait during the check @raise SystemExit: if cgroups are not usable
Run check_cgroup_availability() in a separate thread to detect the following problem: If "cgexec --sticky" is used to tell cgrulesengd to not interfere with our child processes, the sticky flag unfortunately works only for processes spawned by the main thread, not those spawned by other threads (and this will happen if "benchexec -N" is used).
A simple command-line interface for the cgroups check of BenchExec.
This function checks, if all the words appear in the given order in the text.
This function prints the given String immediately and flushes the output.
This function returns True, if a line of the file contains bracket '{'.
This function searches for all "option"-tags and returns a list with all attributes and texts.
Get a single child tag from an XML element. Similar to "elem.find(tag)", but warns if there are multiple child tags with the given name.
This method returns a shallow copy of a XML-Element. This method is for compatibility with Python 2.6 or earlier.. In Python 2.7 you can use 'copyElem = elem.copy()' instead.
Parse a comma-separated list of strings. The list may additionally contain ranges such as "1-5", which will be expanded into "1,2,3,4,5".
Parse a string that consists of a integer number and an optional unit. @param s a non-empty string that starts with an int and is followed by some letters @return a triple of the number (as int) and the unit
Parse a string that contains a number of bytes, optionally with a unit like MB. @return the number of bytes encoded by the string
Parse a string that contains a time span, optionally with a unit like s. @return the number of seconds encoded by the string
Expand a file name pattern containing wildcards, environment variables etc. @param pattern: The pattern string to expand. @param base_dir: The directory where relative paths are based on. @return: A list of file names (possibly empty).
Replace certain keys with respective values in a string. @param template: the string in which replacements should be made @param replacements: a dict or a list of pairs of keys and values
Suited as onerror handler for (sh)util.rmtree() that logs a warning.
Same as shutil.rmtree, but supports directories without write or execute permissions.
Copy all lines from an input file object to an output file object.
Simply write some content to a file, overriding the file if necessary.
Shrink a text file to approximately maxSize bytes by removing lines from the middle of the file.
Read the full content of a file.
Read key value pairs from a file (each pair on a separate line). Key and value are separated by ' ' as often used by the kernel. @return a generator of tuples
Add and commit all files given in a list into a git repository in the base_dir directory. Nothing is done if the git repository has local changes. @param files: the files to commit @param description: the commit message
Setup the logging framework with a basic configuration
Interrupt running process, and provide a python prompt for interactive debugging. This code is based on http://stackoverflow.com/a/133384/396730
This function executes the tool with a sourcefile with options. It also calls functions for output before and after the run.
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Check whether Turbo Boost (scaling CPU frequency beyond nominal frequency) is active on this system. @return: A bool, or None if Turbo Boost is not supported.
Check whether any of the CPU cores monitored by this instance has throttled since this instance was created. @return a boolean value
Check whether any swapping occured on this system since this instance was created. @return a boolean value
Add some basic options for an executor to an argparse argument_parser.
Handle the options specified by add_basic_executor_options().
Try to send signal to given process.
Actually start the tool and the measurements. @param parent_setup_fn a function without parameters that is called in the parent process immediately before the tool is started @param child_setup_fn a function without parameters that is called in the child process before the tool is started @param parent_cleanup_fn a function that is called in the parent process immediately after the tool terminated, with three parameters: the result of parent_setup_fn, the result of the executed process as ProcessExitCode, and the base path for looking up files as parameter values @return: a tuple of PID of process and a blocking function, which waits for the process and a triple of the exit code and the resource usage of the process and the result of parent_cleanup_fn (do not use os.wait)
Wait for the given process to terminate. @return tuple of exit code and resource usage
Return a pretty-printed XML string for the Element. Also allows setting a document type.
Determine the file paths to be adopted
This method replaces special substrings from a list of string and return a new list.
Open and parse a task-definition file in YAML format.
Load the tool-info class. @param tool_name: The name of the tool-info module. Either a full Python package name or a name within the benchexec.tools package. @return: A tuple of the full name of the used tool-info module and an instance of the tool-info class.
This function builds a list of SourcefileSets (containing filename with options). The files and their options are taken from the list of sourcefilesTags.
Get the task-definition files from the XML definition. Task-definition files are files for which we create a run (typically an input file or a YAML task definition).
Create a Run from a direct definition of the main input file (without task definition)
Create a Run from a task definition in yaml format
The function expand_filename_pattern expands a filename pattern to a sorted list of filenames. The pattern can contain variables and wildcards. If base_dir is given and pattern is not absolute, base_dir and pattern are joined.
Set the result of this run. Use this method instead of manually setting the run attributes and calling after_execution(), this method handles all this by itself. @param values: a dictionary with result values as returned by RunExecutor.execute_run(), may also contain arbitrary additional values @param visible_columns: a set of keys of values that should be visible by default (i.e., not marked as hidden), apart from those that BenchExec shows by default anyway
Return status according to result and output of tool.
try to find out whether the tool terminated because of a timeout
Create a dict of property->ExpectedResult from information encoded in a filename.
Return the achieved score of a task according to the SV-COMP scoring scheme. @param category: result category as determined by get_result_category @param result: the result given by the tool
Return the possible score of task, depending on whether the result is correct or not.
Classify the given result into "true" (property holds), "false" (property does not hold), "unknown", and "error". @param result: The result given by the tool (needs to be one of the RESULT_* strings to be recognized). @return One of RESULT_CLASS_* strings
This function determines the relation between actual result and expected result for the given file and properties. @param filename: The file name of the input file. @param result: The result given by the tool (needs to be one of the RESULT_* strings to be recognized). @param properties: The list of property names to check. @return One of the CATEGORY_* strings.
Create a Property instance by attempting to parse the given property file. @param propertyfile: A file name of a property file @param allow_unknown: Whether to accept unknown properties
Create a Property instance from a list of well-known property names @param property_names: a non-empty list of property names
The function get_file_list expands a short filename to a sorted list of filenames. The short filename can contain variables and wildcards.
Open a URL and ensure that the result is seekable, copying it into a buffer if necessary.
Split a string into two parts: a prefix and a suffix. Splitting is done from the end, so the split is done around the position of the last digit in the string (that means the prefix may include any character, mixing digits and chars). The flag 'numbers_into_suffix' determines whether the suffix consists of digits or non-digits.
Helper function for formatting the content of the options line
Take a tuple (values, counts), remove consecutive values and increment their count instead.
Returns a list where sequences of post-fixed entries are shortened to their common prefix. This might be useful in cases of several similar values, where the prefix is identical for several entries. If less than 'number_of_needed_commons' are identically prefixed, they are kept unchanged. Example: ['test', 'pc1', 'pc2', 'pc3', ... , 'pc10'] -> ['test', 'pc*']
Filter out duplicate values while keeping order.
If the value is a number (or number followed by a unit), this function returns a string-representation of the number with the specified number of significant digits, optionally aligned at the decimal point.
Returns the type of the given column based on its row values on the given RunSetResult. @param column: the column to return the correct ColumnType for @param column_values: the column values to consider @return: a tuple of a type object describing the column - the concrete ColumnType is stored in the attribute 'type', the display unit of the column, which may be None, the source unit of the column, which may be None, and the scale factor to convert from the source unit to the display unit. If no scaling is necessary for conversion, this value is 1.
Returns the amount of decimal digits of the given regex match, considering the number of significant digits for the provided column. @param decimal_number_match: a regex match of a decimal number, resulting from REGEX_MEASURE.match(x). @param number_of_significant_digits: the number of significant digits required @return: the number of decimal digits of the given decimal number match's representation, after expanding the number to the required amount of significant digits
Format a value nicely for human-readable output (including rounding). @param value: the value to format @param isToAlign: if True, spaces will be added to the returned String representation to align it to all other values in this column, correctly @param format_target the target the value should be formatted for @return: a formatted String representation of the given value.
Find the path to the executable file that will get executed. This method always needs to be overridden, and most implementations will look similar to this one. The path returned should be relative to the current directory.
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
OPTIONAL, this method is only necessary for situations when the benchmark environment needs to know all files belonging to a tool (to transport them to a cloud service, for example). Returns a list of files or directories that are necessary to run the tool.
Return a Cgroup object with the cgroups of the current process. Note that it is not guaranteed that all subsystems are available in the returned object, as a subsystem may not be mounted. Check with "subsystem in <instance>" before using. A subsystem may also be present but we do not have the rights to create child cgroups, this can be checked with require_subsystem(). @param cgroup_paths: If given, use this instead of reading /proc/self/cgroup.
Return the information which subsystems are mounted where. @return a generator of tuples (subsystem, mountpoint)
For all subsystems, return the information in which (sub-)cgroup this process is in. (Each process is in exactly cgroup in each hierarchy.) @return a generator of tuples (subsystem, cgroup)
Parse a /proc/*/cgroup file into tuples of (subsystem,cgroup). @param content: An iterable over the lines of the file. @return: a generator of tuples
Tell cgrulesengd daemon to not move the given process into other cgroups, if libcgroup is available.
Check whether the given subsystem is enabled and is writable (i.e., new cgroups can be created for it). Produces a log message for the user if one of the conditions is not fulfilled. If the subsystem is enabled but not writable, it will be removed from this instance such that further checks with "in" will return "False". @return A boolean value.
Create child cgroups of the current cgroup for at least the given subsystems. @return: A Cgroup instance representing the new child cgroup(s).
Add a process to the cgroups represented by this instance.
Return a generator of all PIDs currently in this cgroup for the given subsystem.
Kill all tasks in this cgroup and all its children cgroups forcefully. Additionally, the children cgroups will be deleted.
Check whether the given value exists in the given subsystem. Does not make a difference whether the value is readable, writable, or both. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Read the given value from the given subsystem. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Read the lines of the given file from the given subsystem. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Read the lines of the given file from the given subsystem and split the lines into key-value pairs. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Write the given value for the given subsystem. Do not include the subsystem name in the option name. Only call this method if the given subsystem is available.
Remove all cgroups this instance represents from the system. This instance is afterwards not usable anymore!
Parse a time stamp in the "year-month-day hour-minute" format.
The main method of BenchExec for use in a command-line script. In addition to calling benchexec.start(argv), it also handles signals and keyboard interrupts. It does not return but calls sys.exit(). @param benchexec: An instance of BenchExec for executing benchmarks. @param argv: optionally the list of command-line options to use
Start BenchExec. @param argv: command-line options for BenchExec
Create a parser for the command-line options. May be overwritten for adding more configuration options. @return: an argparse.ArgumentParser instance
Configure the logging framework.
Execute a single benchmark as defined in a file. If called directly, ensure that config and executor attributes are set up. @param benchmark_file: the name of a benchmark-definition XML file @return: a result value from the executor module
Check and abort if the target directory for the benchmark results already exists in order to avoid overwriting results.
Stop the execution of a benchmark. This instance cannot be used anymore afterwards. Timely termination is not guaranteed, and this method may return before everything is terminated.
Allocate some memory that can be used as a stack. @return: a ctypes void pointer to the *top* of the stack.
Execute a function in a child process in separate namespaces. @param func: a parameter-less function returning an int (which will be the process' exit value) @return: the PID of the created child process
Write uid_map and gid_map in /proc to create a user mapping that maps our user from outside the container to the same user inside the container (and no other users are mapped). @see: http://man7.org/linux/man-pages/man7/user_namespaces.7.html @param pid: The PID of the process in the container.
Bring up the given network interface. @raise OSError: if interface does not exist or permissions are missing
Get all current mount points of the system. Changes to the mount points during iteration may be reflected in the result. @return a generator of (source, target, fstype, options), where options is a list of bytes instances, and the others are bytes instances (this avoids encoding problems with mount points with problematic characters).
Remount an existing mount point with additional flags. @param mountpoint: the mount point as bytes @param existing_options: dict with current mount existing_options as bytes @param mountflags: int with additional mount existing_options (cf. libc.MS_* constants)
Make a bind mount. @param source: the source directory as bytes @param target: the target directory as bytes @param recursive: whether to also recursively bind mount all mounts below source @param private: whether to mark the bind as private, i.e., changes to the existing mounts won't propagate and vice-versa (changes to files/dirs will still be visible).
Drop all capabilities this process has. @param keep: list of capabilities to not drop
Install all signal handler that forwards all signals to the given process.
Wait for a child to terminate and in the meantime forward all signals the current process receives to this child. @return a tuple of exit code and resource usage of the child as given by os.waitpid
Close all open file descriptors except those in a given set. @param keep_files: an iterable of file descriptors or file-like objects.
Create a minimal system configuration for use in a container. @param basedir: The directory where the configuration files should be placed as bytes. @param mountdir: If present, bind mounts to the configuration files will be added below this path (given as bytes).
Determine whether a given file is one of the files created by setup_container_system_config(). @param file: Absolute file path as string.
Compose the command line to execute from the name of the executable, the user-specified options, and the inputfile to analyze. This method can get overridden, if, for example, some options should be enabled or if the order of arguments must be changed. All paths passed to this method (executable, tasks, and propertyfile) are either absolute or have been made relative to the designated working directory. @param executable: the path to the executable of the tool (typically the result of executable()) @param options: a list of options, in the same order as given in the XML-file. @param tasks: a list of tasks, that should be analysed with the tool in one run. In most cases we we have only _one_ inputfile. @param propertyfile: contains a specification for the verifier. @param rlimits: This dictionary contains resource-limits for a run, for example: time-limit, soft-time-limit, hard-time-limit, memory-limit, cpu-core-limit. All entries in rlimits are optional, so check for existence before usage!
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
OPTIONAL, this method is only necessary for situations when the benchmark environment needs to know all files belonging to a tool (to transport them to a cloud service, for example). Returns a list of files or directories that are necessary to run the tool.
Add content to the represented file. If keep is False, the new content will be forgotten during the next call to this method.
Take the result of an energy measurement and return a flat dictionary that contains all values.
Starts the external measurement program.
Stops the external measurement program and returns the measurement result, if the measurement was running.
Parse the output of the tool and extract the verification result. This method always needs to be overridden. If the tool gave a result, this method needs to return one of the benchexec.result.RESULT_* strings. Otherwise an arbitrary string can be returned that will be shown to the user and should give some indication of the failure reason (e.g., "CRASH", "OUT_OF_MEMORY", etc.).
Render the error document
Call Paste's FileApp (a WSGI application) to serve the file at the specified path
Show login form.
Extract prefix attributes from arbitary dict.
Extract pool attributes from arbitary dict.
List VRFs and return JSON encoded result.
Perform a smart VRF search. The "smart" search function tries extract a query from a text string. This query is then passed to the search_vrf function, which performs the search.
Add a new VRF to NIPAP and return its data.
Remove a VRF.
List pools and return JSON encoded result.
Perform a smart pool search. The "smart" search function tries extract a query from a text string. This query is then passed to the search_pool function, which performs the search.
Edit a pool.
Remove a pool.
List prefixes and return JSON encoded result.
Search prefixes. Does not yet incorporate all the functions of the search_prefix API function due to difficulties with transferring a complete 'dict-to-sql' encoded data structure. Instead, a list of prefix attributes can be given which will be matched with the 'equals' operator if notheing else is specified. If multiple attributes are given, they will be combined with the 'and' operator. Currently, it is not possible to specify different operators for different attributes.
Perform a smart search. The smart search function tries extract a query from a text string. This query is then passed to the search_prefix function, which performs the search.
Add prefix according to the specification. The following keys can be used: vrf ID of VRF to place the prefix in prefix the prefix to add if already known family address family (4 or 6) description A short description expires Expiry time of assignment comment Longer comment node Hostname of node type Type of prefix; reservation, assignment, host status Status of prefix; assigned, reserved, quarantine pool ID of pool country Country where the prefix is used order_id Order identifier customer_id Customer identifier vlan VLAN ID alarm_priority Alarm priority of prefix monitor If the prefix should be monitored or not from-prefix A prefix the prefix is to be allocated from from-pool A pool (ID) the prefix is to be allocated from prefix_length Prefix length of allocated prefix
Remove a prefix.
Add VRF to filter list session variable
Remove VRF to filter list session variable
Return VRF filter list from session variable Before returning list, make a search for all VRFs currently in the list to verify that they still exist.
List Tags and return JSON encoded result.
Read the configuration file
Display NIPAP version info
Initialize auth backends.
Returns an authentication object. Examines the auth backend given after the '@' in the username and returns a suitable instance of a subclass of the BaseAuth class. * `username` [string] Username to authenticate as. * `password` [string] Password to authenticate with. * `authoritative_source` [string] Authoritative source of the query. * `auth_options` [dict] A dict which, if authenticated as a trusted user, can override `username` and `authoritative_source`.
Verify authentication. Returns True/False dependant on whether the authentication succeeded or not.
Set up database Creates tables required for the authentication module.
Verify authentication. Returns True/False dependant on whether the authentication succeeded or not.
Fetch the user from the database The function will return None if the user is not found
Add user to SQLite database. * `username` [string] Username of new user. * `password` [string] Password of new user. * `full_name` [string] Full name of new user. * `trusted` [boolean] Whether the new user should be trusted or not. * `readonly` [boolean] Whether the new user can only read or not
Remove user from the SQLite database. * `username` [string] Username of user to remove.
Modify user in SQLite database. Since username is used as primary key and we only have a single argument for it we can't modify the username right now.
List all users.
Generate password hash.
Adds readwrite authorization This will check if the user is a readonly user and if so reject the query. Apply this decorator to readwrite functions.
Parse the 'expires' attribute, guessing what format it is in and returning a datetime
Create the INET type and an Inet adapter.
Return true if given arg is a valid IPv4 address
Return address-family (4 or 6) for IP or None if invalid address
Open database connection
Execute query, catch and log errors.
Expand a dict so it fits in a INSERT clause
Expand a dict so it fits in a INSERT clause
Get rows updated by last update query * `function` [function] Function to use for searching (one of the search_* functions). Helper function used to fetch all rows which was updated by the latest UPDATE ... RETURNING id query.
Split a query string into its parts
Get the schema version of the nipap psql db.
Install nipap database schema
Upgrade nipap database schema
Expand VRF specification to SQL. id [integer] internal database id of VRF name [string] name of VRF A VRF is referenced either by its internal database id or by its name. Both are used for exact matching and so no wildcard or regular expressions are allowed. Only one key may be used and an error will be thrown if both id and name is specified.
Add a new VRF. * `auth` [BaseAuth] AAA options. * `attr` [vrf_attr] The news VRF's attributes. Add a VRF based on the values stored in the `attr` dict. Returns a dict describing the VRF which was added. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_vrf` for full understanding.
Remove a VRF. * `auth` [BaseAuth] AAA options. * `spec` [vrf_spec] A VRF specification. Remove VRF matching the `spec` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_vrf` for full understanding.
Return a list of VRFs matching `spec`. * `auth` [BaseAuth] AAA options. * `spec` [vrf_spec] A VRF specification. If omitted, all VRFs are returned. Returns a list of dicts. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_vrf` for full understanding.
Get a VRF based on prefix spec Shorthand function to reduce code in the functions below, since more or less all of them needs to perform the actions that are specified here. The major difference to :func:`list_vrf` is that we always return results - empty results if no VRF is specified in prefix spec.
Update VRFs matching `spec` with attributes `attr`. * `auth` [BaseAuth] AAA options. * `spec` [vrf_spec] Attibutes specifying what VRF to edit. * `attr` [vrf_attr] Dict specifying fields to be updated and their new values. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_vrf` for full understanding.
Perform a smart search on VRF list. * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_vrf`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what VRF attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_vrf` function. If multiple search keys are detected, they are combined with a logical AND. It will basically just take each search term and try to match it against the name or description column with regex match or the VRF column with an exact match. See the :func:`search_vrf` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_vrf` for full understanding.
Parse a smart search query for VRFs This is a helper function to smart_search_vrf for easier unit testing of the parser.
Create a pool according to `attr`. * `auth` [BaseAuth] AAA options. * `attr` [pool_attr] A dict containing the attributes the new pool should have. Returns a dict describing the pool which was added. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_pool` for full understanding.
Remove a pool. * `auth` [BaseAuth] AAA options. * `spec` [pool_spec] Specifies what pool(s) to remove. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_pool` for full understanding.
Return a list of pools. * `auth` [BaseAuth] AAA options. * `spec` [pool_spec] Specifies what pool(s) to list. Of omitted, all will be listed. Returns a list of dicts. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_pool` for full understanding.
Check pool attributes.
Get a pool. Shorthand function to reduce code in the functions below, since more or less all of them needs to perform the actions that are specified here. The major difference to :func:`list_pool` is that an exception is raised if no pool matching the spec is found.
Update pool given by `spec` with attributes `attr`. * `auth` [BaseAuth] AAA options. * `spec` [pool_spec] Specifies what pool to edit. * `attr` [pool_attr] Attributes to update and their new values. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_pool` for full understanding.
Perform a smart search on pool list. * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_pool`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what pool attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_pool` function. If multiple search keys are detected, they are combined with a logical AND. It will basically just take each search term and try to match it against the name or description column with regex match. See the :func:`search_pool` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_pool` for full understanding.
Parse a smart search query for pools This is a helper function to smart_search_pool for easier unit testing of the parser.
Expand prefix specification to SQL.
Expand prefix query dict into a WHERE-clause. If you need to prefix each column reference with a table name, that can be supplied via the table_name argument.
Add a prefix and return its ID. * `auth` [BaseAuth] AAA options. * `attr` [prefix_attr] Prefix attributes. * `args` [add_prefix_args] Arguments explaining how the prefix should be allocated. Returns a dict describing the prefix which was added. Prefixes can be added in three ways; manually, from a pool or from a prefix. Manually All prefix data, including the prefix itself is specified in the `attr` argument. The `args` argument shall be omitted. From a pool Most prefixes are expected to be automatically assigned from a pool. In this case, the :attr:`prefix` key is omitted from the `attr` argument. Also the :attr:`type` key can be omitted and the prefix type will then be set to the pools default prefix type. The :func:`find_free_prefix` function is used to find available prefixes for this allocation method, see its documentation for a description of how the `args` argument should be formatted. From a prefix A prefix can also be selected from another prefix. Also in this case the :attr:`prefix` key is omitted from the `attr` argument. See the documentation for the :func:`find_free_prefix` for a description of how the `args` argument is to be formatted. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_prefix` for full understanding.
Update prefix matching `spec` with attributes `attr`. * `auth` [BaseAuth] AAA options. * `spec` [prefix_spec] Specifies the prefix to edit. * `attr` [prefix_attr] Prefix attributes. Note that there are restrictions on when and how a prefix's type can be changed; reservations can be changed to assignments and vice versa, but only if they contain no child prefixes. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_prefix` for full understanding.
Finds free prefixes in the sources given in `args`. * `auth` [BaseAuth] AAA options. * `vrf` [vrf] Full VRF-dict specifying in which VRF the prefix should be unique. * `args` [find_free_prefix_args] Arguments to the find free prefix function. Returns a list of dicts. Prefixes can be found in two ways: from a pool of from a prefix. From a pool The `args` argument is set to a dict with key :attr:`from-pool` set to a pool spec. This is the pool from which the prefix will be assigned. Also the key :attr:`family` needs to be set to the adress family (integer 4 or 6) of the requested prefix. Optionally, also the key :attr:`prefix_length` can be added to the `attr` argument, and will then override the default prefix length. Example:: args = { 'from-pool': { 'name': 'CUSTOMER-' }, 'family': 6, 'prefix_length': 64 } From a prefix Instead of specifying a pool, a prefix which will be searched for new prefixes can be specified. In `args`, the key :attr:`from-prefix` is set to list of prefixes you want to allocate from and the key :attr:`prefix_length` is set to the wanted prefix length. Example:: args = { 'from-prefix': ['192.0.2.0/24'], 'prefix_length': 27 } The key :attr:`count` can also be set in the `args` argument to specify how many prefixes that should be returned. If omitted, the default value is 1000. The internal backend function :func:`find_free_prefix` is used internally by the :func:`add_prefix` function to find available prefixes from the given sources. It's also exposed over XML-RPC, please see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.find_free_prefix` for full understanding.
List prefixes matching the `spec`. * `auth` [BaseAuth] AAA options. * `spec` [prefix_spec] Specifies prefixes to list. If omitted, all will be listed. Returns a list of dicts. This is a quite blunt tool for finding prefixes, mostly useful for fetching data about a single prefix. For more capable alternatives, see the :func:`search_prefix` or :func:`smart_search_prefix` functions. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_prefix` for full understanding.
Do the underlying database operations to delete a prefix
Remove prefix matching `spec`. * `auth` [BaseAuth] AAA options. * `spec` [prefix_spec] Specifies prefixe to remove. * `recursive` [bool] When set to True, also remove child prefixes. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_prefix` for full understanding.
Search prefix list for prefixes matching `query`. * `auth` [BaseAuth] AAA options. * `query` [dict_to_sql] How the search should be performed. * `search_options` [options_dict] Search options, see below. Returns a list of dicts. The `query` argument passed to this function is designed to be able to express quite advanced search filters. It is internally expanded to an SQL WHERE-clause. The `query` is a dict with three elements, where one specifies the operation to perform and the two other specifies its arguments. The arguments can themselves be `query` dicts, i.e. nested, to build more complex queries. The :attr:`operator` key specifies what operator should be used for the comparison. Currently the following operators are supported: * :data:`and` - Logical AND * :data:`or` - Logical OR * :data:`equals_any` - Equality of any element in array * :data:`equals` - Equality; = * :data:`not_equals` - Inequality; != * :data:`less` - Less than; < * :data:`less_or_equal` - Less than or equal to; <= * :data:`greater` - Greater than; > * :data:`greater_or_equal` - Greater than or equal to; >= * :data:`like` - SQL LIKE * :data:`regex_match` - Regular expression match * :data:`regex_not_match` - Regular expression not match * :data:`contains` - IP prefix contains * :data:`contains_equals` - IP prefix contains or is equal to * :data:`contained_within` - IP prefix is contained within * :data:`contained_within_equals` - IP prefix is contained within or equals The :attr:`val1` and :attr:`val2` keys specifies the values which are subjected to the comparison. :attr:`val1` can be either any prefix attribute or a query dict. :attr:`val2` can be either the value you want to compare the prefix attribute to, or a `query` dict. Example 1 - Find the prefixes which contains 192.0.2.0/24:: query = { 'operator': 'contains', 'val1': 'prefix', 'val2': '192.0.2.0/24' } This will be expanded to the pseudo-SQL query:: SELECT * FROM prefix WHERE prefix contains '192.0.2.0/24' Example 2 - Find for all assignments in prefix 192.0.2.0/24:: query = { 'operator': 'and', 'val1': { 'operator': 'equals', 'val1': 'type', 'val2': 'assignment' }, 'val2': { 'operator': 'contained_within', 'val1': 'prefix', 'val2': '192.0.2.0/24' } } This will be expanded to the pseudo-SQL query:: SELECT * FROM prefix WHERE (type == 'assignment') AND (prefix contained within '192.0.2.0/24') If you want to combine more than two expressions together with a boolean expression you need to nest them. For example, to match on three values, in this case the tag 'foobar' and a prefix-length between /10 and /24, the following could be used:: query = { 'operator': 'and', 'val1': { 'operator': 'and', 'val1': { 'operator': 'greater', 'val1': 'prefix_length', 'val2': 9 }, 'val2': { 'operator': 'less_or_equal', 'val1': 'prefix_length', 'val2': 24 } }, 'val2': { 'operator': 'equals_any', 'val1': 'tags', 'val2': 'foobar' } } The `options` argument provides a way to alter the search result to assist in client implementations. Most options regard parent and children prefixes, that is the prefixes which contain the prefix(es) matching the search terms (parents) or the prefixes which are contained by the prefix(es) matching the search terms. The search options can also be used to limit the number of rows returned. The following options are available: * :attr:`parents_depth` - How many levels of parents to return. Set to :data:`-1` to include all parents. * :attr:`children_depth` - How many levels of children to return. Set to :data:`-1` to include all children. * :attr:`include_all_parents` - Include all parents, no matter what depth is specified. * :attr:`include_all_children` - Include all children, no matter what depth is specified. * :attr:`max_result` - The maximum number of prefixes to return (default :data:`50`). * :attr:`offset` - Offset the result list this many prefixes (default :data:`0`). The options above gives the possibility to specify how many levels of parent and child prefixes to return in addition to the prefixes that actually matched the search terms. This is done by setting the :attr:`parents_depth` and :attr:`children depth` keys in the `search_options` dict to an integer value. In addition to this it is possible to get all all parents and/or children included in the result set even though they are outside the limits set with :attr:`*_depth`. The extra prefixes included will have the attribute :attr:`display` set to :data:`false` while the other ones (the actual search result togther with the ones included due to given depth) :attr:`display` set to :data:`true`. This feature is usable obtain search results with some context given around them, useful for example when displaying prefixes in a tree without the need to implement client side IP address logic. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.search_prefix` for full understanding.
Perform a smart search on prefix list. * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_prefix`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what prefix attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_prefix` function. If multiple search keys are detected, they are combined with a logical AND. It tries to automatically detect IP addresses and prefixes and put these into the `query` dict with "contains_within" operators and so forth. See the :func:`search_prefix` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_prefix` for full understanding.
Parse a smart search query for prefixes This is a helper function to smart_search_prefix for easier unit testing of the parser.
List AS numbers matching `spec`. * `auth` [BaseAuth] AAA options. * `spec` [asn_spec] An automous system number specification. If omitted, all ASNs are returned. Returns a list of dicts. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.list_asn` for full understanding.
Add AS number to NIPAP. * `auth` [BaseAuth] AAA options. * `attr` [asn_attr] ASN attributes. Returns a dict describing the ASN which was added. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.add_asn` for full understanding.
Edit AS number * `auth` [BaseAuth] AAA options. * `asn` [integer] AS number to edit. * `attr` [asn_attr] New AS attributes. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.edit_asn` for full understanding.
Remove an AS number. * `auth` [BaseAuth] AAA options. * `spec` [asn] An ASN specification. Remove ASNs matching the `asn` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.remove_asn` for full understanding.
Search ASNs for entries matching 'query' * `auth` [BaseAuth] AAA options. * `query` [dict_to_sql] How the search should be performed. * `search_options` [options_dict] Search options, see below. Returns a list of dicts. The `query` argument passed to this function is designed to be able to specify how quite advanced search operations should be performed in a generic format. It is internally expanded to a SQL WHERE-clause. The `query` is a dict with three elements, where one specifies the operation to perform and the two other specifies its arguments. The arguments can themselves be `query` dicts, to build more complex queries. The :attr:`operator` key specifies what operator should be used for the comparison. Currently the following operators are supported: * :data:`and` - Logical AND * :data:`or` - Logical OR * :data:`equals` - Equality; = * :data:`not_equals` - Inequality; != * :data:`like` - SQL LIKE * :data:`regex_match` - Regular expression match * :data:`regex_not_match` - Regular expression not match The :attr:`val1` and :attr:`val2` keys specifies the values which are subjected to the comparison. :attr:`val1` can be either any prefix attribute or an entire query dict. :attr:`val2` can be either the value you want to compare the prefix attribute to, or an entire `query` dict. The search options can also be used to limit the number of rows returned or set an offset for the result. The following options are available: * :attr:`max_result` - The maximum number of prefixes to return (default :data:`50`). * :attr:`offset` - Offset the result list this many prefixes (default :data:`0`). This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.search_tag` for full understanding.
Perform a smart search operation among AS numbers * `auth` [BaseAuth] AAA options. * `query_str` [string] Search string * `search_options` [options_dict] Search options. See :func:`search_asn`. * `extra_query` [dict_to_sql] Extra search terms, will be AND:ed together with what is extracted from the query string. Return a dict with three elements: * :attr:`interpretation` - How the query string was interpreted. * :attr:`search_options` - Various search_options. * :attr:`result` - The search result. The :attr:`interpretation` is given as a list of dicts, each explaining how a part of the search key was interpreted (ie. what ASN attribute the search operation was performed on). The :attr:`result` is a list of dicts containing the search result. The smart search function tries to convert the query from a text string to a `query` dict which is passed to the :func:`search_asn` function. If multiple search keys are detected, they are combined with a logical AND. See the :func:`search_asn` function for an explanation of the `search_options` argument. This is the documentation of the internal backend function. It's exposed over XML-RPC, please also see the XML-RPC documentation for :py:func:`nipap.xmlrpc.NipapXMLRPC.smart_search_asn` for full understanding.
Parse a smart search query for ASNs This is a helper function to smart_search_asn for easier unit testing of the parser.
Expand Tag query dict into a WHERE-clause. If you need to prefix each column reference with a table name, that can be supplied via the table_name argument.
Create, configure and return the routes Mapper
Set up the global pynipap connection object
Returns pool to work with Returns a pynipap.Pool object representing the pool we are working with.
Returns VRF to work in Returns a pynipap.VRF object representing the VRF we are working in. If there is a VRF set globally, return this. If not, fetch the VRF named 'arg'. If 'arg' is None, fetch the default_vrf attribute from the config file and return this VRF.
List pools matching a search criteria
List VRFs matching a search criteria
List prefixes matching 'arg'
Return a prefix based on options passed from command line Used by add_prefix() and add_prefix_from_pool() to avoid duplicate parsing
Add prefix to NIPAP
Add prefix using from-pool to NIPAP
Add VRF to NIPAP
Add a pool.
View a single VRF
View a single pool
View a single prefix.
Remove VRF
Remove pool
Remove prefix
Modify a VRF with the options set in opts
Modify a pool with the options set in opts
Expand a pool with the ranges set in opts
Shrink a pool by removing the ranges in opts from it
Modify the prefix 'arg' with the options 'opts'
Add attributes to a prefix
Add attributes to a VRF
Remove attributes from a prefix
Add attributes to a pool
Remove attributes from a prefix
Returns valid string completions Takes the string 'key' and compares it to each of the strings in 'haystack'. The ones which beginns with 'key' are returned as result.
Complete NIPAP prefix type
Complete member prefixes of pool
Complete node hostname This function is currently a bit special as it looks in the config file for a command to use to complete a node hostname from an external system. It is configured by setting the config attribute "complete_node_cmd" to a shell command. The string "%search_string%" in the command will be replaced by the current search string.
Returns list of matching pool names
Returns list of matching VRFs
Returns list of matching VRFs Includes "virtual" VRF 'all' which is used in search operations
Parse a smart search string and return it in an AST like form
Parse matching expression in form key <op> value For example; vlan > 1 node = FOO-BAR
Do magic matching of single words or quoted string
Do magic matching of single words or quoted string
Examine the current matching key Extracts information, such as function to execute and command options, from the current key (passed to function as 'key_name' and 'key_val').
Extract command and options from string. The tree argument should contain a specifically formatted dict which describes the available commands, options, arguments and callbacks to methods for completion of arguments. TODO: document dict format The inp_cmd argument should contain a list of strings containing the complete command to parse, such as sys.argv (without the first element which specified the command itself).
Return list of valid completions Returns a list of valid completions on the current level in the tree. If an element of type 'value' is found, its complete callback function is called (if set).
Return list of valid next values
Add a pool.
Edit a pool.
Remove pool.
Remove a prefix from pool 'id'.
Add hosts from ipplan to networks object.
Gather network and host information from ipplan export files.
Put your network information in the prefix object.
Put your host information in the prefix object.
Get version of nipapd we're connected to. Maps to the function :py:func:`nipap.xmlrpc.NipapXMLRPC.version` in the XML-RPC API. Please see the documentation for the XML-RPC function for information regarding the return value.
Get schema version of database we're connected to. Maps to the function :py:func:`nipap.backend.Nipap._get_db_version` in the backend. Please see the documentation for the backend function for information regarding the return value.
Converts XML-RPC Fault objects to Pynipap-exceptions. TODO: Is this one neccesary? Can be done inline...
Create new Tag-object from dict. Suitable for creating objects from XML-RPC data. All available keys must exist.
Search tags. For more information, see the backend function :py:func:`nipap.backend.Nipap.search_tag`.
List VRFs. Maps to the function :py:func:`nipap.backend.Nipap.list_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Create new VRF-object from dict. Suitable for creating objects from XML-RPC data. All available keys must exist.
Get the VRF with id 'id'.
Search VRFs. Maps to the function :py:func:`nipap.backend.Nipap.search_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Perform a smart VRF search. Maps to the function :py:func:`nipap.backend.Nipap.smart_search_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Save changes made to object to NIPAP. If the object represents a new VRF unknown to NIPAP (attribute `id` is `None`) this function maps to the function :py:func:`nipap.backend.Nipap.add_vrf` in the backend, used to create a new VRF. Otherwise it maps to the function :py:func:`nipap.backend.Nipap.edit_vrf` in the backend, used to modify the VRF. Please see the documentation for the backend functions for information regarding input arguments and return values.
Remove VRF. Maps to the function :py:func:`nipap.backend.Nipap.remove_vrf` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Save changes made to pool to NIPAP. If the object represents a new pool unknown to NIPAP (attribute `id` is `None`) this function maps to the function :py:func:`nipap.backend.Nipap.add_pool` in the backend, used to create a new pool. Otherwise it maps to the function :py:func:`nipap.backend.Nipap.edit_pool` in the backend, used to modify the pool. Please see the documentation for the backend functions for information regarding input arguments and return values.
Get the pool with id 'id'.
Search pools. Maps to the function :py:func:`nipap.backend.Nipap.search_pool` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Perform a smart pool search. Maps to the function :py:func:`nipap.backend.Nipap.smart_search_pool` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Create new Pool-object from dict. Suitable for creating objects from XML-RPC data. All available keys must exist.
List pools. Maps to the function :py:func:`nipap.backend.Nipap.list_pool` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Get the prefix with id 'id'.
Finds a free prefix. Maps to the function :py:func:`nipap.backend.Nipap.find_free_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Search for prefixes. Maps to the function :py:func:`nipap.backend.Nipap.search_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Perform a smart prefix search. Maps to the function :py:func:`nipap.backend.Nipap.smart_search_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
List prefixes. Maps to the function :py:func:`nipap.backend.Nipap.list_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Save prefix to NIPAP. If the object represents a new prefix unknown to NIPAP (attribute `id` is `None`) this function maps to the function :py:func:`nipap.backend.Nipap.add_prefix` in the backend, used to create a new prefix. Otherwise it maps to the function :py:func:`nipap.backend.Nipap.edit_prefix` in the backend, used to modify the VRF. Please see the documentation for the backend functions for information regarding input arguments and return values.
Remove the prefix. Maps to the function :py:func:`nipap.backend.Nipap.remove_prefix` in the backend. Please see the documentation for the backend function for information regarding input arguments and return values.
Create a Prefix object from a dict. Suitable for creating Prefix objects from XML-RPC input.
Edit a VRF
Add a new VRF.
Removes a VRF.
Place any commands to setup nipapwww here
Create a Pylons WSGI application and return it ``global_conf`` The inherited configuration for this application. Normally from the [DEFAULT] section of the Paste ini file. ``full_stack`` Whether this application provides a full WSGI stack (by default, meaning it handles its own exceptions and errors). Disable full_stack when this application is "managed" by another WSGI middleware. ``static_files`` Whether this application serves its own static files; disable when another web server is responsible for serving them. ``app_conf`` The application's local configuration. Normally specified in the [app:<name>] section of the Paste ini file (where <name> defaults to main).
Add a prefix.
Edit a prefix.
Remove a prefix.
Configure the Pylons environment via the ``pylons.config`` object
Get prefix data from NIPAP
Write the config to file
Detach a process from the controlling terminal and run it in the background as a daemon.
Parse one line
Mangle prefix result
Class decorator for XML-RPC functions that requires auth
An echo function An API test function which simply echoes what is is passed in the 'message' element in the args-dict.. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `message` [string] String to echo. * `sleep` [integer] Number of seconds to sleep before echoing. Returns a string.
Add a new VRF. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `attr` [struct] VRF attributes. Returns the internal database ID for the VRF.
Add a prefix. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `attr` [struct] Attributes to set on the new prefix. * `args` [srgs] Arguments for addition of prefix, such as what pool or prefix it should be allocated from. Returns ID of created prefix.
Remove a prefix. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `prefix` [struct] Attributes used to select what prefix to remove. * `recursive` [boolean] When set to 1, also remove child prefixes.
Find a free prefix. Valid keys in the `args`-struct: * `auth` [struct] Authentication options passed to the :class:`AuthFactory`. * `args` [struct] Arguments for the find_free_prefix-function such as what prefix or pool to allocate from.
Read content of specified NEWS file
Parse content of DCH file
Formats a record and serializes it as a JSON str. If record message isnt already a dict, initializes a new dict and uses `default_msg_fieldname` as a key as the record msg as the value.
:type record: aiologger.loggers.json.LogRecord
:type record: aiologger.loggers.json.LogRecord
Open the current base file with the (original) mode and encoding. Return the resulting stream.
Emit a record. Output the record to the file, catering for rollover as described in `do_rollover`.
Modify the filename of a log file when rotating. This is provided so that a custom filename can be provided. :param default_name: The default name for the log file.
When rotating, rotate the current log. The default implementation calls the 'rotator' attribute of the handler, if it's callable, passing the source and dest arguments to it. If the attribute isn't callable (the default is None), the source is simply renamed to the destination. :param source: The source filename. This is normally the base filename, e.g. 'test.log' :param dest: The destination filename. This is normally what the source is rotated to, e.g. 'test.log.1'.
Work out the rollover time based on the specified time. If we are rolling over at midnight or weekly, then the interval is already known. need to figure out is WHEN the next interval is. In other words, if you are rolling over at midnight, then your base interval is 1 day, but you want to start that one day clock at midnight, not now. So, we have to fudge the `rollover_at` value in order to trigger the first rollover at the right time. After that, the regular interval will take care of the rest. Note that this code doesn't care about leap seconds. :)
Determine if rollover should occur. record is not used, as we are just comparing times, but it is needed so the method signatures are the same
Determine the files to delete when rolling over.
do a rollover; in this case, a date/time stamp is appended to the filename when the rollover happens. However, you want the file to be named for the start of the interval, not the current time. If there is a backup count, then we have to get a list of matching filenames, sort them and remove the one with the oldest suffix.
Low-level logging routine which creates a LogRecord and then calls all the handlers of this logger to handle the record. Overwritten to properly handle log methods kwargs
Conditionally emit the specified logging record. Emission depends on filters which may have been added to the handler.
Actually log the specified logging record to the stream.
Pass a record to all relevant handlers. Loop through all handlers for this logger and its parents in the logger hierarchy. If no handler was found, raises an error. Stop searching up the hierarchy whenever a logger with the "propagate" attribute set to zero is found - that will be the last logger whose handlers are called.
Call the handlers for the specified record. This method is used for unpickled records received from a socket, as well as those created locally. Logger-level filtering is applied.
Creates an asyncio.Task for a msg if logging is enabled for level. Returns a dummy task otherwise.
Log msg with severity 'DEBUG'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.debug("Houston, we have a %s", "thorny problem", exc_info=1)
Log msg with severity 'INFO'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.info("Houston, we have an interesting problem", exc_info=1)
Log msg with severity 'WARNING'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.warning("Houston, we have a bit of a problem", exc_info=1)
Log msg with severity 'ERROR'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.error("Houston, we have a major problem", exc_info=1)
Log msg with severity 'CRITICAL'. To pass exception information, use the keyword argument exc_info with a true value, e.g. await logger.critical("Houston, we have a major disaster", exc_info=1)
Convenience method for logging an ERROR with exception information.
Perform any cleanup actions in the logging system (e.g. flushing buffers). Should be called at application exit.
From an array-like input, infer the correct vega typecode ('ordinal', 'nominal', 'quantitative', or 'temporal') Parameters ---------- data: Numpy array or Pandas Series data for which the type will be inferred ordinal_threshold: integer (default: 0) integer data will result in a 'quantitative' type, unless the number of unique values is smaller than ordinal_threshold. Adapted from code at http://github.com/altair-viz/altair/ Licence: BSD-3
Unpivot a dataframe for use with Vega/Vega-Lite The input is a frame with any number of columns, output is a frame with three columns: x value, y values, and variable names.
Validate an aggregation for use in Vega-Lite. Translate agg to one of the following supported named aggregations: ['mean', 'sum', 'median', 'min', 'max', 'count'] Parameters ---------- agg : string or callable A string Supported reductions are ['mean', 'sum', 'median', 'min', 'max', 'count']. If agg is a numpy function, the return value is the string representation. If agg is unrecognized, raise a ValueError
Obtain the packge version from a python file e.g. pkg/__init__.py See <https://packaging.python.org/en/latest/single_source_version.html>.
Draw a matrix of scatter plots. The result is an interactive pan/zoomable plot, with linked-brushing enabled by holding the shift key. Parameters ---------- frame : DataFrame The dataframe for which to draw the scatter matrix. c : string (optional) If specified, the name of the column to be used to determine the color of each point. s : string (optional) If specified, the name of the column to be used to determine the size of each point, figsize : tuple (optional) A length-2 tuple speficying the size of the figure in inches dpi : float (default=72) The dots (i.e. pixels) per inch used to convert the figure size from inches to pixels. Returns ------- chart: alt.Chart object The alt.Chart representation of the plot. See Also -------- pandas.plotting.scatter_matrix : matplotlib version of this routine
Generates an Andrews curves visualization for visualising clusters of multivariate data. Andrews curves have the functional form: f(t) = x_1/sqrt(2) + x_2 sin(t) + x_3 cos(t) + x_4 sin(2t) + x_5 cos(2t) + ... Where x coefficients correspond to the values of each dimension and t is linearly spaced between -pi and +pi. Each row of frame then corresponds to a single curve. Parameters: ----------- data : DataFrame Data to be plotted, preferably normalized to (0.0, 1.0) class_column : string Name of the column containing class names samples : integer Number of points to plot in each curve alpha: float, optional The transparency of the lines width : int, optional the width of the plot in pixels height : int, optional the height of the plot in pixels **kwds: keywords Additional options Returns: -------- chart: alt.Chart object
Parallel coordinates plotting. Parameters ---------- frame: DataFrame class_column: str Column name containing class names cols: list, optional A list of column names to use alpha: float, optional The transparency of the lines interactive : bool, optional if True (default) then produce an interactive plot width : int, optional the width of the plot in pixels height : int, optional the height of the plot in pixels var_name : string, optional the legend title value_name : string, optional the y-axis label Returns ------- chart: alt.Chart object The altair representation of the plot. See Also -------- pandas.plotting.parallel_coordinates : matplotlib version of this routine
Lag plot for time series. Parameters ---------- data: pandas.Series the time series to plot lag: integer The lag of the scatter plot, default=1 kind: string The kind of plot to use (e.g. 'scatter', 'line') **kwds: Additional keywords passed to data.vgplot.scatter Returns ------- chart: alt.Chart object
Return a hash of the contents of a dictionary
Exec a code block & return evaluation of the last line
Import the object given by clsname. If default_module is specified, import from this module.
Strip the vega-lite extension (either vl.json or json) from filename
Utility to return (prev, this, next) tuples from an iterator
Get TurboCARTO CartoCSS based on input parameters
Create a custom scheme. Args: colors (list of str): List of hex values for styling data bins (int, optional): Number of bins to style by. If not given, the number of colors will be used. bin_method (str, optional): Classification method. One of the values in :obj:`BinMethod`. Defaults to `quantiles`, which only works with quantitative data.
Return a custom scheme based on CARTOColors. Args: name (str): Name of a CARTOColor. bins (int or iterable): If an `int`, the number of bins for classifying data. CARTOColors have 7 bins max for quantitative data, and 11 max for qualitative data. If `bins` is a `list`, it is the upper range for classifying data. E.g., `bins` can be of the form ``(10, 20, 30, 40, 50)``. bin_method (str, optional): One of methods in :obj:`BinMethod`. Defaults to ``quantiles``. If `bins` is an interable, then that is the bin method that will be used and this will be ignored. .. Warning:: Input types are particularly sensitive in this function, and little feedback is given for errors. ``name`` and ``bin_method`` arguments are case-sensitive.
Checks if credentials allow for authenticated carto access
Read a table from CARTO into a pandas DataFrames. Args: table_name (str): Name of table in user's CARTO account. limit (int, optional): Read only `limit` lines from `table_name`. Defaults to ``None``, which reads the full table. decode_geom (bool, optional): Decodes CARTO's geometries into a `Shapely <https://github.com/Toblerity/Shapely>`__ object that can be used, for example, in `GeoPandas <http://geopandas.org/>`__. shared_user (str, optional): If a table has been shared with you, specify the user name (schema) who shared it. retry_times (int, optional): If the read call is rate limited, number of retries to be made Returns: pandas.DataFrame: DataFrame representation of `table_name` from CARTO. Example: .. code:: python import cartoframes cc = cartoframes.CartoContext(BASEURL, APIKEY) df = cc.read('acadia_biodiversity')
List all tables in user's CARTO account Returns: :obj:`list` of :py:class:`Table <cartoframes.analysis.Table>`
Write a DataFrame to a CARTO table. Examples: Write a pandas DataFrame to CARTO. .. code:: python cc.write(df, 'brooklyn_poverty', overwrite=True) Scrape an HTML table from Wikipedia and send to CARTO with content guessing to create a geometry from the country column. This uses a CARTO Import API param `content_guessing` parameter. .. code:: python url = 'https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy' # retrieve first HTML table from that page df = pd.read_html(url, header=0)[0] # send to carto, let it guess polygons based on the 'country' # column. Also set privacy to 'public' cc.write(df, 'life_expectancy', content_guessing=True, privacy='public') cc.map(layers=Layer('life_expectancy', color='both_sexes_life_expectancy')) Args: df (pandas.DataFrame): DataFrame to write to ``table_name`` in user CARTO account table_name (str): Table to write ``df`` to in CARTO. temp_dir (str, optional): Directory for temporary storage of data that is sent to CARTO. Defaults are defined by `appdirs <https://github.com/ActiveState/appdirs/blob/master/README.rst>`__. overwrite (bool, optional): Behavior for overwriting ``table_name`` if it exits on CARTO. Defaults to ``False``. lnglat (tuple, optional): lng/lat pair that can be used for creating a geometry on CARTO. Defaults to ``None``. In some cases, geometry will be created without specifying this. See CARTO's `Import API <https://carto.com/developers/import-api/reference/#tag/Standard-Tables>`__ for more information. encode_geom (bool, optional): Whether to write `geom_col` to CARTO as `the_geom`. geom_col (str, optional): The name of the column where geometry information is stored. Used in conjunction with `encode_geom`. **kwargs: Keyword arguments to control write operations. Options are: - `compression` to set compression for files sent to CARTO. This will cause write speedups depending on the dataset. Options are ``None`` (no compression, default) or ``gzip``. - Some arguments from CARTO's Import API. See the `params listed in the documentation <https://carto.com/developers/import-api/reference/#tag/Standard-Tables>`__ for more information. For example, when using `content_guessing='true'`, a column named 'countries' with country names will be used to generate polygons for each country. Another use is setting the privacy of a dataset. To avoid unintended consequences, avoid `file`, `url`, and other similar arguments. Returns: :py:class:`Dataset <cartoframes.datasets.Dataset>` .. note:: DataFrame indexes are changed to ordinary columns. CARTO creates an index called `cartodb_id` for every table that runs from 1 to the length of the DataFrame.
gets current privacy of a table
Updates the privacy of a dataset
Delete a table in user's CARTO account. Args: table_name (str): Name of table to delete Returns: bool: `True` if table is removed
Pull the result from an arbitrary SELECT SQL query from a CARTO account into a pandas DataFrame. Args: query (str): SELECT query to run against CARTO user database. This data will then be converted into a pandas DataFrame. decode_geom (bool, optional): Decodes CARTO's geometries into a `Shapely <https://github.com/Toblerity/Shapely>`__ object that can be used, for example, in `GeoPandas <http://geopandas.org/>`__. Returns: pandas.DataFrame: DataFrame representation of query supplied. Pandas data types are inferred from PostgreSQL data types. In the case of PostgreSQL date types, dates are attempted to be converted, but on failure a data type 'object' is used. Examples: This query gets the 10 highest values from a table and returns a dataframe. .. code:: python topten_df = cc.query( ''' SELECT * FROM my_table ORDER BY value_column DESC LIMIT 10 ''' ) This query joins points to polygons based on intersection, and aggregates by summing the values of the points in each polygon. The query returns a dataframe, with a geometry column that contains polygons. .. code:: python points_aggregated_to_polygons = cc.query( ''' SELECT polygons.*, sum(points.values) FROM polygons JOIN points ON ST_Intersects(points.the_geom, polygons.the_geom) GROUP BY polygons.the_geom, polygons.cartodb_id ''', decode_geom=True )
Pull the result from an arbitrary SQL SELECT query from a CARTO account into a pandas DataFrame. This is the default behavior, when `is_select=True` Can also be used to perform database operations (creating/dropping tables, adding columns, updates, etc.). In this case, you have to explicitly specify `is_select=False` This method is a helper for the `CartoContext.fetch` and `CartoContext.execute` methods. We strongly encourage you to use any of those methods depending on the type of query you want to run. If you want to get the results of a `SELECT` query into a pandas DataFrame, then use `CartoContext.fetch`. For any other query that performs an operation into the CARTO database, use `CartoContext.execute` Args: query (str): Query to run against CARTO user database. This data will then be converted into a pandas DataFrame. table_name (str, optional): If set (and `is_select=True`), this will create a new table in the user's CARTO account that is the result of the SELECT query provided. Defaults to None (no table created). decode_geom (bool, optional): Decodes CARTO's geometries into a `Shapely <https://github.com/Toblerity/Shapely>`__ object that can be used, for example, in `GeoPandas <http://geopandas.org/>`__. It only works for SELECT queries when `is_select=True` is_select (bool, optional): This argument has to be set depending on the query performed. True for SELECT queries, False for any other query. For the case of a SELECT SQL query (`is_select=True`) the result will be stored into a pandas DataFrame. When an arbitrary SQL query (`is_select=False`) it will perform a database operation (UPDATE, DROP, INSERT, etc.) By default `is_select=None` that means that the method will return a dataframe if the `query` starts with a `select` clause, otherwise it will just execute the query and return `None` Returns: pandas.DataFrame: When `is_select=True` and the query is actually a SELECT query this method returns a pandas DataFrame representation of query supplied otherwise returns None. Pandas data types are inferred from PostgreSQL data types. In the case of PostgreSQL date types, dates are attempted to be converted, but on failure a data type 'object' is used. Raises: CartoException: If there's any error when executing the query Examples: Query a table in CARTO and write a new table that is result of query. This query gets the 10 highest values from a table and returns a dataframe, as well as creating a new table called 'top_ten' in the CARTO account. .. code:: python topten_df = cc.query( ''' SELECT * FROM my_table ORDER BY value_column DESC LIMIT 10 ''', table_name='top_ten' ) This query joins points to polygons based on intersection, and aggregates by summing the values of the points in each polygon. The query returns a dataframe, with a geometry column that contains polygons and also creates a new table called 'points_aggregated_to_polygons' in the CARTO account. .. code:: python points_aggregated_to_polygons = cc.query( ''' SELECT polygons.*, sum(points.values) FROM polygons JOIN points ON ST_Intersects(points.the_geom, polygons.the_geom) GROUP BY polygons.the_geom, polygons.cartodb_id ''', table_name='points_aggregated_to_polygons', decode_geom=True ) Drops `my_table` .. code:: python cc.query( ''' DROP TABLE my_table ''' ) Updates the column `my_column` in the table `my_table` .. code:: python cc.query( ''' UPDATE my_table SET my_column = 1 ''' )
Produce a CARTO map visualizing data layers. Examples: Create a map with two data :py:class:`Layer <cartoframes.layer.Layer>`\s, and one :py:class:`BaseMap <cartoframes.layer.BaseMap>` layer:: import cartoframes from cartoframes import Layer, BaseMap, styling cc = cartoframes.CartoContext(BASEURL, APIKEY) cc.map(layers=[BaseMap(), Layer('acadia_biodiversity', color={'column': 'simpson_index', 'scheme': styling.tealRose(7)}), Layer('peregrine_falcon_nest_sites', size='num_eggs', color={'column': 'bird_id', 'scheme': styling.vivid(10))], interactive=True) Create a snapshot of a map at a specific zoom and center:: cc.map(layers=Layer('acadia_biodiversity', color='simpson_index'), interactive=False, zoom=14, lng=-68.3823549, lat=44.3036906) Args: layers (list, optional): List of zero or more of the following: - :py:class:`Layer <cartoframes.layer.Layer>`: cartoframes :py:class:`Layer <cartoframes.layer.Layer>` object for visualizing data from a CARTO table. See :py:class:`Layer <cartoframes.layer.Layer>` for all styling options. - :py:class:`BaseMap <cartoframes.layer.BaseMap>`: Basemap for contextualizng data layers. See :py:class:`BaseMap <cartoframes.layer.BaseMap>` for all styling options. - :py:class:`QueryLayer <cartoframes.layer.QueryLayer>`: Layer from an arbitrary query. See :py:class:`QueryLayer <cartoframes.layer.QueryLayer>` for all styling options. interactive (bool, optional): Defaults to ``True`` to show an interactive slippy map. Setting to ``False`` creates a static map. zoom (int, optional): Zoom level of map. Acceptable values are usually in the range 0 to 19. 0 has the entire earth on a single tile (256px square). Zoom 19 is the size of a city block. Must be used in conjunction with ``lng`` and ``lat``. Defaults to a view to have all data layers in view. lat (float, optional): Latitude value for the center of the map. Must be used in conjunction with ``zoom`` and ``lng``. Defaults to a view to have all data layers in view. lng (float, optional): Longitude value for the center of the map. Must be used in conjunction with ``zoom`` and ``lat``. Defaults to a view to have all data layers in view. size (tuple, optional): List of pixel dimensions for the map. Format is ``(width, height)``. Defaults to ``(800, 400)``. ax: matplotlib axis on which to draw the image. Only used when ``interactive`` is ``False``. Returns: IPython.display.HTML or matplotlib Axes: Interactive maps are rendered as HTML in an `iframe`, while static maps are returned as matplotlib Axes objects or IPython Image.
gets geometry type(s) of specified layer
Find all boundaries available for the world or a `region`. If `boundary` is specified, get all available boundary polygons for the region specified (if any). This method is espeically useful for getting boundaries for a region and, with :py:meth:`CartoContext.data <cartoframes.context.CartoContext.data>` and :py:meth:`CartoContext.data_discovery <cartoframes.context.CartoContext.data_discovery>`, getting tables of geometries and the corresponding raw measures. For example, if you want to analyze how median income has changed in a region (see examples section for more). Examples: Find all boundaries available for Australia. The columns `geom_name` gives us the name of the boundary and `geom_id` is what we need for the `boundary` argument. .. code:: python import cartoframes cc = cartoframes.CartoContext('base url', 'api key') au_boundaries = cc.data_boundaries(region='Australia') au_boundaries[['geom_name', 'geom_id']] Get the boundaries for Australian Postal Areas and map them. .. code:: python from cartoframes import Layer au_postal_areas = cc.data_boundaries(boundary='au.geo.POA') cc.write(au_postal_areas, 'au_postal_areas') cc.map(Layer('au_postal_areas')) Get census tracts around Idaho Falls, Idaho, USA, and add median income from the US census. Without limiting the metadata, we get median income measures for each census in the Data Observatory. .. code:: python cc = cartoframes.CartoContext('base url', 'api key') # will return DataFrame with columns `the_geom` and `geom_ref` tracts = cc.data_boundaries( boundary='us.census.tiger.census_tract', region=[-112.096642,43.429932,-111.974213,43.553539]) # write geometries to a CARTO table cc.write(tracts, 'idaho_falls_tracts') # gather metadata needed to look up median income median_income_meta = cc.data_discovery( 'idaho_falls_tracts', keywords='median income', boundaries='us.census.tiger.census_tract') # get median income data and original table as new dataframe idaho_falls_income = cc.data( 'idaho_falls_tracts', median_income_meta, how='geom_refs') # overwrite existing table with newly-enriched dataframe cc.write(idaho_falls_income, 'idaho_falls_tracts', overwrite=True) Args: boundary (str, optional): Boundary identifier for the boundaries that are of interest. For example, US census tracts have a boundary ID of ``us.census.tiger.census_tract``, and Brazilian Municipios have an ID of ``br.geo.municipios``. Find IDs by running :py:meth:`CartoContext.data_boundaries <cartoframes.context.CartoContext.data_boundaries>` without any arguments, or by looking in the `Data Observatory catalog <http://cartodb.github.io/bigmetadata/>`__. region (str, optional): Region where boundary information or, if `boundary` is specified, boundary polygons are of interest. `region` can be one of the following: - table name (str): Name of a table in user's CARTO account - bounding box (list of float): List of four values (two lng/lat pairs) in the following order: western longitude, southern latitude, eastern longitude, and northern latitude. For example, Switzerland fits in ``[5.9559111595,45.8179931641,10.4920501709,47.808380127]`` timespan (str, optional): Specific timespan to get geometries from. Defaults to use the most recent. See the Data Observatory catalog for more information. decode_geom (bool, optional): Whether to return the geometries as Shapely objects or keep them encoded as EWKB strings. Defaults to False. include_nonclipped (bool, optional): Optionally include non-shoreline-clipped boundaries. These boundaries are the raw boundaries provided by, for example, US Census Tiger. Returns: pandas.DataFrame: If `boundary` is specified, then all available boundaries and accompanying `geom_refs` in `region` (or the world if `region` is ``None`` or not specified) are returned. If `boundary` is not specified, then a DataFrame of all available boundaries in `region` (or the world if `region` is ``None``)
Discover Data Observatory measures. This method returns the full Data Observatory metadata model for each measure or measures that match the conditions from the inputs. The full metadata in each row uniquely defines a measure based on the timespan, geographic resolution, and normalization (if any). Read more about the metadata response in `Data Observatory <https://carto.com/docs/carto-engine/data/measures-functions/#obs_getmetaextent-geometry-metadata-json-max_timespan_rank-max_score_rank-target_geoms>`__ documentation. Internally, this method finds all measures in `region` that match the conditions set in `keywords`, `regex`, `time`, and `boundaries` (if any of them are specified). Then, if `boundaries` is not specified, a geographical resolution for that measure will be chosen subject to the type of region specified: 1. If `region` is a table name, then a geographical resolution that is roughly equal to `region size / number of subunits`. 2. If `region` is a country name or bounding box, then a geographical resolution will be chosen roughly equal to `region size / 500`. Since different measures are in some geographic resolutions and not others, different geographical resolutions for different measures are oftentimes returned. .. tip:: To remove the guesswork in how geographical resolutions are selected, specify one or more boundaries in `boundaries`. See the boundaries section for each region in the `Data Observatory catalog <http://cartodb.github.io/bigmetadata/>`__. The metadata returned from this method can then be used to create raw tables or for augmenting an existing table from these measures using :py:meth:`CartoContext.data <cartoframes.context.CartoContext.data>`. For the full Data Observatory catalog, visit https://cartodb.github.io/bigmetadata/. When working with the metadata DataFrame returned from this method, be careful to only remove rows not columns as `CartoContext.data <cartoframes.context.CartoContext.data>` generally needs the full metadata. .. note:: Narrowing down a discovery query using the `keywords`, `regex`, and `time` filters is important for getting a manageable metadata set. Besides there being a large number of measures in the DO, a metadata response has acceptable combinations of measures with demonimators (normalization and density), and the same measure from other years. For example, setting the region to be United States counties with no filter values set will result in many thousands of measures. Examples: Get all European Union measures that mention ``freight``. .. code:: meta = cc.data_discovery('European Union', keywords='freight', time='2010') print(meta['numer_name'].values) Arguments: region (str or list of float): Information about the region of interest. `region` can be one of three types: - region name (str): Name of region of interest. Acceptable values are limited to: 'Australia', 'Brazil', 'Canada', 'European Union', 'France', 'Mexico', 'Spain', 'United Kingdom', 'United States'. - table name (str): Name of a table in user's CARTO account with geometries. The region will be the bounding box of the table. .. Note:: If a table name is also a valid Data Observatory region name, the Data Observatory name will be chosen over the table. - bounding box (list of float): List of four values (two lng/lat pairs) in the following order: western longitude, southern latitude, eastern longitude, and northern latitude. For example, Switzerland fits in ``[5.9559111595,45.8179931641,10.4920501709,47.808380127]`` .. Note:: Geometry levels are generally chosen by subdividing the region into the next smallest administrative unit. To override this behavior, specify the `boundaries` flag. For example, set `boundaries` to ``'us.census.tiger.census_tract'`` to choose US census tracts. keywords (str or list of str, optional): Keyword or list of keywords in measure description or name. Response will be matched on all keywords listed (boolean `or`). regex (str, optional): A regular expression to search the measure descriptions and names. Note that this relies on PostgreSQL's case insensitive operator ``~*``. See `PostgreSQL docs <https://www.postgresql.org/docs/9.5/static/functions-matching.html>`__ for more information. boundaries (str or list of str, optional): Boundary or list of boundaries that specify the measure resolution. See the boundaries section for each region in the `Data Observatory catalog <http://cartodb.github.io/bigmetadata/>`__. include_quantiles (bool, optional): Include quantiles calculations which are a calculation of how a measure compares to all measures in the full dataset. Defaults to ``False``. If ``True``, quantiles columns will be returned for each column which has it pre-calculated. Returns: pandas.DataFrame: A dataframe of the complete metadata model for specific measures based on the search parameters. Raises: ValueError: If `region` is a :obj:`list` and does not consist of four elements, or if `region` is not an acceptable region CartoException: If `region` is not a table in user account
Get an augmented CARTO dataset with `Data Observatory <https://carto.com/data-observatory>`__ measures. Use `CartoContext.data_discovery <#context.CartoContext.data_discovery>`__ to search for available measures, or see the full `Data Observatory catalog <https://cartodb.github.io/bigmetadata/index.html>`__. Optionally persist the data as a new table. Example: Get a DataFrame with Data Observatory measures based on the geometries in a CARTO table. .. code:: cc = cartoframes.CartoContext(BASEURL, APIKEY) median_income = cc.data_discovery('transaction_events', regex='.*median income.*', time='2011 - 2015') df = cc.data('transaction_events', median_income) Pass in cherry-picked measures from the Data Observatory catalog. The rest of the metadata will be filled in, but it's important to specify the geographic level as this will not show up in the column name. .. code:: median_income = [{'numer_id': 'us.census.acs.B19013001', 'geom_id': 'us.census.tiger.block_group', 'numer_timespan': '2011 - 2015'}] df = cc.data('transaction_events', median_income) Args: table_name (str): Name of table on CARTO account that Data Observatory measures are to be added to. metadata (pandas.DataFrame): List of all measures to add to `table_name`. See :py:meth:`CartoContext.data_discovery <cartoframes.context.CartoContext.data_discovery>` outputs for a full list of metadata columns. persist_as (str, optional): Output the results of augmenting `table_name` to `persist_as` as a persistent table on CARTO. Defaults to ``None``, which will not create a table. how (str, optional): **Not fully implemented**. Column name for identifying the geometry from which to fetch the data. Defaults to `the_geom`, which results in measures that are spatially interpolated (e.g., a neighborhood boundary's population will be calculated from underlying census tracts). Specifying a column that has the geometry identifier (for example, GEOID for US Census boundaries), results in measures directly from the Census for that GEOID but normalized how it is specified in the metadata. Returns: pandas.DataFrame: A DataFrame representation of `table_name` which has new columns for each measure in `metadata`. Raises: NameError: If the columns in `table_name` are in the ``suggested_name`` column of `metadata`. ValueError: If metadata object is invalid or empty, or if the number of requested measures exceeds 50. CartoException: If user account consumes all of Data Observatory quota
Checks if query from Layer or QueryLayer is valid
Return the bounds of all data layers involved in a cartoframes map. Args: layers (list): List of cartoframes layers. See `cartoframes.layers` for all types. Returns: dict: Dictionary of northern, southern, eastern, and western bounds of the superset of data layers. Keys are `north`, `south`, `east`, and `west`. Units are in WGS84.
CARTO VL-powered interactive map Args: layers (list of Layer-types): List of layers. One or more of :py:class:`Layer <cartoframes.contrib.vector.Layer>`, :py:class:`QueryLayer <cartoframes.contrib.vector.QueryLayer>`, or :py:class:`LocalLayer <cartoframes.contrib.vector.LocalLayer>`. context (:py:class:`CartoContext <cartoframes.context.CartoContext>`): A :py:class:`CartoContext <cartoframes.context.CartoContext>` instance size (tuple of int or str): a (width, height) pair for the size of the map. Default is None, which makes the map 100% wide and 640px tall. If specified as int, will be used as pixels, but you can also use string values for the CSS attributes. So, you could specify it as size=('75%', 250). basemap (str): - if a `str`, name of a CARTO vector basemap. One of `positron`, `voyager`, or `darkmatter` from the :obj:`BaseMaps` class - if a `dict`, Mapbox or other style as the value of the `style` key. If a Mapbox style, the access token is the value of the `token` key. bounds (dict or list): a dict with `east`,`north`,`west`,`south` properties, or a list of floats in the following order: [west, south, east, north]. If not provided the bounds will be automatically calculated to fit all features. viewport (dict): Configure where and how map will be centered. If not specified, or specified without lat / lng, automatic bounds or the bounds argument will be used to center the map. You can specify only zoom, bearing or pitch if you desire automatic bounds but want to tweak the viewport. - lng (float): Longitude to center the map on. Must specify lat as well. - lat (float): Latitude to center the map on. Must specify lng as well. - zoom (float): Zoom level. - bearing (float): A bearing, or heading, is the direction you're facing, measured clockwise as an angle from true north on a compass. (north is 0, east is 90, south is 180, and west is 270). - pitch (float): The angle towards the horizon measured in degrees, with a range between 0 and 60 degrees. Zero degrees results in a two-dimensional map, as if your line of sight forms a perpendicular angle with the earth's surface. Example: .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://your_user_name.carto.com', api_key='your api key' ) vector.vmap([vector.Layer('table in your account'), ], cc) CARTO basemap style. .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://your_user_name.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, basemap=vector.BaseMaps.darkmatter ) Custom basemap style. Here we use the Mapbox streets style, which requires an access token. .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://<username>.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, basemap={ 'style': 'mapbox://styles/mapbox/streets-v9', 'token: '<your mapbox token>' } ) Custom bounds .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://<username>.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, bounds={'west': -10, 'east': 10, 'north': -10, 'south': 10} ) Adjusting the map's viewport. .. code:: from cartoframes.contrib import vector from cartoframes import CartoContext cc = CartoContext( base_url='https://<username>.carto.com', api_key='your api key' ) vector.vmap( [vector.Layer('table in your account'), ], context=cc, viewport={'lng': 10, 'lat': 15, 'zoom': 10, 'bearing': 90, 'pitch': 45} )
Aggregates bounding boxes of all local layers return: dict of bounding box of all bounds in layers
Takes two bounding boxes dicts and gives a new bbox that encompasses them both
Appends `prop` with `style` to layer styling
Adds interactivity syntax to the styling
normalize country name to match data obs
Creates a map named based on supplied parameters
Creates a map template based on custom parameters supplied
Setup the color scheme
Parse time inputs
Parse size inputs
Validate the options in the styles
Setups layers once geometry types and data types are known, and when a map is requested to be rendered from zero or more data layers
Choose color scheme
generates CartoCSS for time-based maps (torque)
Generate cartocss for class properties
Given an arbitrary column name, translate to a SQL-normalized column name a la CARTO's Import API will translate to Examples * 'Field: 2' -> 'field_2' * '2 Items' -> '_2_items' * 'Unnamed: 0' -> 'unnamed_0', * '201moore' -> '_201moore', * '201moore' -> '_201moore_1', * 'Acadia 1.2.3' -> 'acadia_1_2_3', * 'old_soaker' -> 'old_soaker', * '_testingTesting' -> '_testingtesting', * 1 -> '_1', * 1.0 -> '_1_0', * 'public' -> 'public', * 'SELECT' -> '_select', * 'à' -> 'a', * 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabesplittedrightnow' -> \ 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabespli', * 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabesplittedrightnow' -> \ 'longcolumnshouldbesplittedsomehowanditellyouwhereitsgonnabe_1', * 'all' -> '_all' Args: column_names (list): List of column names that will be SQL normalized Returns: list: List of SQL-normalized column names
Function to get CartoCSS from Python dicts
htmlify string
Temporarily ignores warnings like those emitted by the carto python sdk
Get list of cartoframes.columns.Column
Get column names and types from a query
decorator for encoding and decoding geoms
Decode encoded wkb into a shapely geometry
Checks to see if table exists
Create the read (COPY TO) query
Get column names and types from a table
Get column names and types from a table
Saves current user credentials to user directory. Args: config_loc (str, optional): Location where credentials are to be stored. If no argument is provided, it will be send to the default location. Example: .. code:: from cartoframes import Credentials creds = Credentials(username='eschbacher', key='abcdefg') creds.save() # save to default location
Retrives credentials from a file. Defaults to the user config directory
Deletes the credentials file specified in `config_file`. If no file is specified, it deletes the default user credential file. Args: config_file (str): Path to configuration file. Defaults to delete the user default location if `None`. .. Tip:: To see if there is a default user credential file stored, do the following:: >>> creds = Credentials() >>> print(creds) Credentials(username=eschbacher, key=abcdefg, base_url=https://eschbacher.carto.com/)
Update the credentials of a Credentials instance instead with new values. Args: key (str): API key of user account. Defaults to previous value if not specified. username (str): User name of account. This parameter is optional if `base_url` is not specified, but defaults to the previous value if not set. base_url (str): Base URL of user account. This parameter is optional if `username` is specified and on CARTO's cloud-based account. Generally of the form ``https://your_user_name.carto.com/`` for cloud-based accounts. If on-prem or otherwise, contact your admin. Example: .. code:: from cartoframes import Credentials # load credentials saved in previous session creds = Credentials() # set new API key creds.set(key='new_api_key') # save new creds to default user config directory creds.save() Note: If the `username` is specified but the `base_url` is not, the `base_url` will be updated to ``https://<username>.carto.com/``.
Return or set `base_url`. Args: base_url (str, optional): If set, updates the `base_url`. Otherwise returns current `base_url`. Note: This does not update the `username` attribute. Separately update the username with ``Credentials.username`` or update `base_url` and `username` at the same time with ``Credentials.set``. Example: .. code:: >>> from cartoframes import Credentials # load credentials saved in previous session >>> creds = Credentials() # returns current base_url >>> creds.base_url() 'https://eschbacher.carto.com/' # updates base_url with new value >>> creds.base_url('new_base_url')
Quick setup for a chatroom. :param str room: Roomname, if not given, a random sequence is generated and printed. :param MediaStream stream: The media stream to share, if not given a CameraStream will be created. :rtype: WebRTCRoom
Adds an object as a child in the scene graph.
Conveniience function: Adds objects as children in the scene graph.
Returns a copy of the projection matrix
Updates Camera.aspect to match the viewport's aspect ratio.
np.array: The Camera's lens-shift matrix.
np.array: The Camera's Projection Matrix. Will be an Orthographic matrix if ortho_mode is set to True.
Save Camera to a pickle file, given a filename.
Loads and Returns a Camera from a pickle file, given a filename.
Converges the two cameras to look at the specific point
Bind the FBO. Anything drawn afterward will be stored in the FBO's texture.
Unbind the FBO.
Returns int pointing to an OpenGL texture
Makes GLfloat or GLuint vector containing float or uint args. By default, newtype is 'float', but can be set to 'int' to make uint list.
Return Nx3 normal array from Nx3 vertex array.
Experimental anaglyph drawing function for VR system with red/blue glasses, used in Sirota lab. Draws a virtual scene in red and blue, from subject's (heda trackers) perspective in active scene. Note: assumes shader uses playerPos like ratcave's default shader Args: cube_fbo: texture frameBuffer object. vr_scene: virtual scene object active_scene: active scene object eye_poses: the eye positions Returns:
Draw each visible mesh in the scene from the perspective of the scene's camera and lit by its light.
Draw each visible mesh in the scene from the perspective of the scene's camera and lit by its light, and applies it to each face of cubetexture, which should be currently bound to an FBO.
Load data into a vbo
Returns a copy of the Mesh.
Loads and Returns a Mesh from a pickle file, given a filename.
Resets the uniforms to the Mesh object to the ""global"" coordinate system
Return a Mesh with (vertices, normals, texcoords) as arrays, in that order. Useful for when you want a standardized array location format across different amounts of info in each mesh.
Put array location in VAO for shader in same order as arrays given to Mesh.
Draw the Mesh if it's visible, from the perspective of the camera and lit by the light. The function sends the uniforms
Returns a 3x3 cross-product matrix from a 3-element vector.
Returns a rotation matrix to rotate from 3d vector "from_vec" to 3d vector "to_vec". Equation from https://math.stackexchange.com/questions/180418/calculate-rotation-matrix-to-align-vector-a-to-vector-b-in-3d
Generates combinations of named coordinate values, mapping them to the internal array. For Example: x, xy, xyz, y, yy, zyx, etc
The maximum number of textures available for this graphic card's fragment shader.
Applies some hard-coded texture filtering settings.
Attach the texture to a bound FBO object, for rendering to texture.
Uses Pyglet's image.load function to generate a Texture from an image file. If 'mipmap', then texture will have mipmap layers calculated.
Generate an empty texture in OpenGL
Builds Mesh from geom name in the wavefront file. Takes all keyword arguments that Mesh takes.
Sends all the key-value pairs to the graphics card. These uniform variables will be available in the currently-bound shader.
Activate this Shader, making it the currently-bound program. Any Mesh.draw() calls after bind() will have their data processed by this Shader. To unbind, call Shader.unbind(). Example:: shader.bind() mesh.draw() shader.unbind() .. note:: Shader.bind() and Shader.unbind() can be also be called implicitly by using the 'with' statement. Example of with statement with Shader:: with shader: mesh.draw()
Reads the shader programs, given the vert and frag filenames Arguments: - vert (str): The filename of the vertex shader program (ex: 'vertshader.vert') - frag (str): The filename of the fragment shader program (ex: 'fragshader.frag') Returns: - shader (Shader): The Shader using these files.
link the program, making it the active shader. .. note:: Shader.bind() is preferred here, because link() Requires the Shader to be compiled already.
Rotate so orientation is toward (x, y, z) coordinates.
Adds an object as a child in the scene graph. With modify=True, model_matrix_transform gets change from identity and prevents the changes of the coordinates of the child
Setup an example Treeview
Put widgets in the grid
Take a screenshot, crop and save
Take a screenshot for all themes available
Load the themes into the Tkinter interpreter
Append a theme dir to the Tk interpreter auto_path
Set new theme to use. Uses a direct tk call to allow usage of the themes supplied with this package. :param theme_name: name of theme to activate
Load an advanced theme that is dynamically created Applies the given modifiers to the images of the theme given and then creates a theme from these new images with the name 'advanced' and then applies this theme. Is not available without support for PNG-based themes, then raises RuntimeError.
Setup all the files required to enable an advanced theme. Copies all the files over and creates the required directories if they do not exist. :param theme_name: theme to copy the files over from :param output_dir: output directory to place the files in
Apply modifiers to the images of a theme Modifies the images using the PIL.ImageEnhance module. Using this function, theme images are modified to given them a unique look and feel. Works best with PNG-based images.
Redirect the set_theme call to also set Tk background color
Setup Toplevel.__init__ hook for background color
configure redirect to support additional options
cget redirect to support additional options
:param command: command to run on os.system :return: exit code
Build a binary distribution wheel and install it
Run the most common CI tasks
Setup Travis-CI macOS for wheel building
Set a new theme to use or return current theme name :param theme_name: name of theme to use :returns: active theme name
Like os.chdir(), but always restores the old working directory For example, code like this... old_curdir = os.getcwd() os.chdir('stuff') do_some_stuff() os.chdir(old_curdir) ...leaves the current working directory unchanged if do_some_stuff() raises an error, so it should be rewritten like this: old_curdir = os.getcwd() os.chdir('stuff') try: do_some_stuff() finally: os.chdir(old_curdir) Or equivalently, like this: with utils.temporary_chdir('stuff'): do_some_stuff()
Return an absolute path to an existing temporary directory
Return an absolute path the to /themes directory
Create directory but first delete it if it exists
Shifts the hue of an image in HSV format. :param image: PIL Image to perform operation on :param hue: value between 0 and 2.0
Turn all black pixels in an image into transparent ones
Setup platform specific network settings
Connects to WIFI
Disables any Accesspoint
Depending on the interval: returns True if its time for an update, returns False if its not yet time for an update
Return the property id from topic as integer
add a node class of HomieNode to this device
subscribe to all registered device and node topics
publish device and node properties
publish node data if node has updates
publish device and node properties, run forever
Factory method for shared memory arrays supporting all numpy dtypes.
Get configuration value from PYMP/OMP env variables.
Print synchronized.
Get the correctly distributed parallel chunks. This corresponds to using the OpenMP 'static' schedule.
Get an iterator for this threads chunk of work. This corresponds to using the OpenMP 'dynamic' schedule.
Iterate over an iterable. The iterator is executed in the host thread. The threads dynamically grab the elements. The iterator elements must hence be picklable to be transferred through the queue. If there is only one thread, no special operations are performed. Otherwise, effectively n-1 threads are used to process the iterable elements, and the host thread is used to provide them. You can specify a timeout for the clients to adhere.
Iterator implementation.
Iterator implementation.
creates (if needed) and updates the value of the key in the config with a value entered by the user Parameters ---------- config: ConfigParser object existing configuration key: string key to update instruction: string text to show in the prompt is_sensitive: bool if true, require confirmation and do not show typed characters Notes ----- sets key in config passed in
Configure information about Databricks account and default behavior. Configuration is stored in a `.apparatecfg` file. A config file must exist before this package can be used, and can be supplied either directly as a text file or generated using this configuration tool.
upload an egg to the Databricks filesystem. Parameters ---------- filename: string local location of file to upload match: FilenameMatch object match object with library_type, library_name, and version folder: string Databricks folder to upload to (e.g. '/Users/htorrence@shoprunner.com/') token: string Databricks API key host: string Databricks host (e.g. https://my-organization.cloud.databricks.com) Side Effects ------------ uploads egg to Databricks
get a list of jobs using the major version of the given library Parameters ---------- logger: logging object configured in cli_commands.py match: FilenameMatch object match object with suffix library_mapping: dict first element of get_library_mapping output token: string Databricks API key host: string Databricks host (e.g. https://my-organization.cloud.databricks.com) Returns ------- list of dictionaries containing the job id, job name, and library path for each job
returns a pair of library mappings, the first mapping library uri to a library name for all libraries in the production folder, and the second mapping library name to info for libraries in the production folder with parsable versions Parameters ---------- logger: logging object configured in cli_commands.py prod_folder: string name of folder in Databricks UI containing production libraries token: string Databricks API key host: string Databricks account url (e.g. https://fake-organization.cloud.databricks.com) Returns ------- dictionary mapping a library uri to a library name dictionary mapping library UI path to base name, major version, minor version, and id number
update libraries on jobs using same major version Parameters ---------- logger: logging object configured in cli_commands.py job_list: list of strings output of get_job_list match: FilenameMatch object match object with suffix new_library_path: string path to library in dbfs (including uri) token: string Databricks API key with admin permissions host: string Databricks account url (e.g. https://fake-organization.cloud.databricks.com) Side Effects ------------ jobs now require updated version of library
delete any other versions of the same library where: it has the same major version it has a smaller minor version it lives in prod_folder Parameters ---------- logger: logging object configured in cli_commands.py match: FilenameMatch object match object with library_name_, major_version, minor_version id_nums: dict second output of get_library_mapping token: string Databricks API key with admin permissions prod_folder: string name of folder in Databricks UI containing production libraries host: string Databricks account url (e.g. https://fake-organization.cloud.databricks.com) Side Effects ------------ delete any other versions of the same library with the same major version and smaller minor versions
upload library, update jobs using the same major version, and delete libraries with the same major and lower minor versions (depending on update_jobs and cleanup flags) Parameters ---------- logger: logging object configured in cli_commands.py path: string path with name of egg as output from setuptools (e.g. dist/new_library-1.0.0-py3.6.egg) token: string Databricks API key folder: string Databricks folder to upload to (e.g. '/Users/my_email@fake_organization.com/') update_jobs: bool if true, jobs using this library will be updated to point to the new version if false, will not touch jobs or other library versions at all cleanup: bool if true, outdated libraries will be deleted if false, nothing will be deleted Side Effects ------------ new library in Databricks if update_jobs is true, then updated jobs if update_jobs and cleanup are true, removed outdated libraries
True if self can safely replace other based on version numbers only - snapshot and branch tags are ignored
Resolve input entered as option values with config values If option values are provided (passed in as `variable`), then they are returned unchanged. If `variable` is None, then we first look for a config value to use. If no config value is found, then raise an error. Parameters ---------- variable: string or numeric value passed in as input by the user variable_name: string name of the variable, for clarity in the error message config_key: string key in the config whose value could be used to fill in the variable config: ConfigParser contains keys/values in .apparatecfg
The egg that the provided path points to will be uploaded to Databricks.
The egg that the provided path points to will be uploaded to Databricks. All jobs which use the same major version of the library will be updated to use the new version, and all version of this library in the production folder with the same major version and a lower minor version will be deleted. Unlike `upload`, `upload_and_update` does not ask for a folder because it relies on the production folder specified in the config. This is to protect against accidentally updating jobs to versions of a library still in testing/development. All egg names already in Databricks must be properly formatted with versions of the form <name>-0.0.0.
Parse a SAS token into its components. :param sas_token: The SAS token. :type sas_token: str :rtype: dict[str, str]
The offset of the event data object. :rtype: ~azure.eventhub.common.Offset
The enqueued timestamp of the event data object. :rtype: datetime.datetime
The partition key of the event data object. :rtype: bytes
Set the partition key of the event data object. :param value: The partition key to set. :type value: str or bytes
Application defined properties on the message. :param value: The application properties for the EventData. :type value: dict
The body of the event data as a string if the data is of a compatible type. :param encoding: The encoding to use for decoding message data. Default is 'UTF-8' :rtype: str or unicode
The body of the event loaded as a JSON object is the data is compatible. :param encoding: The encoding to use for decoding message data. Default is 'UTF-8' :rtype: dict
Creates a selector expression of the offset. :rtype: bytes
Returns an auth token dictionary for making calls to eventhub REST API. :rtype: str
Returns an auth token for making calls to eventhub REST API. :rtype: str
Open the Sender using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.connection.Connection
Whether the handler has completed all start up processes such as establishing the connection, session, link and authentication, and is not ready to process messages. **This function is now deprecated and will be removed in v2.0+.** :rtype: bool
Close down the handler. If the handler has already closed, this will be a no op. An optional exception can be passed in to indicate that the handler was shutdown due to error. :param exception: An optional exception if the handler is closing due to an error. :type exception: Exception
Sends an event data and blocks until acknowledgement is received or operation times out. :param event_data: The event to be sent. :type event_data: ~azure.eventhub.common.EventData :raises: ~azure.eventhub.common.EventHubError if the message fails to send. :return: The outcome of the message send. :rtype: ~uamqp.constants.MessageSendResult
Transfers an event data and notifies the callback when the operation is done. :param event_data: The event to be sent. :type event_data: ~azure.eventhub.common.EventData :param callback: Callback to be run once the message has been send. This must be a function that accepts two arguments. :type callback: callable[~uamqp.constants.MessageSendResult, ~azure.eventhub.common.EventHubError]
Wait until all transferred events have been sent.
Called when the outcome is received for a delivery. :param outcome: The outcome of the message delivery - success or failure. :type outcome: ~uamqp.constants.MessageSendResult
The EventProcessorHost can't pass itself to the AzureStorageCheckpointLeaseManager constructor because it is still being constructed. Do other initialization here also because it might throw and hence we don't want it in the constructor.
Get the checkpoint data associated with the given partition. Could return null if no checkpoint has been created for that partition. :param partition_id: The partition ID. :type partition_id: str :return: Given partition checkpoint info, or `None` if none has been previously stored. :rtype: ~azure.eventprocessorhost.checkpoint.Checkpoint
Create the given partition checkpoint if it doesn't exist.Do nothing if it does exist. The offset/sequenceNumber for a freshly-created checkpoint should be set to StartOfStream/0. :param partition_id: The partition ID. :type partition_id: str :return: The checkpoint for the given partition, whether newly created or already existing. :rtype: ~azure.eventprocessorhost.checkpoint.Checkpoint
Update the checkpoint in the store with the offset/sequenceNumber in the provided checkpoint checkpoint:offset/sequeceNumber to update the store with. :param lease: The stored lease to be updated. :type lease: ~azure.eventprocessorhost.lease.Lease :param checkpoint: The checkpoint to update the lease with. :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint
Create the lease store if it does not exist, do nothing if it does exist. :return: `True` if the lease store already exists or was created successfully, `False` if not. :rtype: bool
Return the lease info for the specified partition. Can return null if no lease has been created in the store for the specified partition. :param partition_id: The partition ID. :type partition_id: str :return: lease info for the partition, or `None`. :rtype: ~azure.eventprocessorhost.lease.Lease
Return the lease info for all partitions. A typical implementation could just call get_lease_async() on all partitions. :return: A list of lease info. :rtype: list[~azure.eventprocessorhost.lease.Lease]
Create in the store the lease info for the given partition, if it does not exist. Do nothing if it does exist in the store already. :param partition_id: The ID of a given parition. :type partition_id: str :return: the existing or newly-created lease info for the partition. :rtype: ~azure.eventprocessorhost.lease.Lease
Delete the lease info for the given partition from the store. If there is no stored lease for the given partition, that is treated as success. :param lease: The stored lease to be deleted. :type lease: ~azure.eventprocessorhost.lease.Lease
Acquire the lease on the desired partition for this EventProcessorHost. Note that it is legal to acquire a lease that is already owned by another host. Lease-stealing is how partitions are redistributed when additional hosts are started. :param lease: The stored lease to be acquired. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the lease was acquired successfully, `False` if not. :rtype: bool
Renew a lease currently held by this host. If the lease has been stolen, or expired, or released, it is not possible to renew it. You will have to call getLease() and then acquireLease() again. :param lease: The stored lease to be renewed. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the lease was renewed successfully, `False` if not. :rtype: bool
Give up a lease currently held by this host. If the lease has been stolen, or expired, releasing it is unnecessary, and will fail if attempted. :param lease: The stored lease to be released. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the lease was released successfully, `False` if not. :rtype: bool
Update the store with the information in the provided lease. It is necessary to currently hold a lease in order to update it. If the lease has been stolen, or expired, or released, it cannot be updated. Updating should renew the lease before performing the update to avoid lease expiration during the process. :param lease: The stored lease to be updated. :type lease: ~azure.eventprocessorhost.lease.Lease :return: `True` if the updated was performed successfully, `False` if not. :rtype: bool
Open the Receiver using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.async_ops.connection_async.ConnectionAsync
Whether the handler has completed all start up processes such as establishing the connection, session, link and authentication, and is not ready to process messages. **This function is now deprecated and will be removed in v2.0+.** :rtype: bool
Create a shared access signiture token as a string literal. :returns: SAS token as string literal. :rtype: str
Create an EventHubClient from an existing auth token or token generator. :param address: The Event Hub address URL :type address: str :param sas_token: A SAS token or function that returns a SAS token. If a function is supplied, it will be used to retrieve subsequent tokens in the case of token expiry. The function should take no arguments. :type sas_token: str or callable :param eventhub: The name of the EventHub, if not already included in the address URL. :type eventhub: str :param debug: Whether to output network trace logs to the logger. Default is `False`. :type debug: bool :param http_proxy: HTTP proxy settings. This must be a dictionary with the following keys: 'proxy_hostname' (str value) and 'proxy_port' (int value). Additionally the following keys may also be present: 'username', 'password'. :type http_proxy: dict[str, Any] :param auth_timeout: The time in seconds to wait for a token to be authorized by the service. The default value is 60 seconds. If set to 0, no timeout will be enforced from the client. :type auth_timeout: int
Create an EventHubClient from a connection string. :param conn_str: The connection string. :type conn_str: str :param eventhub: The name of the EventHub, if the EntityName is not included in the connection string. :type eventhub: str :param debug: Whether to output network trace logs to the logger. Default is `False`. :type debug: bool :param http_proxy: HTTP proxy settings. This must be a dictionary with the following keys: 'proxy_hostname' (str value) and 'proxy_port' (int value). Additionally the following keys may also be present: 'username', 'password'. :type http_proxy: dict[str, Any] :param auth_timeout: The time in seconds to wait for a token to be authorized by the service. The default value is 60 seconds. If set to 0, no timeout will be enforced from the client. :type auth_timeout: int
Create an EventHubClient from an IoTHub connection string. :param conn_str: The connection string. :type conn_str: str :param debug: Whether to output network trace logs to the logger. Default is `False`. :type debug: bool :param http_proxy: HTTP proxy settings. This must be a dictionary with the following keys: 'proxy_hostname' (str value) and 'proxy_port' (int value). Additionally the following keys may also be present: 'username', 'password'. :type http_proxy: dict[str, Any] :param auth_timeout: The time in seconds to wait for a token to be authorized by the service. The default value is 60 seconds. If set to 0, no timeout will be enforced from the client. :type auth_timeout: int
Format the properties with which to instantiate the connection. This acts like a user agent over HTTP. :rtype: dict
Run the EventHubClient in blocking mode. Opens the connection and starts running all Sender/Receiver clients. Returns a list of the start up results. For a succcesful client start the result will be `None`, otherwise the exception raised. If all clients failed to start, then run will fail, shut down the connection and raise an exception. If at least one client starts up successfully the run command will succeed. :rtype: list[~azure.eventhub.common.EventHubError]
Stop the EventHubClient and all its Sender/Receiver clients.
Get details on the specified EventHub. Keys in the details dictionary include: -'name' -'type' -'created_at' -'partition_count' -'partition_ids' :rtype: dict
Add a receiver to the client for a particular consumer group and partition. :param consumer_group: The name of the consumer group. :type consumer_group: str :param partition: The ID of the partition. :type partition: str :param offset: The offset from which to start receiving. :type offset: ~azure.eventhub.common.Offset :param prefetch: The message prefetch count of the receiver. Default is 300. :type prefetch: int :operation: An optional operation to be appended to the hostname in the source URL. The value must start with `/` character. :type operation: str :rtype: ~azure.eventhub.receiver.Receiver
Add a sender to the client to EventData object to an EventHub. :param partition: Optionally specify a particular partition to send to. If omitted, the events will be distributed to available partitions via round-robin. :type parition: str :operation: An optional operation to be appended to the hostname in the target URL. The value must start with `/` character. :type operation: str :param send_timeout: The timeout in seconds for an individual event to be sent from the time that it is queued. Default value is 60 seconds. If set to 0, there will be no timeout. :type send_timeout: int :param keep_alive: The time interval in seconds between pinging the connection to keep it alive during periods of inactivity. The default value is 30 seconds. If set to `None`, the connection will not be pinged. :type keep_alive: int :param auto_reconnect: Whether to automatically reconnect the sender if a retryable error occurs. Default value is `True`. :rtype: ~azure.eventhub.sender.Sender
Create an ~uamqp.authentication.cbs_auth_async.SASTokenAuthAsync instance to authenticate the session. :param username: The name of the shared access policy. :type username: str :param password: The shared access key. :type password: str
Run the EventHubClient asynchronously. Opens the connection and starts running all AsyncSender/AsyncReceiver clients. Returns a list of the start up results. For a succcesful client start the result will be `None`, otherwise the exception raised. If all clients failed to start, then run will fail, shut down the connection and raise an exception. If at least one client starts up successfully the run command will succeed. :rtype: list[~azure.eventhub.common.EventHubError]
Stop the EventHubClient and all its Sender/Receiver clients.
Get details on the specified EventHub async. :rtype: dict
Add an async receiver to the client for a particular consumer group and partition. :param consumer_group: The name of the consumer group. :type consumer_group: str :param partition: The ID of the partition. :type partition: str :param offset: The offset from which to start receiving. :type offset: ~azure.eventhub.common.Offset :param prefetch: The message prefetch count of the receiver. Default is 300. :type prefetch: int :operation: An optional operation to be appended to the hostname in the source URL. The value must start with `/` character. :type operation: str :rtype: ~azure.eventhub.async_ops.receiver_async.ReceiverAsync
Add an async sender to the client to send ~azure.eventhub.common.EventData object to an EventHub. :param partition: Optionally specify a particular partition to send to. If omitted, the events will be distributed to available partitions via round-robin. :type partition: str :operation: An optional operation to be appended to the hostname in the target URL. The value must start with `/` character. :type operation: str :param send_timeout: The timeout in seconds for an individual event to be sent from the time that it is queued. Default value is 60 seconds. If set to 0, there will be no timeout. :type send_timeout: int :param keep_alive: The time interval in seconds between pinging the connection to keep it alive during periods of inactivity. The default value is 30 seconds. If set to `None`, the connection will not be pinged. :type keep_alive: int :param auto_reconnect: Whether to automatically reconnect the sender if a retryable error occurs. Default value is `True`. :type auto_reconnect: bool :rtype: ~azure.eventhub.async_ops.sender_async.SenderAsync
Creates a new Checkpoint from an existing checkpoint. :param checkpoint: Existing checkpoint. :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint
Init Azure Blob Lease with existing blob.
Init Azure Blob Lease from existing.
Check and return Azure Blob Lease state using Storage API.
Makes pump sync so that it can be run in a thread.
Updates pump status and logs update to console.
Sets a new partition lease to be processed by the pump. :param lease: The lease to set. :type lease: ~azure.eventprocessorhost.lease.Lease
Opens partition pump.
Safely closes the pump. :param reason: The reason for the shutdown. :type reason: str
Process pump events. :param events: List of events to be processed. :type events: list[~azure.eventhub.common.EventData]
Eventhub Override for on_open_async.
Responsible for establishing connection to event hub client throws EventHubsException, IOException, InterruptedException, ExecutionException.
Resets the pump swallows all exceptions.
Overides partition pump on closing. :param reason: The reason for the shutdown. :type reason: str
Runs the async partion reciever event loop to retrive messages from the event queue.
Handles processing errors this is never called since python recieve client doesn't have error handling implemented (TBD add fault pump handling). :param error: An error the occurred. :type error: Exception
Init with partition Id. :param partition_id: ID of a given partition. :type partition_id: str
Init with existing lease. :param lease: An existing Lease. :type lease: ~azure.eventprocessorhost.lease.Lease
Starts the host.
Updates offset based on event. :param event_data: A received EventData with valid offset and sequenceNumber. :type event_data: ~azure.eventhub.common.EventData
Gets the initial offset for processing the partition. :rtype: str
Generates a checkpoint for the partition using the curren offset and sequenceNumber for and persists to the checkpoint manager. :param event_processor_context An optional custom state value for the Event Processor. This data must be in a JSON serializable format. :type event_processor_context: str or dict
Stores the offset and sequenceNumber from the provided received EventData instance, then writes those values to the checkpoint store via the checkpoint manager. Optionally stores the state of the Event Processor along the checkpoint. :param event_data: A received EventData with valid offset and sequenceNumber. :type event_data: ~azure.eventhub.common.EventData :param event_processor_context An optional custom state value for the Event Processor. This data must be in a JSON serializable format. :type event_processor_context: str or dict :raises: ValueError if suplied event_data is None. :raises: ValueError if the sequenceNumber is less than the last checkpointed value.
Returns the parition context in the following format: "PartitionContext({EventHubPath}{ConsumerGroupName}{PartitionId}{SequenceNumber})" :rtype: str
Persists the checkpoint, and - optionally - the state of the Event Processor. :param checkpoint: The checkpoint to persist. :type checkpoint: ~azure.eventprocessorhost.checkpoint.Checkpoint :param event_processor_context An optional custom state value for the Event Processor. This data must be in a JSON serializable format. :type event_processor_context: str or dict
Called by processor host to indicate that the event processor is being stopped. :param context: Information about the partition :type context: ~azure.eventprocessorhost.PartitionContext
Called by the processor host when a batch of events has arrived. This is where the real work of the event processor is done. :param context: Information about the partition :type context: ~azure.eventprocessorhost.PartitionContext :param messages: The events to be processed. :type messages: list[~azure.eventhub.common.EventData]
Open the Receiver using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.connection.Connection
Receive events from the EventHub. :param max_batch_size: Receive a batch of events. Batch size will be up to the maximum specified, but will return as soon as service returns no new events. If combined with a timeout and no events are retrieve before the time, the result will be empty. If no batch size is supplied, the prefetch size will be the maximum. :type max_batch_size: int :rtype: list[~azure.eventhub.common.EventData]
Returns a list of all the event hub partition IDs. :rtype: list[str]
Intializes the partition checkpoint and lease store and then calls run async.
Terminiates the partition manger.
Starts the run loop and manages exceptions and cleanup.
Intializes the partition checkpoint and lease store ensures that a checkpoint exists for all partitions. Note in this case checkpoint and lease stores are the same storage manager construct. :return: Returns the number of partitions. :rtype: int
Make attempt_renew_lease async call sync.
Throws if it runs out of retries. If it returns, action succeeded.
This is the main execution loop for allocating and manging pumps.
Updates the lease on an exisiting pump. :param partition_id: The partition ID. :type partition_id: str :param lease: The lease to be used. :type lease: ~azure.eventprocessorhost.lease.Lease
Create a new pump thread with a given lease. :param partition_id: The partition ID. :type partition_id: str :param lease: The lease to be used. :type lease: ~azure.eventprocessorhost.lease.Lease
Stops a single partiton pump. :param partition_id: The partition ID. :type partition_id: str :param reason: A reason for closing. :type reason: str
Stops all partition pumps (Note this might be wrong and need to await all tasks before returning done). :param reason: A reason for closing. :type reason: str :rtype: bool
Determines and return which lease to steal If the number of leases is a multiple of the number of hosts, then the desired configuration is that all hosts own the name number of leases, and the difference between the "biggest" owner and any other is 0. If the number of leases is not a multiple of the number of hosts, then the most even configurationpossible is for some hosts to have (self, leases/hosts) leases and others to have (self, (self, leases/hosts) + 1). For example, for 16 partitions distributed over five hosts, the distribution would be 4, 3, 3, 3, 3, or any of the possible reorderings. In either case, if the difference between this host and the biggest owner is 2 or more, then thesystem is not in the most evenly-distributed configuration, so steal one lease from the biggest. If there is a tie for biggest, we pick whichever appears first in the list because it doesn't really matter which "biggest" is trimmed down. Stealing one at a time prevents flapping because it reduces the difference between the biggest and this host by two at a time. If the starting difference is two or greater, then the difference cannot end up below 0. This host may become tied for biggest, but it cannot become larger than the host that it is stealing from. :param stealable_leases: List of leases to determine which can be stolen. :type stealable_leases: list[~azure.eventprocessorhost.lease.Lease] :param have_lease_count: Lease count. :type have_lease_count: int :rtype: ~azure.eventprocessorhost.lease.Lease
Returns a dictionary of leases by current owner.
Make attempt_renew_lease async call sync.
Attempts to renew a potential lease if possible and marks in the queue as none adds to adds to the queue.
Open the Sender using the supplied conneciton. If the handler has previously been redirected, the redirect context will be used to create a new handler before opening it. :param connection: The underlying client shared connection. :type: connection: ~uamqp.async_ops.connection_async.ConnectionAsync
Close down the handler. If the handler has already closed, this will be a no op. An optional exception can be passed in to indicate that the handler was shutdown due to error. :param exception: An optional exception if the handler is closing due to an error. :type exception: Exception
Wait until all transferred events have been sent.
This method opens an IP connection on the IP device :return: None
This method writes sends data to the IP device :param data: :return: None
Setup pins
Function that gets called again as soon as it finishes (forever).
Retrieve the last data update for the specified digital pin. It is intended for a polling application. :param pin: Digital pin number :returns: Last value reported for the digital pin
This command enables the rotary encoder (2 pin + ground) and will enable encoder reporting. This is a FirmataPlus feature. Encoder data is retrieved by performing a digital_read from pin a (encoder pin 1) :param pin_a: Encoder pin 1. :param pin_b: Encoder pin 2. :param cb: callback function to report encoder changes :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param hall_encoder: wheel hall_encoder - set to True to select hall encoder support support. :returns: No return value
This method retrieves the latest encoder data value. It is a FirmataPlus feature. :param pin: Encoder Pin :returns: encoder data value
Enables digital reporting. By turning reporting on for all 8 bits in the "port". This is part of Firmata's protocol specification. :param pin: Pin and all pins for this port :returns: No return value
This method will send an extended-data analog write command to the selected pin.. :param pin: 0 - 127 :param data: 0 - 0-0x4000 (14 bits) :returns: No return value
A list is returned containing the latch state for the pin, the latched value, and the time stamp [pin_num, latch_state, latched_value, time_stamp] If the the latch state is LATCH_LATCHED, the table is reset (data and timestamp set to zero) It is intended to be used for a polling application. :param pin: Pin number. :returns: [latched_state, threshold_type, threshold_value, latched_data, time_stamp]
This method requests and returns an analog map. :param cb: Optional callback reference :returns: An analog map response or None if a timeout occurs
This method retrieves the Firmata capability report :param raw: If True, it either stores or provides the callback with a report as list. If False, prints a formatted report to the console :param cb: Optional callback reference to receive a raw report :returns: capability report
This method retrieves the Firmata firmware version :param cb: Reference to a callback function :returns:If no callback is specified, the firmware version
This method retrieves a pin state report for the specified pin :param pin: Pin of interest :param cb: optional callback reference :returns: pin state report
This method retrieves the PyMata version number :returns: PyMata version number.
This method configures Arduino i2c with an optional read delay time. :param read_delay_time: firmata i2c delay time :returns: No return value
Retrieve result of last data read from i2c device. i2c_read_request should be called before trying to retrieve data. It is intended for use by a polling application. :param address: i2c :returns: last data read or None if no data is present.
This method issues an i2c read request for a single read,continuous read or a stop, specified by the read_type. Because different i2c devices return data at different rates, if a callback is not specified, the user must first call this method and then call i2c_read_data after waiting for sufficient time for the i2c device to respond. Some devices require that transmission be restarted (e.g. MMA8452Q accelerometer). Use I2C_READ | I2C_RESTART_TX for those cases. :param address: i2c device :param register: i2c register number :param number_of_bytes: number of bytes to be returned :param read_type: Constants.I2C_READ, Constants.I2C_READ_CONTINUOUSLY or Constants.I2C_STOP_READING. Constants.I2C_RESTART_TX may be OR'ed when required :param cb: optional callback reference :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value
Write data to an i2c device. :param address: i2c device address :param args: A variable number of bytes to be sent to the device passed in as a list. :returns: No return value
Periodically send a keep alive message to the Arduino. Frequency of keep alive transmission is calculated as follows: keep_alive_sent = period - (period * margin) :param period: Time period between keepalives. Range is 0-10 seconds. 0 disables the keepalive mechanism. :param margin: Safety margin to assure keepalives are sent before period expires. Range is 0.1 to 0.9 :returns: No return value
This method will call the Tone library for the selected pin. It requires FirmataPlus to be loaded onto the arduino If the tone command is set to TONE_TONE, then the specified tone will be played. Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled. :param pin: Pin number :param tone_command: Either TONE_TONE, or TONE_NO_TONE :param frequency: Frequency of tone :param duration: Duration of tone in milliseconds :returns: No return value
Send a Firmata reset command :returns: No return value
This method configures the Arduino for servo operation. :param pin: Servo control pin :param min_pulse: Minimum pulse width :param max_pulse: Maximum pulse width :returns: No return value
This method "arms" an analog pin for its data to be latched and saved in the latching table. If a callback method is provided, when latching criteria is achieved, the callback function is called with latching data notification. :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5 :param threshold_type: Constants.LATCH_GT | Constants.LATCH_LT | Constants.LATCH_GTE | Constants.LATCH_LTE :param threshold_value: numerical value - between 0 and 1023 :param cb: callback method :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: True if successful, False if parameter data is invalid
This method sets the pin mode for the specified pin. :param pin_number: Arduino Pin Number :param pin_state: INPUT/OUTPUT/ANALOG/PWM/PULLUP - for SERVO use servo_config() :param callback: Optional: A reference to a call back function to be called when pin data value changes :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value
This method sets the sampling interval for the Firmata loop method :param interval: time in milliseconds :returns: No return value
Perform an asyncio sleep for the time specified in seconds. T his method should be used in place of time.sleep() :param time: time in seconds :returns: No return value
Shutdown the application and exit :returns: No return value
Retrieve Ping (HC-SR04 type) data. The data is presented as a dictionary. The 'key' is the trigger pin specified in sonar_config() and the 'data' is the current measured distance (in centimeters) for that pin. If there is no data, the value is set to None. This is a FirmataPlus feature. :param trigger_pin: trigger pin specified in sonar_config :returns: active_sonar_map
Configure the pins,ping interval and maximum distance for an HC-SR04 type device. Single pin configuration may be used. To do so, set both the trigger and echo pins to the same value. Up to a maximum of 6 SONAR devices is supported If the maximum is exceeded a message is sent to the console and the request is ignored. NOTE: data is measured in centimeters This is FirmataPlus feature. :param trigger_pin: The pin number of for the trigger (transmitter). :param echo_pin: The pin number for the received echo. :param cb: optional callback function to report sonar data changes :param ping_interval: Minimum interval between pings. Lowest number to use is 33 ms.Max is 127 :param max_distance: Maximum distance in cm. Max is 200. :param cb_type: direct call or asyncio yield from :returns: No return value
Configure stepper motor prior to operation. This is a FirmataPlus feature. :param steps_per_revolution: number of steps per motor revolution :param stepper_pins: a list of control pin numbers - either 4 or 2 :returns: No return value
Move a stepper motor for the number of steps at the specified speed This is a FirmataPlus feature. :param motor_speed: 21 bits of data to set motor speed :param number_of_steps: 14 bits for number of steps & direction positive is forward, negative is reverse
Initialize Pixy and will enable Pixy block reporting. This is a FirmataPlusRB feature. :param cb: callback function to report Pixy blocks :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param max_blocks: Maximum number of Pixy blocks to report when many signatures are found. :returns: No return value.
Sends the setServos Pixy command. This method sets the pan/tilt servos that are plugged into Pixy's two servo ports. :param s0: value 0 to 1000 :param s1: value 0 to 1000 :returns: No return value.
Sends the setBrightness Pixy command. This method sets the brightness (exposure) of Pixy's camera. :param brightness: range between 0 and 255 with 255 being the brightest setting :returns: No return value.
Sends the setLed Pixy command. This method sets the RGB LED on front of Pixy. :param r: red range between 0 and 255 :param g: green range between 0 and 255 :param b: blue range between 0 and 255 :returns: No return value.
This is an asyncio adapted version of pyserial write. It provides a non-blocking write and returns the number of bytes written upon completion :param data: Data to be written :return: Number of bytes written
This is an asyncio adapted version of pyserial read. It provides a non-blocking read and returns a line of data read. :return: A line of data
Prints the Pixy blocks data.
Prints the Pixy blocks data.
This method checks verifies the device ID. @return: True if valid, False if not
Put the device into standby mode so that the registers can be set. @return: No return value
Set the device scale register. Device must be in standby before calling this function @param scale: scale factor @return: No return value
Set the device output data rate. Device must be in standby before calling this function @param output_data_rate: Desired data rate @return: No return value.
This method checks to see if new xyz data is available @return: Returns 0 if not available. 1 if it is available
The device returns an MSB and LSB (in that order) for each axis. These are 12 bit values - that is only the upper 4 bits of the LSB are used. To make things more confusing, firmata returns each axis as 4 bytes, and reverses the order because it looks at the world as lsb, msb order. :param callback: Callback function :returns: callback data is set with x,y,z raw (integers) followed by x,y,z corrected ( floating point) Call available() first to make sure new data is really available.
This is a utility function to wait for return data call back @return: Returns resultant data from callback
Basically the same as drive(), but omitting the right motor.
Basically the same as drive(), but omitting the left motor.
allows left motor to coast to a stop
allows left motor to coast to a stop
allows left motor to coast to a stop
allows left motor to coast to a stop
pivot() controls the pivot speed of the RedBot. The values of the pivot function inputs range from -255:255, with -255 indicating a full speed counter-clockwise rotation. 255 indicates a full speed clockwise rotation
This function reads the portrait/landscape status register of the MMA8452Q. It will return either PORTRAIT_U, PORTRAIT_D, LANDSCAPE_R, LANDSCAPE_L, or LOCKOUT. LOCKOUT indicates that the sensor is in neither p or ls. :param callback: Callback function :returns: See above.
This method sets the tap thresholds. Device must be in standby before calling this function. Set up single and double tap - 5 steps: for more info check out this app note: http://cache.freescale.com/files/sensors/doc/app_note/AN4072.pdf Set the threshold - minimum required acceleration to cause a tap. @param x_ths: x tap threshold @param y_ths: y tap threshold @param z_ths: z tap threshold @return: No return value.
This function returns any taps read by the MMA8452Q. If the function returns 0, no new taps were detected. Otherwise the function will return the lower 7 bits of the PULSE_SRC register. :param callback: Callback function :returns: 0 or lower 7 bits of the PULSE_SRC register.
This is a utility function to wait for return data call back @return: Returns resultant data from callback
Prints the Pixy blocks data.
Set digital pin 6 as a PWM output and set its output value to 128 @param my_board: A PymataCore instance @return: No Return Value
Blink LED 13 @return: No Return Value
turns RedBot to the Right
turns RedBot to the Left
This method must be called immediately after the class is instantiated. It instantiates the serial interface and then performs auto pin discovery. It is intended for use by pymata3 applications that do not use asyncio coroutines directly. :returns: No return value.
Set the selected pin to the specified value. :param pin: PWM pin number :param value: Pin value (0 - 0x4000) :returns: No return value
Set the specified pin to the specified value directly without port manipulation. :param pin: pin number :param value: pin value :returns: No return value
Set the specified pin to the specified value. :param pin: pin number :param value: pin value :returns: No return value
Disables analog reporting for a single analog pin. :param pin: Analog pin number. For example for A0, the number is 0. :returns: No return value
Disables digital reporting. By turning reporting off for this pin, Reporting is disabled for all 8 bits in the "port" :param pin: Pin and all pins for this port :returns: No return value
This command enables the rotary encoder support and will enable encoder reporting. This command is not part of StandardFirmata. For 2 pin + ground encoders, FirmataPlus is required to be used for 2 pin rotary encoder, and for hell effect wheel encoder support, FirmataPlusRB is required. Encoder data is retrieved by performing a digital_read from pin a (encoder pin_a). When using 2 hall effect sensors (e.g. 2 wheel robot) specify pin_a for 1st encoder and pin_b for 2nd encoder. :param pin_a: Encoder pin 1. :param pin_b: Encoder pin 2. :param cb: callback function to report encoder changes :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param hall_encoder: wheel hall_encoder - set to True to select hall encoder support support. :returns: No return value
Enables analog reporting. By turning reporting on for a single pin, :param pin: Analog pin number. For example for A0, the number is 0. :returns: No return value
Enables digital reporting. By turning reporting on for all 8 bits in the "port" - this is part of Firmata's protocol specification. :param pin: Pin and all pins for this port :returns: No return value
This method will send an extended-data analog write command to the selected pin. :param pin: 0 - 127 :param data: 0 - 0xfffff :returns: No return value
A list is returned containing the latch state for the pin, the latched value, and the time stamp [latched_state, threshold_type, threshold_value, latched_data, time_stamp] :param pin: Pin number. :returns: [latched_state, threshold_type, threshold_value, latched_data, time_stamp]
This method requests a Firmata analog map query and returns the results. :returns: An analog map response or None if a timeout occurs
This method requests and returns a Firmata capability query report :returns: A capability report in the form of a list
A list is returned containing the latch state for the pin, the latched value, and the time stamp [pin_num, latch_state, latched_value, time_stamp] :param pin: Pin number. :returns: [latched_state, threshold_type, threshold_value, latched_data, time_stamp]
This method retrieves the Firmata firmware version :returns: Firmata firmware version
This method returns the major and minor values for the protocol version, i.e. 2.4 :returns: Firmata protocol version
This method retrieves a pin state report for the specified pin :param pin: Pin of interest :returns: pin state report
NOTE: THIS METHOD MUST BE CALLED BEFORE ANY I2C REQUEST IS MADE This method initializes Firmata for I2c operations. :param read_delay_time (in microseconds): an optional parameter, default is 0 :returns: No Return Value
This method retrieves cached i2c data to support a polling mode. :param address: I2C device address :returns: Last cached value read
This method requests the read of an i2c device. Results are retrieved by a call to i2c_get_read_data(). or by callback. If a callback method is provided, when data is received from the device it will be sent to the callback method. Some devices require that transmission be restarted (e.g. MMA8452Q accelerometer). Use Constants.I2C_READ | Constants.I2C_END_TX_MASK for those cases. :param address: i2c device address :param register: register number (can be set to zero) :param number_of_bytes: number of bytes expected to be returned :param read_type: I2C_READ or I2C_READ_CONTINUOUSLY. I2C_END_TX_MASK may be OR'ed when required :param cb: Optional callback function to report i2c data as a result of read command :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value.
Write data to an i2c device. :param address: i2c device address :param args: A variable number of bytes to be sent to the device passed in as a list :returns: No return value.
Periodically send a keep alive message to the Arduino. Frequency of keep alive transmission is calculated as follows: keep_alive_sent = period - (period * margin) :param period: Time period between keepalives. Range is 0-10 seconds. 0 disables the keepalive mechanism. :param margin: Safety margin to assure keepalives are sent before period expires. Range is 0.1 to 0.9 :returns: No return value
This method will call the Tone library for the selected pin. It requires FirmataPlus to be loaded onto the arduino If the tone command is set to TONE_TONE, then the specified tone will be played. Else, if the tone command is TONE_NO_TONE, then any currently playing tone will be disabled. :param pin: Pin number :param tone_command: Either TONE_TONE, or TONE_NO_TONE :param frequency: Frequency of tone :param duration: Duration of tone in milliseconds :returns: No return value
Configure a pin as a servo pin. Set pulse min, max in ms. Use this method (not set_pin_mode) to configure a pin for servo operation. :param pin: Servo Pin. :param min_pulse: Min pulse width in ms. :param max_pulse: Max pulse width in ms. :returns: No return value
This method "arms" an analog pin for its data to be latched and saved in the latching table If a callback method is provided, when latching criteria is achieved, the callback function is called with latching data notification. Data returned in the callback list has the pin number as the first element, :param pin: Analog pin number (value following an 'A' designator, i.e. A5 = 5 :param threshold_type: ANALOG_LATCH_GT | ANALOG_LATCH_LT | ANALOG_LATCH_GTE | ANALOG_LATCH_LTE :param threshold_value: numerical value - between 0 and 1023 :param cb: callback method :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: True if successful, False if parameter data is invalid
This method "arms" a digital pin for its data to be latched and saved in the latching table If a callback method is provided, when latching criteria is achieved, the callback function is called with latching data notification. Data returned in the callback list has the pin number as the first element, :param pin: Digital pin number :param threshold_value: 0 or 1 :param cb: callback function :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: True if successful, False if parameter data is invalid
This method sets the pin mode for the specified pin. For Servo, use servo_config() instead. :param pin_number: Arduino Pin Number :param pin_state: INPUT/OUTPUT/ANALOG/PWM/PULLUP - for SERVO use servo_config() :param callback: Optional: A reference to a call back function to be called when pin data value changes :param callback_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value
This method sends the desired sampling interval to Firmata. Note: Standard Firmata will ignore any interval less than 10 milliseconds :param interval: Integer value for desired sampling interval in milliseconds :returns: No return value.
This method attempts an orderly shutdown If any exceptions are thrown, just ignore them. :returns: No return value
This method is a proxy method for asyncio.sleep :param sleep_time: Sleep interval in seconds :returns: No return value.
Configure the pins,ping interval and maximum distance for an HC-SR04 type device. Single pin configuration may be used. To do so, set both the trigger and echo pins to the same value. Up to a maximum of 6 SONAR devices is supported If the maximum is exceeded a message is sent to the console and the request is ignored. NOTE: data is measured in centimeters :param trigger_pin: The pin number of for the trigger (transmitter). :param echo_pin: The pin number for the received echo. :param cb: optional callback function to report sonar data changes :param ping_interval: Minimum interval between pings. Lowest number to use is 33 ms. Max is 127ms. :param max_distance: Maximum distance in cm. Max is 200. :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :returns: No return value.
Retrieve Ping (HC-SR04 type) data. The data is presented as a dictionary. The 'key' is the trigger pin specified in sonar_config() and the 'data' is the current measured distance (in centimeters) for that pin. If there is no data, the value is set to None. :param trigger_pin: key into sonar data map :returns: active_sonar_map
Configure stepper motor prior to operation. This is a FirmataPlus feature. :param steps_per_revolution: number of steps per motor revolution :param stepper_pins: a list of control pin numbers - either 4 or 2 :returns: No return value.
Move a stepper motor for the number of steps at the specified speed This is a FirmataPlus feature. :param motor_speed: 21 bits of data to set motor speed :param number_of_steps: 14 bits for number of steps & direction positive is forward, negative is reverse :returns: No return value.
Initialize Pixy and enable Pixy block reporting. This is a FirmataPlusRB feature. :param cb: callback function to report Pixy blocks :param cb_type: Constants.CB_TYPE_DIRECT = direct call or Constants.CB_TYPE_ASYNCIO = asyncio coroutine :param max_blocks: Maximum number of Pixy blocks to report when many signatures are found. :returns: No return value.
Sends the setServos Pixy command. This method sets the pan/tilt servos that are plugged into Pixy's two servo ports. :param s0: value 0 to 1000 :param s1: value 0 to 1000 :returns: No return value.
Sends the setBrightness Pixy command. This method sets the brightness (exposure) of Pixy's camera. :param brightness: range between 0 and 255 with 255 being the brightest setting :returns: No return value.
Sends the setLed Pixy command. This method sets the RGB LED on front of Pixy. :param r: red range between 0 and 255 :param g: green range between 0 and 255 :param b: blue range between 0 and 255 :returns: No return value.
This is a private method. It continually accepts and interprets data coming from Firmata,and then dispatches the correct handler to process the data. :returns: This method never returns
This is a private message handler method. It is a message handler for analog messages. :param data: message data :returns: None - but saves the data in the pins structure
This is a private message handler method. It is a message handler for Digital Messages. :param data: digital message :returns: None - but update is saved in pins structure
This is a private message handler method. It handles encoder data messages. :param data: encoder data :returns: None - but update is saved in the digital pins structure
This is a private message handler method. It handles pixy data messages. :param data: pixy data :returns: None - but update is saved in the digital pins structure
This is a private message handler method. It handles replies to i2c_read requests. It stores the data for each i2c device address in a dictionary called i2c_map. The data may be retrieved via a polling call to i2c_get_read_data(). It a callback was specified in pymata.i2c_read, the raw data is sent through the callback :param data: raw data returned from i2c device
This is a private message handler method. This method handles the sysex 'report firmware' command sent by Firmata (0x79). It assembles the firmware version by concatenating the major and minor version number components and the firmware identifier into a string. e.g. "2.3 StandardFirmata.ino" :param sysex_data: Sysex data sent from Firmata :returns: None
This is a private message handler method. This method reads the following 2 bytes after the report version command (0xF9 - non sysex). The first byte is the major number and the second byte is the minor number. :returns: None
This method handles the incoming sonar data message and stores the data in the response table. :param data: Message data from Firmata :returns: No return value.
This is a private message handler method. It is the message handler for String data messages that will be printed to the console. :param data: message :returns: None - message is sent to console
This is a private utility method. When a data change message is received this method checks to see if latching needs to be processed :param key: encoded pin number :param data: data change :returns: None
This is a private utility method. This method attempts to discover the com port that the arduino is connected to. :returns: Detected Comport
This is a private utility method. This method formats a capability report if the user wishes to send it to the console. If log_output = True, no output is generated :param data: Capability report :returns: None
This is a private utility method. This method process latching events and either returns them via callback or stores them in the latch map :param key: Encoded pin :param latching_entry: a latch table entry :returns: Callback or store data in latch map
This is a private utility method. The method sends a non-sysex command to Firmata. :param command: command data :returns: length of data sent
This is a private utility method. This method sends a sysex command to Firmata. :param sysex_command: sysex command :param sysex_data: data for command :returns : No return value.
This is a private utility method. This method accumulates the requested number of bytes and then returns the full command :param current_command: command id :param number_of_bytes: how many bytes to wait for :returns: command
This method writes a value to an analog pin. It is used to set the output of a PWM pin or the angle of a Servo. :param command: {"method": "analog_write", "params": [PIN, WRITE_VALUE]} :returns: No return message.
This method reads and returns the last reported value for a digital pin. Normally not used since digital pin updates will be provided automatically as they occur with the digital_message_reply being sent to the client after set_pin_mode is called.. (see enable_digital_reporting for message format) :param command: {"method": "digital_read", "params": [PIN]} :returns: {"method": "digital_read_reply", "params": [PIN, DIGITAL_DATA_VALUE]}
This method writes a zero or one to a digital pin. :param command: {"method": "digital_write", "params": [PIN, DIGITAL_DATA_VALUE]} :returns: No return message..
Disable Firmata reporting for an analog pin. :param command: {"method": "disable_analog_reporting", "params": [PIN]} :returns: No return message..
Disable Firmata reporting for a digital pin. :param command: {"method": "disable_digital_reporting", "params": [PIN]} :returns: No return message.
Enable Firmata reporting for an analog pin. :param command: {"method": "enable_analog_reporting", "params": [PIN]} :returns: {"method": "analog_message_reply", "params": [PIN, ANALOG_DATA_VALUE]}
Enable Firmata reporting for a digital pin. :param command: {"method": "enable_digital_reporting", "params": [PIN]} :returns: {"method": "digital_message_reply", "params": [PIN, DIGITAL_DATA_VALUE]}
Configure 2 pins for FirmataPlus encoder operation. :param command: {"method": "encoder_config", "params": [PIN_A, PIN_B]} :returns: {"method": "encoder_data_reply", "params": [ENCODER_DATA]}
This is a polling method to read the last cached FirmataPlus encoder value. Normally not used. See encoder config for the asynchronous report message format. :param command: {"method": "encoder_read", "params": [PIN_A]} :returns: {"method": "encoder_read_reply", "params": [PIN_A, ENCODER_VALUE]}
This method retrieves the Firmata capability report. Refer to http://firmata.org/wiki/Protocol#Capability_Query The command format is: {"method":"get_capability_report","params":["null"]} :returns: {"method": "capability_report_reply", "params": [RAW_CAPABILITY_REPORT]}
This method retrieves a Firmata pin_state report for a pin.. See: http://firmata.org/wiki/Protocol#Pin_State_Query :param command: {"method": "get_pin_state", "params": [PIN]} :returns: {"method": "get_pin_state_reply", "params": [PIN_NUMBER, PIN_MODE, PIN_STATE]}
This method retrieves the Firmata protocol version. JSON command: {"method": "get_protocol_version", "params": ["null"]} :returns: {"method": "protocol_version_reply", "params": [PROTOCOL_VERSION]}
This method initializes the I2c and sets the optional read delay (in microseconds). It must be called before doing any other i2c operations for a given device. :param command: {"method": "i2c_config", "params": [DELAY]} :returns: No Return message.
This method retrieves the last value read for an i2c device identified by address. This is a polling implementation and i2c_read_request and i2c_read_request_reply may be a better alternative. :param command: {"method": "i2c_read_data", "params": [I2C_ADDRESS ]} :returns:{"method": "i2c_read_data_reply", "params": i2c_data}
This method sends an I2C read request to Firmata. It is qualified by a single shot, continuous read, or stop reading command. Special Note: for the read type supply one of the following string values: "0" = I2C_READ "1" = I2C_READ | I2C_END_TX_MASK" "2" = I2C_READ_CONTINUOUSLY "3" = I2C_READ_CONTINUOUSLY | I2C_END_TX_MASK "4" = I2C_STOP_READING :param command: {"method": "i2c_read_request", "params": [I2C_ADDRESS, I2C_REGISTER, NUMBER_OF_BYTES, I2C_READ_TYPE ]} :returns: {"method": "i2c_read_request_reply", "params": [DATA]}
This method performs an I2C write at a given I2C address, :param command: {"method": "i2c_write_request", "params": [I2C_DEVICE_ADDRESS, [DATA_TO_WRITE]]} :returns:No return message.
Periodically send a keep alive message to the Arduino. Frequency of keep alive transmission is calculated as follows: keep_alive_sent = period - (period * margin) :param command: {"method": "keep_alive", "params": [PERIOD, MARGIN]} Period is time period between keepalives. Range is 0-10 seconds. 0 disables the keepalive mechanism. Margin is a safety margin to assure keepalives are sent before period expires. Range is 0.1 to 0.9 :returns: No return value
This method controls a piezo device to play a tone. It is a FirmataPlus feature. Tone command is TONE_TONE to play, TONE_NO_TONE to stop playing. :param command: {"method": "play_tone", "params": [PIN, TONE_COMMAND, FREQUENCY(Hz), DURATION(MS)]} :returns:No return message.
This method sets the an analog latch for a given analog pin, providing the threshold type, and latching threshold. :param command: {"method": "set_analog_latch", "params": [PIN, THRESHOLD_TYPE, THRESHOLD_VALUE]} :returns:{"method": "analog_latch_data_reply", "params": [PIN, DATA_VALUE_LATCHED, TIMESTAMP_STRING]}
This method sets the a digital latch for a given digital pin, the threshold type, and latching threshold. :param command:{"method": "set_digital_latch", "params": [PIN, THRESHOLD (0 or 1)]} :returns:{"method": digital_latch_data_reply", "params": [PIN, DATA_VALUE_LATCHED, TIMESTAMP_STRING]}
This method sets the pin mode for the selected pin. It handles: Input, Analog(Input) PWM, and OUTPUT. Servo is handled by servo_config(). :param command: {"method": "set_pin_mode", "params": [PIN, MODE]} :returns:No return message.
This method sets the Firmata sampling interval in ms. :param command:{"method": "set_sampling_interval", "params": [INTERVAL]} :returns:No return message.
This method configures 2 pins to support HC-SR04 Ping devices. This is a FirmataPlus feature. :param command: {"method": "sonar_config", "params": [TRIGGER_PIN, ECHO_PIN, PING_INTERVAL(default=50), MAX_DISTANCE(default= 200 cm]} :returns:{"method": "sonar_data_reply", "params": [DISTANCE_IN_CM]}
This method retrieves the last sonar data value that was cached. This is a polling method. After sonar config, sonar_data_reply messages will be sent automatically. :param command: {"method": "sonar_read", "params": [TRIGGER_PIN]} :returns:{"method": "sonar_read_reply", "params": [TRIGGER_PIN, DATA_VALUE]}
This method configures a pin for servo operation. The servo angle is set by using analog_write(). :param command: {"method": "servo_config", "params": [PIN, MINIMUM_PULSE(ms), MAXIMUM_PULSE(ms)]} :returns:No message returned.
This method configures 4 pins for stepper motor operation. This is a FirmataPlus feature. :param command: {"method": "stepper_config", "params": [STEPS_PER_REVOLUTION, [PIN1, PIN2, PIN3, PIN4]]} :returns:No message returned.
This method activates a stepper motor motion. This is a FirmataPlus feature. :param command: {"method": "stepper_step", "params": [SPEED, NUMBER_OF_STEPS]} :returns:No message returned.
This method handles the analog message received from pymata_core :param data: analog callback message :returns:{"method": "analog_message_reply", "params": [PIN, DATA_VALUE}
This method handles analog_latch data received from pymata_core :param data: analog latch callback message :returns:{"method": "analog_latch_data_reply", "params": [ANALOG_PIN, VALUE_AT_TRIGGER, TIME_STAMP_STRING]}
Prints the Pixy blocks data.
Helper method to shutdown the RedBot if Ctrl-c is pressed
Set digital pin 6 as a PWM output and set its output value to 128 @return:
Exchange a code (and 'state' token) for a bearer token
Returns the complete tag record for a single tag. Parameters ---------- tag : {Id} The tag to get. [params] : {Object} Parameters for the request
Updates the properties of a tag. Only the fields provided in the `data` block will be updated; any unspecified fields will remain unchanged. When using this method, it is best to specify only those fields you wish to change, or else you may overwrite changes made by another user since you last retrieved the task. Returns the complete updated tag record. Parameters ---------- tag : {Id} The tag to update. [data] : {Object} Data for the request
A specific, existing tag can be deleted by making a DELETE request on the URL for that tag. Returns an empty data record. Parameters ---------- tag : {Id} The tag to delete.
Returns the compact tag records for all tags in the workspace. Parameters ---------- workspace : {Id} The workspace or organization to find tags in. [params] : {Object} Parameters for the request
Returns the compact task records for all tasks with the given tag. Tasks can have more than one tag at a time. Parameters ---------- tag : {Id} The tag to fetch tasks from. [params] : {Object} Parameters for the request
Returns details of a previously-requested Organization export. Parameters ---------- organization_export : {Id} Globally unique identifier for the Organization export. [params] : {Object} Parameters for the request
Returns the complete definition of a custom field's metadata. Parameters ---------- custom_field : {Id} Globally unique identifier for the custom field. [params] : {Object} Parameters for the request
A specific, existing custom field can be updated by making a PUT request on the URL for that custom field. Only the fields provided in the `data` block will be updated; any unspecified fields will remain unchanged When using this method, it is best to specify only those fields you wish to change, or else you may overwrite changes made by another user since you last retrieved the custom field. A custom field's `type` cannot be updated. An enum custom field's `enum_options` cannot be updated with this endpoint. Instead see "Work With Enum Options" for information on how to update `enum_options`. Returns the complete updated custom field record. Parameters ---------- custom_field : {Id} Globally unique identifier for the custom field. [data] : {Object} Data for the request
A specific, existing custom field can be deleted by making a DELETE request on the URL for that custom field. Returns an empty data record. Parameters ---------- custom_field : {Id} Globally unique identifier for the custom field.
Creates an enum option and adds it to this custom field's list of enum options. A custom field can have at most 50 enum options (including disabled options). By default new enum options are inserted at the end of a custom field's list. Returns the full record of the newly created enum option. Parameters ---------- custom_field : {Id} Globally unique identifier for the custom field. [data] : {Object} Data for the request - name : {String} The name of the enum option. - [color] : {String} The color of the enum option. Defaults to 'none'. - [insert_before] : {Id} An existing enum option within this custom field before which the new enum option should be inserted. Cannot be provided together with after_enum_option. - [insert_after] : {Id} An existing enum option within this custom field after which the new enum option should be inserted. Cannot be provided together with before_enum_option.
Updates an existing enum option. Enum custom fields require at least one enabled enum option. Returns the full record of the updated enum option. Parameters ---------- enum_option : {Id} Globally unique identifier for the enum option. [data] : {Object} Data for the request - name : {String} The name of the enum option. - [color] : {String} The color of the enum option. Defaults to 'none'. - [enabled] : {Boolean} Whether or not the enum option is a selectable value for the custom field.
Moves a particular enum option to be either before or after another specified enum option in the custom field. Parameters ---------- custom_field : {Id} Globally unique identifier for the custom field. [data] : {Object} Data for the request - enum_option : {Id} The ID of the enum option to relocate. - name : {String} The name of the enum option. - [color] : {String} The color of the enum option. Defaults to 'none'. - [before_enum_option] : {Id} An existing enum option within this custom field before which the new enum option should be inserted. Cannot be provided together with after_enum_option. - [after_enum_option] : {Id} An existing enum option within this custom field after which the new enum option should be inserted. Cannot be provided together with before_enum_option.
Returns the compact project membership records for the project. Parameters ---------- project : {Id} The project for which to fetch memberships. [params] : {Object} Parameters for the request - [user] : {String} If present, the user to filter the memberships to.
Returns the project membership record. Parameters ---------- project_membership : {Id} Globally unique identifier for the project membership. [params] : {Object} Parameters for the request
Returns the full workspace record for a single workspace. Parameters ---------- workspace : {Id} Globally unique identifier for the workspace or organization. [params] : {Object} Parameters for the request
A specific, existing workspace can be updated by making a PUT request on the URL for that workspace. Only the fields provided in the data block will be updated; any unspecified fields will remain unchanged. Currently the only field that can be modified for a workspace is its `name`. Returns the complete, updated workspace record. Parameters ---------- workspace : {Id} The workspace to update. [data] : {Object} Data for the request
Retrieves objects in the workspace based on an auto-completion/typeahead search algorithm. This feature is meant to provide results quickly, so do not rely on this API to provide extremely accurate search results. The result set is limited to a single page of results with a maximum size, so you won't be able to fetch large numbers of results. Parameters ---------- workspace : {Id} The workspace to fetch objects from. [params] : {Object} Parameters for the request - type : {Enum} The type of values the typeahead should return. You can choose from one of the following: custom_field, project, tag, task, and user. Note that unlike in the names of endpoints, the types listed here are in singular form (e.g. `task`). Using multiple types is not yet supported. - [query] : {String} The string that will be used to search for relevant objects. If an empty string is passed in, the API will currently return an empty result set. - [count] : {Number} The number of results to return. The default is `20` if this parameter is omitted, with a minimum of `1` and a maximum of `100`. If there are fewer results found than requested, all will be returned.
The user can be referenced by their globally unique user ID or their email address. Returns the full user record for the invited user. Parameters ---------- workspace : {Id} The workspace or organization to invite the user to. [data] : {Object} Data for the request - user : {String} An identifier for the user. Can be one of an email address, the globally unique identifier for the user, or the keyword `me` to indicate the current user making the request.
The user making this call must be an admin in the workspace. Returns an empty data record. Parameters ---------- workspace : {Id} The workspace or organization to invite the user to. [data] : {Object} Data for the request - user : {String} An identifier for the user. Can be one of an email address, the globally unique identifier for the user, or the keyword `me` to indicate the current user making the request.
Returns the full record for a single attachment. Parameters ---------- attachment : {Id} Globally unique identifier for the attachment. [params] : {Object} Parameters for the request
Returns the compact records for all attachments on the task. Parameters ---------- task : {Id} Globally unique identifier for the task. [params] : {Object} Parameters for the request
Upload an attachment for a task. Accepts a file object or string, file name, and optional file Content-Type
Returns the full record for a single team. Parameters ---------- team : {Id} Globally unique identifier for the team. [params] : {Object} Parameters for the request
Returns the compact records for all teams in the organization visible to the authorized user. Parameters ---------- organization : {Id} Globally unique identifier for the workspace or organization. [params] : {Object} Parameters for the request
Returns the compact records for all teams to which user is assigned. Parameters ---------- user : {String} An identifier for the user. Can be one of an email address, the globally unique identifier for the user, or the keyword `me` to indicate the current user making the request. [params] : {Object} Parameters for the request - [organization] : {Id} The workspace or organization to filter teams on.
Returns the compact records for all users that are members of the team. Parameters ---------- team : {Id} Globally unique identifier for the team. [params] : {Object} Parameters for the request
The user making this call must be a member of the team in order to add others. The user to add must exist in the same organization as the team in order to be added. The user to add can be referenced by their globally unique user ID or their email address. Returns the full user record for the added user. Parameters ---------- team : {Id} Globally unique identifier for the team. [data] : {Object} Data for the request - user : {String} An identifier for the user. Can be one of an email address, the globally unique identifier for the user, or the keyword `me` to indicate the current user making the request.
The user to remove can be referenced by their globally unique user ID or their email address. Removes the user from the specified team. Returns an empty data record. Parameters ---------- team : {Id} Globally unique identifier for the team. [data] : {Object} Data for the request - user : {String} An identifier for the user. Can be one of an email address, the globally unique identifier for the user, or the keyword `me` to indicate the current user making the request.
Changes the parent of a task. Each task may only be a subtask of a single parent, or no parent task at all. Returns an empty data block. Parameters ---------- task : {Id} Globally unique identifier for the task. [data] : {Object} Data for the request - parent : {Id} The new parent of the task, or `null` for no parent.
Creates a new section in a project. Returns the full record of the newly created section. Parameters ---------- project : {Id} The project to create the section in [data] : {Object} Data for the request - name : {String} The text to be displayed as the section name. This cannot be an empty string.
Returns the compact records for all sections in the specified project. Parameters ---------- project : {Id} The project to get sections from. [params] : {Object} Parameters for the request
Returns the complete record for a single section. Parameters ---------- section : {Id} The section to get. [params] : {Object} Parameters for the request
A specific, existing section can be updated by making a PUT request on the URL for that project. Only the fields provided in the `data` block will be updated; any unspecified fields will remain unchanged. (note that at this time, the only field that can be updated is the `name` field.) When using this method, it is best to specify only those fields you wish to change, or else you may overwrite changes made by another user since you last retrieved the task. Returns the complete updated section record. Parameters ---------- section : {Id} The section to update. [data] : {Object} Data for the request
A specific, existing section can be deleted by making a DELETE request on the URL for that section. Note that sections must be empty to be deleted. The last remaining section in a board view cannot be deleted. Returns an empty data block. Parameters ---------- section : {Id} The section to delete.
Move sections relative to each other in a board view. One of `before_section` or `after_section` is required. Sections cannot be moved between projects. At this point in time, moving sections is not supported in list views, only board views. Returns an empty data block. Parameters ---------- project : {Id} The project in which to reorder the given section [data] : {Object} Data for the request - section : {Id} The section to reorder - [before_section] : {Id} Insert the given section immediately before the section specified by this parameter. - [after_section] : {Id} Insert the given section immediately after the section specified by this parameter.
Returns the full record for a single story. Parameters ---------- story : {Id} Globally unique identifier for the story. [params] : {Object} Parameters for the request
Adds a comment to a task. The comment will be authored by the currently authenticated user, and timestamped when the server receives the request. Returns the full record for the new story added to the task. Parameters ---------- task : {Id} Globally unique identifier for the task. [data] : {Object} Data for the request - text : {String} The plain text of the comment to add.
Updates the story and returns the full record for the updated story. Only comment stories can have their text updated, and only comment stories and attachment stories can be pinned. Only one of `text` and `html_text` can be specified. Parameters ---------- story : {Id} Globally unique identifier for the story. [data] : {Object} Data for the request - [text] : {String} The plain text with which to update the comment. - [html_text] : {String} The rich text with which to update the comment. - [is_pinned] : {Boolean} Whether the story should be pinned on the resource.
Deletes a story. A user can only delete stories they have created. Returns an empty data record. Parameters ---------- story : {Id} Globally unique identifier for the story.
Creating a new task is as easy as POSTing to the `/tasks` endpoint with a data block containing the fields you'd like to set on the task. Any unspecified fields will take on default values. Every task is required to be created in a specific workspace, and this workspace cannot be changed once set. The workspace need not be set explicitly if you specify a `project` or a `parent` task instead. Parameters ---------- workspace : {Id} The workspace to create a task in. [data] : {Object} Data for the request
Returns the complete task record for a single task. Parameters ---------- task : {Id} The task to get. [params] : {Object} Parameters for the request
A specific, existing task can be updated by making a PUT request on the URL for that task. Only the fields provided in the `data` block will be updated; any unspecified fields will remain unchanged. When using this method, it is best to specify only those fields you wish to change, or else you may overwrite changes made by another user since you last retrieved the task. Returns the complete updated task record. Parameters ---------- task : {Id} The task to update. [data] : {Object} Data for the request
A specific, existing task can be deleted by making a DELETE request on the URL for that task. Deleted tasks go into the "trash" of the user making the delete request. Tasks can be recovered from the trash within a period of 30 days; afterward they are completely removed from the system. Returns an empty data record. Parameters ---------- task : {Id} The task to delete.
Returns the compact task records for all tasks within the given project, ordered by their priority within the project. Parameters ---------- projectId : {Id} The project in which to search for tasks. [params] : {Object} Parameters for the request
Returns the compact task records for all tasks with the given tag. Parameters ---------- tag : {Id} The tag in which to search for tasks. [params] : {Object} Parameters for the request
<b>Board view only:</b> Returns the compact section records for all tasks within the given section. Parameters ---------- section : {Id} The section in which to search for tasks. [params] : {Object} Parameters for the request
The search endpoint allows you to build complex queries to find and fetch exactly the data you need from Asana. For a more comprehensive description of all the query parameters and limitations of this endpoint, see our [long-form documentation](/developers/documentation/getting-started/search-api) for this feature. Parameters ---------- workspace : {Id} The workspace or organization in which to search for tasks. [params] : {Object} Parameters for the request
Returns the compact representations of all of the dependencies of a task. Parameters ---------- task : {Id} The task to get dependencies on. [params] : {Object} Parameters for the request
Returns the compact representations of all of the dependents of a task. Parameters ---------- task : {Id} The task to get dependents on. [params] : {Object} Parameters for the request
Marks a set of tasks as dependencies of this task, if they are not already dependencies. *A task can have at most 15 dependencies.* Parameters ---------- task : {Id} The task to add dependencies to. [data] : {Object} Data for the request - dependencies : {Array} An array of task IDs that this task should depend on.
Marks a set of tasks as dependents of this task, if they are not already dependents. *A task can have at most 30 dependents.* Parameters ---------- task : {Id} The task to add dependents to. [data] : {Object} Data for the request - dependents : {Array} An array of task IDs that should depend on this task.
Unlinks a set of dependencies from this task. Parameters ---------- task : {Id} The task to remove dependencies from. [data] : {Object} Data for the request - dependencies : {Array} An array of task IDs to remove as dependencies.
Unlinks a set of dependents from this task. Parameters ---------- task : {Id} The task to remove dependents from. [data] : {Object} Data for the request - dependents : {Array} An array of task IDs to remove as dependents.
Adds each of the specified followers to the task, if they are not already following. Returns the complete, updated record for the affected task. Parameters ---------- task : {Id} The task to add followers to. [data] : {Object} Data for the request - followers : {Array} An array of followers to add to the task.
Removes each of the specified followers from the task if they are following. Returns the complete, updated record for the affected task. Parameters ---------- task : {Id} The task to remove followers from. [data] : {Object} Data for the request - followers : {Array} An array of followers to remove from the task.
Returns a compact representation of all of the projects the task is in. Parameters ---------- task : {Id} The task to get projects on. [params] : {Object} Parameters for the request
Adds the task to the specified project, in the optional location specified. If no location arguments are given, the task will be added to the end of the project. `addProject` can also be used to reorder a task within a project or section that already contains it. At most one of `insert_before`, `insert_after`, or `section` should be specified. Inserting into a section in an non-order-dependent way can be done by specifying `section`, otherwise, to insert within a section in a particular place, specify `insert_before` or `insert_after` and a task within the section to anchor the position of this task. Returns an empty data block. Parameters ---------- task : {Id} The task to add to a project. [data] : {Object} Data for the request - project : {Id} The project to add the task to. - [insert_after] : {Id} A task in the project to insert the task after, or `null` to insert at the beginning of the list. - [insert_before] : {Id} A task in the project to insert the task before, or `null` to insert at the end of the list. - [section] : {Id} A section in the project to insert the task into. The task will be inserted at the bottom of the section.
Removes the task from the specified project. The task will still exist in the system, but it will not be in the project anymore. Returns an empty data block. Parameters ---------- task : {Id} The task to remove from a project. [data] : {Object} Data for the request - project : {Id} The project to remove the task from.
Returns a compact representation of all of the tags the task has. Parameters ---------- task : {Id} The task to get tags on. [params] : {Object} Parameters for the request
Adds a tag to a task. Returns an empty data block. Parameters ---------- task : {Id} The task to add a tag to. [data] : {Object} Data for the request - tag : {Id} The tag to add to the task.
Removes a tag from the task. Returns an empty data block. Parameters ---------- task : {Id} The task to remove a tag from. [data] : {Object} Data for the request - tag : {Id} The tag to remove from the task.
Returns a compact representation of all of the subtasks of a task. Parameters ---------- task : {Id} The task to get the subtasks of. [params] : {Object} Parameters for the request
Creates a new subtask and adds it to the parent task. Returns the full record for the newly created subtask. Parameters ---------- task : {Id} The task to add a subtask to. [data] : {Object} Data for the request
Returns a compact representation of all of the stories on the task. Parameters ---------- task : {Id} The task containing the stories to get. [params] : {Object} Parameters for the request
Adds a comment to a task. The comment will be authored by the currently authenticated user, and timestamped when the server receives the request. Returns the full record for the new story added to the task. Parameters ---------- task : {Id} Globally unique identifier for the task. [data] : {Object} Data for the request - text : {String} The plain text of the comment to add.
Returns the complete record for a single status update. Parameters ---------- project-status : {Id} The project status update to get. [params] : {Object} Parameters for the request
Deletes a specific, existing project status update. Returns an empty data record. Parameters ---------- project-status : {Id} The project status update to delete.
Dispatches a request to the Asana HTTP API
Sleep based on the type of :class:`RetryableAsanaError`
Parses GET request options and dispatches a request.
Get a collection from a collection endpoint. Parses GET request options for a collection endpoint and dispatches a request.
Parses PUT request options and dispatches a request.
Select all unknown options. Select all unknown options (not query string, API, or request options)
Select API options out of the provided options object. Selects API string options out of the provided options object and formats for either request body (default) or query string.
Select request options out of the provided options object. Select and formats options to be passed to the 'requests' library's request methods.
Select the provided keys out of an options object. Selects the provided keys (or everything except the provided keys) out of an options object.
Generate the client version header to send on each request.
Returns the full record for the given webhook. Parameters ---------- webhook : {Id} The webhook to get. [params] : {Object} Parameters for the request
This method permanently removes a webhook. Note that it may be possible to receive a request that was already in flight after deleting the webhook, but no further requests will be issued. Parameters ---------- webhook : {Id} The webhook to delete.
Dispatches a GET request to /events of the API to get a set of recent changes to a resource.
Returns a tuple containing the next page of events and a sync token for the given query (and optional 'sync' token)
Returns an event iterator for the given query (and optional 'sync' token)
Creates a project shared with the given team. Returns the full record of the newly created project. Parameters ---------- team : {Id} The team to create the project in. [data] : {Object} Data for the request
Returns the complete project record for a single project. Parameters ---------- project : {Id} The project to get. [params] : {Object} Parameters for the request
A specific, existing project can be updated by making a PUT request on the URL for that project. Only the fields provided in the `data` block will be updated; any unspecified fields will remain unchanged. When using this method, it is best to specify only those fields you wish to change, or else you may overwrite changes made by another user since you last retrieved the task. Returns the complete updated project record. Parameters ---------- project : {Id} The project to update. [data] : {Object} Data for the request
A specific, existing project can be deleted by making a DELETE request on the URL for that project. Returns an empty data record. Parameters ---------- project : {Id} The project to delete.
Returns the compact project records for all projects in the team. Parameters ---------- team : {Id} The team to find projects in. [params] : {Object} Parameters for the request - [archived] : {Boolean} Only return projects whose `archived` field takes on the value of this parameter.
Returns the compact task records for all tasks within the given project, ordered by their priority within the project. Tasks can exist in more than one project at a time. Parameters ---------- project : {Id} The project in which to search for tasks. [params] : {Object} Parameters for the request
Adds the specified list of users as followers to the project. Followers are a subset of members, therefore if the users are not already members of the project they will also become members as a result of this operation. Returns the updated project record. Parameters ---------- project : {Id} The project to add followers to. [data] : {Object} Data for the request - followers : {Array} An array of followers to add to the project.
Removes the specified list of users from following the project, this will not affect project membership status. Returns the updated project record. Parameters ---------- project : {Id} The project to remove followers from. [data] : {Object} Data for the request - followers : {Array} An array of followers to remove from the project.
Adds the specified list of users as members of the project. Returns the updated project record. Parameters ---------- project : {Id} The project to add members to. [data] : {Object} Data for the request - members : {Array} An array of members to add to the project.
Removes the specified list of members from the project. Returns the updated project record. Parameters ---------- project : {Id} The project to remove members from. [data] : {Object} Data for the request - members : {Array} An array of members to remove from the project.
Create a new custom field setting on the project. Parameters ---------- project : {Id} The project to associate the custom field with [data] : {Object} Data for the request - custom_field : {Id} The id of the custom field to associate with this project. - [is_important] : {Boolean} Whether this field should be considered important to this project. - [insert_before] : {Id} An id of a Custom Field Settings on this project, before which the new Custom Field Settings will be added. `insert_before` and `insert_after` parameters cannot both be specified. - [insert_after] : {Id} An id of a Custom Field Settings on this project, after which the new Custom Field Settings will be added. `insert_before` and `insert_after` parameters cannot both be specified.
Remove a custom field setting on the project. Parameters ---------- project : {Id} The project to associate the custom field with [data] : {Object} Data for the request - [custom_field] : {Id} The id of the custom field to remove from this project.
Returns the full user record for the single user with the provided ID. Parameters ---------- user : {String} An identifier for the user. Can be one of an email address, the globally unique identifier for the user, or the keyword `me` to indicate the current user making the request. [params] : {Object} Parameters for the request
Returns the shipping cost for a given country If the shipping cost for the given country has not been set, it will fallback to the default shipping cost if it has been enabled in the app settings
Get all shipping countries
Get the shipping options for a given country
Return the shipping cost for a given country code and shipping option (shipping rate name)
Add an item to the basket
Put multiple items in the basket, removing anything that already exists
Remove an item from the basket
Get total number of items in the basket
Get quantity of a single item in the basket
Compile the front end assets
Table display of each request for a given product. Allows the given Page pk to refer to a direct parent of the ProductVariant model or be the ProductVariant model itself. This allows for the standard longclaw product modelling philosophy where ProductVariant refers to the actual product (in the case where there is only 1 variant) or to be variants of the product page.
Refund the order specified by the pk
Mark the order specified by pk as fulfilled
Renders a 'requests' button on the page index showing the number of times the product has been requested. Attempts to only show such a button for valid product/variant pages
Template tag which provides a `script` tag for each javascript item required by the payment gateway
Get all items in the basket
Delete all items in the basket
Return the shipping rate for a country & shipping option name.
Calculate the price range of the products variants
Create a new django project using the longclaw template
Build the longclaw assets
Setup the parser and call the command function
Get all sales for a given time period
Generic function for creating a payment token from the payment backend. Some payment backends (e.g. braintree) support creating a payment token, which should be imported from the backend as 'get_token'
Create an order using an existing transaction ID. This is useful for capturing the payment outside of longclaw - e.g. using paypals' express checkout or similar
Capture the payment for a basket and create an order request.data should contain: 'address': Dict with the following fields: shipping_name shipping_address_line1 shipping_address_city shipping_address_zip shipping_address_country billing_name billing_address_line1 billing_address_city billing_address_zip billing_address_country 'email': Email address of the customer 'shipping': The shipping rate (in the sites' currency)
Create an order from a basket and customer infomation
Create a new product request
Get all the requests for a single variant
Dummy function for creating a payment through a payment gateway. Should be overridden in gateway implementations. Can be used for testing - to simulate a failed payment/error, pass `error: true` in the request data.
Check user has cookies enabled
Instantiates a class-based view to provide 'inspect' functionality for the assigned model. The view class used can be overridden by changing the 'inspect_view_class' attribute.
Utilised by Wagtail's 'register_admin_urls' hook to register urls for our the views that class offers.
Create a stripe token for a card
Total cost of the order
Issue a full refund for this order
Cancel this order, optionally refunding it
Sets the attribute_name of this CatalogQueryRange. The name of the attribute to be searched. :param attribute_name: The attribute_name of this CatalogQueryRange. :type: str
Sets the idempotency_key of this BatchUpsertCatalogObjectsRequest. A value you specify that uniquely identifies this request among all your requests. A common way to create a valid idempotency key is to use a Universally unique identifier (UUID). If you're unsure whether a particular request was successful, you can reattempt it with the same idempotency key without worrying about creating duplicate objects. See [Idempotency](/basics/api101/idempotency) for more information. :param idempotency_key: The idempotency_key of this BatchUpsertCatalogObjectsRequest. :type: str
Sets the start_of_day_local_time of this WorkweekConfig. The local time at which a business week cuts over. Represented as a string in `HH:MM` format (`HH:MM:SS` is also accepted, but seconds are truncated). :param start_of_day_local_time: The start_of_day_local_time of this WorkweekConfig. :type: str
:param method: http request method :param url: http request url :param query_params: query parameters in the url :param headers: http request headers :param body: request json body, for `application/json` :param post_params: request post parameters, `application/x-www-form-urlencode` and `multipart/form-data`
Sets the catalog_object_id of this CreateOrderRequestModifier. The catalog object ID of a [CatalogModifier](#type-catalogmodifier). :param catalog_object_id: The catalog_object_id of this CreateOrderRequestModifier. :type: str
Sets the domain_name of this RegisterDomainRequest. A domain name as described in RFC-1034 that will be registered with ApplePay :param domain_name: The domain_name of this RegisterDomainRequest. :type: str
Sets the attribute_prefix of this CatalogQueryPrefix. The desired prefix of the search attribute value. :param attribute_prefix: The attribute_prefix of this CatalogQueryPrefix. :type: str
Sets the id of this Shift. UUID for this object :param id: The id of this Shift. :type: str
Sets the employee_id of this Shift. The ID of the employee this shift belongs to. :param employee_id: The employee_id of this Shift. :type: str
Sets the start_at of this Shift. RFC 3339; shifted to location timezone + offset. Precision up to the minute is respected; seconds are truncated. :param start_at: The start_at of this Shift. :type: str
Sets the percentage of this OrderLineItemTax. The percentage of the tax, as a string representation of a decimal number. A value of `7.25` corresponds to a percentage of 7.25%. :param percentage: The percentage of this OrderLineItemTax. :type: str
Sets the modifier_list_id of this CatalogItemModifierListInfo. The ID of the [CatalogModifierList](#type-catalogmodifierlist) controlled by this [CatalogModifierListInfo](#type-catalogmodifierlistinfo). :param modifier_list_id: The modifier_list_id of this CatalogItemModifierListInfo. :type: str
Sets the location_id of this Order. The ID of the merchant location this order is associated with. :param location_id: The location_id of this Order. :type: str
Sets the reference_id of this Order. A client specified identifier to associate an entity in another system with this order. :param reference_id: The reference_id of this Order. :type: str
Sets the note of this OrderFulfillmentPickupDetails. A general note about the pickup fulfillment. Notes are useful for providing additional instructions and are displayed in Square apps. :param note: The note of this OrderFulfillmentPickupDetails. :type: str
Sets the cancel_reason of this OrderFulfillmentPickupDetails. A description of why the pickup was canceled. Max length is 100 characters. :param cancel_reason: The cancel_reason of this OrderFulfillmentPickupDetails. :type: str
Sets the modifier_id of this CatalogModifierOverride. The ID of the [CatalogModifier](#type-catalogmodifier) whose default behavior is being overridden. :param modifier_id: The modifier_id of this CatalogModifierOverride. :type: str
Sets the merchant_support_email of this CreateCheckoutRequest. The email address to display on the Square Checkout confirmation page and confirmation email that the buyer can use to contact the merchant. If this value is not set, the confirmation page and email will display the primary email address associated with the merchant's Square account. Default: none; only exists if explicitly set. :param merchant_support_email: The merchant_support_email of this CreateCheckoutRequest. :type: str
Sets the pre_populate_buyer_email of this CreateCheckoutRequest. If provided, the buyer's email is pre-populated on the checkout page as an editable text field. Default: none; only exists if explicitly set. :param pre_populate_buyer_email: The pre_populate_buyer_email of this CreateCheckoutRequest. :type: str
Sets the redirect_url of this CreateCheckoutRequest. The URL to redirect to after checkout is completed with `checkoutId`, Square's `orderId`, `transactionId`, and `referenceId` appended as URL parameters. For example, if the provided redirect_url is `http://www.example.com/order-complete`, a successful transaction redirects the customer to: `http://www.example.com/order-complete?checkoutId=xxxxxx&orderId=xxxxxx&referenceId=xxxxxx&transactionId=xxxxxx` If you do not provide a redirect URL, Square Checkout will display an order confirmation page on your behalf; however Square strongly recommends that you provide a redirect URL so you can verify the transaction results and finalize the order through your existing/normal confirmation workflow. Default: none; only exists if explicitly set. :param redirect_url: The redirect_url of this CreateCheckoutRequest. :type: str
Sets the description of this AdditionalRecipient. The description of the additional recipient. :param description: The description of this AdditionalRecipient. :type: str
Sets the break_type_id of this ModelBreak. The `BreakType` this `Break` was templated on. :param break_type_id: The break_type_id of this ModelBreak. :type: str
Sets the expected_duration of this ModelBreak. Format: RFC-3339 P[n]Y[n]M[n]DT[n]H[n]M[n]S. The expected length of the break. :param expected_duration: The expected_duration of this ModelBreak. :type: str
Sets the limit of this ListEmployeeWagesRequest. Maximum number of Employee Wages to return per page. Can range between 1 and 200. The default is the maximum at 200. :param limit: The limit of this ListEmployeeWagesRequest. :type: int
Sets the receivable_id of this AdditionalRecipientReceivableRefund. The ID of the receivable that the refund was applied to. :param receivable_id: The receivable_id of this AdditionalRecipientReceivableRefund. :type: str
Sets the refund_id of this AdditionalRecipientReceivableRefund. The ID of the refund that is associated to this receivable refund. :param refund_id: The refund_id of this AdditionalRecipientReceivableRefund. :type: str
Sets the transaction_location_id of this AdditionalRecipientReceivableRefund. The ID of the location that created the receivable. This is the location ID on the associated transaction. :param transaction_location_id: The transaction_location_id of this AdditionalRecipientReceivableRefund. :type: str
Sets the card_nonce of this ChargeRequest. A nonce generated from the `SqPaymentForm` that represents the card to charge. The application that provides a nonce to this endpoint must be the _same application_ that generated the nonce with the `SqPaymentForm`. Otherwise, the nonce is invalid. Do not provide a value for this field if you provide a value for `customer_card_id`. :param card_nonce: The card_nonce of this ChargeRequest. :type: str
Sets the customer_card_id of this ChargeRequest. The ID of the customer card on file to charge. Do not provide a value for this field if you provide a value for `card_nonce`. If you provide this value, you _must_ also provide a value for `customer_id`. :param customer_card_id: The customer_card_id of this ChargeRequest. :type: str
Sets the customer_id of this ChargeRequest. The ID of the customer to associate this transaction with. This field is required if you provide a value for `customer_card_id`, and optional otherwise. :param customer_id: The customer_id of this ChargeRequest. :type: str
Sets the order_id of this ChargeRequest. The ID of the order to associate with this transaction. If you provide this value, the `amount_money` value of your request must __exactly match__ the value of the order's `total_money` field. :param order_id: The order_id of this ChargeRequest. :type: str
Sets the quantity of this OrderLineItem. The quantity purchased, as a string representation of a number. This string must have a positive integer value. :param quantity: The quantity of this OrderLineItem. :type: str
Sets the variation_name of this OrderLineItem. The name of the variation applied to this line item. :param variation_name: The variation_name of this OrderLineItem. :type: str
Sets the transaction_id of this AdditionalRecipientReceivable. The ID of the transaction that the additional recipient receivable was applied to. :param transaction_id: The transaction_id of this AdditionalRecipientReceivable. :type: str
Sets the page_index of this V1Page. The page's position in the merchant's list of pages. Always an integer between 0 and 6, inclusive. :param page_index: The page_index of this V1Page. :type: int
Sets the break_name of this BreakType. A human-readable name for this type of break. Will be displayed to employees in Square products. :param break_name: The break_name of this BreakType. :type: str
Sets the display_name of this OrderFulfillmentRecipient. The display name of the fulfillment recipient. If provided, overrides the value from customer profile indicated by customer_id. :param display_name: The display_name of this OrderFulfillmentRecipient. :type: str
Sets the email_address of this OrderFulfillmentRecipient. The email address of the fulfillment recipient. If provided, overrides the value from customer profile indicated by customer_id. :param email_address: The email_address of this OrderFulfillmentRecipient. :type: str
Sets the phone_number of this OrderFulfillmentRecipient. The phone number of the fulfillment recipient. If provided, overrides the value from customer profile indicated by customer_id. :param phone_number: The phone_number of this OrderFulfillmentRecipient. :type: str
Sets the amount of this Money. The amount of money, in the smallest denomination of the currency indicated by `currency`. For example, when `currency` is `USD`, `amount` is in cents. :param amount: The amount of this Money. :type: int
Sets the tender_id of this CreateRefundRequest. The ID of the tender to refund. A [`Transaction`](#type-transaction) has one or more `tenders` (i.e., methods of payment) associated with it, and you refund each tender separately with the Connect API. :param tender_id: The tender_id of this CreateRefundRequest. :type: str
Sets the reason of this CreateRefundRequest. A description of the reason for the refund. Default value: `Refund via API` :param reason: The reason of this CreateRefundRequest. :type: str
Takes value and turn it into a string suitable for inclusion in the path, by url-encoding. :param obj: object or string value. :return string: quoted value.
Builds a JSON POST object. If obj is None, return None. If obj is str, int, float, bool, return directly. If obj is datetime.datetime, datetime.date convert to string in iso8601 format. If obj is list, sanitize each element in the list. If obj is dict, return the dict. If obj is swagger model, return the properties dict. :param obj: The data to serialize. :return: The serialized form of data.
Deserializes response into an object. :param response: RESTResponse object to be deserialized. :param response_type: class literal for deserialzied object, or string of class name. :return: deserialized object.
Deserializes dict, list, str into an object. :param data: dict, list or str. :param klass: class literal, or string of class name. :return: object.
Returns `Accept` based on an array of accepts provided. :param accepts: List of headers. :return: Accept (e.g. application/json).
Deserializes string to primitive type. :param data: str. :param klass: class literal. :return: int, float, str, bool.
Aligns the AST so that the argument with the highest cardinality is on the left. :return: a new AST.
This function processes the list of truisms and finds bounds for ASTs.
Checks whether we can handle this truism. The truism should already be aligned.
Swap the operands of the truism if the unknown variable is on the right side and the concrete value is on the left side.
Given a constraint, _get_assumptions() returns a set of constraints that are implicitly assumed to be true. For example, `x <= 10` would return `x >= 0`.
Given a constraint, _unpack_truisms() returns a set of constraints that must be True this constraint to be True.
Handles all comparisons.
Parses a list of expressions from the tokens
Make a copy of self and return. :return: A new ValueSet object. :rtype: ValueSet
Apply a new annotation onto self, and return a new ValueSet object. :param RegionAnnotation annotation: The annotation to apply. :return: A new ValueSet object :rtype: ValueSet
The minimum integer value of a value-set. It is only defined when there is exactly one region. :return: A integer that represents the minimum integer value of this value-set. :rtype: int
The maximum integer value of a value-set. It is only defined when there is exactly one region. :return: A integer that represents the maximum integer value of this value-set. :rtype: int
Operation extract - A cheap hack is implemented: a copy of self is returned if (high_bit - low_bit + 1 == self.bits), which is a ValueSet instance. Otherwise a StridedInterval is returned. :param high_bit: :param low_bit: :return: A ValueSet or a StridedInterval
Used to make exact comparisons between two ValueSets. :param o: The other ValueSet to compare with. :return: True if they are exactly same, False otherwise.
Evaluates expression e, returning the results in the form of concrete ASTs.
Returns independent constraints, split from this Frontend's `constraints`.
Creates a boolean symbol (i.e., a variable). :param name: The name of the symbol :param explicit_name: If False, an identifier is appended to the name to ensure uniqueness. :return: A Bool object representing this symbol.
Convert a constraint to SI if possible. :param expr: :return:
Fill up `self._op_expr` dict. :param op_list: A list of operation names. :param op_dict: A dictionary of operation methods. :param op_class: Where the operation method comes from. :return:
Clears all caches associated with this backend.
Resolves a claripy.ast.Base into something usable by the backend. :param expr: The expression. :param save: Save the result in the expression's object cache :return: A backend object.
Calls operation `op` on args `args` with this backend. :return: A backend object representing the result.
_call :param op: :param args: :return:
Should return True if `e` can be easily found to be True. :param e: The AST. :param extra_constraints: Extra constraints (as ASTs) to add to the solver for this solve. :param solver: A solver, for backends that require it. :param model_callback: a function that will be executed with recovered models (if any) :returns: A boolean.
Should return True if e can be easily found to be False. :param e: The AST :param extra_constraints: Extra constraints (as ASTs) to add to the solver for this solve. :param solver: A solver, for backends that require it :param model_callback: a function that will be executed with recovered models (if any) :return: A boolean.
Should return True if `e` can possible be True. :param e: The AST. :param extra_constraints: Extra constraints (as ASTs) to add to the solver for this solve. :param solver: A solver, for backends that require it. :param model_callback: a function that will be executed with recovered models (if any) :return: A boolean
Should return False if `e` can possibly be False. :param e: The AST. :param extra_constraints: Extra constraints (as ASTs) to add to the solver for this solve. :param solver: A solver, for backends that require it. :param model_callback: a function that will be executed with recovered models (if any) :return: A boolean.
This function adds constraints to the backend solver. :param c: A sequence of ASTs :param s: A backend solver object :param bool track: True to enable constraint tracking, which is used in unsat_core()
This function returns the unsat core from the backend solver. :param s: A backend solver object. :return: The unsat core.
This function returns up to `n` possible solutions for expression `expr`. :param expr: expression (an AST) to evaluate :param n: number of results to return :param solver: a solver object, native to the backend, to assist in the evaluation (for example, a z3.Solver) :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve :param model_callback: a function that will be executed with recovered models (if any) :return: A sequence of up to n results (backend objects)
Evaluate one or multiple expressions. :param exprs: A list of expressions to evaluate. :param n: Number of different solutions to return. :param extra_constraints: Extra constraints (as ASTs) to add to the solver for this solve. :param solver: A solver object, native to the backend, to assist in the evaluation. :param model_callback: a function that will be executed with recovered models (if any) :return: A list of up to n tuples, where each tuple is a solution for all expressions.
Return the minimum value of `expr`. :param expr: expression (an AST) to evaluate :param solver: a solver object, native to the backend, to assist in the evaluation (for example, a z3.Solver) :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve :param model_callback: a function that will be executed with recovered models (if any) :return: the minimum possible value of expr (backend object)
Return the maximum value of expr. :param expr: expression (an AST) to evaluate :param solver: a solver object, native to the backend, to assist in the evaluation (for example, a z3.Solver) :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve :param model_callback: a function that will be executed with recovered models (if any) :return: the maximum possible value of expr (backend object)
This function does a constraint check and returns the solvers state :param solver: The backend solver object. :param extra_constraints: Extra constraints (as ASTs) to add to s for this solve :param model_callback: a function that will be executed with recovered models (if any) :return: 'SAT', 'UNSAT', or 'UNKNOWN'
This function does a constraint check and returns the solvers state :param solver: The backend solver object. :param extra_constraints: Extra constraints (as ASTs) to add to s for this solve :param model_callback: a function that will be executed with recovered models (if any) :return: 'SAT', 'UNSAT', or 'UNKNOWN'
This function does a constraint check and checks if the solver is in a sat state. :param solver: The backend solver object. :param extra_constraints: Extra constraints (as ASTs) to add to s for this solve :param model_callback: a function that will be executed with recovered models (if any) :return: True if sat, otherwise false
Return True if `v` is a solution of `expr` with the extra constraints, False otherwise. :param expr: An expression (an AST) to evaluate :param v: The proposed solution (an AST) :param solver: A solver object, native to the backend, to assist in the evaluation (for example, a z3.Solver). :param extra_constraints: Extra constraints (as ASTs) to add to the solver for this solve. :param model_callback: a function that will be executed with recovered models (if any) :return: True if `v` is a solution of `expr`, False otherwise
This should return whether `a` is identical to `b`. Of course, this isn't always clear. True should mean that it is definitely identical. False eans that, conservatively, it might not be. :param a: an AST :param b: another AST
:param name: :param bits: :param stride: :param lower_bound: :param upper_bound: :param to_conv: :param bool discrete_set: :param int discrete_set_max_cardinality: :return:
Lower bound of result of ORing 2-intervals. :param a: Lower bound of first interval :param b: Upper bound of first interval :param c: Lower bound of second interval :param d: Upper bound of second interval :param w: bit width :return: Lower bound of ORing 2-intervals
Upper bound of result of ORing 2-intervals. :param a: Lower bound of first interval :param b: Upper bound of first interval :param c: Lower bound of second interval :param d: Upper bound of second interval :param w: bit width :return: Upper bound of ORing 2-intervals
Lower bound of result of ANDing 2-intervals. :param a: Lower bound of first interval :param b: Upper bound of first interval :param c: Lower bound of second interval :param d: Upper bound of second interval :param w: bit width :return: Lower bound of ANDing 2-intervals
Upper bound of result of ANDing 2-intervals. :param a: Lower bound of first interval :param b: Upper bound of first interval :param c: Lower bound of second interval :param d: Upper bound of second interval :param w: bit width :return: Upper bound of ANDing 2-intervals
Lower bound of result of XORing 2-intervals. :param a: Lower bound of first interval :param b: Upper bound of first interval :param c: Lower bound of second interval :param d: Upper bound of second interval :param w: bit width :return: Lower bound of XORing 2-intervals
Upper bound of result of XORing 2-intervals. :param a: Lower bound of first interval :param b: Upper bound of first interval :param c: Lower bound of second interval :param d: Upper bound of second interval :param w: bit width :return: Upper bound of XORing 2-intervals
Evaluate this StridedInterval to obtain a list of concrete integers. :param n: Upper bound for the number of concrete integers :param signed: Treat this StridedInterval as signed or unsigned :return: A list of at most `n` concrete integers
Checks whether an integer is solution of the current strided Interval :param b: integer to check :return: True if b belongs to the current Strided Interval, False otherwhise
Split `self` at the south pole, which is the same as in unsigned arithmetic. When returning two StridedIntervals (which means a splitting occurred), it is guaranteed that the first StridedInterval is on the right side of the south pole. :return: a list of split StridedIntervals, that contains either one or two StridedIntervals
Split `self` at the north pole, which is the same as in signed arithmetic. :return: A list of split StridedIntervals
Split `self` at both north and south poles. :return: A list of split StridedIntervals
Get lower bound and upper bound for `self` in signed arithmetic. :return: a list of (lower_bound, upper_bound) tuples
Get lower bound and upper bound for `self` in unsigned arithmetic. :return: a list of (lower_bound, upper_bound) tuples.
Logical shift right with a concrete shift amount :param int shift_amount: Number of bits to shift right. :return: The new StridedInterval after right shifting :rtype: StridedInterval
Arithmetic shift right with a concrete shift amount :param int shift_amount: Number of bits to shift right. :return: The new StridedInterval after right shifting :rtype: StridedInterval
Used to make exact comparisons between two StridedIntervals. Usually it is only used in test cases. :param o: The other StridedInterval to compare with. :return: True if they are exactly same, False otherwise.
Signed less than :param o: The other operand :return: TrueResult(), FalseResult(), or MaybeResult()
Unsigned less than. :param o: The other operand :return: TrueResult(), FalseResult(), or MaybeResult()
Equal :param o: The ohter operand :return: TrueResult(), FalseResult(), or MaybeResult()
Return the complement of the interval Refer section 3.1 augmented for managing strides :return:
If this is a TOP value. :return: True if this is a TOP
Refer section 3.1; gap function. :param src_interval: first argument or interval 1 :param tar_interval: second argument or interval 2 :return: Interval representing gap between two intervals
Get a TOP StridedInterval. :return:
Return the cardinality for a set of number (| x, y |) on the wrapped-interval domain. :param x: The first operand (an integer) :param y: The second operand (an integer) :return: The cardinality
Convert an unsigned integer to a signed integer. :param v: The unsigned integer :param bits: How many bits this integer should be :return: The converted signed integer
Determines if an overflow happens during the addition of `a` and `b`. :param a: The first operand (StridedInterval) :param b: The other operand (StridedInterval) :return: True if overflows, False otherwise
Perform wrapped unsigned multiplication on two StridedIntervals. :param a: The first operand (StridedInterval) :param b: The second operand (StridedInterval) :return: The multiplication result
Perform wrapped signed multiplication on two StridedIntervals. :param a: The first operand (StridedInterval) :param b: The second operand (StridedInterval) :return: The product
Perform wrapped unsigned division on two StridedIntervals. :param a: The dividend (StridedInterval) :param b: The divisor (StridedInterval) :return: The quotient
Perform wrapped unsigned division on two StridedIntervals. :param a: The dividend (StridedInterval) :param b: The divisor (StridedInterval) :return: The quotient
Perform a wrapped LTE comparison only considering the SI bounds :param a: The first operand :param b: The second operand :return: True if a <= b, False otherwise
Unary operation: neg :return: 0 - self
Binary operation: add :param b: The other operand :return: self + b
Binary operation: sub :param b: The other operand :return: self - b
Binary operation: multiplication :param o: The other operand :return: self * o
Binary operation: signed division :param o: The divisor :return: (self / o) in signed arithmetic
Binary operation: unsigned division :param o: The divisor :return: (self / o) in unsigned arithmetic
Unary operation: bitwise not :return: ~self
Binary operation: logical or :param b: The other operand :return: self | b
Binary operation: logical and :param b: The other operand :return:
Operation xor :param t: The other operand.
Logical shift right. :param StridedInterval shift_amount: The amount of shifting :return: The shifted StridedInterval :rtype: StridedInterval
Arithmetic shift right. :param StridedInterval shift_amount: The amount of shifting :return: The shifted StridedInterval :rtype: StridedInterval
Unary operation: SignExtend :param new_length: New length after sign-extension :return: A new StridedInterval
Unary operation: ZeroExtend :param new_length: New length after zero-extension :return: A new StridedInterval
Unary operation: SignExtend :param new_length: New length after sign-extension :return: A new StridedInterval
The union operation. It might return a DiscreteStridedIntervalSet to allow for better precision in analysis. :param b: Operand :return: A new DiscreteStridedIntervalSet, or a new StridedInterval.
Return interval with bigger cardinality Refer Section 3.1 :param interval1: first interval :param interval2: second interval :return: Interval or interval2 whichever has greater cardinality
Get the number of consecutive zeros :param x: :return:
Pseudo least upper bound. Join the given set of intervals into a big interval. The resulting strided interval is the one which in all the possible joins of the presented SI, presented the least number of values. The number of joins to compute is linear with the number of intervals to join. Draft of proof: Considering three generic SI (a,b, and c) ordered from their lower bounds, such that a.lower_bund <= b.lower_bound <= c.lower_bound, where <= is the lexicographic less or equal. The only joins which have sense to compute are: * a U b U c * b U c U a * c U a U b All the other combinations fall in either one of these cases. For example: b U a U c does not make make sense to be calculated. In fact, if one draws this union, the result is exactly either (b U c U a) or (a U b U c) or (c U a U b). :param intervals_to_join: Intervals to join :return: Interval that contains all intervals
It two intervals in a way that the resulting SI is the one that has the least SI cardinality (i.e., which represents the least number of elements) possible if the smart_join flag is enabled, otherwise it just joins the SI according the order they are passed to the function. The pseudo-join operation is not associative in wrapping intervals (please refer to section 3.1 paper 'Signedness-Agnostic Program Analysis: Precise Integer Bounds for Low-Level Code'), Therefore the join of three WI may give us different results according on the order we join them. All of the results will be sound, though. Please use the function least_upper_bound as a stub. :param s: The first SI :param b: The other SI. :param smart_join: Enable the smart join behavior. If this flag is set, this function joins the two SI in a way that the resulting Si has least number of elements (more precise). If it is unset, this function will join the two SI according on the order they are passed to the function. :return: A new StridedInterval
Calculates the minimal integer that appears in both StridedIntervals. As a wrapper method of _minimal_common_integer_splitted(), this method takes arbitrary StridedIntervals. For more information, please refer to the comment of _minimal_common_integer_splitted(). :param si_0: the first StridedInterval :type si_0: StridedInterval :param si_1: the second StridedInterval :type si_1: StridedInterval :return: the minimal common integer, or None if there is no common integer
It calculates the GCD of a and b, and two values x and y such that: a*x + b*y = GCD(a,b). This code has been taken from the project sympy. :param a: first integer :param b: second integer :return: x,y and the GCD of a and b
:param a: First integer :param b: Second integer :return: the integer GCD between a and b
It finds the fist natural solution of the diophantine equation a*x + b*y = c. Some lines of this code are taken from the project sympy. :param c: constant :param a: quotient of x :param b: quotient of y :return: the first natural solution of the diophatine equation
Calculates the minimal integer that appears in both StridedIntervals. It's equivalent to finding an integral solution for equation `ax + b = cy + d` that makes `ax + b` minimal si_0.stride, si_1.stride being a and c, and si_0.lower_bound, si_1.lower_bound being b and d, respectively. Upper bounds are used to check whether the minimal common integer exceeds the bound or not. None is returned if no minimal common integers can be found within the range. Some assumptions: # - None of the StridedIntervals straddles the south pole. Consequently, we have x <= max_int(si.bits) and y <= # max_int(si.bits) # - a, b, c, d are all positive integers # - x >= 0, y >= 0 :param StridedInterval si_0: the first StridedInterval :param StridedInterval si_1: the second StrideInterval :return: the minimal common integer, or None if there is no common integer
This is a delayed reversing function. All it really does is to invert the _reversed property of this StridedInterval object. :return: None
This method reverses the StridedInterval object for real. Do expect loss of precision for most cases! :return: A new reversed StridedInterval instance
This method reverses the StridedInterval object for real. Do expect loss of precision for most cases! :return: A new reversed StridedInterval instance
Return an smt-lib script that check the satisfiability of the current constraints :return string: smt-lib script
Apply an annotation on the backend object. :param BackendObject bo: The backend object. :param Annotation annotation: The annotation to be applied :return: A new BackendObject :rtype: BackendObject
Converts a Z3 model to a name->primitive dict.
This function is the deserializer for ASTs. It exists to work around the fact that pickle will (normally) call __new__() with no arguments during deserialization. For ASTs, this does not work.
Calculates the hash of an AST, given the operation, args, and kwargs. :param op: The operation. :param args: The arguments to the operation. :param keywords: A dict including the 'symbolic', 'variables', and 'length' items. :returns: a hash. We do it using md5 to avoid hash collisions. (hash(-1) == hash(-2), for example)
Removes an annotation from this AST. :param a: the annotation to remove :returns: a new AST, with the annotation removed
Removes several annotations from this AST. :param remove_sequence: a sequence/set of the annotations to remove :returns: a new AST, with the annotations removed
Returns a string representation of this AST, but with a maximum depth to prevent floods of text being printed. :param max_depth: The maximum depth to print. :param explicit_length: Print lengths of BVV arguments. :param details: An integer value specifying how detailed the output should be: LITE_REPR - print short repr for both operations and BVs, MID_REPR - print full repr for operations and short for BVs, FULL_REPR - print full repr of both operations and BVs. :return: A string representing the AST
Return an iterator over the nested children ASTs.
Return an iterator over the leaf ASTs.
This returns the same AST, with the arguments swapped out for new_args.
Splits the AST if its operation is `split_on` (i.e., return all the arguments). Otherwise, return a list with just the AST.
Structurally compares two A objects, and check if their corresponding leaves are definitely the same A object (name-wise or hash-identity wise). :param o: the other claripy A object :return: True/False
Returns this AST with subexpressions replaced by those that can be found in `replacements` dict. :param variable_set: For optimization, ast's without these variables are not checked for replacing. :param replacements: A dictionary of hashes to their replacements. :param leaf_operation: An operation that should be applied to the leaf nodes. :return: An AST with all instances of ast's in replacements.
Returns this AST but with the AST 'old' replaced with AST 'new' in its subexpressions.
Returns an equivalent AST that "burrows" the ITE expressions as deep as possible into the ast, for simpler printing.
Returns an equivalent AST that "excavates" the ITE expressions out as far as possible toward the root of the AST, for processing in static analyses.
Handles the following case: ((A << a) | (A >> (_N - a))) & mask, where A being a BVS, a being a integer that is less than _N, _N is either 32 or 64, and mask can be evaluated to 0xffffffff (64-bit) or 0xffff (32-bit) after reversing the rotate-shift operation. It will be simplified to: (A & (mask >>> a)) <<< a
Creates a floating-point symbol. :param name: The name of the symbol :param sort: The sort of the floating point :param explicit_name: If False, an identifier is appended to the name to ensure uniqueness. :return: An FP AST.
Convert this float to a different sort :param sort: The sort to convert to :param rm: Optional: The rounding mode to use :return: An FP AST
Convert this floating point value to an integer. :param size: The size of the bitvector to return :param signed: Optional: Whether the target integer is signed :param rm: Optional: The rounding mode to use :return: A bitvector whose value is the rounded version of this FP's value
This is an over-approximation of the cardinality of this DSIS. :return:
Collapse into a StridedInterval instance. :return: A new StridedInterval instance.
Return the collapsed object if ``should_collapse()`` is True, otherwise return self. :return: A DiscreteStridedIntervalSet object.
Operation extract :param high_bit: The highest bit to begin extraction. :param low_bit: The lowest bit to end extraction. :return: Extracted bits.
Union with another StridedInterval. :param si: :return:
Union with another DiscreteStridedIntervalSet. :param dsis: :return:
Intersection with another :class:`StridedInterval`. :param si: The other operand :return:
Intersection with another :class:`DiscreteStridedIntervalSet`. :param dsis: The other operand. :return:
Dump the symbol in its smt-format depending on its type :param e: symbol to dump :param daggify: The daggify parameter can be used to switch from a linear-size representation that uses ‘let’ operators to represent the formula as a dag or a simpler (but possibly exponential) representation that expands the formula as a tree :return string: smt-lib representation of the symbol
Since we decided to emulate integer with bitvector, this method transform their concrete value (if any) in the corresponding integer
Returns a SMT script that declare all the symbols and constraint and checks their satisfiability (check-sat) :param extra-constraints: list of extra constraints that we want to evaluate only in the scope of this call :return string: smt-lib representation of the script that checks the satisfiability
Returns a SMT script that declare all the symbols and constraint and checks their satisfiability (check-sat) :param extra-constraints: list of extra constraints that we want to evaluate only in the scope of this call :return string: smt-lib representation of the script that checks the satisfiability
Create a new symbolic string (analogous to z3.String()) :param name: The name of the symbolic string (i. e. the name of the variable) :param size: The size in bytes of the string (i. e. the length of the string) :param uninitialized: Whether this value should be counted as an "uninitialized" value in the course of an analysis. :param bool explicit_name: If False, an identifier is appended to the name to ensure uniqueness. :returns: The String object representing the symbolic string
Create a new Concrete string (analogous to z3.StringVal()) :param value: The constant value of the concrete string :returns: The String object representing the concrete string
Return the start index of the pattern inside the input string in a Bitvector representation, otherwise it returns -1 (always using a BitVector) :param bitlength: size of the biitvector holding the result
A counterpart to FP.raw_to_bv - does nothing and returns itself.
Create a concrete version of the concatenated string :param args: List of string that has to be concatenated :return : a concrete version of the concatenated string
Create a concrete version of the substring :param start_idx : starting index of the substring :param end_idx : last index of the substring :param initial_string :return : a concrete version of the substring
Create a concrete version of the replaced string (replace ONLY th efirst occurrence of the pattern) :param initial_string: string in which the pattern needs to be replaced :param pattern_to_be_replaced: substring that has to be replaced inside initial_string :param replacement_poattern: pattern that has to be inserted in initial_string t replace pattern_to_be_replaced :return: a concrete representation of the replaced string
Return True if the concrete value of the input_string starts with prefix otherwise false. :param prefix: prefix we want to check :param input_string: the string we want to check :return: True if the input_string starts with prefix else false
Return True if the concrete value of the input_string ends with suffix otherwise false. :param suffix: suffix we want to check :param input_string: the string we want to check :return : True if the input_string ends with suffix else false
Return True if the concrete value of the input_string ends with suffix otherwise false. :param input_string: the string we want to check :param substring: the substring we want to find the index :param startIndex: the index to start searching at :param bitlength: bitlength of the bitvector representing the index of the substring :return BVV: index of the substring in bit-vector representation or -1 in bitvector representation
Return True if the concrete value of the input_string ends with suffix otherwise false. :param input_string: the string we want to transform in an integer :param bitlength: bitlength of the bitvector representing the index of the substring :return BVV: bit-vector representation of the integer resulting from ythe string or -1 in bitvector representation if the string cannot be transformed into an integer
Returns a sequence of the solvers that self and others share.
Creates a bit-vector symbol (i.e., a variable). If you want to specify the maximum or minimum value of a normal symbol that is not part of value-set analysis, you should manually add constraints to that effect. **Do not use ``min`` and ``max`` for symbolic execution.** :param name: The name of the symbol. :param size: The size (in bits) of the bit-vector. :param min: The minimum value of the symbol, used only for value-set analysis :param max: The maximum value of the symbol, used only for value-set analysis :param stride: The stride of the symbol, used only for value-set analysis :param uninitialized: Whether this value should be counted as an "uninitialized" value in the course of an analysis. :param bool explicit_name: If False, an identifier is appended to the name to ensure uniqueness. :param bool discrete_set: If True, a DiscreteStridedIntervalSet will be used instead of a normal StridedInterval. :param int discrete_set_max_card: The maximum cardinality of the discrete set. It is ignored if discrete_set is set to False or None. :returns: a BV object representing this symbol.
Creates a bit-vector value (i.e., a concrete value). :param value: The value. Either an integer or a string. If it's a string, it will be interpreted as the bytes of a big-endian constant. :param size: The size (in bits) of the bit-vector. Optional if you provide a string, required for an integer. :returns: A BV object representing this value.
Chops a BV into consecutive sub-slices. Obviously, the length of this BV must be a multiple of bits. :returns: A list of smaller bitvectors, each ``bits`` in length. The first one will be the left-most (i.e. most significant) bits.
Extracts a byte from a BV, where the index refers to the byte in a big-endian order :param index: the byte to extract :return: An 8-bit BV
Extracts several bytes from a bitvector, where the index refers to the byte in a big-endian order :param index: the byte index at which to start extracting :param size: the number of bytes to extract :return: A BV of size ``size * 8``
Interpret this bitvector as an integer, and return the floating-point representation of that integer. :param sort: The sort of floating point value to return :param signed: Optional: whether this value is a signed integer :param rm: Optional: the rounding mode to use :return: An FP AST whose value is the same as this BV
Interpret the bits of this bitvector as an IEEE754 floating point number. The inverse of this function is raw_to_bv. :return: An FP AST whose bit-pattern is the same as this BV
Override Backend.convert() to add fast paths for BVVs and BoolVs.
Eval the ast, replacing symbols by their last value in the model.
Returns whether the constraints is satisfied trivially by using the last model.
Updates this cache mixin with results discovered by the other split off one.
This function returns up to `n` possible solutions for expression `expr`. :param expr: expression (an AST) to evaluate :param n: number of results to return :param solver: a solver object, native to the backend, to assist in the evaluation (for example, a z3.Solver) :param extra_constraints: extra constraints (as ASTs) to add to the solver for this solve :param model_callback: a function that will be executed with recovered models (if any) :return: A sequence of up to n results (backend objects)
r"""Convert seekpath-formatted kpoints path to sumo-preferred format. If 'GAMMA' is used as a label this will be replaced by '\Gamma'. Args: seekpath (list): A :obj:`list` of 2-tuples containing the labels at each side of each segment of the k-point path:: [(A, B), (B, C), (C, D), ...] where a break in the sequence is indicated by a non-repeating label. E.g.:: [(A, B), (B, C), (D, E), ...] for a break between C and D. point_coords (dict): Dict of coordinates corresponding to k-point labels:: {'GAMMA': [0., 0., 0.], ...} Returns: dict: The path and k-points as:: { 'path', [[l1, l2, l3], [l4, l5], ...], 'kpoints', {l1: [a1, b1, c1], l2: [a2, b2, c2], ...} }
r"""Read Bradley--Cracknell k-points path from data file Args: bravais (str): Lattice code including orientation e.g. 'trig_p_c' Returns: dict: kpoint path and special point locations, formatted as e.g.:: {'kpoints': {'\Gamma': [0., 0., 0.], 'X': [0., 0.5, 0.], ...}, 'path': [['\Gamma', 'X', ..., 'P'], ['H', 'N', ...]]}
Get Bravais lattice symbol from symmetry data
Get a colour for a particular elemental and orbital combination. If the element is not specified in the colours dictionary, the cache is checked. If this element-orbital combination has not been chached before, a new colour is drawn from the current matplotlib colour cycle and cached. The default cache is sumo.plotting.colour_cache. To reset this cache, use ``sumo.plotting.colour_cache.clear()``. Args: element (:obj:`str`): The element. orbital (:obj:`str`): The orbital. colours (:obj:`dict`, optional): Use custom colours for specific element and orbital combinations. Specified as a :obj:`dict` of :obj:`dict` of the colours. For example:: { 'Sn': {'s': 'r', 'p': 'b'}, 'O': {'s': '#000000'} } The colour can be a hex code, series of rgb value, or any other format supported by matplotlib. cache (:obj:`dict`, optional): Cache of colour values already assigned. The format is the same as the custom colours dict. If None, the module-level cache ``sumo.plotting.colour_cache`` is used. Returns: tuple: (colour, cache)
Get the plotting data. Args: yscale (:obj:`float`, optional): Scaling factor for the y-axis. xmin (:obj:`float`, optional): The minimum energy to mask the energy and density of states data (reduces plotting load). xmax (:obj:`float`, optional): The maximum energy to mask the energy and density of states data (reduces plotting load). colours (:obj:`dict`, optional): Use custom colours for specific element and orbital combinations. Specified as a :obj:`dict` of :obj:`dict` of the colours. For example:: { 'Sn': {'s': 'r', 'p': 'b'}, 'O': {'s': '#000000'} } The colour can be a hex code, series of rgb value, or any other format supported by matplotlib. plot_total (:obj:`bool`, optional): Plot the total density of states. Defaults to ``True``. legend_cutoff (:obj:`float`, optional): The cut-off (in % of the maximum density of states within the plotting range) for an elemental orbital to be labelled in the legend. This prevents the legend from containing labels for orbitals that have very little contribution in the plotting range. subplot (:obj:`bool`, optional): Plot the density of states for each element on separate subplots. Defaults to ``False``. zero_to_efermi (:obj:`bool`, optional): Normalise the plot such that the Fermi level is set as 0 eV. cache (:obj:`dict`, optional): Cache object tracking how colours have been assigned to orbitals. The format is the same as the "colours" dict. This defaults to the module-level sumo.plotting.colour_cache object, but an empty dict can be used as a fresh cache. This object will be modified in-place. Returns: dict: The plotting data. Formatted with the following keys: "energies" (:obj:`numpy.ndarray`) The energies. "mask" (:obj:`numpy.ndarray`) A mask used to trim the density of states data and prevent unwanted data being included in the output file. "lines" (:obj:`list`) A :obj:`list` of :obj:`dict` containing the density data and some metadata. Each line :obj:`dict` contains the keys: "label" (:obj:`str`) The label for the legend. "dens" (:obj:`numpy.ndarray`) The density of states data. "colour" (:obj:`str`) The colour of the line. "alpha" (:obj:`float`) The alpha value for line fill. "ymin" (:obj:`float`) The minimum y-axis limit. "ymax" (:obj:`float`) The maximum y-axis limit.
Get a :obj:`matplotlib.pyplot` object of the density of states. Args: subplot (:obj:`bool`, optional): Plot the density of states for each element on separate subplots. Defaults to ``False``. width (:obj:`float`, optional): The width of the plot. height (:obj:`float`, optional): The height of the plot. xmin (:obj:`float`, optional): The minimum energy on the x-axis. xmax (:obj:`float`, optional): The maximum energy on the x-axis. yscale (:obj:`float`, optional): Scaling factor for the y-axis. colours (:obj:`dict`, optional): Use custom colours for specific element and orbital combinations. Specified as a :obj:`dict` of :obj:`dict` of the colours. For example:: { 'Sn': {'s': 'r', 'p': 'b'}, 'O': {'s': '#000000'} } The colour can be a hex code, series of rgb value, or any other format supported by matplotlib. plot_total (:obj:`bool`, optional): Plot the total density of states. Defaults to ``True``. legend_on (:obj:`bool`, optional): Plot the graph legend. Defaults to ``True``. num_columns (:obj:`int`, optional): The number of columns in the legend. legend_frame_on (:obj:`bool`, optional): Plot a frame around the graph legend. Defaults to ``False``. legend_cutoff (:obj:`float`, optional): The cut-off (in % of the maximum density of states within the plotting range) for an elemental orbital to be labelled in the legend. This prevents the legend from containing labels for orbitals that have very little contribution in the plotting range. xlabel (:obj:`str`, optional): Label/units for x-axis (i.e. energy) ylabel (:obj:`str`, optional): Label/units for y-axis (i.e. DOS) zero_to_efermi (:obj:`bool`, optional): Normalise the plot such that the Fermi level is set as 0 eV. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. Returns: :obj:`matplotlib.pyplot`: The density of states plot.
Returns orbital projections for each branch in a band structure. Args: bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`): The band structure. selection (list): A list of :obj:`tuple` or :obj:`string` identifying which projections to return. Projections can be specified by both element and orbital, for example:: [('Sn', 's'), ('Bi', 'p'), ('S', 'p')] If just the element is specified then all the orbitals of that element are combined. For example, the following will combine all the S orbitals into a single projection:: [('Bi', 's'), ('Bi', 'p'), 'S'] Particular orbitals can also be combined, for example:: [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))] normalise (:obj:`str`, optional): Normalisation the projections. Options are: * ``'all'``: Projections normalised against the sum of all other projections. * ``'select'``: Projections normalised against the sum of the selected projections. * ``None``: No normalisation performed. Defaults to ``None``. Returns: list: A ``list`` of orbital projections for each branch of the band structure, in the same order as specified in ``selection``, with the format:: [ [ {spin: projections} ], [ {spin: projections} ], ... ] Where spin is a :obj:`pymatgen.electronic_structure.core.Spin` object and projections is a :obj:`numpy.array` of:: projections[band_index][kpoint_index] If there are no projections in the band structure, then an array of zeros is returned for each spin.
Returns orbital projections from a band structure. Args: bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`): The band structure. selection (list): A list of :obj:`tuple` or :obj:`string` identifying which projections to return. Projections can be specified by both element and orbital, for example:: [('Bi', 's'), ('Bi', 'p'), ('S', 'p')] If just the element is specified then all the orbitals of that element are combined. For example, the following will combine all the S orbitals into a single projection:: [('Bi', 's'), ('Bi', 'p'), 'S'] Particular orbitals can also be combined, for example:: [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))] normalise (:obj:`str`, optional): Normalisation the projections. Options are: * ``'all'``: Projections normalised against the sum of all other projections. * ``'select'``: Projections normalised against the sum of the selected projections. * ``None``: No normalisation performed. Defaults to ``None``. Returns: list: A ``list`` of orbital projections, in the same order as specified in ``selection``, with the format:: [ {spin: projections}, {spin: projections} ... ] Where spin is a :obj:`pymatgen.electronic_structure.core.Spin` object and projections is a :obj:`numpy.array` of:: projections[band_index][kpoint_index] If there are no projections in the band structure, then an array of zeros is returned for each spin.
Get a :obj:`matplotlib.pyplot` object of the band structure. If the system is spin polarised, orange lines are spin up, dashed blue lines are spin down. For metals, all bands are coloured blue. For semiconductors, blue lines indicate valence bands and orange lines indicates conduction bands. Args: zero_to_efermi (:obj:`bool`): Normalise the plot such that the valence band maximum is set as 0 eV. ymin (:obj:`float`, optional): The minimum energy on the y-axis. ymax (:obj:`float`, optional): The maximum energy on the y-axis. width (:obj:`float`, optional): The width of the plot. height (:obj:`float`, optional): The height of the plot. vbm_cbm_marker (:obj:`bool`, optional): Plot markers to indicate the VBM and CBM locations. ylabel (:obj:`str`, optional): y-axis (i.e. energy) label/units dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. dos_plotter (:obj:`~sumo.plotting.dos_plotter.SDOSPlotter`, \ optional): Plot the density of states alongside the band structure. This should be a :obj:`~sumo.plotting.dos_plotter.SDOSPlotter` object initialised with the data to plot. dos_options (:obj:`dict`, optional): The options for density of states plotting. This should be formatted as a :obj:`dict` containing any of the following keys: "yscale" (:obj:`float`) Scaling factor for the y-axis. "xmin" (:obj:`float`) The minimum energy to mask the energy and density of states data (reduces plotting load). "xmax" (:obj:`float`) The maximum energy to mask the energy and density of states data (reduces plotting load). "colours" (:obj:`dict`) Use custom colours for specific element and orbital combinations. Specified as a :obj:`dict` of :obj:`dict` of the colours. For example:: { 'Sn': {'s': 'r', 'p': 'b'}, 'O': {'s': '#000000'} } The colour can be a hex code, series of rgb value, or any other format supported by matplotlib. "plot_total" (:obj:`bool`) Plot the total density of states. Defaults to ``True``. "legend_cutoff" (:obj:`float`) The cut-off (in % of the maximum density of states within the plotting range) for an elemental orbital to be labelled in the legend. This prevents the legend from containing labels for orbitals that have very little contribution in the plotting range. "subplot" (:obj:`bool`) Plot the density of states for each element on separate subplots. Defaults to ``False``. dos_label (:obj:`str`, optional): DOS axis label/units dos_aspect (:obj:`float`, optional): Aspect ratio for the band structure and density of states subplot. For example, ``dos_aspect = 3``, results in a ratio of 3:1, for the band structure:dos plots. aspect (:obj:`float`, optional): The aspect ratio of the band structure plot. By default the dimensions of the figure size are used to determine the aspect ratio. Set to ``1`` to force the plot to be square. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. Returns: :obj:`matplotlib.pyplot`: The electronic band structure plot.
Get a :obj:`matplotlib.pyplot` of the projected band structure. If the system is spin polarised and ``mode = 'rgb'`` spin up and spin down bands are differentiated by solid and dashed lines, respectively. For the other modes, spin up and spin down are plotted separately. Args: selection (list): A list of :obj:`tuple` or :obj:`string` identifying which elements and orbitals to project on to the band structure. These can be specified by both element and orbital, for example, the following will project the Bi s, p and S p orbitals:: [('Bi', 's'), ('Bi', 'p'), ('S', 'p')] If just the element is specified then all the orbitals of that element are combined. For example, to sum all the S orbitals:: [('Bi', 's'), ('Bi', 'p'), 'S'] You can also choose to sum particular orbitals by supplying a :obj:`tuple` of orbitals. For example, to sum the S s, p, and d orbitals into a single projection:: [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))] If ``mode = 'rgb'``, a maximum of 3 orbital/element combinations can be plotted simultaneously (one for red, green and blue), otherwise an unlimited number of elements/orbitals can be selected. mode (:obj:`str`, optional): Type of projected band structure to plot. Options are: "rgb" The band structure line color depends on the character of the band. Each element/orbital contributes either red, green or blue with the corresponding line colour a mixture of all three colours. This mode only supports up to 3 elements/orbitals combinations. The order of the ``selection`` :obj:`tuple` determines which colour is used for each selection. "stacked" The element/orbital contributions are drawn as a series of stacked circles, with the colour depending on the composition of the band. The size of the circles can be scaled using the ``circle_size`` option. interpolate_factor (:obj:`int`, optional): The factor by which to interpolate the band structure (necessary to make smooth lines). A larger number indicates greater interpolation. circle_size (:obj:`float`, optional): The area of the circles used when ``mode = 'stacked'``. projection_cutoff (:obj:`float`): Don't plot projections with intensities below this number. This option is useful for stacked plots, where small projections clutter the plot. zero_to_efermi (:obj:`bool`): Normalise the plot such that the valence band maximum is set as 0 eV. ymin (:obj:`float`, optional): The minimum energy on the y-axis. ymax (:obj:`float`, optional): The maximum energy on the y-axis. width (:obj:`float`, optional): The width of the plot. height (:obj:`float`, optional): The height of the plot. vbm_cbm_marker (:obj:`bool`, optional): Plot markers to indicate the VBM and CBM locations. ylabel (:obj:`str`, optional): y-axis (i.e. energy) label/units dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. dos_plotter (:obj:`~sumo.plotting.dos_plotter.SDOSPlotter`, \ optional): Plot the density of states alongside the band structure. This should be a :obj:`~sumo.plotting.dos_plotter.SDOSPlotter` object initialised with the data to plot. dos_options (:obj:`dict`, optional): The options for density of states plotting. This should be formatted as a :obj:`dict` containing any of the following keys: "yscale" (:obj:`float`) Scaling factor for the y-axis. "xmin" (:obj:`float`) The minimum energy to mask the energy and density of states data (reduces plotting load). "xmax" (:obj:`float`) The maximum energy to mask the energy and density of states data (reduces plotting load). "colours" (:obj:`dict`) Use custom colours for specific element and orbital combinations. Specified as a :obj:`dict` of :obj:`dict` of the colours. For example:: { 'Sn': {'s': 'r', 'p': 'b'}, 'O': {'s': '#000000'} } The colour can be a hex code, series of rgb value, or any other format supported by matplotlib. "plot_total" (:obj:`bool`) Plot the total density of states. Defaults to ``True``. "legend_cutoff" (:obj:`float`) The cut-off (in % of the maximum density of states within the plotting range) for an elemental orbital to be labelled in the legend. This prevents the legend from containing labels for orbitals that have very little contribution in the plotting range. "subplot" (:obj:`bool`) Plot the density of states for each element on separate subplots. Defaults to ``False``. dos_label (:obj:`str`, optional): DOS axis label/units dos_aspect (:obj:`float`, optional): Aspect ratio for the band structure and density of states subplot. For example, ``dos_aspect = 3``, results in a ratio of 3:1, for the band structure:dos plots. aspect (:obj:`float`, optional): The aspect ratio of the band structure plot. By default the dimensions of the figure size are used to determine the aspect ratio. Set to ``1`` to force the plot to be square. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. Returns: :obj:`matplotlib.pyplot`: The projected electronic band structure plot.
Tidy the band structure & add the density of states if required.
This is basically the same as the SDOSPlotter get_plot function.
Implement label hacks: Hide trailing @, remove label with leading @ Labels split with $\mid$ symbol will be treated for each part.
Utility method to add tick marks to a band structure.
Get a :obj:`matplotlib.pyplot` object of the optical spectra. Args: width (:obj:`float`, optional): The width of the plot. height (:obj:`float`, optional): The height of the plot. xmin (:obj:`float`, optional): The minimum energy on the x-axis. xmax (:obj:`float`, optional): The maximum energy on the x-axis. ymin (:obj:`float`, optional): The minimum absorption intensity on the y-axis. ymax (:obj:`float`, optional): The maximum absorption intensity on the y-axis. colours (:obj:`list`, optional): A :obj:`list` of colours to use in the plot. The colours can be specified as a hex code, set of rgb values, or any other format supported by matplotlib. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. Returns: :obj:`matplotlib.pyplot`: The plot of optical spectra.
Determine if the structure matches the standard primitive structure. The standard primitive will be different between seekpath and pymatgen high-symmetry paths, but this is handled by the specific subclasses. Args: atol (:obj:`float`, optional): Absolute tolerance used to compare the input structure with the primitive standard structure. Returns: bool: ``True`` if the structure is the same as the standard primitive, otherwise ``False``.
r"""Return a list of k-points and labels along the high-symmetry path. The format of the returned data will be different if phonopy is ``True`` or ``False``. This is because phonopy requires the labels and kpoints to be provided in a different format than kgen. Adapted from :obj:`pymatgen.symmetry.bandstructure.HighSymmKpath.get_kpoints`. Args: line_density (:obj:`int`, optional): Density of k-points along the path. cart_coords (:obj:`bool`, optional): Whether the k-points are returned in cartesian or reciprocal coordinates. Defaults to ``False`` (fractional coordinates). phonopy (:obj:`bool`, optional): Format the k-points and labels for use with phonopy. Defaults to ``False``. Returns: tuple: A :obj:`tuple` of the k-points along the high-symmetry path, and k-point labels. Returned as ``(kpoints, labels)``. If ``phonopy == False``, then: * ``kpoints`` is a :obj:`numpy.ndarray` of the k-point coordinates along the high-symmetry path. For example:: [[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0], [0.5, 0, 0.25], [0.5, 0, 0.5]] * ``labels`` is a :obj:`list` of the high symmetry labels for each k-point (will be an empty :obj:`str` if the k-point has no label). For example:: ['\Gamma', '', 'X', '', 'Y'] If ``phonopy == True``, then: * ``kpoints`` is a :obj:`list` of :obj:`numpy.ndarray` containing the k-points for each branch of the band structure. This means that the first and last k-points of a particular branch may be repeated. For example:: [[[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0]], [[0.5, 0, 0], [0.5, 0, 0.25], [0.5, 0, 0.5]]] * ``labels`` is a :obj:`list` of the high symmetry labels. For example:: ['\Gamma', 'X', 'Y']
Return the lattice crystal system. Hexagonal cells are differentiated into rhombohedral and hexagonal lattices. Args: number (int): The international space group number. Returns: str: The lattice crystal system.
Extract fitting data for band extrema based on spin, kpoint and band. Searches forward and backward from the extrema point, but will only sample there data if there are enough points in that direction. Args: bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`): The band structure. spin (:obj:`~pymatgen.electronic_structure.core.Spin`): Which spin channel to sample. band_id (int): Index of the band to sample. kpoint_id (int): Index of the kpoint to sample. Returns: list: The data necessary to calculate the effective mass, along with some metadata. Formatted as a :obj:`list` of :obj:`dict`, each with the keys: 'energies' (:obj:`numpy.ndarray`) Band eigenvalues in eV. 'distances' (:obj:`numpy.ndarray`) Distances of the k-points in reciprocal space. 'band_id' (:obj:`int`) The index of the band, 'spin' (:obj:`~pymatgen.electronic_structure.core.Spin`) The spin channel 'start_kpoint' (:obj:`int`) The index of the k-point at which the band extrema occurs 'end_kpoint' (:obj:`int`) The k-point towards which the data has been sampled.
Fit the effective masses using either a parabolic or nonparabolic fit. Args: distances (:obj:`numpy.ndarray`): The x-distances between k-points in reciprocal Angstroms, normalised to the band extrema. energies (:obj:`numpy.ndarray`): The band eigenvalues normalised to the eigenvalue of the band extrema. parabolic (:obj:`bool`, optional): Use a parabolic fit of the band edges. If ``False`` then nonparabolic fitting will be attempted. Defaults to ``True``. Returns: float: The effective mass in units of electron rest mass, :math:`m_0`.
r"""Get the k-point path, coordinates and symmetry labels for a structure. If a manual :obj:`list` of kpoints is supplied using the ``kpt_list`` variable, the ``mode`` option will be ignored. The format of the returned data will be different if phonopy is ``True`` or ``False``. This is because phonopy requires the labels and kpoints to be provided in a different format than kgen. Args: structure (:obj:`~pymatgen.core.structure.Structure`): The structure. mode (:obj:`str`, optional): Method used for calculating the high-symmetry path. The options are: bradcrack Use the paths from Bradley and Cracknell. See [brad]_. pymatgen Use the paths from pymatgen. See [curt]_. seekpath Use the paths from SeeK-path. See [seek]_. symprec (:obj:`float`, optional): The tolerance for determining the crystal symmetry. spg (:obj:`~pymatgen.symmetry.groups.SpaceGroup`, optional): Space group used to override the symmetry determined by spglib. This is not recommended and only provided for testing purposes. This option will only take effect when ``mode = 'bradcrack'``. line_density (:obj:`int`, optional): Density of k-points along the path. cart_coords (:obj:`bool`, optional): Whether the k-points are returned in cartesian or reciprocal coordinates. Defaults to ``False`` (fractional coordinates). kpt_list (:obj:`list`, optional): List of k-points to use, formatted as a list of subpaths, each containing a list of fractional k-points. For example:: [ [[0., 0., 0.], [0., 0., 0.5]], [[0.5, 0., 0.], [0.5, 0.5, 0.]] ] Will return points along ``0 0 0 -> 0 0 1/2 | 1/2 0 0 -> 1/2 1/2 0`` path_labels (:obj:`list`, optional): The k-point labels. These should be provided as a :obj:`list` of :obj:`str` for each subpath of the overall path. For example:: [ ['Gamma', 'Z'], ['X', 'M'] ] combined with the above example for ``kpt_list`` would indicate the path: Gamma -> Z | X -> M. If no labels are provided, letters from A -> Z will be used instead. phonopy (:obj:`bool`, optional): Format the k-points and labels for use with phonopy. Defaults to ``False``. Returns: tuple: A tuple of a :obj:`~sumo.symmetry.kpath` object, the k-points along the high-symmetry path, and the k-point labels. Returned as ``(kpath, kpoints, labels)``. The type of ``kpath`` object will depend on the value of ``mode`` and whether ``kpt_list`` is set. If ``phonopy == False``, then: * ``kpoints`` is a :obj:`numpy.ndarray` of the k-point coordinates along the high-symmetry path. For example:: [[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0], [0.5, 0, 0.25], [0.5, 0, 0.5]] * ``labels`` is a :obj:`list` of the high symmetry labels for each k-point (will be an empty :obj:`str` if the k-point has no label). For example:: ['\Gamma', '', 'X', '', 'Y'] If ``phonopy == True``, then: * ``kpoints`` is a :obj:`list` of :obj:`numpy.ndarray` containing the k-points for each branch of the band structure. This means that the first and last k-points of a particular branch may be repeated. For example:: [[[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0]], [[0.5, 0, 0], [0.5, 0, 0.25], [0.5, 0, 0.5]]] * ``labels`` is a :obj:`list` of the high symmetry labels. For example:: ['\Gamma', 'X', 'Y']
r"""Write the k-points data to VASP KPOINTS files. Folders are named as 'split-01', 'split-02', etc ... KPOINTS files are named KPOINTS_band_split_01 etc ... Args: filename (:obj:`str`): Path to VASP structure file. kpoints (:obj:`numpy.ndarray`): The k-point coordinates along the high-symmetry path. For example:: [[0, 0, 0], [0.25, 0, 0], [0.5, 0, 0], [0.5, 0, 0.25], [0.5, 0, 0.5]] labels (:obj:`list`) The high symmetry labels for each k-point (will be an empty :obj:`str` if the k-point has no label). For example:: ['\Gamma', '', 'X', '', 'Y'] make_folders (:obj:`bool`, optional): Generate folders and copy in required files (INCAR, POTCAR, POSCAR, and possibly CHGCAR) from the current directory. ibzkpt (:obj:`str`, optional): Path to IBZKPT file. If set, the generated k-points will be appended to the k-points in this file and given a weight of 0. This is necessary for hybrid band structure calculations. kpts_per_split (:obj:`int`, optional): If set, the k-points are split into separate k-point files (or folders) each containing the number of k-points specified. This is useful for hybrid band structure calculations where it is often intractable to calculate all k-points in the same calculation. directory (:obj:`str`, optional): The output file directory. cart_coords (:obj:`bool`, optional): Whether the k-points are returned in cartesian or reciprocal coordinates. Defaults to ``False`` (fractional coordinates).
Return a decorator that will apply matplotlib style sheets to a plot. ``style_sheets`` is a base set of styles, which will be ignored if ``no_base_style`` is set in the decorated function arguments. The style will further be overwritten by any styles in the ``style`` optional argument of the decorated function. Args: style_sheets (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib supported definition of a style sheet. Can be a list of style of style sheets.
Get a :obj:`matplotlib.pyplot` object with publication ready defaults. Args: width (:obj:`float`, optional): The width of the plot. height (:obj:`float`, optional): The height of the plot. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the plot. Returns: :obj:`matplotlib.pyplot`: A :obj:`matplotlib.pyplot` object with publication ready defaults set.
Get a :obj:`matplotlib.pyplot` subplot object with pretty defaults. Args: nrows (int): The number of rows in the subplot. ncols (int): The number of columns in the subplot. width (:obj:`float`, optional): The width of the plot. height (:obj:`float`, optional): The height of the plot. sharex (:obj:`bool`, optional): All subplots share the same x-axis. Defaults to ``True``. sharey (:obj:`bool`, optional): All subplots share the same y-axis. Defaults to ``True``. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the plot. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. gridspec_kw (:obj:`dict`, optional): Gridspec parameters. Please see: :obj:`matplotlib.pyplot.subplot` for more information. Defaults to ``None``. Returns: :obj:`matplotlib.pyplot`: A :obj:`matplotlib.pyplot` subplot object with publication ready defaults set.
Custom power ticker function.
Get a RGB coloured line for plotting. Args: x (list): x-axis data. y (list): y-axis data (can be multidimensional array). red (list): Red data (must have same shape as ``y``). green (list): Green data (must have same shape as ``y``). blue (list): blue data (must have same shape as ``y``). alpha (:obj:`list` or :obj:`int`, optional): Alpha (transparency) data (must have same shape as ``y`` or be an :obj:`int`). linestyles (:obj:`str`, optional): Linestyle for plot. Options are ``"solid"`` or ``"dotted"``.
Apply gaussian broadening to the dielectric response. Args: dielectric_data (tuple): The high-frequency dielectric data, following the same format as :attr:`pymatgen.io.vasp.outputs.Vasprun.dielectric`. This is a :obj:`tuple` containing the energy, the real part of the dielectric tensor, and the imaginary part of the tensor, as a :obj:`list` of :obj:`floats`. E.g.:: ( [energies], [[real_xx, real_yy, real_zz, real_xy, real_yz, real_xz]], [[imag_xx, imag_yy, imag_zz, imag_xy, imag_yz, imag_xz]] ) sigma (float): Standard deviation for gaussian broadening. Returns: :obj:`tuple` of :obj:`list` of :obj:`list` of :obj:`float`: The broadened dielectric response. Returned as a tuple containing the energy, the real part of the dielectric tensor, and the imaginary part of the tensor. E.g.:: ( [energies], [[real_xx, real_yy, real_zz, real_xy, real_yz, real_xz]], [[imag_xx, imag_yy, imag_zz, imag_xy, imag_yz, imag_xz]] )
r"""Calculate optical properties from the dielectric function Supported properties: Absorption ~~~~~~~~~~ The unit of alpha is :math:`\mathrm{cm}^{-1}`. Refractive index :math:`n` has real and imaginary parts: .. math:: n = [(e^\prime + ie^{\prime\prime} / e_0]^{1/2} = n^\prime + in^{\prime\prime} Relationship between :math:`a` and imaginary :math:`n^{\prime\prime}`: .. math:: a = 4 \pi n^{\prime\prime} / \lambda Where: .. math:: \lambda = hc/E Args: dielectric_data (tuple): The high-frequency dielectric data, following the same format as :obj:`pymatgen.io.vasp.Vasprun.dielectric`. This is a :obj:`tuple` containing the energy, the real part of the dielectric tensor, and the imaginary part of the tensor, as a :obj:`list` of :obj:`floats`. E.g.:: ( [energies], [[real_xx, real_yy, real_zz, real_xy, real_yz, real_xz]], [[imag_xx, imag_yy, imag_zz, imag_xy, imag_yz, imag_xz]] ) properties (set): The set of properties to return. Intermediate properties will be calculated as needed. Accepted values: 'eps_real', 'eps_im', 'absorption', 'loss', 'n_real', 'n_imag' average (:obj:`bool`, optional): Average the dielectric response across the xx, yy, zz directions and calculate properties with scalar maths. Defaults to ``True``. If False, solve dielectric matrix to obtain directional properties, returning xx, yy, zz components. This may be significantly slower! Returns: :obj:`tuple` of :obj:`list` of :obj:`float`: The optical absorption in :math:`\mathrm{cm}^{-1}`. If ``average`` is ``True``, the data will be returned as:: ([energies], [property]). If ``average`` is ``False``, the data will be returned as:: ([energies], [property_xx, property_yy, property_zz]).
Write the absorption or loss spectra to a file. Note that this function expects to receive an iterable series of spectra. Args: abs_data (tuple): Series (either :obj:`list` or :obj:`tuple`) of optical absorption or loss spectra. Each spectrum should be formatted as a :obj:`tuple` of :obj:`list` of :obj:`float`. If the data has been averaged, each spectrum should be:: ([energies], [alpha]) Else, if the data has not been averaged, each spectrum should be:: ([energies], [alpha_xx, alpha_yy, alpha_zz]). prefix (:obj:`str`, optional): Prefix for file names. directory (:obj:`str`, optional): The directory in which to save files.
A script to plot optical absorption spectra from VASP calculations. Args: modes (:obj:`list` or :obj:`tuple`): Ordered list of :obj:`str` determining properties to plot. Accepted options are 'absorption' (default), 'eps', 'eps-real', 'eps-im', 'n', 'n-real', 'n-im', 'loss' (equivalent to n-im). filenames (:obj:`str` or :obj:`list`, optional): Path to vasprun.xml file (can be gzipped). Alternatively, a list of paths can be provided, in which case the absorption spectra for each will be plotted concurrently. prefix (:obj:`str`, optional): Prefix for file names. directory (:obj:`str`, optional): The directory in which to save files. gaussian (:obj:`float`): Standard deviation for gaussian broadening. band_gaps (:obj:`float` or :obj:`list`, optional): The band gap as a :obj:`float`, plotted as a dashed line. If plotting multiple spectra then a :obj:`list` of band gaps can be provided. labels (:obj:`str` or :obj:`list`): A label to identify the spectra. If plotting multiple spectra then a :obj:`list` of labels can be provided. average (:obj:`bool`, optional): Average the dielectric response across all lattice directions. Defaults to ``True``. height (:obj:`float`, optional): The height of the plot. width (:obj:`float`, optional): The width of the plot. xmin (:obj:`float`, optional): The minimum energy on the x-axis. xmax (:obj:`float`, optional): The maximum energy on the x-axis. ymin (:obj:`float`, optional): The minimum absorption intensity on the y-axis. ymax (:obj:`float`, optional): The maximum absorption intensity on the y-axis. colours (:obj:`list`, optional): A :obj:`list` of colours to use in the plot. The colours can be specified as a hex code, set of rgb values, or any other format supported by matplotlib. style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. image_format (:obj:`str`, optional): The image file format. Can be any format supported by matplotlib, including: png, jpg, pdf, and svg. Defaults to pdf. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. Returns: A matplotlib pyplot object.
Load phonopy output and return an ``phonopy.Phonopy`` object. Args: filename (str): Path to phonopy output. Can be any of ``FORCE_SETS``, ``FORCE_CONSTANTS``, or ``force_constants.hdf5``. structure (:obj:`~pymatgen.core.structure.Structure`): The unitcell structure. dim (list): The supercell size, as a :obj:`list` of :obj:`float`. symprec (:obj:`float`, optional): The tolerance for determining the crystal symmetry. primitive_matrix (:obj:`list`, optional): The transformation matrix from the conventional to primitive cell. Only required when the conventional cell was used as the starting structure. Should be provided as a 3x3 :obj:`list` of :obj:`float`. factor (:obj:`float`, optional): The conversion factor for phonon frequency. Defaults to :obj:`phonopy.units.VaspToTHz`. symmetrise (:obj:`bool`, optional): Symmetrise the force constants. Defaults to ``True``. born (:obj:`str`, optional): Path to file containing Born effective charges. Should be in the same format as the file produced by the ``phonopy-vasp-born`` script provided by phonopy. write_fc (:obj:`bool` or :obj:`str`, optional): Write the force constants to disk. If ``True``, a ``FORCE_CONSTANTS`` file will be written. Alternatively, if set to ``"hdf5"``, a ``force_constants.hdf5`` file will be written. Defaults to ``False`` (force constants not written).
Get a default set of labels (1), (2), (3)... for a k-point path Repeated points will be identified and the labels re-used. Args: kpt_list (list): Nested list representing k-point path segments, e.g.:: [[[0., 0., 0.], [0., 0., 0.5], [0., 0.5, 0.5]], [[0.5, 0.5, 0.], [0., 0., 0.]]] Returns: list: Corresponding nested list of labels, e.g.:: [['(1)', '(2)', '(3)'], ['(4)', '(1)']]
Load a vasprun and extract the total and projected density of states. Args: vasprun (str): Path to a vasprun.xml or vasprun.xml.gz file or a :obj:`pymatgen.io.vasp.outputs.Vasprun` object. elements (:obj:`dict`, optional): The elements and orbitals to extract from the projected density of states. Should be provided as a :obj:`dict` with the keys as the element names and corresponding values as a :obj:`tuple` of orbitals. For example, the following would extract the Bi s, px, py and d orbitals:: {'Bi': ('s', 'px', 'py', 'd')} If an element is included with an empty :obj:`tuple`, all orbitals for that species will be extracted. If ``elements`` is not set or set to ``None``, all elements for all species will be extracted. lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into their lm contributions (e.g. p -> px, py, pz). Should be provided as a :obj:`dict`, with the elements names as keys and a :obj:`tuple` of orbitals as the corresponding values. For example, the following would be used to decompose the oxygen p and d orbitals:: {'O': ('p', 'd')} atoms (:obj:`dict`, optional): Which atomic sites to use when calculating the projected density of states. Should be provided as a :obj:`dict`, with the element names as keys and a :obj:`tuple` of :obj:`int` specifying the atomic indices as the corresponding values. The elemental projected density of states will be summed only over the atom indices specified. If an element is included with an empty :obj:`tuple`, then all sites for that element will be included. The indices are 0 based for each element specified in the POSCAR. For example, the following will calculate the density of states for the first 4 Sn atoms and all O atoms in the structure:: {'Sn': (1, 2, 3, 4), 'O': (, )} If ``atoms`` is not set or set to ``None`` then all atomic sites for all elements will be considered. gaussian (:obj:`float`, optional): Broaden the density of states using convolution with a gaussian function. This parameter controls the sigma or standard deviation of the gaussian distribution. total_only (:obj:`bool`, optional): Only extract the total density of states. Defaults to ``False``. log (:obj:`bool`): Print logging messages. Defaults to ``False``. adjust_fermi (:obj:`bool`, optional): Shift the Fermi level to sit at the valence band maximum (does not affect metals). Returns: dict: The total and projected density of states. Formatted as a :obj:`tuple` of ``(dos, pdos)``, where ``dos`` is a :obj:`~pymatgen.electronic_structure.dos.Dos` object containing the total density of states and ``pdos`` is a :obj:`dict` of :obj:`dict` mapping the elements and their orbitals to :obj:`~pymatgen.electronic_structure.dos.Dos` objects. For example:: { 'Bi': {'s': Dos, 'p': Dos ... }, 'S': {'s': Dos} }
Extract the projected density of states from a CompleteDos object. Args: dos (:obj:`~pymatgen.electronic_structure.dos.CompleteDos`): The density of states. elements (:obj:`dict`, optional): The elements and orbitals to extract from the projected density of states. Should be provided as a :obj:`dict` with the keys as the element names and corresponding values as a :obj:`tuple` of orbitals. For example, the following would extract the Bi s, px, py and d orbitals:: {'Bi': ('s', 'px', 'py', 'd')} If an element is included with an empty :obj:`tuple`, all orbitals for that species will be extracted. If ``elements`` is not set or set to ``None``, all elements for all species will be extracted. lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into their lm contributions (e.g. p -> px, py, pz). Should be provided as a :obj:`dict`, with the elements names as keys and a :obj:`tuple` of orbitals as the corresponding values. For example, the following would be used to decompose the oxygen p and d orbitals:: {'O': ('p', 'd')} atoms (:obj:`dict`, optional): Which atomic sites to use when calculating the projected density of states. Should be provided as a :obj:`dict`, with the element names as keys and a :obj:`tuple` of :obj:`int` specifying the atomic indices as the corresponding values. The elemental projected density of states will be summed only over the atom indices specified. If an element is included with an empty :obj:`tuple`, then all sites for that element will be included. The indices are 0 based for each element specified in the POSCAR. For example, the following will calculate the density of states for the first 4 Sn atoms and all O atoms in the structure:: {'Sn': (1, 2, 3, 4), 'O': (, )} If ``atoms`` is not set or set to ``None`` then all atomic sites for all elements will be considered. Returns: dict: The projected density of states. Formatted as a :obj:`dict` of :obj:`dict` mapping the elements and their orbitals to :obj:`~pymatgen.electronic_structure.dos.Dos` objects. For example:: { 'Bi': {'s': Dos, 'p': Dos ... }, 'S': {'s': Dos} }
Get the projected density of states for an element. Args: dos (:obj:`~pymatgen.electronic_structure.dos.CompleteDos`): The density of states. element (str): Element symbol. E.g. 'Zn'. sites (tuple): The atomic indices over which to sum the density of states, as a :obj:`tuple`. Indices are zero based for each element. For example, ``(0, 1, 2)`` will sum the density of states for the 1st, 2nd and 3rd sites of the element specified. lm_orbitals (:obj:`tuple`, optional): The orbitals to decompose into their lm contributions (e.g. p -> px, py, pz). Should be provided as a :obj:`tuple` of :obj:`str`. For example, ``('p')``, will extract the projected density of states for the px, py, and pz orbitals. Defaults to ``None``. orbitals (:obj:`tuple`, optional): The orbitals to extract from the projected density of states. Should be provided as a :obj:`tuple` of :obj:`str`. For example, ``('s', 'px', 'dx2')`` will extract the s, px, and dx2 orbitals, only. If ``None``, all orbitals will be extracted. Defaults to ``None``. Returns: dict: The projected density of states. Formatted as a :obj:`dict` mapping the orbitals to :obj:`~pymatgen.electronic_structure.dos.Dos` objects. For example:: { 's': Dos, 'p': Dos }
Write the density of states data to disk. Args: dos (:obj:`~pymatgen.electronic_structure.dos.Dos` or \ :obj:`~pymatgen.electronic_structure.dos.CompleteDos`): The total density of states. pdos (dict): The projected density of states. Formatted as a :obj:`dict` of :obj:`dict` mapping the elements and their orbitals to :obj:`~pymatgen.electronic_structure.dos.Dos` objects. For example:: { 'Bi': {'s': Dos, 'p': Dos}, 'S': {'s': Dos} } prefix (:obj:`str`, optional): A prefix for file names. directory (:obj:`str`, optional): The directory in which to save files. zero_to_efermi (:obj:`bool`, optional): Normalise the energy such that the Fermi level is set as 0 eV.
Sort the orbitals of an element's projected density of states. Sorts the orbitals based on a standard format. E.g. s < p < d. Will also sort lm decomposed orbitals. This is useful for plotting/saving. Args: element_pdos (dict): An element's pdos. Should be formatted as a :obj:`dict` of ``{orbital: dos}``. Where dos is a :obj:`~pymatgen.electronic_structure.dos.Dos` object. For example:: {'s': dos, 'px': dos} Returns: list: The sorted orbitals.
Calculate the effective masses of the bands of a semiconductor. Args: filenames (:obj:`str` or :obj:`list`, optional): Path to vasprun.xml or vasprun.xml.gz file. If no filenames are provided, the code will search for vasprun.xml or vasprun.xml.gz files in folders named 'split-0*'. Failing that, the code will look for a vasprun in the current directory. If a :obj:`list` of vasprun files is provided, these will be combined into a single band structure. num_sample_points (:obj:`int`, optional): Number of k-points to sample when fitting the effective masses. temperature (:obj:`int`, optional): Find band edges within kB * T of the valence band maximum and conduction band minimum. Not currently implemented. degeneracy_tol (:obj:`float`, optional): Tolerance for determining the degeneracy of the valence band maximum and conduction band minimum. parabolic (:obj:`bool`, optional): Use a parabolic fit of the band edges. If ``False`` then nonparabolic fitting will be attempted. Defaults to ``True``. Returns: dict: The hole and electron effective masses. Formatted as a :obj:`dict` with keys: ``'hole_data'`` and ``'electron_data'``. The data is a :obj:`list` of :obj:`dict` with the keys: 'effective_mass' (:obj:`float`) The effective mass in units of electron rest mass, :math:`m_0`. 'energies' (:obj:`numpy.ndarray`) Band eigenvalues in eV. 'distances' (:obj:`numpy.ndarray`) Distances of the k-points in reciprocal space. 'band_id' (:obj:`int`) The index of the band, 'spin' (:obj:`~pymatgen.electronic_structure.core.Spin`) The spin channel 'start_kpoint' (:obj:`int`) The index of the k-point at which the band extrema occurs 'end_kpoint' (:obj:`int`)
Log data about the direct and indirect band gaps. Args: bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`):
Log data about the valence band maximum or conduction band minimum. Args: bs (:obj:`~pymatgen.electronic_structure.bandstructure.BandStructureSymmLine`): The band structure. edge_data (dict): The :obj:`dict` from ``bs.get_vbm()`` or ``bs.get_cbm()``
Log data about the effective masses and their directions. Args: data (dict): The effective mass data. Formatted as a :obj:`dict` with the keys: 'effective_mass' (:obj:`float`) The effective mass in units of electron rest mass, :math:`m_0`. 'energies' (:obj:`numpy.ndarray`) Band eigenvalues in eV. 'band_id' (:obj:`int`) The index of the band, 'spin' (:obj:`~pymatgen.electronic_structure.core.Spin`) The spin channel 'start_kpoint' (:obj:`int`) The index of the k-point at which the band extrema occurs 'end_kpoint' (:obj:`int`) The k-point towards which the data has been sampled. is_spin_polarized (bool): Whether the system is spin polarized.
Get a :obj:`matplotlib.pyplot` object of the phonon band structure. Args: units (:obj:`str`, optional): Units of phonon frequency. Accepted (case-insensitive) values are Thz, cm-1, eV, meV. ymin (:obj:`float`, optional): The minimum energy on the y-axis. ymax (:obj:`float`, optional): The maximum energy on the y-axis. width (:obj:`float`, optional): The width of the plot. height (:obj:`float`, optional): The height of the plot. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. dos (:obj:`np.ndarray`): 2D Numpy array of total DOS data dos_aspect (float): Width division for vertical DOS color (:obj:`str` or :obj:`tuple`, optional): Line/fill colour in any matplotlib-accepted format style (:obj:`list`, :obj:`str`, or :obj:`dict`): Any matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. Returns: :obj:`matplotlib.pyplot`: The phonon band structure plot.
Utility method to tidy phonon band structure diagrams.
Utility method to add tick marks to a band structure.
Plot electronic band structure diagrams from vasprun.xml files. Args: filenames (:obj:`str` or :obj:`list`, optional): Path to vasprun.xml or vasprun.xml.gz file. If no filenames are provided, the code will search for vasprun.xml or vasprun.xml.gz files in folders named 'split-0*'. Failing that, the code will look for a vasprun in the current directory. If a :obj:`list` of vasprun files is provided, these will be combined into a single band structure. prefix (:obj:`str`, optional): Prefix for file names. directory (:obj:`str`, optional): The directory in which to save files. vbm_cbm_marker (:obj:`bool`, optional): Plot markers to indicate the VBM and CBM locations. projection_selection (list): A list of :obj:`tuple` or :obj:`string` identifying which elements and orbitals to project on to the band structure. These can be specified by both element and orbital, for example, the following will project the Bi s, p and S p orbitals:: [('Bi', 's'), ('Bi', 'p'), ('S', 'p')] If just the element is specified then all the orbitals of that element are combined. For example, to sum all the S orbitals:: [('Bi', 's'), ('Bi', 'p'), 'S'] You can also choose to sum particular orbitals by supplying a :obj:`tuple` of orbitals. For example, to sum the S s, p, and d orbitals into a single projection:: [('Bi', 's'), ('Bi', 'p'), ('S', ('s', 'p', 'd'))] If ``mode = 'rgb'``, a maximum of 3 orbital/element combinations can be plotted simultaneously (one for red, green and blue), otherwise an unlimited number of elements/orbitals can be selected. mode (:obj:`str`, optional): Type of projected band structure to plot. Options are: "rgb" The band structure line color depends on the character of the band. Each element/orbital contributes either red, green or blue with the corresponding line colour a mixture of all three colours. This mode only supports up to 3 elements/orbitals combinations. The order of the ``selection`` :obj:`tuple` determines which colour is used for each selection. "stacked" The element/orbital contributions are drawn as a series of stacked circles, with the colour depending on the composition of the band. The size of the circles can be scaled using the ``circle_size`` option. circle_size (:obj:`float`, optional): The area of the circles used when ``mode = 'stacked'``. dos_file (:obj:'str', optional): Path to vasprun.xml file from which to read the density of states information. If set, the density of states will be plotted alongside the bandstructure. elements (:obj:`dict`, optional): The elements and orbitals to extract from the projected density of states. Should be provided as a :obj:`dict` with the keys as the element names and corresponding values as a :obj:`tuple` of orbitals. For example, the following would extract the Bi s, px, py and d orbitals:: {'Bi': ('s', 'px', 'py', 'd')} If an element is included with an empty :obj:`tuple`, all orbitals for that species will be extracted. If ``elements`` is not set or set to ``None``, all elements for all species will be extracted. lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into their lm contributions (e.g. p -> px, py, pz). Should be provided as a :obj:`dict`, with the elements names as keys and a :obj:`tuple` of orbitals as the corresponding values. For example, the following would be used to decompose the oxygen p and d orbitals:: {'O': ('p', 'd')} atoms (:obj:`dict`, optional): Which atomic sites to use when calculating the projected density of states. Should be provided as a :obj:`dict`, with the element names as keys and a :obj:`tuple` of :obj:`int` specifying the atomic indices as the corresponding values. The elemental projected density of states will be summed only over the atom indices specified. If an element is included with an empty :obj:`tuple`, then all sites for that element will be included. The indices are 0 based for each element specified in the POSCAR. For example, the following will calculate the density of states for the first 4 Sn atoms and all O atoms in the structure:: {'Sn': (1, 2, 3, 4), 'O': (, )} If ``atoms`` is not set or set to ``None`` then all atomic sites for all elements will be considered. total_only (:obj:`bool`, optional): Only extract the total density of states. Defaults to ``False``. plot_total (:obj:`bool`, optional): Plot the total density of states. Defaults to ``True``. legend_cutoff (:obj:`float`, optional): The cut-off (in % of the maximum density of states within the plotting range) for an elemental orbital to be labelled in the legend. This prevents the legend from containing labels for orbitals that have very little contribution in the plotting range. gaussian (:obj:`float`, optional): Broaden the density of states using convolution with a gaussian function. This parameter controls the sigma or standard deviation of the gaussian distribution. height (:obj:`float`, optional): The height of the plot. width (:obj:`float`, optional): The width of the plot. ymin (:obj:`float`, optional): The minimum energy on the y-axis. ymax (:obj:`float`, optional): The maximum energy on the y-axis. style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. colours (:obj:`dict`, optional): Use custom colours for specific element and orbital combinations. Specified as a :obj:`dict` of :obj:`dict` of the colours. For example:: { 'Sn': {'s': 'r', 'p': 'b'}, 'O': {'s': '#000000'} } The colour can be a hex code, series of rgb value, or any other format supported by matplotlib. yscale (:obj:`float`, optional): Scaling factor for the y-axis. image_format (:obj:`str`, optional): The image file format. Can be any format supported by matplotlib, including: png, jpg, pdf, and svg. Defaults to pdf. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. Returns: If ``plt`` set then the ``plt`` object will be returned. Otherwise, the method will return a :obj:`list` of filenames written to disk.
Search for vasprun files from the current directory. The precedence order for file locations is: 1. First search for folders named: 'split-0*' 2. Else, look in the current directory. The split folder names should always be zero based, therefore easily sortable.
Write the band structure data files to disk. Args: vs (`Vasprun`): Pymatgen `Vasprun` object. bs (`BandStructureSymmLine`): Calculated band structure. prefix (`str`, optional): Prefix for data file. directory (`str`, optional): Directory in which to save the data. Returns: The filename of the written data file.
Parse the element and orbital argument strings. The presence of an element without any orbitals means that we want to plot all of its orbitals. Args: string (`str`): The selected elements and orbitals in in the form: `"Sn.s.p,O"`. Returns: A list of tuples specifying which elements/orbitals to plot. The output for the above example would be: `[('Sn', ('s', 'p')), 'O']`
A script to plot phonon band structure diagrams. Args: filename (str): Path to phonopy output. Can be a band structure yaml file, ``FORCE_SETS``, ``FORCE_CONSTANTS``, or ``force_constants.hdf5``. poscar (:obj:`str`, optional): Path to POSCAR file of unitcell. Not required if plotting the phonon band structure from a yaml file. If not specified, the script will search for a POSCAR file in the current directory. prefix (:obj:`str`, optional): Prefix for file names. directory (:obj:`str`, optional): The directory in which to save files. born (:obj:`str`, optional): Path to file containing Born effective charges. Should be in the same format as the file produced by the ``phonopy-vasp-born`` script provided by phonopy. qmesh (:obj:`list` of :obj:`int`, optional): Q-point mesh to use for calculating the density of state. Formatted as a 3x1 :obj:`list` of :obj:`int`. spg (:obj:`str` or :obj:`int`, optional): The space group international number or symbol to override the symmetry determined by spglib. This is not recommended and only provided for testing purposes. This option will only take effect when ``mode = 'bradcrack'``. primitive_matrix (:obj:`list`, optional): The transformation matrix from the conventional to primitive cell. Only required when the conventional cell was used as the starting structure. Should be provided as a 3x3 :obj:`list` of :obj:`float`. line_density (:obj:`int`, optional): Density of k-points along the path. units (:obj:`str`, optional): Units of phonon frequency. Accepted (case-insensitive) values are Thz, cm-1, eV, meV. symprec (:obj:`float`, optional): Tolerance for space-group-finding operations mode (:obj:`str`, optional): Method used for calculating the high-symmetry path. The options are: bradcrack Use the paths from Bradley and Cracknell. See [brad]_. pymatgen Use the paths from pymatgen. See [curt]_. seekpath Use the paths from SeeK-path. See [seek]_. kpt_list (:obj:`list`, optional): List of k-points to use, formatted as a list of subpaths, each containing a list of fractional k-points. For example:: [ [[0., 0., 0.], [0., 0., 0.5]], [[0.5, 0., 0.], [0.5, 0.5, 0.]] ] Will return points along ``0 0 0 -> 0 0 1/2 | 1/2 0 0 -> 1/2 1/2 0`` path_labels (:obj:`list`, optional): The k-point labels. These should be provided as a :obj:`list` of :obj:`str` for each subpath of the overall path. For example:: [ ['Gamma', 'Z'], ['X', 'M'] ] combined with the above example for ``kpt_list`` would indicate the path: Gamma -> Z | X -> M. If no labels are provided, letters from A -> Z will be used instead. eigenvectors (:obj:`bool`, optional): Write the eigenvectors to the yaml file. dos (str): Path to Phonopy total dos .dat file height (:obj:`float`, optional): The height of the plot. width (:obj:`float`, optional): The width of the plot. ymin (:obj:`float`, optional): The minimum energy on the y-axis. ymax (:obj:`float`, optional): The maximum energy on the y-axis. style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. image_format (:obj:`str`, optional): The image file format. Can be any format supported by matplotlib, including: png, jpg, pdf, and svg. Defaults to pdf. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. Returns: A matplotlib pyplot object.
Write the phonon band structure data files to disk. Args: bs (:obj:`~pymatgen.phonon.bandstructure.PhononBandStructureSymmLine`): The phonon band structure. prefix (:obj:`str`, optional): Prefix for data file. directory (:obj:`str`, optional): Directory in which to save the data. Returns: str: The filename of the written data file.
Generate KPOINTS files for VASP band structure calculations. This script provides a wrapper around several frameworks used to generate k-points along a high-symmetry path. The paths found in Bradley and Cracknell, SeeK-path, and pymatgen are all supported. It is important to note that the standard primitive cell symmetry is different between SeeK-path and pymatgen. If the correct the structure is not used, the high-symmetry points (and band path) may be invalid. Args: filename (:obj:`str`, optional): Path to VASP structure file. Default is ``POSCAR``. directory (:obj:`str`, optional): The output file directory. make_folders (:obj:`bool`, optional): Generate folders and copy in required files (INCAR, POTCAR, POSCAR, and possibly CHGCAR) from the current directory. symprec (:obj:`float`, optional): The precision used for determining the cell symmetry. kpts_per_split (:obj:`int`, optional): If set, the k-points are split into separate k-point files (or folders) each containing the number of k-points specified. This is useful for hybrid band structure calculations where it is often intractable to calculate all k-points in the same calculation. ibzkpt (:obj:`str`, optional): Path to IBZKPT file. If set, the generated k-points will be appended to the k-points in this file and given a weight of 0. This is necessary for hybrid band structure calculations. spg (:obj:`str` or :obj:`int`, optional): The space group international number or symbol to override the symmetry determined by spglib. This is not recommended and only provided for testing purposes. This option will only take effect when ``mode = 'bradcrack'``. line_density (:obj:`int`, optional): Density of k-points along the path. mode (:obj:`str`, optional): Method used for calculating the high-symmetry path. The options are: bradcrack Use the paths from Bradley and Cracknell. See [brad]_. pymatgen Use the paths from pymatgen. See [curt]_. seekpath Use the paths from SeeK-path. See [seek]_. cart_coords (:obj:`bool`, optional): Whether the k-points are returned in cartesian or reciprocal coordinates. Defaults to ``False`` (fractional coordinates). kpt_list (:obj:`list`, optional): List of k-points to use, formatted as a list of subpaths, each containing a list of fractional k-points. For example:: [ [[0., 0., 0.], [0., 0., 0.5]], [[0.5, 0., 0.], [0.5, 0.5, 0.]] ] Will return points along ``0 0 0 -> 0 0 1/2 | 1/2 0 0 -> 1/2 1/2 0`` path_labels (:obj:`list`, optional): The k-point labels. These should be provided as a :obj:`list` of :obj:`str` for each subpath of the overall path. For example:: [ ['Gamma', 'Z'], ['X', 'M'] ] combined with the above example for ``kpt_list`` would indicate the path: Gamma -> Z | X -> M. If no labels are provided, letters from A -> Z will be used instead. If a label begins with '@' it will be concealed when plotting with sumo-bandplot.
A script to plot the density of states from a vasprun.xml file. Args: filename (:obj:`str`, optional): Path to a vasprun.xml file (can be gzipped). prefix (:obj:`str`, optional): Prefix for file names. directory (:obj:`str`, optional): The directory in which to save files. elements (:obj:`dict`, optional): The elements and orbitals to extract from the projected density of states. Should be provided as a :obj:`dict` with the keys as the element names and corresponding values as a :obj:`tuple` of orbitals. For example, the following would extract the Bi s, px, py and d orbitals:: {'Bi': ('s', 'px', 'py', 'd')} If an element is included with an empty :obj:`tuple`, all orbitals for that species will be extracted. If ``elements`` is not set or set to ``None``, all elements for all species will be extracted. lm_orbitals (:obj:`dict`, optional): The orbitals to decompose into their lm contributions (e.g. p -> px, py, pz). Should be provided as a :obj:`dict`, with the elements names as keys and a :obj:`tuple` of orbitals as the corresponding values. For example, the following would be used to decompose the oxygen p and d orbitals:: {'O': ('p', 'd')} atoms (:obj:`dict`, optional): Which atomic sites to use when calculating the projected density of states. Should be provided as a :obj:`dict`, with the element names as keys and a :obj:`tuple` of :obj:`int` specifying the atomic indices as the corresponding values. The elemental projected density of states will be summed only over the atom indices specified. If an element is included with an empty :obj:`tuple`, then all sites for that element will be included. The indices are 0 based for each element specified in the POSCAR. For example, the following will calculate the density of states for the first 4 Sn atoms and all O atoms in the structure:: {'Sn': (1, 2, 3, 4), 'O': (, )} If ``atoms`` is not set or set to ``None`` then all atomic sites for all elements will be considered. subplot (:obj:`bool`, optional): Plot the density of states for each element on separate subplots. Defaults to ``False``. shift (:obj:`bool`, optional): Shift the energies such that the valence band maximum (or Fermi level for metals) is at 0 eV. Defaults to ``True``. total_only (:obj:`bool`, optional): Only extract the total density of states. Defaults to ``False``. plot_total (:obj:`bool`, optional): Plot the total density of states. Defaults to ``True``. legend_on (:obj:`bool`, optional): Plot the graph legend. Defaults to ``True``. legend_frame_on (:obj:`bool`, optional): Plot a frame around the graph legend. Defaults to ``False``. legend_cutoff (:obj:`float`, optional): The cut-off (in % of the maximum density of states within the plotting range) for an elemental orbital to be labelled in the legend. This prevents the legend from containing labels for orbitals that have very little contribution in the plotting range. gaussian (:obj:`float`, optional): Broaden the density of states using convolution with a gaussian function. This parameter controls the sigma or standard deviation of the gaussian distribution. height (:obj:`float`, optional): The height of the plot. width (:obj:`float`, optional): The width of the plot. xmin (:obj:`float`, optional): The minimum energy on the x-axis. xmax (:obj:`float`, optional): The maximum energy on the x-axis. num_columns (:obj:`int`, optional): The number of columns in the legend. colours (:obj:`dict`, optional): Use custom colours for specific element and orbital combinations. Specified as a :obj:`dict` of :obj:`dict` of the colours. For example:: { 'Sn': {'s': 'r', 'p': 'b'}, 'O': {'s': '#000000'} } The colour can be a hex code, series of rgb value, or any other format supported by matplotlib. xlabel (:obj:`str`, optional): Label/units for x-axis (i.e. energy) ylabel (:obj:`str`, optional): Label/units for y-axis (i.e. DOS) yscale (:obj:`float`, optional): Scaling factor for the y-axis. style (:obj:`list` or :obj:`str`, optional): (List of) matplotlib style specifications, to be composed on top of Sumo base style. no_base_style (:obj:`bool`, optional): Prevent use of sumo base style. This can make alternative styles behave more predictably. image_format (:obj:`str`, optional): The image file format. Can be any format supported by matplotlib, including: png, jpg, pdf, and svg. Defaults to pdf. dpi (:obj:`int`, optional): The dots-per-inch (pixel density) for the image. plt (:obj:`matplotlib.pyplot`, optional): A :obj:`matplotlib.pyplot` object to use for plotting. fonts (:obj:`list`, optional): Fonts to use in the plot. Can be a a single font, specified as a :obj:`str`, or several fonts, specified as a :obj:`list` of :obj:`str`. Returns: A matplotlib pyplot object.
Parse the element and orbital argument strings. The presence of an element without any orbitals means that we want to plot all of its orbitals. Args: string (str): The element and orbitals as a string, in the form ``"C.s.p,O"``. Returns: dict: The elements and orbitals as a :obj:`dict`. For example:: {'Bi': ['s', 'px', 'py', 'd']}. If an element symbol is included with an empty list, then all orbitals for that species are considered.
Parse the atom string. Args: atoms_string (str): The atoms to plot, in the form ``"C.1.2.3,"``. Returns: dict: The atomic indices over which to sum the DOS. Formatted as:: {Element: [atom_indices]}. Indices are zero indexed for each atomic species. If an element symbol is included with an empty list, then all sites for that species are considered.
Basic support for 3.6's f-strings, in 3.5! Formats "s" using appropriate globals and locals dictionaries. This f-string: f"hello a is {a}" simply becomes f("hello a is {a}") In other words, just throw parentheses around the string, and you're done! Implemented internally using str.format_map(). This means it doesn't support expressions: f("two minus three is {2-3}") And it doesn't support function calls: f("how many elements? {len(my_list)}") But most other f-string features work.
Accepts either a string or an iterable of strings. (Iterable is assumed to be individual lines.) Returns a string.
Find cmd on PATH.
Print help for subcommands. Prints the help text for the specified subcommand. If subcommand is not specified, prints one-line summaries for every command.
Add a blurb (a Misc/NEWS entry) to the current CPython repo.
Move all new blurbs to a single blurb file for the release. This is used by the release manager when cutting a new release.
Merge all blurbs together into a single Misc/NEWS file. Optional output argument specifies where to write to. Default is <cpython-root>/Misc/NEWS. If overwriting, blurb merge will prompt you to make sure it's okay. To force it to overwrite, use -f.
Creates and populates the Misc/NEWS.d directory tree.
Split the current Misc/NEWS into a zillion little blurb files. Assumes that the newest version section in Misc/NEWS is under development, and splits those entries into the "next" subdirectory. If the current version has actually been released, use the --released flag. Also runs "blurb populate" for you.
Parses a string. Appends a list of blurb ENTRIES to self, as tuples: (metadata, body) metadata is a dict. body is a string.
Read a blurb file. Broadly equivalent to blurb.parse(open(filename).read()).
Parses a "next" filename into its equivalent blurb metadata. Returns a dict.
changes metadata!
Save out blurbs created from "blurb split". They don't have dates, so we have to get creative.
cherry-pick COMMIT_SHA1 into target BRANCHES.
return '2.7' from 'backport-sha-2.7' raises ValueError if the specified branch name is not of a form that cherry_picker would have created
Validate that a hexdigest sha is a valid commit in the repo raises ValueError if the sha does not reference a commit within the repo
return version information from a git branch name
Return the current branch
Return a tuple of title and body from the commit message
Check whether the current folder is a Git repo.
Locate and return the default config for current revison.
Choose and return the config path and it's contents as dict.
Save a set of options into Git config.
Remove a set of options from Git config.
Retrieve one option from Git config.
Retrieve given file path contents of certain Git revision.
Save paused progress state into Git config.
Get the remote name to use for upstream branches Uses "upstream" if it exists, "origin" otherwise
git fetch <upstream>
git checkout -b <branch_name>
Return the commit message for the current commit hash, replace #<PRID> with GH-<PRID>
git checkout default branch
git cherry-pick -x <commit_sha1>
prefix the commit message with (X.Y)
git push <origin> <branchname>
Create PR in GitHub
open url in the web browser
Remove the temporary backport branch. Switch to the default branch before that.
run `git cherry-pick --abort` and then clean up the branch
git push origin <current_branch> open the PR clean up branch
Return the run progress state stored in the Git config. Raises ValueError if the retrieved state is not of a form that cherry_picker would have stored in the config.
Open a file-like object using a pkg relative path. Example: fd = openDatFile('foopkg.barpkg/wootwoot.bin')
Scrape types from a blob of text and return node tuples. Args: text (str): Text to scrape. ptype (str): Optional ptype to scrape. If present, only scrape rules which match the provided type. Returns: (str, str): Yield tuples of type, valu strings.
Use msgpack to serialize a compatible python object. Args: item (obj): The object to serialize Notes: String objects are encoded using utf8 encoding. In order to handle potentially malformed input, ``unicode_errors='surrogatepass'`` is set to allow encoding bad input strings. Returns: bytes: The serialized bytes in msgpack format.
Use msgpack to de-serialize a python object. Args: byts (bytes): The bytes to de-serialize Notes: String objects are decoded using utf8 encoding. In order to handle potentially malformed input, ``unicode_errors='surrogatepass'`` is set to allow decoding bad input strings. Returns: obj: The de-serialized object
Generator which unpacks a file object of msgpacked content. Args: fd: File object to consume data from. Notes: String objects are decoded using utf8 encoding. In order to handle potentially malformed input, ``unicode_errors='surrogatepass'`` is set to allow decoding bad input strings. Yields: Objects from a msgpack stream.
Generator which yields msgpack objects from a file path. Args: path: File path to open and consume data from. Notes: String objects are decoded using utf8 encoding. In order to handle potentially malformed input, ``unicode_errors='surrogatepass'`` is set to allow decoding bad input strings. Yields: Objects from a msgpack stream.
Dump an object to a file by path. Args: item (object): The object to serialize. path (str): The file path to save. Returns: None
Feed bytes to the unpacker and return completed objects. Args: byts (bytes): Bytes to unpack. Notes: It is intended that this function is called multiple times with bytes from some sort of a stream, as it will unpack and return objects as they are available. Returns: list: List of tuples containing the item size and the unpacked item.
Clear the cached model rows and rebuild them only if they have been loaded already.
Get byts for a message
Calculate the imei check byte.
Execute a function in an executor thread. Args: todo ((func,args,kwargs)): A todo tuple.
Initializes (or re-initializes for testing purposes) all of a task's task-local variables Precondition: If task is None, this must be called from task context
Note: No locking is provided. Under normal circumstances, like the other task is not running (e.g. this is running from the same event loop as the task) or task is the current task, this is fine.
Access a task local variable by name Precondition: If task is None, this must be called from task context
Walk the phone info tree to find the best-match info for the given number. Example: info = getPhoneInfo(17035551212) country = info.get('cc')
Open a HiveDict at the given full path.
Load a node from storage into the tree. ( used by initialization routines to build the tree)
A set operation at the hive level (full path).
Atomically increments a node's value.
Remove and return the value for the given node.
Add the structured data from items to the CryoTank. Args: items (list): A list of objects to store in the CryoTank. seqn (iden, offs): An iden / offset pair to record. Returns: int: The ending offset of the items or seqn.
Yield metrics rows starting at offset. Args: offs (int): The index offset. size (int): The maximum number of records to yield. Yields: ((int, dict)): An index offset, info tuple for metrics.
Yield a number of items from the CryoTank starting at a given offset. Args: offs (int): The index of the desired datum (starts at 0) size (int): The max number of items to yield. Yields: ((index, object)): Index and item values.
Yield a number of raw items from the CryoTank starting at a given offset. Args: offs (int): The index of the desired datum (starts at 0) size (int): The max number of items to yield. Yields: ((indx, bytes)): Index and msgpacked bytes.
Returns information about the CryoTank instance. Returns: dict: A dict containing items and metrics indexes.
Generate a new CryoTank with a given name or get an reference to an existing CryoTank. Args: name (str): Name of the CryoTank. Returns: CryoTank: A CryoTank instance.
Get a list of (name, info) tuples for the CryoTanks. Returns: list: A list of tufos.
Argparse expects exit() to be a terminal function and not return. As such, this function must raise an exception which will be caught by Cmd.hasValidOpts.
Note: this overrides an existing method in ArgumentParser
Generate a uniq hash for the JSON compatible primitive data structure.
Argparse expects exit() to be a terminal function and not return. As such, this function must raise an exception instead.
Retrieve volume usage info for the given path.
Consume chars in set from the string and return (subtxt,offset). Example: text = "foo(bar)" chars = set('abcdefghijklmnopqrstuvwxyz') name,off = nom(text,0,chars)
Parse a list (likely for comp type) coming from a command line input. The string elements within the list may optionally be quoted.
Parse in a command line string which may be quoted.
Special syntax for the right side of equals in a macro
Parse a foo:bar=<valu> kwarg into (prop,valu),off
Parse a foo:bar=<valu>[,...] kwarg list into (prop,valu),off
A storm sub-query aware command line splitter. ( not for storm commands, but for commands which may take storm )
Consume and return one command argument, stopping when it hits a character (not in a quotation) in `until`.
foo:bar = hehe
:foo=10
.foo = bar
<- * / <- prop
<+- * / <+- prop
-> * -> #tag.match -> form:prop -> form
Ignore whitespace as well as comment syntax ``//`` and ``/* ... */``
:foo:bar -> baz:faz
:foo:bar -+> baz:faz
foo:bar
:foo:bar
.foo
$foo $foo.bar $foo.bar() $foo[0] $foo.bar(10)
cmdargv *must* have leading whitespace to prevent foo@bar from becoming cmdname foo with argv=[@bar]
Parse a time string into an epoch millis value.
Return a date string for an epoch-millis timestamp. Args: tick (int): The timestamp in milliseconds since the epoch. Returns: (str): A date time string
Parse a simple time delta string and return the delta.
Encrypt the given bytes and return an envelope dict in msgpack form. Args: byts (bytes): The message to be encrypted. asscd (bytes): Extra data that needs to be authenticated (but not encrypted). Returns: bytes: The encrypted message. This is a msgpacked dictionary containing the IV, ciphertext, and associated data.
Decode an envelope dict and decrypt the given bytes. Args: byts (bytes): Bytes to decrypt. Returns: bytes: Decrypted message.
Wrap a message with a sequence number and encrypt it. Args: mesg: The mesg to encrypt. Returns: bytes: The encrypted message.
Decrypt a message, validating its sequence number is as we expect. Args: ciphertext (bytes): The message to decrypt and verify. Returns: mesg: A mesg. Raises: s_exc.CryptoErr: If the message decryption fails or the sequence number was unexpected.
Parse a Semantic Version string into is component parts. Args: text (str): A text string to parse into semver components. This string has whitespace and leading 'v' characters stripped off of it. Examples: Parse a string into it semvar parts:: parts = parseSemver('v1.2.3') Returns: dict: The dictionary will contain the keys 'major', 'minor' and 'patch' pointing to integer values. The dictionary may also contain keys for 'build' and 'pre' information if that data is parsed out of a semver string. None is returned if the string is not a valid Semver string.
Pack a set of major/minor/patch integers into a single integer for storage. Args: major (int): Major version level integer. minor (int): Minor version level integer. patch (int): Patch version level integer. Returns: int: System normalized integer value to represent a software version.
Unpack a system normalized integer representing a softare version into its component parts. Args: ver (int): System normalized integer value to unpack into a tuple. Returns: (int, int, int): A tuple containing the major, minor and patch values shifted out of the integer.
Join a string of parts together with a . separator. Args: *vsnparts: Returns:
Extract a list of major/minor/version integer strings from a string. Args: text (str): String to parse seps (tuple): A tuple or list of separators to use when parsing the version string. Examples: Parse a simple version string into a major and minor parts:: parts = parseVersionParts('1.2') Parse a complex version string into a major and minor parts:: parts = parseVersionParts('wowsoft_1.2') Parse a simple version string into a major, minor and patch parts. Parts after the "3." are dropped from the results:: parts = parseVersionParts('1.2.3.4.5') Notes: This attempts to brute force out integers from the version string by stripping any leading ascii letters and part separators, and then regexing out numeric parts optionally followed by part separators. It will stop at the first mixed-character part encountered. For example, "1.2-3a" would only parse out the "1" and "2" from the string. Returns: dict: Either a empty dictionary or dictionary containing up to three keys, 'major', 'minor' and 'patch'.
Set a name in the SlabDict. Args: name (str): The key name. valu (obj): A msgpack compatible value. Returns: None
Pop a name from the SlabDict. Args: name (str): The name to remove. defval (obj): The default value to return if the name is not present. Returns: object: The object stored in the SlabDict, or defval if the object was not present.
Note: This method may raise a MapFullError
Deletes an **entire database** (i.e. a table), losing all data.
Return the last key/value pair from the given db.
Returns: Tuple of number of items consumed, number of items added
Copy an entire database in this slab to a new database in potentially another slab. Args: sourcedb (LmdbDatabase): which database in this slab to copy rows from destslab (LmdbSlab): which slab to copy rows to destdbname (str): the name of the database to copy rows to in destslab progresscb (Callable[int]): if not None, this function will be periodically called with the number of rows completed Returns: (int): the number of rows copied Note: If any rows already exist in the target database, this method returns an error. This means that one cannot use destdbname=None unless there are no explicit databases in the destination slab.
Like put, but returns the previous value if existed
Note: This method may raise a MapFullError
Retrieve the ipv4 address for this host ( optionally as seen from dest ). Example: addr = s_socket.hostaddr()
Returns the iden that starts with prefix. Prints out error and returns None if it doesn't match exactly one.
Decode the given byts with the named decoder. If name is a comma separated list of decoders, loop through and do them all. Example: byts = s_encoding.decode('base64',byts) Note: Decoder names may also be prefixed with + to *encode* for that name/layer.
Add an additional ingest file format
Iterate through the data provided by a file like object. Optional parameters may be used to control how the data is deserialized. Examples: The following example show use of the iterdata function.:: with open('foo.csv','rb') as fd: for row in iterdata(fd, format='csv', encoding='utf8'): dostuff(row) Args: fd (file) : File like object to iterate over. close_fd (bool) : Default behavior is to close the fd object. If this is not true, the fd will not be closed. **opts (dict): Ingest open directive. Causes the data in the fd to be parsed according to the 'format' key and any additional arguments. Yields: An item to process. The type of the item is dependent on the format parameters.
Remove any v0 (i.e. pre-010) rules from storage and replace them with v1 rules. Notes: v0 had two differences user was a username. Replaced with iden of user as 'iden' field. Also 'iden' was storage as binary. Now it is stored as hex string.
Initialize a new (min,max) tuple interval from values. Args: *vals ([int,...]): A list of values (or Nones) Returns: ((int,int)): A (min,max) interval tuple or None
Determine if two interval tuples have overlap. Args: iv0 ((int,int)): An interval tuple iv1 ((int,int)); An interval tuple Returns: (bool): True if the intervals overlap, otherwise False
Parse an interval time string and return a (min,max) tuple. Args: text (str): A time interval string Returns: ((int,int)): A epoch millis epoch time string
Get a proxy to a cortex backed by a temporary directory. Args: mods (list): A list of modules which are loaded into the cortex. Notes: The cortex and temporary directory are town down on exit. This should only be called from synchronous code. Returns: Proxy to the cortex.
Set the view layers from a list of idens. NOTE: view layers are stored "top down" ( write is layers[0] )
Yield Node.pack() tuples which match the query.
Adds a trigger to the cortex
Check that, as a non-admin, may only manipulate resources created by you.
Deletes a trigger from the cortex
Change an existing trigger's query
Lists all the triggers that the current user is authorized to access
Add a cron job to the cortex A cron job is a persistently-stored item that causes storm queries to be run in the future. The specification for the times that the queries run can be one-shot or recurring. Args: query (str): The storm query to execute in the future reqs (Union[Dict[str, Union[int, List[int]]], List[Dict[...]]]): Either a dict of the fixed time fields or a list of such dicts. The keys are in the set ('year', 'month', 'dayofmonth', 'dayofweek', 'hour', 'minute'. The values must be positive integers, except for the key of 'dayofmonth' in which it may also be a negative integer which represents the number of days from the end of the month with -1 representing the last day of the month. All values may also be lists of valid values. incunit (Optional[str]): A member of the same set as above, with an additional member 'day'. If is None (default), then the appointment is one-shot and will not recur. incval (Union[int, List[int]): A integer or a list of integers of the number of units Returns (bytes): An iden that can be used to later modify, query, and delete the job. Notes: reqs must have fields present or incunit must not be None (or both) The incunit if not None it must be larger in unit size than all the keys in all reqs elements.
Delete a cron job Args: iden (bytes): The iden of the cron job to be deleted
Change an existing cron job's query Args: iden (bytes): The iden of the cron job to be changed
Get information about all the cron jobs accessible to the current user
Add a tag to a node specified by iden. Args: iden (str): A hex encoded node BUID. tag (str): A tag string. valu (tuple): A time interval tuple or (None, None).
Add a list of packed nodes to the cortex. Args: nodes (list): [ ( (form, valu), {'props':{}, 'tags':{}}), ... ] Yields: (tuple): Packed node tuples ((form,valu), {'props': {}, 'tags':{}})
Count the number of nodes which result from a storm query. Args: text (str): Storm query text. opts (dict): Storm query options. Returns: (int): The number of nodes resulting from the query.
Evalute a storm query and yield packed nodes.
Evaluate a storm query and yield result messages. Yields: ((str,dict)): Storm messages.
Return the list of splices at the given offset.
Return stream of (iden, provenance stack) tuples at the given offset.
Return the providence stack associated with the given iden. Args: iden (str): the iden from splice Note: the iden appears on each splice entry as the 'prov' property
Registration for built-in Storm commands.
Registration for built-in Storm Libraries
Registration for splice handlers.
Registration for built-in Layer ctors
Registration for built-in Cortex feed functions.
Registration for built-in Cortex httpapi endpoints
Recalculate form counts from scratch.
Register a callback for tag addition. Args: name (str): The name of the tag or tag glob. func (function): The callback func(node, tagname, tagval).
Unregister a callback for tag addition. Args: name (str): The name of the tag or tag glob. func (function): The callback func(node, tagname, tagval).
Register a callback for tag deletion. Args: name (str): The name of the tag or tag glob. func (function): The callback func(node, tagname, tagval).
Unregister a callback for tag deletion. Args: name (str): The name of the tag or tag glob. func (function): The callback func(node, tagname, tagval).
Execute a runt lift function. Args: full (str): Property to lift by. valu: cmpr: Returns: bytes, list: Yields bytes, list tuples where the list contains a series of key/value pairs which are used to construct a Node object.
Delete a cortex view by iden.
Args: layers ([str]): A top-down list of of layer guids iden (str): The view iden ( defaults to default view ).
Add a Layer to the cortex. Notes: The addLayer ``**info`` arg is expected to be shaped like the following:: info = { 'iden': <str>, ( optional iden. default guid() ) 'type': <str>, ( optional type. default lmdb ) 'owner': <str>, ( optional owner. default root ) 'config': {}, # type specific config options. }
Convenience function to join a remote telepath layer into this cortex and default view.
Add a synapse.lib.storm.Cmd class to the cortex.
feeds: - cryotank: tcp://cryo.vertex.link/cryo00/tank01 type: syn.splice
Get a list of packed nodes from a ingest definition.
Evaluate a storm query and yield Nodes only.
Evaluate a storm query and yield (node, path) tuples. Yields: (Node, Path) tuples
A simple non-streaming way to return a list of nodes.
Evaluate a storm query and yield result messages. Yields: ((str,dict)): Storm messages.
Parse storm query text and return a Query object.
Log a storm query.
Return a single Node() instance by (form,valu) tuple.
Get nodes by a property value or lift syntax. Args: full (str): The full name of a property <form>:<prop>. valu (obj): A value that the type knows how to lift by. cmpr (str): The comparison operator you are lifting by. Some node property types allow special syntax here. Examples: # simple lift by property equality core.getNodesBy('file:bytes:size', 20) # The inet:ipv4 type knows about cidr syntax core.getNodesBy('inet:ipv4', '1.2.3.0/24')
Quickly add/modify a list of nodes from node definition tuples. This API is the simplest/fastest way to add nodes, set node props, and add tags to nodes remotely. Args: nodedefs (list): A list of node definition tuples. See below. A node definition tuple is defined as: ( (form, valu), {'props':{}, 'tags':{}) The "props" or "tags" keys may be omitted.
Add data using a feed/parser function. Args: name (str): The name of the feed record format. items (list): A list of items to ingest. seqn ((str,int)): An (iden, offs) tuple for this feed chunk. Returns: (int): The next expected offset (or None) if seqn is None.
Return a transaction object for the default view. Args: write (bool): Set to True for a write transaction. Returns: (synapse.lib.snap.Snap) NOTE: This must be used in a with block.
Load a single cortex module with the given ctor and conf. Args: ctor (str): The python module class path conf (dict):Config dictionary for the module
Get the normalized property value based on the Cortex data model. Args: prop (str): The property to normalize. valu: The value to normalize. Returns: (tuple): A two item tuple, containing the normed value and the info dictionary. Raises: s_exc.NoSuchProp: If the prop does not exist. s_exc.BadTypeValu: If the value fails to normalize.
Get the normalized type value based on the Cortex data model. Args: name (str): The type to normalize. valu: The value to normalize. Returns: (tuple): A two item tuple, containing the normed value and the info dictionary. Raises: s_exc.NoSuchType: If the type does not exist. s_exc.BadTypeValu: If the value fails to normalize.
Execute a series of storage operations. Overrides implementation in layer.py to avoid unnecessary async calls.
Check for any pre-010 entries in 'dbname' in my slab and migrate those to the new slab. Once complete, drop the database from me with the name 'dbname' Returns (bool): True if a migration occurred, else False
Check for any pre-010 provstacks and migrate those to the new slab.
Migration-only method Notes: Precondition: buid cache must be disabled
Migration-only method Notes: Precondition: buid cache must be disabled
Migration-only function
Iterate (buid, valu) rows for the given form in this layer.
Iterate (buid, valu) rows for the given form:prop in this layer.
Iterate (buid, valu) rows for the given universal prop
Resolve a telepath alias via ~/.syn/aliases.yaml Args: name (str): Name of the alias to resolve. Notes: An exact match against the aliases will always be returned first. If no exact match is found and the name contains a '/' in it, the value before the slash is looked up and the remainder of the path is joined to any result. This is done to support dynamic Telepath share names. Returns: str: The url string, if present in the alias. None will be returned if there are no matches.
Open a URL to a remote telepath object. Args: url (str): A telepath URL. **opts (dict): Telepath connect options. Returns: (synapse.telepath.Proxy): A telepath proxy object. The telepath proxy may then be used for sync or async calls: proxy = openurl(url) value = proxy.getFooThing() ... or ... proxy = await openurl(url) valu = await proxy.getFooThing() ... or ... async with await openurl(url) as proxy: valu = await proxy.getFooThing()
Call a remote method by name. Args: methname (str): The name of the remote method. *args: Arguments to the method call. **kwargs: Keyword arguments to the method call. Most use cases will likely use the proxy methods directly: The following two are effectively the same: valu = proxy.getFooBar(x, y) valu = proxy.call('getFooBar', x, y)
Returns True if the rate limit has not been reached. Example: if not rlimit.allows(): rasie RateExceeded() # ok to go...
Disable and invalidate the layer buid cache for migration
Returns: Iterable[Tuple[bytes, Dict[str, Any]]]: yield a stream of tuple (buid, propdict)
Return a list of "fully qualified" class names for an instance. Example: for name in getClsNames(foo): print(name)
Return a fully qualified string for the <mod>.<class>.<func> name of a given method.
Iterate the locals of an item and yield (name,valu) pairs. Example: for name,valu in getItemLocals(item): dostuff()
Get a dictionary of special annotations for a Telepath Proxy. Args: item: Item to inspect. Notes: This will set the ``_syn_telemeth`` attribute on the item and the items class, so this data is only computed once. Returns: dict: A dictionary of methods requiring special handling by the proxy.
Switch to another user (admin only). This API allows remote admin/service accounts to impersonate a user. Used mostly by services that manage their own authentication/sessions.
Get the value of a key in the cell default hive
Set or change the value of a key in the cell default hive
Remove and return the value of a key in the cell default hive
Set the admin status of the given user/role.
An admin only API endpoint for getting user info.
A signal handler used to print asyncio task stacks and thread stacks.
Schedule a coroutine to run on the global loop and return it's result. Args: coro (coroutine): The coroutine instance. Notes: This API is thread safe and should only be called by non-loop threads.
The synchelp decorator allows the transparent execution of a coroutine using the global loop from a thread other than the event loop. In both use cases, teh actual work is done by the global event loop. Examples: Use as a decorator:: @s_glob.synchelp async def stuff(x, y): await dostuff() Calling the stuff function as regular async code using the standard await syntax:: valu = await stuff(x, y) Calling the stuff function as regular sync code outside of the event loop thread:: valu = stuff(x, y)
Try to match a day-of-week abbreviation, then try a day-of-week full name
Parse a non-day increment value. Should be an integer or a comma-separated integer list.
Parse a non-day fixed value
Parse a --day argument
Prints details about a particular cron job. Not actually a different API call
Promote the currently running task.
Create a synapse task from the given coroutine.
Add a function/coroutine/Base to be called on fini().
Remove a callback function previously added with link() Example: base.unlink( callback )
Add an base function callback for a specific event with optional filtering. If the function returns a coroutine, it will be awaited. Args: evnt (str): An event name func (function): A callback function to receive event tufo Examples: Add a callback function and fire it: async def baz(event): x = event[1].get('x') y = event[1].get('y') return x + y d.on('foo', baz) # this fire triggers baz... await d.fire('foo', x=10, y=20) Returns: None:
Remove a previously registered event handler function. Example: base.off( 'foo', onFooFunc )
Fire the given event name on the Base. Returns a list of the return values of each callback. Example: for ret in d.fire('woot',foo='asdf'): print('got: %r' % (ret,))
Distribute an existing event tuple. Args: mesg ((str,dict)): An event tuple. Example: await base.dist( ('foo',{'bar':'baz'}) )
Shut down the object and notify any onfini() coroutines. Returns: Remaining ref count
A context manager which can be used to add a callback and remove it when using a ``with`` statement. Args: evnt (str): An event name func (function): A callback function to receive event tufo
Wait for the base to fini() Returns: None if timed out, True if fini happened Example: base.waitfini(timeout=30)
Schedules a free-running coroutine to run on this base's event loop. Kills the coroutine if Base is fini'd. It does not pend on coroutine completion. Precondition: This function is *not* threadsafe and must be run on the Base's event loop Returns: asyncio.Task: An asyncio.Task object.
Schedules a coroutine to run as soon as possible on the same event loop that this Base is running on Note: This method may *not* be run inside an event loop
Register SIGTERM/SIGINT signal handlers with the ioloop to fini this object.
Construct and return a new Waiter for events on this base. Example: # wait up to 3 seconds for 10 foo:bar events... waiter = base.waiter(10,'foo:bar') # .. fire thread that will cause foo:bar events events = waiter.wait(timeout=3) if events == None: # handle the timout case... for event in events: # parse the events if you need... NOTE: use with caution... it's easy to accidentally construct race conditions with this mechanism ;)
Wait for the required number of events and return them or None on timeout. Example: evnts = waiter.wait(timeout=30) if evnts == None: handleTimedOut() return for evnt in evnts: doStuff(evnt)
Add a Base (or sub-class) to the BaseRef by name. Args: name (str): The name/iden of the Base base (Base): The Base instance Returns: (None)
Atomically get/gen a Base and incref. (requires ctor during BaseRef init) Args: name (str): The name/iden of the Base instance.
Get a 16 byte guid value. By default, this is a random guid value. Args: valu: Object used to construct the guid valu from. This must be able to be msgpack'd. Returns: str: 32 character, lowercase ascii string.
A binary GUID like sequence of 32 bytes. Args: valu (object): Optional, if provided, the hash of the msgpack encoded form of the object is returned. This can be used to create stable buids. Notes: By default, this returns a random 32 byte value. Returns: bytes: A 32 byte value.
Ensure ( or coerce ) a value into being an integer or None. Args: x (obj): An object to intify Returns: (int): The int value ( or None )
Create or open ( for read/write ) a file path join. Args: *paths: A list of paths to join together to make the file. Notes: If the file already exists, the fd returned is opened in ``r+b`` mode. Otherwise, the fd is opened in ``w+b`` mode. Returns: io.BufferedRandom: A file-object which can be read/written too.
A file lock with-block helper. Args: path (str): A path to a lock file. Examples: Get the lock on a file and dostuff while having the lock:: path = '/hehe/haha.lock' with lockfile(path): dostuff() Notes: This is curently based on fcntl.lockf(), and as such, it is purely advisory locking. If multiple processes are attempting to obtain a lock on the same file, this will block until the process which has the current lock releases it. Yields: None
List the (optionally glob filtered) full paths from a dir. Args: *paths ([str,...]): A list of path elements glob (str): An optional fnmatch glob str
Combines/creates a yaml file and combines with obj. obj and file must be maps/dict or empty.
Get an err tufo from an exception. Args: e (Exception): An Exception (or Exception subclass). Notes: This can be called outside of the context of an exception handler, however details such as file, line, function name and source may be missing. Returns: ((str, dict)):
Populate err,errmsg,errtrace info from exc.
Divide an iterable into chunks. Args: item: Item to slice size (int): Maximum chunk size. Notes: This supports Generator objects and objects which support calling the __getitem__() method with a slice object. Yields: Slices of the item containing up to "size" number of items.
Generator which yields bytes from a file descriptor. Args: fd (file): A file-like object to read bytes from. size (int): Size, in bytes, of the number of bytes to read from the fd at a given time. Notes: If the first read call on the file descriptor is a empty bytestring, that zero length bytestring will be yielded and the generator will then be exhuasted. This behavior is intended to allow the yielding of contents of a zero byte file. Yields: bytes: Bytes from the file descriptor.
A decorator for making a function fire a thread.
Configure synapse logging. Args: mlogger (logging.Logger): Reference to a logging.Logger() defval (str): Default log level Notes: This calls logging.basicConfig and should only be called once per process. Returns: None
Return a value or raise an exception from a retn tuple.
Initialize a config dict using the given confdef tuples.
Migrate old cell Auth() data into a HiveAuth().
Fire the onset() handlers for this property. Args: node (synapse.lib.node.Node): The node whose property was set. oldv (obj): The previous value of the property.
Get a list of storage operations to delete this property from the buid. Args: buid (bytes): The node buid. Returns: (tuple): The storage operations
Fire the onAdd() callbacks for node creation.
Fire the onDel() callbacks for node deletion.
Get a set of lift operations for use with an Xact.
Adds a model definition (same format as input to Model.addDataModels and output of Model.getModelDef).
Add a list of (name, mdef) tuples. A model definition (mdef) is structured as follows:: { "ctors":( ('name', 'class.path.ctor', {}, {'doc': 'The foo thing.'}), ), "types":( ('name', ('basetype', {typeopts}), {info}), ), "forms":( (formname, (typename, typeopts), {info}, ( (propname, (typename, typeopts), {info}), )), ), "univs":( (propname, (typename, typeopts), {info}), ) } Args: mods (list): The list of tuples. Returns: None
Add a Type instance to the data model.
Bind and listen on the given host/port with possible SSL. Args: host (str): A hostname or IP address. port (int): The TCP port to bind.
Share an object via the telepath protocol. Args: name (str): Name of the shared object item (object): The object to share over telepath.
Returns a valid day of the month given the desired value. Negative values are interpreted as offset backwards from the last day of the month, with -1 representing the last day of the month. Out-of-range values are clamped to the first or last day of the month.
Make ApptRec json/msgpack-friendly
Convert from json/msgpack-friendly
Returns next timestamp that meets requirements, incrementing by (self.incunit * incval) if not increasing, or 0.0 if there are no future matches
Return a datetime incremented by incunit * incval
Find the next time this appointment should be scheduled. Delete any nonrecurring record that just happened.
Enable cron jobs to start running, start the scheduler loop Go through all the appointments, making sure the query is valid, and remove the ones that aren't. (We can't evaluate queries until enabled because not all the modules are loaded yet.)
Load all the appointments from persistent storage
Updates the data structures to add an appointment
Store a single appointment
Yields a series of dicts that cover the combination of all multiple-value (e.g. lists or tuples) values, with non-multiple-value values remaining the same.
Persistently adds an appointment Args: query (str): storm query to run reqs (Union[None, Dict[TimeUnit, Union[int, Tuple[int]], List[...]): one or more dicts of the fixed aspects of the appointment. dict value may be a single or multiple. May be an empty dict or None. incunit (Union[None, TimeUnit]): the unit that changes for recurring, or None for non-recurring. It is an error for this value to match a key in reqdict. incvals (Union[None, int, Iterable[int]): count of units of incunit or explicit day of week or day of month. Not allowed for incunit == None, required for others (1 would be a typical value) Notes: For values in reqs that are lists and incvals if a list, all combinations of all values (the product) are used Returns: iden of new appointment
Change the query of an appointment
Delete an appointment
Task loop to issue query tasks at the right times.
Fire off the task to make the storm query
Actually run the storm query, updating the appropriate statistics and results
Return an object from the embedded synapse data folder. Example: for tld in syanpse.data.get('iana.tlds'): dostuff(tld) NOTE: Files are named synapse/data/<name>.mpk
Return the valu of a given property on the node. Args: pode (tuple): A packed node. prop (str): Property to retrieve. Notes: The prop argument may be the full property name (foo:bar:baz), relative property name (:baz) , or the unadorned property name (baz). Returns:
Get all the tags for a given node. Args: pode (tuple): A packed node. leaf (bool): If True, only return the full tags. Returns: list: A list of tag strings.
Check if a packed node has a given tag. Args: pode (tuple): A packed node. tag (str): The tag to check. Examples: Check if a node is tagged with "woot" and dostuff if it is. if s_node.tagged(node,'woot'): dostuff() Notes: If the tag starts with `#`, this is removed prior to checking. Returns: bool: True if the tag is present. False otherwise.
Args: path (Path): If set, then vars from path are copied into the new runtime, and vars are copied back out into path at the end Note: If opts is not None and opts['vars'] is set and path is not None, then values of path vars take precedent
Return the serializable/packed version of the node. Returns: (tuple): An (iden, info) node tuple.
Update the .seen interval and optionally a source specific seen node.
Return a list of (prop, (form, valu)) refs out for the node.
Set a property on the node. Args: name (str): The name of the property. valu (obj): The value of the property. init (bool): Set to True to disable read-only enforcement Returns: (bool): True if the property was changed.
Generate operations to set a property on a node.
Return a secondary property value from the Node. Args: name (str): The name of a secondary property. Returns: (obj): The secondary property value or None.
Remove a property from a node and return the value
Delete a tag from the node.
Delete a node from the cortex. The following tear-down operations occur in order: * validate that you have permissions to delete the node * validate that you have permissions to delete all tags * validate that there are no remaining references to the node. * delete all the tags (bottom up) * fire onDelTag() handlers * delete tag properties from storage * log tag:del splices * delete all secondary properties * fire onDelProp handler * delete secondary property from storage * log prop:del splices * delete the primary property * fire onDel handlers for the node * delete primary property from storage * log node:del splices
Returns: a regular expression string with ** and * interpreted as tag globs Precondition: tag is a valid tagmatch Notes: A single asterisk will replace exactly one dot-delimited component of a tag A double asterisk will replace one or more of any character. The returned string does not contain a starting '^' or trailing '$'.
Note: we override default impl from parent to avoid costly KeyError
A helper method for Type subclasses to use for a simple way to truncate indx bytes.
Compare the two values using the given type specific comparator.
Normalize the value for a given type. Args: valu (obj): The value to normalize. Returns: ((obj,dict)): The normalized valu, info tuple. Notes: The info dictionary uses the following key conventions: subs (dict): The normalized sub-fields as name: valu entries.
Return the property index bytes for the given *normalized* value.
Extend this type to construct a sub-type. Args: name (str): The name of the new sub-type. opts (dict): The type options for the sub-type. info (dict): The type info for the sub-type. Returns: (synapse.types.Type): A new sub-type instance.
Create a new instance of this type with the specified options. Args: opts (dict): The type specific options for the new instance.
Return a list of index operation tuples to lift values in a table. Valid index operations include: ('eq', <indx>) ('pref', <indx>) ('range', (<minindx>, <maxindx>))
Get a tick, tock time pair. Args: vals (list): A pair of values to norm. Returns: (int, int): A ordered pair of integers.
Override default ``*range=`` handler to account for relative computation.
Override default *range= handler to account for relative computation.
Async connect and return a Link().
Listen on the given host/port and fire onlink(Link). Returns a server object that contains the listening sockets
Start an PF_UNIX server listening on the given path.
Connect to a PF_UNIX server listening on the given path.
Async transmit routine which will wait for writer drain().
Return sibling node by relative offset from self.
Yield "rightward" siblings until None.
Execute a non-coroutine function in the ioloop executor pool. Args: func: Function to execute. *args: Args for the function. **kwargs: Kwargs for the function. Examples: Execute a blocking API call in the executor pool:: import requests def block(url, params=None): return requests.get(url, params=params).json() fut = s_coro.executor(block, 'http://some.tld/thign') resp = await fut Returns: asyncio.Future: An asyncio future.
Wait on an an asyncio event with an optional timeout Returns: true if the event got set, None if timed out
Calls func and awaits it if a returns a coroutine. Note: This is useful for implementing a function that might take a telepath proxy object or a local object, and you must call a non-async method on that object. This is also useful when calling a callback that might either be a coroutine function or a regular function. Usage: ok = await s_coro.ornot(maybeproxy.allowed, 'path')
Ensure a string is valid hex. Args: text (str): String to normalize. Examples: Norm a few strings: hexstr('0xff00') hexstr('ff00') Notes: Will accept strings prefixed by '0x' or '0X' and remove them. Returns: str: Normalized hex string.
Divide a normalized tag string into hierarchical layers.
Dynamically import a python module and return a local. Example: cls = getDynLocal('foopkg.barmod.BlahClass') blah = cls()
Retrieve and return an unbound method by python path.
Dynamically import a python module or exception.
Dynamically import a module and return a module local or raise an exception.
Run a dynamic task and return the result. Example: foo = runDynTask( ('baz.faz.Foo', (), {} ) )
Hold this during a series of renames to delay ndef secondary property processing until the end....
Rename a form within all the layers.
Change all props as a result of an ndef change.
Produce a deconflicted list of form values across layers as a *copy* to avoid iter vs edit issues in the indexes.
Yield packed node tuples for the given storm query text.
Execute a storm query and yield (Node(), Path()) tuples.
Run a storm query and yield Node() objects.
Retrieve a node tuple by binary id. Args: buid (bytes): The binary ID for the node. Returns: Optional[s_node.Node]: The node object or None.
Return a single Node by (form,valu) tuple. Args: ndef ((str,obj)): A (form,valu) ndef tuple. valu must be normalized. Returns: (synapse.lib.node.Node): The Node or None.
The main function for retrieving nodes by prop. Args: full (str): The property/tag name. valu (obj): A lift compatible value for the type. cmpr (str): An optional alternate comparator. Yields: (synapse.lib.node.Node): Node instances.
Add a node by form name and value with optional props. Args: name (str): The form of node to add. valu (obj): The value for the node. props (dict): Optional secondary properties for the node.
Call a feed function and return what it returns (typically yields Node()s). Args: name (str): The name of the feed record type. items (list): A list of records of the given feed type. Returns: (object): The return value from the feed function. Typically Node() generator.
Add a node via (form, norm, info, buid) and add ops to editatom
return a form, norm, info, buid tuple
Add/merge nodes in bulk. The addNodes API is designed for bulk adds which will also set properties and add tags to existing nodes. Nodes are specified as a list of the following tuples: ( (form, valu), {'props':{}, 'tags':{}}) Args: nodedefs (list): A list of nodedef tuples. Returns: (list): A list of xact messages.
Yield row tuples from a series of lift operations. Row tuples only requirement is that the first element be the binary id of a node. Args: lops (list): A list of lift operations. Yields: (tuple): (layer_indx, (buid, ...)) rows.
Join a row generator into (row, Node()) tuples. A row generator yields tuples of node buid, rawprop dict Args: rows: A generator of (layer_idx, (buid, ...)) tuples. rawprop(str): "raw" propname e.g. if a tag, starts with "#". Used for filtering so that we skip the props for a buid if we're asking from a higher layer than the row was from (and hence, we'll presumable get/have gotten the row when that layer is lifted. cmpf (func): A comparison function used to filter nodes. Yields: (tuple): (row, node)
Return the lowercased name of this module. Notes: This pulls the ``mod_name`` attribute on the class. This allows an implementer to set a arbitrary name for the module. If this attribute is not set, it defaults to ``self.__class__.__name__.lower()`` and sets ``mod_name`` to that value. Returns: (str): The module name.
Construct a path relative to this module's working directory. Args: *paths: A list of path strings Notes: This creates the module specific directory if it does not exist. Returns: (str): The full path (or None if no cortex dir is configured).
Add a ctor callback to the global scope.
Retrieve a value from the closest scope frame.
Add values as iter() compatible items in the current scope frame.
Add a constructor to be called when a specific property is not present. Example: scope.ctor('foo',FooThing) ... foo = scope.get('foo')
Iterate through values added with add() from each scope frame.
Add an item to the queue.
Return a node if it is currently being made, mark as a dependency, else None if none found
Update the shared map with my in-construction node
Allow any other editatoms waiting on me to complete to resume
Wait on the other editatoms who are constructing nodes my new nodes refer to
Push the recorded changes to disk, notify all the listeners
Construct and return a cmdr for the given remote cell. Example: cmdr = await getItemCmdr(foo)
Create a cmdr for the given item and run the cmd loop. Example: runItemCmdr(foo)
Helper for getting a documentation data file paths. Args: fn (str): Name of the file to retrieve the full path for. root (str): Optional root path to look for a docdata in. Notes: Defaults to looking for the ``docdata`` directory in the current working directory. This behavior works fine for notebooks nested in the docs directory of synapse; but this root directory that is looked for may be overridden by providing an alternative root. Returns: str: A file path. Raises: ValueError if the file does not exist or directory traversal attempted..
Get a temporary cortex proxy.
Get a Cmdr instance with prepopulated locs
Get a Telepath Proxt to a Cortex instance which is backed by a temporary Cortex. Args: mods (list): A list of additional CoreModules to load in the Cortex. Notes: The Proxy returned by this should be fini()'d to tear down the temporary Cortex. Returns: s_telepath.Proxy
Get a CmdrCore instance which is backed by a temporary Cortex. Args: mods (list): A list of additional CoreModules to load in the Cortex. outp: A output helper. Will be used for the Cmdr instance. Notes: The CmdrCore returned by this should be fini()'d to tear down the temporary Cortex. Returns: CmdrCore: A CmdrCore instance.
Add feed data to the cortex.
A helper for executing a storm command and getting a list of storm messages. Args: text (str): Storm command to execute. opts (dict): Opt to pass to the cortex during execution. num (int): Number of nodes to expect in the output query. Checks that with an assert statement. cmdr (bool): If True, executes the line via the Cmdr CLI and will send output to outp. Notes: The opts dictionary will not be used if cmdr=True. Returns: list: A list of storm messages.
Emulate a small bit of readline behavior. Returns: (bool) True if current user enabled vi mode ("set editing-mode vi") in .inputrc
Run a line of command input for this command. Args: line (str): Line to execute Examples: Run the foo command with some arguments: await foo.runCmdLine('foo --opt baz woot.com')
Use the _cmd_syntax def to split/parse/normalize the cmd line. Args: text (str): Command to process. Notes: This is implemented independent of argparse (et al) due to the need for syntax aware argument splitting. Also, allows different split per command type Returns: dict: An opts dictionary.
Register SIGINT signal handler with the ioloop to cancel the currently running cmdloop task.
Prompt for user input from stdin.
Add a Cmd subclass to this cli.
Run commands from a user in an interactive fashion until fini() or EOFError is raised.
Run a single command line. Args: line (str): Line to execute. Examples: Execute the 'woot' command with the 'help' switch: await cli.runCmdLine('woot --help') Returns: object: Arbitrary data from the cmd class.
Generates a CA keypair. Args: name (str): The name of the CA keypair. signas (str): The CA keypair to sign the new CA with. outp (synapse.lib.output.Output): The output buffer. Examples: Make a CA named "myca": mycakey, mycacert = cdir.genCaCert('myca') Returns: ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the private key and certificate objects.
Generates a host keypair. Args: name (str): The name of the host keypair. signas (str): The CA keypair to sign the new host keypair with. outp (synapse.lib.output.Output): The output buffer. csr (OpenSSL.crypto.PKey): The CSR public key when generating the keypair from a CSR. sans (list): List of subject alternative names. Examples: Make a host keypair named "myhost": myhostkey, myhostcert = cdir.genHostCert('myhost') Returns: ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the private key and certificate objects.
Generates a user keypair. Args: name (str): The name of the user keypair. signas (str): The CA keypair to sign the new user keypair with. outp (synapse.lib.output.Output): The output buffer. csr (OpenSSL.crypto.PKey): The CSR public key when generating the keypair from a CSR. Examples: Generate a user cert for the user "myuser": myuserkey, myusercert = cdir.genUserCert('myuser') Returns: ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the key and certificate objects.
Generates a user PKCS #12 archive. Please note that the resulting file will contain private key material. Args: name (str): The name of the user keypair. outp (synapse.lib.output.Output): The output buffer. Examples: Make the PKC12 object for user "myuser": myuserpkcs12 = cdir.genClientCert('myuser') Returns: OpenSSL.crypto.PKCS12: The PKCS #12 archive.
Validate the PEM encoded x509 user certificate bytes and return it. Args: byts (bytes): The bytes for the User Certificate. cacerts (tuple): A tuple of OpenSSL.crypto.X509 CA Certificates. Raises: OpenSSL.crypto.X509StoreContextError: If the certificate is not valid. Returns: OpenSSL.crypto.X509: The certificate, if it is valid.
Return a list of CA certs from the CertDir. Returns: [OpenSSL.crypto.X509]: List of CA certificates.
Gets the path to the CA certificate that issued a given host keypair. Args: name (str): The name of the host keypair. Examples: Get the path to the CA cert which issue the cert for "myhost": mypath = cdir.getHostCaPath('myhost') Returns: str: The path if exists.
Gets the path to a host certificate. Args: name (str): The name of the host keypair. Examples: Get the path to the host certificate for the host "myhost": mypath = cdir.getHostCertPath('myhost') Returns: str: The path if exists.
Gets the path to the CA certificate that issued a given user keypair. Args: name (str): The name of the user keypair. Examples: Get the path to the CA cert which issue the cert for "myuser": mypath = cdir.getUserCaPath('myuser') Returns: str: The path if exists.
Gets the name of the first existing user cert for a given user and host. Args: user (str): The name of the user. host (str): The name of the host. Examples: Get the name for the "myuser" user cert at "cool.vertex.link": usercertname = cdir.getUserForHost('myuser', 'cool.vertex.link') Returns: str: The cert name, if exists.
Imports certs and keys into the Synapse cert directory Args: path (str): The path of the file to be imported. mode (str): The certdir subdirectory to import the file into. Examples: Import CA certifciate 'mycoolca.crt' to the 'cas' directory. certdir.importFile('mycoolca.crt', 'cas') Notes: importFile does not perform any validation on the files it imports. Returns: None
Checks if a CA certificate exists. Args: name (str): The name of the CA keypair. Examples: Check if the CA certificate for "myca" exists: exists = cdir.isCaCert('myca') Returns: bool: True if the certificate is present, False otherwise.
Checks if a user client certificate (PKCS12) exists. Args: name (str): The name of the user keypair. Examples: Check if the client certificate "myuser" exists: exists = cdir.isClientCert('myuser') Returns: bool: True if the certificate is present, False otherwise.
Checks if a host certificate exists. Args: name (str): The name of the host keypair. Examples: Check if the host cert "myhost" exists: exists = cdir.isUserCert('myhost') Returns: bool: True if the certificate is present, False otherwise.
Checks if a user certificate exists. Args: name (str): The name of the user keypair. Examples: Check if the user cert "myuser" exists: exists = cdir.isUserCert('myuser') Returns: bool: True if the certificate is present, False otherwise.
Signs a certificate with a CA keypair. Args: cert (OpenSSL.crypto.X509): The certificate to sign. signas (str): The CA keypair name to sign the new keypair with. Examples: Sign a certificate with the CA "myca": cdir.signCertAs(mycert, 'myca') Returns: None
Signs a host CSR with a CA keypair. Args: cert (OpenSSL.crypto.X509Req): The certificate signing request. signas (str): The CA keypair name to sign the CSR with. outp (synapse.lib.output.Output): The output buffer. sans (list): List of subject alternative names. Examples: Sign a host key with the CA "myca": cdir.signHostCsr(mycsr, 'myca') Returns: ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the public key and certificate objects.
Self-sign a certificate. Args: cert (OpenSSL.crypto.X509): The certificate to sign. pkey (OpenSSL.crypto.PKey): The PKey with which to sign the certificate. Examples: Sign a given certificate with a given private key: cdir.selfSignCert(mycert, myotherprivatekey) Returns: None
Signs a user CSR with a CA keypair. Args: cert (OpenSSL.crypto.X509Req): The certificate signing request. signas (str): The CA keypair name to sign the CSR with. outp (synapse.lib.output.Output): The output buffer. Examples: cdir.signUserCsr(mycsr, 'myca') Returns: ((OpenSSL.crypto.PKey, OpenSSL.crypto.X509)): Tuple containing the public key and certificate objects.
Returns an ssl.SSLContext appropriate for initiating a TLS session
Returns an ssl.SSLContext appropriate to listen on a socket Args: hostname: if None, the value from socket.gethostname is used to find the key in the servers directory. This name should match the not-suffixed part of two files ending in .key and .crt in the hosts subdirectory
Save a certificate in PEM format to a file outside the certdir.
Save a private key in PEM format to a file outside the certdir.
Perform one side of an Ecliptic Curve Diffie Hellman Ephemeral key exchange. Args: statprv_u (PriKey): Static Private Key for U statpub_v (PubKey: Static Public Key for V ephmprv_u (PriKey): Ephemeral Private Key for U ephmpub_v (PubKey): Ephemeral Public Key for V length (int): Number of bytes to return salt (bytes): Salt to use when computing the key. info (bytes): Additional information to use when computing the key. Notes: This makes no assumption about the reuse of the Ephemeral keys passed to the function. It is the caller's responsibility to destroy the keys after they are used for doing key generation. This implementation is the dhHybrid1 scheme described in NIST 800-56A Revision 2. Returns: bytes: The derived key.
Compute the ECC signature for the given bytestream. Args: byts (bytes): The bytes to sign. Returns: bytes: The RSA Signature bytes.
Perform a ECDH key exchange with a public key. Args: pubkey (PubKey): A PubKey to perform the ECDH with. Returns: bytes: The ECDH bytes. This is deterministic for a given pubkey and private key.
Get the private key bytes in DER/PKCS8 format. Returns: bytes: The DER/PKCS8 encoded private key.
Get the public key bytes in DER/SubjectPublicKeyInfo format. Returns: bytes: The DER/SubjectPublicKeyInfo encoded public key.
Verify the signature for the given bytes using the ECC public key. Args: byts (bytes): The data bytes. sign (bytes): The signature bytes. Returns: bool: True if the data was verified, False otherwise.
Brute force the version out of a string. Args: valu (str): String to attempt to get version information for. Notes: This first attempts to parse strings using the it:semver normalization before attempting to extract version parts out of the string. Returns: int, dict: The system normalized version integer and a subs dictionary.
A sane "stand alone" url parser. Example: info = chopurl(url)
Use elements from this hash set to create a unique (re)identifier.
Consume all the bytes from a file like object. Example: hset = HashSet() hset.eatfd(fd)
Update all the hashes in the set with the given bytes.
Add an entry to the provenance stack for the duration of the context
Duplicate the current provenance stack onto another task
Returns the provenance stack given the iden to it
Returns a stream of provenance stacks at the given offset
Returns the iden corresponding to a provenance stack and stores if it hasn't seen it before
Writes the current provenance stack to storage if it wasn't already there and returns it Returns (Tuple[bool, str, List[]]): Whether the stack was not cached, the iden of the prov stack, and the provstack
Save a series of items to a sequence. Args: items (tuple): The series of items to save into the sequence. Returns: The index of the first item
Determine the next insert offset according to storage. Returns: int: The next insert offset.
Iterate over items in a sequence from a given offset. Args: offs (int): The offset to begin iterating from. Yields: (indx, valu): The index and valu of the item.
Iterate over raw indx, bytes tuples from a given offset.
Chop a latlong string and return (float,float). Does not perform validation on the coordinates. Args: text (str): A longitude,latitude string. Returns: (float,float): A longitude, latitude float tuple.
Determine if the given point is within dist of any of points. Args: point ((float,float)): A latitude, longitude float tuple. dist (int): A distance in mm ( base units ) points (list): A list of latitude, longitude float tuples to compare against.
Calculate the haversine distance between two points defined by (lat,lon) tuples. Args: px ((float,float)): lat/long position 1 py ((float,float)): lat/long position 2 r (float): Radius of sphere Returns: (int): Distance in mm.
Calculate a min/max bounding box for the circle defined by lalo/dist. Args: lat (float): The latitude in degrees lon (float): The longitude in degrees dist (int): A distance in geo:dist base units (mm) Returns: (float,float,float,float): (latmin, latmax, lonmin, lonmax)
SumoLogic REST API endpoint changes based on the geo location of the client. For example, If the client geolocation is Australia then the REST end point is https://api.au.sumologic.com/api/v1 When the default REST endpoint (https://api.sumologic.com/api/v1) is used the server responds with a 401 and causes the SumoLogic class instantiation to fail and this very unhelpful message is shown 'Full authentication is required to access this resource' This method makes a request to the default REST endpoint and resolves the 401 to learn the right endpoint
Perform a single Sumo metrics query
call inner component connect
Translate CardConnection protocol mask into PCSC protocol mask.
Translate protocol into PCSC protocol header.
Connect to the card. If protocol is not specified, connect with the default connection protocol. If mode is not specified, connect with SCARD_SHARE_SHARED.
Disconnect from the card.
Return card ATR
Transmit an apdu to the card and return response apdu. @param bytes: command apdu to transmit (list of bytes) @param protocol: the transmission protocol, from CardConnection.T0_protocol, CardConnection.T1_protocol, or CardConnection.RAW_protocol @return: a tuple (response, sw1, sw2) where sw1 is status word 1, e.g. 0x90 sw2 is status word 2, e.g. 0x1A response are the response bytes excluding status words
Transmit a control command to the reader and return response. controlCode: control command bytes: command data to transmit (list of bytes) return: response are the response bytes (if any)
get an attribute attribId: Identifier for the attribute to get return: response are the attribute byte array
Return the ATR of the card inserted into the reader.
expand all nodes
recursivly walk tree control
This will get us the program's directory, even if we are frozen using py2exe. From WhereAmI page on py2exe wiki.
Remove a reader group
Connect to card. @param protocol: a bit mask of the protocols to use, from L{CardConnection.T0_protocol}, L{CardConnection.T1_protocol}, L{CardConnection.RAW_protocol}, L{CardConnection.T15_protocol} @param mode: SCARD_SHARE_SHARED (default), SCARD_SHARE_EXCLUSIVE or SCARD_SHARE_DIRECT @param disposition: SCARD_LEAVE_CARD (default), SCARD_RESET_CARD, SCARD_UNPOWER_CARD or SCARD_EJECT_CARD
Transmit an apdu. Internally calls doTransmit() class method and notify observers upon command/response APDU events. Subclasses must override the doTransmit() class method. @param bytes: list of bytes to transmit @param protocol: the transmission protocol, from CardConnection.T0_protocol, CardConnection.T1_protocol, or CardConnection.RAW_protocol
Send a control command and buffer. Internally calls doControl() class method and notify observers upon command/response events. Subclasses must override the doControl() class method. @param controlCode: command code @param bytes: list of bytes to transmit
return the requested attribute @param attribId: attribute id like SCARD_ATTR_VENDOR_NAME
Return a card connection thru a remote reader.
Called when a card is activated by double-clicking on the card or reader tree control or toolbar. In this sample, we just connect to the card on the first activation.
Called when a card is selected by clicking on the card or reader tree control or toolbar.
Called when a card is selected by clicking on the card or reader tree control or toolbar.
Called when a reader is selected by clicking on the reader tree control or toolbar.
Create and display application frame.
return command line arguments for shutting down the server; this command line is built from the name server startup arguments.
Starts Pyro naming server with command line arguments (see pyro documentation)
Shutdown pyro naming server.
wait until name server is started.
Return a CardConnection to the Card object.
Returns the list of smartcard reader groups.
Add a reader group
Remove a reader group
Get the list of Part10 features supported by the reader. @param response: result of CM_IOCTL_GET_FEATURE_REQUEST commmand @rtype: list @return: a list of list [[tag1, value1], [tag2, value2]]
return the controlCode for a feature or None @param feature: feature to look for @param featureList: feature list as returned by L{getFeatureRequest()} @return: feature value or None
return the PIN_PROPERTIES structure @param cardConnection: L{CardConnection} object @param featureList: feature list as returned by L{getFeatureRequest()} @param controlCode: control code for L{FEATURE_IFD_PIN_PROPERTIES} @rtype: dict @return: a dict
return the GET_TLV_PROPERTIES structure @param cardConnection: L{CardConnection} object @param featureList: feature list as returned by L{getFeatureRequest()} @param controlCode: control code for L{FEATURE_GET_TLV_PROPERTIES} @rtype: dict @return: a dict
return the GET_TLV_PROPERTIES structure @param response: result of L{FEATURE_GET_TLV_PROPERTIES} @rtype: dict @return: a dict
Return a card connection thru the reader.
Called when a local reader is added or removed. Create remote pyro reader objects for added readers. Delete remote pyro reader objects for removed readers.
Add an observer.
Remove an observer.
Runs until stopEvent is notified, and notify observers of all reader insertion/removal.
Remove from other items already in list.
CardConnectionObserver callback.
Starts Pyro naming server with command line arguments (see pyro documentation)
wait until name server is started.
Synchronize methods in the given class. Only synchronize the methods whose names are given, or all methods if names=None.
Lock card with SCardBeginTransaction.
Unlock card with SCardEndTransaction.
Gain exclusive access to card during APDU transmission for if this decorator decorates a PCSCCardConnection.
Static method to create a reader from a reader clazz. @param clazz: the reader class name @param readername: the reader name
Toolbar ReaderObserver callback that is notified when readers are added or removed.
Send an APDU command to the connected smartcard. @param command: list of APDU bytes, e.g. [0xA0, 0xA4, 0x00, 0x00, 0x02] @return: a tuple (response, sw1, sw2) where response is the APDU response sw1, sw2 are the two status words
Returns next error checking strategy.
Add an exception filter to the error checking chain. @param exClass: the exception to exclude, e.g. L{smartcard.sw.SWExceptions.WarningProcessingException} A filtered exception will not be raised when the sw1,sw2 conditions that would raise the excption are met.
Add reader to a reader group.
Remove a reader from a reader group
Activate a card.
Deactivate a card.
Called when the user activates a reader in the tree.
Called when user right-clicks a node in the card tree control.
Called when the user selects a card in the tree.
Called when the user selects a reader in the tree.
Called when the user activates a reader in the toolbar combo box.
Retrieve a function object from a full dotted-package name.
Load a module and retrieve a class (NOT an instance). If the parentClass is supplied, className must be of parentClass or a subclass of parentClass (or None is returned).
Returns the list or PCSC readers on which to wait for cards.
Wait for card insertion and returns a card service.
Wait for card insertion or removal.
Converts a GUID string into a list of bytes. >>> strToGUID('{AD4F1667-EA75-4124-84D4-641B3B197C65}') [103, 22, 79, 173, 117, 234, 36, 65, 132, 212, 100, 27, 59, 25, 124, 101]
Converts a GUID sequence of bytes into a string. >>> GUIDToStr([103,22,79,173, 117,234, 36,65, ... 132, 212, 100, 27, 59, 25, 124, 101]) '{AD4F1667-EA75-4124-84D4-641B3B197C65}'
If 'changed' indicates that this object has changed, notify all its observers, then call clearChanged(). Each observer has its update() called with two arguments: this observable object and the generic 'arg'.
Returns true if the atr matches the masked CardType atr. @param atr: the atr to chek for matching @param reader: the reader (optional); default is None When atr is compared to the CardType ATR, matches returns true if and only if CardType.atr & CardType.mask = atr & CardType.mask, where & is the bitwise logical AND.
Cypher/uncypher APDUs before transmission
Called when a reader is activated by double-clicking on the reader tree control or toolbar.
Called when a card is deactivated in the reader tree control or toolbar.
Called when a card is selected by clicking on the card or reader tree control or toolbar.
Called when a reader is selected by clicking on the reader tree control or toolbar.
Returns a dictionnary of supported protocols.
Dump the details of an ATR.
Called when a card is inserted. Adds a smart card to the smartcards tree.
Called when a card is removed. Removes a card from the tree.
Add an ATR to a reader node.
Return the ATR of the card inserted into the reader.
Called when a card is inserted. Adds the smart card child to the reader node.
Called when a reader is inserted. Adds the smart card reader to the smartcard readers tree.
Called when a card is removed. Removes the card from the tree.
Called when a reader is removed. Removes the reader from the smartcard readers tree.
Called on panel destruction.
Disconnect and reconnect in exclusive mode PCSCCardconnections.
Create a module from imported objects. Parameters ---------- name : str New module name. objs : dict Dictionary of the objects (or their name) to import into the module, keyed by the name they will take in the created module. doc : str Docstring of the new module. source : Module object Module where objects are defined if not explicitly given. mode : {'raise', 'warn', 'ignore'} How to deal with missing objects. Returns ------- ModuleType A module built from a list of objects' name.
r"""Base flow index Return the base flow index, defined as the minimum 7-day average flow divided by the mean flow. Parameters ---------- q : xarray.DataArray Rate of river discharge [m³/s] freq : str, optional Resampling frequency Returns ------- xarray.DataArrray Base flow index. Notes ----- Let :math:`\mathbf{q}=q_0, q_1, \ldots, q_n` be the sequence of daily discharge and :math:`\overline{\mathbf{q}}` the mean flow over the period. The base flow index is given by: .. math:: \frac{\min(\mathrm{CMA}_7(\mathbf{q}))}{\overline{\mathbf{q}}} where :math:`\mathrm{CMA}_7` is the seven days moving average of the daily flow: .. math:: \mathrm{CMA}_7(q_i) = \frac{\sum_{j=i-3}^{i+3} q_j}{7}
r"""Cold spell duration index Number of days with at least six consecutive days where the daily minimum temperature is below the 10th percentile. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature. tn10 : float 10th percentile of daily minimum temperature. window : int Minimum number of days with temperature below threshold to qualify as a cold spell. Default: 6. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Count of days with at least six consecutive days where the daily minimum temperature is below the 10th percentile [days]. Notes ----- Let :math:`TN_i` be the minimum daily temperature for the day of the year :math:`i` and :math:`TN10_i` the 10th percentile of the minimum daily temperature over the 1961-1990 period for day of the year :math:`i`, the cold spell duration index over period :math:`\phi` is defined as: .. math:: \sum_{i \in \phi} \prod_{j=i}^{i+6} \left[ TN_j < TN10_j \right] where :math:`[P]` is 1 if :math:`P` is true, and 0 if false. References ---------- From the Expert Team on Climate Change Detection, Monitoring and Indices (ETCCDMI). Example ------- >>> tn10 = percentile_doy(historical_tasmin, per=.1) >>> cold_spell_duration_index(reference_tasmin, tn10)
r"""Cold spell days The number of days that are part of a cold spell, defined as five or more consecutive days with mean daily temperature below a threshold in °C. Parameters ---------- tas : xarrray.DataArray Mean daily temperature [℃] or [K] thresh : str Threshold temperature below which a cold spell begins [℃] or [K]. Default : '-10 degC' window : int Minimum number of days with temperature below threshold to qualify as a cold spell. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Cold spell days. Notes ----- Let :math:`T_i` be the mean daily temperature on day :math:`i`, the number of cold spell days during period :math:`\phi` is given by .. math:: \sum_{i \in \phi} \prod_{j=i}^{i+5} [T_j < thresh] where :math:`[P]` is 1 if :math:`P` is true, and 0 if false.
r"""Average daily precipitation intensity Return the average precipitation over wet days. Parameters ---------- pr : xarray.DataArray Daily precipitation [mm/d or kg/m²/s] thresh : str precipitation value over which a day is considered wet. Default : '1 mm/day' freq : str, optional Resampling frequency defining the periods defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling. Default : '1 mm/day' Returns ------- xarray.DataArray The average precipitation over wet days for each period Notes ----- Let :math:`\mathbf{p} = p_0, p_1, \ldots, p_n` be the daily precipitation and :math:`thresh` be the precipitation threshold defining wet days. Then the daily precipitation intensity is defined as .. math:: \frac{\sum_{i=0}^n p_i [p_i \leq thresh]}{\sum_{i=0}^n [p_i \leq thresh]} where :math:`[P]` is 1 if :math:`P` is true, and 0 if false. Examples -------- The following would compute for each grid cell of file `pr.day.nc` the average precipitation fallen over days with precipitation >= 5 mm at seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.: >>> pr = xr.open_dataset('pr.day.nc') >>> daily_int = daily_pr_intensity(pr, thresh='5 mm/day', freq="QS-DEC")
r"""Maximum number of consecutive dry days Return the maximum number of consecutive days within the period where precipitation is below a certain threshold. Parameters ---------- pr : xarray.DataArray Mean daily precipitation flux [mm] thresh : str Threshold precipitation on which to base evaluation [mm]. Default : '1 mm/day' freq : str, optional Resampling frequency Returns ------- xarray.DataArray The maximum number of consecutive dry days. Notes ----- Let :math:`\mathbf{p}=p_0, p_1, \ldots, p_n` be a daily precipitation series and :math:`thresh` the threshold under which a day is considered dry. Then let :math:`\mathbf{s}` be the sorted vector of indices :math:`i` where :math:`[p_i < thresh] \neq [p_{i+1} < thresh]`, that is, the days when the temperature crosses the threshold. Then the maximum number of consecutive dry days is given by .. math:: \max(\mathbf{d}) \quad \mathrm{where} \quad d_j = (s_j - s_{j-1}) [p_{s_j} > thresh] where :math:`[P]` is 1 if :math:`P` is true, and 0 if false. Note that this formula does not handle sequences at the start and end of the series, but the numerical algorithm does.
r"""Maximum number of consecutive frost days (Tmin < 0℃). Resample the daily minimum temperature series by computing the maximum number of days below the freezing point over each period. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature values [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray The maximum number of consecutive days below the freezing point. Notes ----- Let :math:`\mathbf{x}=x_0, x_1, \ldots, x_n` be a daily minimum temperature series and :math:`\mathbf{s}` be the sorted vector of indices :math:`i` where :math:`[p_i < 0\celsius] \neq [p_{i+1} < 0\celsius]`, that is, the days when the temperature crosses the freezing point. Then the maximum number of consecutive frost days is given by .. math:: \max(\mathbf{d}) \quad \mathrm{where} \quad d_j = (s_j - s_{j-1}) [x_{s_j} > 0\celsius] where :math:`[P]` is 1 if :math:`P` is true, and 0 if false. Note that this formula does not handle sequences at the start and end of the series, but the numerical algorithm does.
r"""Number of days with a diurnal freeze-thaw cycle The number of days where Tmax > 0℃ and Tmin < 0℃. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature [℃] or [K] tasmin : xarray.DataArray Minimum daily temperature values [℃] or [K] freq : str Resampling frequency Returns ------- xarray.DataArray Number of days with a diurnal freeze-thaw cycle Notes ----- Let :math:`TX_{i}` be the maximum temperature at day :math:`i` and :math:`TN_{i}` be the daily minimum temperature at day :math:`i`. Then the number of freeze thaw cycles during period :math:`\phi` is given by : .. math:: \sum_{i \in \phi} [ TX_{i} > 0℃ ] [ TN_{i} < 0℃ ] where :math:`[P]` is 1 if :math:`P` is true, and 0 if false.
r"""Mean of daily temperature range. The mean difference between the daily maximum temperature and the daily minimum temperature. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature values [℃] or [K] tasmin : xarray.DataArray Minimum daily temperature values [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray The average variation in daily temperature range for the given time period. Notes ----- Let :math:`TX_{ij}` and :math:`TN_{ij}` be the daily maximum and minimum temperature at day :math:`i` of period :math:`j`. Then the mean diurnal temperature range in period :math:`j` is: .. math:: DTR_j = \frac{ \sum_{i=1}^I (TX_{ij} - TN_{ij}) }{I}
r"""Mean absolute day-to-day variation in daily temperature range. Mean absolute day-to-day variation in daily temperature range. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature values [℃] or [K] tasmin : xarray.DataArray Minimum daily temperature values [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray The average day-to-day variation in daily temperature range for the given time period. Notes ----- Let :math:`TX_{ij}` and :math:`TN_{ij}` be the daily maximum and minimum temperature at day :math:`i` of period :math:`j`. Then calculated is the absolute day-to-day differences in period :math:`j` is: .. math:: vDTR_j = \frac{ \sum_{i=2}^{I} |(TX_{ij}-TN_{ij})-(TX_{i-1,j}-TN_{i-1,j})| }{I}
r"""Extreme intra-period temperature range. The maximum of max temperature (TXx) minus the minimum of min temperature (TNn) for the given time period. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature values [℃] or [K] tasmin : xarray.DataArray Minimum daily temperature values [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Extreme intra-period temperature range for the given time period. Notes ----- Let :math:`TX_{ij}` and :math:`TN_{ij}` be the daily maximum and minimum temperature at day :math:`i` of period :math:`j`. Then the extreme temperature range in period :math:`j` is: .. math:: ETR_j = max(TX_{ij}) - min(TN_{ij})
r"""First day consistently exceeding threshold temperature. Returns first day of period where a temperature threshold is exceeded over a given number of days. Parameters ---------- tas : xarray.DataArray Mean daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K]. Default '0 degC' window : int Minimum number of days with temperature above threshold needed for evaluation freq : str, optional Resampling frequency Returns ------- float Day of the year when temperature exceeds threshold over a given number of days for the first time. If there are no such day, return np.nan. Notes ----- Let :math:`x_i` be the daily mean temperature at day of the year :math:`i` for values of :math:`i` going from 1 to 365 or 366. The start date of the freshet is given by the smallest index :math:`i` for which .. math:: \prod_{j=i}^{i+w} [x_j > thresh] is true, where :math:`w` is the number of days the temperature threshold should be exceeded, and :math:`[P]` is 1 if :math:`P` is true, and 0 if false.
r"""Frost days index Number of days where daily minimum temperatures are below 0℃. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Frost days index. Notes ----- Let :math:`TN_{ij}` be the daily minimum temperature at day :math:`i` of period :math:`j`. Then counted is the number of days where: .. math:: TN_{ij} < 0℃
r"""Growing season length. The number of days between the first occurrence of at least six consecutive days with mean daily temperature over 5℃ and the first occurrence of at least six consecutive days with mean daily temperature below 5℃ after July 1st in the northern hemisphere and January 1st in the southern hemisphere. Parameters --------- tas : xarray.DataArray Mean daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K]. Default: '5.0 degC'. window : int Minimum number of days with temperature above threshold to mark the beginning and end of growing season. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Growing season length. Notes ----- Let :math:`TG_{ij}` be the mean temperature at day :math:`i` of period :math:`j`. Then counted is the number of days between the first occurrence of at least 6 consecutive days with: .. math:: TG_{ij} > 5 ℃ and the first occurrence after 1 July of at least 6 consecutive days with: .. math:: TG_{ij} < 5 ℃
r"""Heat wave frequency Number of heat waves over a given period. A heat wave is defined as an event where the minimum and maximum daily temperature both exceeds specific thresholds over a minimum number of days. Parameters ---------- tasmin : xarrray.DataArray Minimum daily temperature [℃] or [K] tasmax : xarrray.DataArray Maximum daily temperature [℃] or [K] thresh_tasmin : str The minimum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '22 degC' thresh_tasmax : str The maximum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '30 degC' window : int Minimum number of days with temperatures above thresholds to qualify as a heatwave. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Number of heatwave at the wanted frequency Notes ----- The thresholds of 22° and 25°C for night temperatures and 30° and 35°C for day temperatures were selected by Health Canada professionals, following a temperature–mortality analysis. These absolute temperature thresholds characterize the occurrence of hot weather events that can result in adverse health outcomes for Canadian communities (Casati et al., 2013). In Robinson (2001), the parameters would be `thresh_tasmin=27.22, thresh_tasmax=39.44, window=2` (81F, 103F). References ---------- Casati, B., A. Yagouti, and D. Chaumont, 2013: Regional Climate Projections of Extreme Heat Events in Nine Pilot Canadian Communities for Public Health Planning. J. Appl. Meteor. Climatol., 52, 2669–2698, https://doi.org/10.1175/JAMC-D-12-0341.1 Robinson, P.J., 2001: On the Definition of a Heat Wave. J. Appl. Meteor., 40, 762–775, https://doi.org/10.1175/1520-0450(2001)040<0762:OTDOAH>2.0.CO;2
r"""Heat wave index. Number of days that are part of a heatwave, defined as five or more consecutive days over 25℃. Parameters ---------- tasmax : xarrray.DataArray Maximum daily temperature [℃] or [K] thresh : str Threshold temperature on which to designate a heatwave [℃] or [K]. Default: '25.0 degC'. window : int Minimum number of days with temperature above threshold to qualify as a heatwave. freq : str, optional Resampling frequency Returns ------- DataArray Heat wave index.
r"""Heat wave max length Maximum length of heat waves over a given period. A heat wave is defined as an event where the minimum and maximum daily temperature both exceeds specific thresholds over a minimum number of days. By definition heat_wave_max_length must be >= window. Parameters ---------- tasmin : xarrray.DataArray Minimum daily temperature [℃] or [K] tasmax : xarrray.DataArray Maximum daily temperature [℃] or [K] thresh_tasmin : str The minimum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '22 degC' thresh_tasmax : str The maximum temperature threshold needed to trigger a heatwave event [℃] or [K]. Default : '30 degC' window : int Minimum number of days with temperatures above thresholds to qualify as a heatwave. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Maximum length of heatwave at the wanted frequency Notes ----- The thresholds of 22° and 25°C for night temperatures and 30° and 35°C for day temperatures were selected by Health Canada professionals, following a temperature–mortality analysis. These absolute temperature thresholds characterize the occurrence of hot weather events that can result in adverse health outcomes for Canadian communities (Casati et al., 2013). In Robinson (2001), the parameters would be `thresh_tasmin=27.22, thresh_tasmax=39.44, window=2` (81F, 103F). References ---------- Casati, B., A. Yagouti, and D. Chaumont, 2013: Regional Climate Projections of Extreme Heat Events in Nine Pilot Canadian Communities for Public Health Planning. J. Appl. Meteor. Climatol., 52, 2669–2698, https://doi.org/10.1175/JAMC-D-12-0341.1 Robinson, P.J., 2001: On the Definition of a Heat Wave. J. Appl. Meteor., 40, 762–775, https://doi.org/10.1175/1520-0450(2001)040<0762:OTDOAH>2.0.CO;2
r"""Heating degree days Sum of degree days below the temperature threshold at which spaces are heated. Parameters ---------- tas : xarray.DataArray Mean daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K]. Default: '17.0 degC'. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Heating degree days index. Notes ----- Let :math:`TG_{ij}` be the daily mean temperature at day :math:`i` of period :math:`j`. Then the heating degree days are: .. math:: HD17_j = \sum_{i=1}^{I} (17℃ - TG_{ij})
r"""Number of ice/freezing days Number of days where daily maximum temperatures are below 0℃. Parameters ---------- tasmax : xarrray.DataArray Maximum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Number of ice/freezing days. Notes ----- Let :math:`TX_{ij}` be the daily maximum temperature at day :math:`i` of period :math:`j`. Then counted is the number of days where: .. math:: TX_{ij} < 0℃
r"""Ratio of rainfall to total precipitation The ratio of total liquid precipitation over the total precipitation. If solid precipitation is not provided, then precipitation is assumed solid if the temperature is below 0°C. Parameters ---------- pr : xarray.DataArray Mean daily precipitation flux [Kg m-2 s-1] or [mm]. prsn : xarray.DataArray Mean daily solid precipitation flux [Kg m-2 s-1] or [mm]. tas : xarray.DataArray Mean daily temperature [℃] or [K] freq : str Resampling frequency Returns ------- xarray.DataArray Ratio of rainfall to total precipitation Notes ----- Let :math:`PR_i` be the mean daily precipitation of day :math:`i`, then for a period :math:`j` starting at day :math:`a` and finishing on day :math:`b`: .. math:: PR_{ij} = \sum_{i=a}^{b} PR_i PRwet_{ij} See also -------- winter_rain_ratio
r"""Number of days with tmin below a threshold in Number of days where daily minimum temperature is below a threshold. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K] . Default: '-10 degC'. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Number of days Tmin < threshold. Notes ----- Let :math:`TN_{ij}` be the daily minimum temperature at day :math:`i` of period :math:`j`. Then counted is the number of days where: .. math:: TX_{ij} < Threshold [℃]
r"""Number of summer days Number of days where daily maximum temperature exceed a threshold. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K]. Default: '25 degC'. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Number of summer days. Notes ----- Let :math:`TX_{ij}` be the daily maximum temperature at day :math:`i` of period :math:`j`. Then counted is the number of days where: .. math:: TX_{ij} > Threshold [℃]
r"""Highest precipitation amount cumulated over a n-day moving window. Calculate the n-day rolling sum of the original daily total precipitation series and determine the maximum value over each period. Parameters ---------- da : xarray.DataArray Daily precipitation values [Kg m-2 s-1] or [mm] window : int Window size in days. freq : str, optional Resampling frequency : default 'YS' (yearly) Returns ------- xarray.DataArray The highest cumulated n-day precipitation value at the given time frequency. Examples -------- The following would compute for each grid cell of file `pr.day.nc` the highest 5-day total precipitation at an annual frequency: >>> da = xr.open_dataset('pr.day.nc').pr >>> window = 5 >>> output = max_n_day_precipitation_amount(da, window, freq="YS")
r"""Highest 1-day precipitation amount for a period (frequency). Resample the original daily total precipitation temperature series by taking the max over each period. Parameters ---------- pr : xarray.DataArray Daily precipitation values [Kg m-2 s-1] or [mm] freq : str, optional Resampling frequency one of : 'YS' (yearly) ,'M' (monthly), or 'QS-DEC' (seasonal - quarters starting in december) Returns ------- xarray.DataArray The highest 1-day precipitation value at the given time frequency. Notes ----- Let :math:`PR_i` be the mean daily precipitation of day `i`, then for a period `j`: .. math:: PRx_{ij} = max(PR_{ij}) Examples -------- The following would compute for each grid cell of file `pr.day.nc` the highest 1-day total at an annual frequency: >>> pr = xr.open_dataset('pr.day.nc').pr >>> rx1day = max_1day_precipitation_amount(pr, freq="YS")
r"""Accumulated total (liquid + solid) precipitation. Resample the original daily mean precipitation flux and accumulate over each period. Parameters ---------- pr : xarray.DataArray Mean daily precipitation flux [Kg m-2 s-1] or [mm]. freq : str, optional Resampling frequency as defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling. Returns ------- xarray.DataArray The total daily precipitation at the given time frequency. Notes ----- Let :math:`PR_i` be the mean daily precipitation of day :math:`i`, then for a period :math:`j` starting at day :math:`a` and finishing on day :math:`b`: .. math:: PR_{ij} = \sum_{i=a}^{b} PR_i Examples -------- The following would compute for each grid cell of file `pr_day.nc` the total precipitation at the seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.: >>> pr_day = xr.open_dataset('pr_day.nc').pr >>> prcp_tot_seasonal = precip_accumulation(pr_day, freq="QS-DEC")
Number of rain on frozen ground events Number of days with rain above a threshold after a series of seven days below freezing temperature. Precipitation is assumed to be rain when the temperature is above 0℃. Parameters ---------- pr : xarray.DataArray Mean daily precipitation flux [Kg m-2 s-1] or [mm] tas : xarray.DataArray Mean daily temperature [℃] or [K] thresh : str Precipitation threshold to consider a day as a rain event. Default : '1 mm/d' freq : str, optional Resampling frequency Returns ------- xarray.DataArray The number of rain on frozen ground events per period [days] Notes ----- Let :math:`PR_i` be the mean daily precipitation and :math:`TG_i` be the mean daily temperature of day :math:`i`. Then for a period :math:`j`, rain on frozen grounds days are counted where: .. math:: PR_{i} > Threshold [mm] and where .. math:: TG_{i} ≤ 0℃ is true for continuous periods where :math:`i ≥ 7`
r"""Number of days with daily mean temperature over the 90th percentile. Number of days with daily mean temperature over the 90th percentile. Parameters ---------- tas : xarray.DataArray Mean daily temperature [℃] or [K] t90 : xarray.DataArray 90th percentile of daily mean temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Count of days with daily mean temperature below the 10th percentile [days] Notes ----- The 90th percentile should be computed for a 5 day window centered on each calendar day for a reference period. Example ------- >>> t90 = percentile_doy(historical_tas, per=0.9) >>> hot_days = tg90p(tas, t90)
r"""Number of days with daily mean temperature below the 10th percentile. Number of days with daily mean temperature below the 10th percentile. Parameters ---------- tas : xarray.DataArray Mean daily temperature [℃] or [K] t10 : xarray.DataArray 10th percentile of daily mean temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Count of days with daily mean temperature below the 10th percentile [days] Notes ----- The 10th percentile should be computed for a 5 day window centered on each calendar day for a reference period. Example ------- >>> t10 = percentile_doy(historical_tas, per=0.1) >>> cold_days = tg10p(tas, t10)
r"""Highest mean temperature. The maximum of daily mean temperature. Parameters ---------- tas : xarray.DataArray Mean daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Maximum of daily minimum temperature. Notes ----- Let :math:`TN_{ij}` be the mean temperature at day :math:`i` of period :math:`j`. Then the maximum daily mean temperature for period :math:`j` is: .. math:: TNx_j = max(TN_{ij})
r"""Mean of daily average temperature. Resample the original daily mean temperature series by taking the mean over each period. Parameters ---------- tas : xarray.DataArray Mean daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray The mean daily temperature at the given time frequency Notes ----- Let :math:`TN_i` be the mean daily temperature of day :math:`i`, then for a period :math:`p` starting at day :math:`a` and finishing on day :math:`b`: .. math:: TG_p = \frac{\sum_{i=a}^{b} TN_i}{b - a + 1} Examples -------- The following would compute for each grid cell of file `tas.day.nc` the mean temperature at the seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.: >>> t = xr.open_dataset('tas.day.nc') >>> tg = tm_mean(t, freq="QS-DEC")
r"""Lowest mean temperature Minimum of daily mean temperature. Parameters ---------- tas : xarray.DataArray Mean daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Minimum of daily minimum temperature. Notes ----- Let :math:`TG_{ij}` be the mean temperature at day :math:`i` of period :math:`j`. Then the minimum daily mean temperature for period :math:`j` is: .. math:: TGn_j = min(TG_{ij})
r"""Highest minimum temperature. The maximum of daily minimum temperature. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Maximum of daily minimum temperature. Notes ----- Let :math:`TN_{ij}` be the minimum temperature at day :math:`i` of period :math:`j`. Then the maximum daily minimum temperature for period :math:`j` is: .. math:: TNx_j = max(TN_{ij})
r"""Mean minimum temperature. Mean of daily minimum temperature. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Mean of daily minimum temperature. Notes ----- Let :math:`TN_{ij}` be the minimum temperature at day :math:`i` of period :math:`j`. Then mean values in period :math:`j` are given by: .. math:: TN_{ij} = \frac{ \sum_{i=1}^{I} TN_{ij} }{I}
r"""Lowest minimum temperature Minimum of daily minimum temperature. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Minimum of daily minimum temperature. Notes ----- Let :math:`TN_{ij}` be the minimum temperature at day :math:`i` of period :math:`j`. Then the minimum daily minimum temperature for period :math:`j` is: .. math:: TNn_j = min(TN_{ij})
r"""Tropical nights The number of days with minimum daily temperature above threshold. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K]. Default: '20 degC'. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Number of days with minimum daily temperature above threshold. Notes ----- Let :math:`TN_{ij}` be the daily minimum temperature at day :math:`i` of period :math:`j`. Then counted is the number of days where: .. math:: TN_{ij} > Threshold [℃]
r"""Highest max temperature The maximum value of daily maximum temperature. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Maximum value of daily maximum temperature. Notes ----- Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`. Then the maximum daily maximum temperature for period :math:`j` is: .. math:: TXx_j = max(TX_{ij})
r"""Mean max temperature The mean of daily maximum temperature. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Mean of daily maximum temperature. Notes ----- Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`. Then mean values in period :math:`j` are given by: .. math:: TX_{ij} = \frac{ \sum_{i=1}^{I} TX_{ij} }{I}
r"""Lowest max temperature The minimum of daily maximum temperature. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature [℃] or [K] freq : str, optional Resampling frequency Returns ------- xarray.DataArray Minimum of daily maximum temperature. Notes ----- Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`. Then the minimum daily maximum temperature for period :math:`j` is: .. math:: TXn_j = min(TX_{ij})
r"""Frequency of extreme warm days Return the number of days with tasmax > thresh per period Parameters ---------- tasmax : xarray.DataArray Mean daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K]. Default : '30 degC' freq : str, optional Resampling frequency Returns ------- xarray.DataArray Number of days exceeding threshold. Notes: Let :math:`TX_{ij}` be the daily maximum temperature at day :math:`i` of period :math:`j`. Then counted is the number of days where: .. math:: TN_{ij} > Threshold [℃]
r"""Number of days with both hot maximum and minimum daily temperatures. The number of days per period with tasmin above a threshold and tasmax above another threshold. Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] tasmax : xarray.DataArray Maximum daily temperature [℃] or [K] thresh_tasmin : str Threshold temperature for tasmin on which to base evaluation [℃] or [K]. Default : '22 degC' thresh_tasmax : str Threshold temperature for tasmax on which to base evaluation [℃] or [K]. Default : '30 degC' freq : str, optional Resampling frequency Returns ------- xarray.DataArray the number of days with tasmin > thresh_tasmin and tasmax > thresh_tasamax per period Notes ----- Let :math:`TX_{ij}` be the maximum temperature at day :math:`i` of period :math:`j`, :math:`TN_{ij}` the daily minimum temperature at day :math:`i` of period :math:`j`, :math:`TX_{thresh}` the threshold for maximum daily temperature, and :math:`TN_{thresh}` the threshold for minimum daily temperature. Then counted is the number of days where: .. math:: TX_{ij} > TX_{thresh} [℃] and where: .. math:: TN_{ij} > TN_{thresh} [℃]
r"""Frequency of extreme warm nights Return the number of days with tasmin > thresh per period Parameters ---------- tasmin : xarray.DataArray Minimum daily temperature [℃] or [K] thresh : str Threshold temperature on which to base evaluation [℃] or [K]. Default : '22 degC' freq : str, optional Resampling frequency Returns ------- xarray.DataArray The number of days with tasmin > thresh per period
r"""Warm spell duration index Number of days with at least six consecutive days where the daily maximum temperature is above the 90th percentile. The 90th percentile should be computed for a 5-day window centred on each calendar day in the 1961-1990 period. Parameters ---------- tasmax : xarray.DataArray Maximum daily temperature [℃] or [K] tx90 : float 90th percentile of daily maximum temperature [℃] or [K] window : int Minimum number of days with temperature below threshold to qualify as a warm spell. freq : str, optional Resampling frequency Returns ------- xarray.DataArray Count of days with at least six consecutive days where the daily maximum temperature is above the 90th percentile [days]. References ---------- From the Expert Team on Climate Change Detection, Monitoring and Indices (ETCCDMI). Used in Alexander, L. V., et al. (2006), Global observed changes in daily climate extremes of temperature and precipitation, J. Geophys. Res., 111, D05109, doi: 10.1029/2005JD006290.
r"""Wet days Return the total number of days during period with precipitation over threshold. Parameters ---------- pr : xarray.DataArray Daily precipitation [mm] thresh : str Precipitation value over which a day is considered wet. Default: '1 mm/day'. freq : str, optional Resampling frequency defining the periods defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling. Returns ------- xarray.DataArray The number of wet days for each period [day] Examples -------- The following would compute for each grid cell of file `pr.day.nc` the number days with precipitation over 5 mm at the seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.: >>> pr = xr.open_dataset('pr.day.nc') >>> wd = wetdays(pr, pr_min = 5., freq="QS-DEC")
Ratio of rainfall to total precipitation during winter The ratio of total liquid precipitation over the total precipitation over the winter months (DJF. If solid precipitation is not provided, then precipitation is assumed solid if the temperature is below 0°C. Parameters ---------- pr : xarray.DataArray Mean daily precipitation flux [Kg m-2 s-1] or [mm]. prsn : xarray.DataArray Mean daily solid precipitation flux [Kg m-2 s-1] or [mm]. tas : xarray.DataArray Mean daily temperature [℃] or [K] freq : str Resampling frequency Returns ------- xarray.DataArray Ratio of rainfall to total precipitation during winter months (DJF)
Select entries according to a time period. Parameters ---------- da : xarray.DataArray Input data. **indexer : {dim: indexer, }, optional Time attribute and values over which to subset the array. For example, use season='DJF' to select winter values, month=1 to select January, or month=[6,7,8] to select summer months. If not indexer is given, all values are considered. Returns ------- xr.DataArray Selected input values.
Apply operation over each period that is part of the index selection. Parameters ---------- da : xarray.DataArray Input data. op : str {'min', 'max', 'mean', 'std', 'var', 'count', 'sum', 'argmax', 'argmin'} or func Reduce operation. Can either be a DataArray method or a function that can be applied to a DataArray. freq : str Resampling frequency defining the periods defined in http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling. **indexer : {dim: indexer, }, optional Time attribute and values over which to subset the array. For example, use season='DJF' to select winter values, month=1 to select January, or month=[6,7,8] to select summer months. If not indexer is given, all values are considered. Returns ------- xarray.DataArray The maximum value for each period.
Return the day of year of the maximum value.
Fit an array to a univariate distribution along the time dimension. Parameters ---------- arr : xarray.DataArray Time series to be fitted along the time dimension. dist : str Name of the univariate distribution, such as beta, expon, genextreme, gamma, gumbel_r, lognorm, norm (see scipy.stats). Returns ------- xarray.DataArray An array of distribution parameters fitted using the method of Maximum Likelihood.
Return the value corresponding to the given return period. Parameters ---------- arr : xarray.DataArray Maximized/minimized input data with a `time` dimension. t : int or sequence Return period. The period depends on the resolution of the input data. If the input array's resolution is yearly, then the return period is in years. dist : str Name of the univariate distribution, such as beta, expon, genextreme, gamma, gumbel_r, lognorm, norm (see scipy.stats). mode : {'min', 'max} Whether we are looking for a probability of exceedance (max) or a probability of non-exceedance (min). Returns ------- xarray.DataArray An array of values with a 1/t probability of exceedance (if mode=='max').
Return the value corresponding to a return period. Parameters ---------- da : xarray.DataArray Input data. t : int or sequence Return period. The period depends on the resolution of the input data. If the input array's resolution is yearly, then the return period is in years. dist : str Name of the univariate distribution, such as beta, expon, genextreme, gamma, gumbel_r, lognorm, norm (see scipy.stats). mode : {'min', 'max'} Whether we are looking for a probability of exceedance (high) or a probability of non-exceedance (low). window : int Averaging window length (days). freq : str Resampling frequency. If None, the frequency is assumed to be 'YS' unless the indexer is season='DJF', in which case `freq` would be set to `YS-DEC`. **indexer : {dim: indexer, }, optional Time attribute and values over which to subset the array. For example, use season='DJF' to select winter values, month=1 to select January, or month=[6,7,8] to select summer months. If not indexer is given, all values are considered. Returns ------- xarray.DataArray An array of values with a 1/t probability of exceedance or non-exceedance when mode is high or low respectively.
Return the default frequency.
Return a distribution object from scipy.stats.
Return whether an output is considered missing or not.
For all modules or classes listed, return the children that are instances of xclim.utils.Indicator. modules : sequence Sequence of modules to inspect.
Return a sequence of dicts storing metadata about all available indices.
Return the length of the longest consecutive run of True values. Parameters ---------- arr : N-dimensional array (boolean) Input array dim : Xarray dimension (default = 'time') Dimension along which to calculate consecutive run Returns ------- N-dimensional array (int) Length of longest run of True values along dimension
Return the number of runs of a minimum length. Parameters ---------- da: N-dimensional Xarray data array (boolean) Input data array window : int Minimum run length. dim : Xarray dimension (default = 'time') Dimension along which to calculate consecutive run Returns ------- out : N-dimensional xarray data array (int) Number of distinct runs of a minimum length.
Return the number of consecutive true values in array for runs at least as long as given duration. Parameters ---------- da: N-dimensional Xarray data array (boolean) Input data array window : int Minimum run length. dim : Xarray dimension (default = 'time') Dimension along which to calculate consecutive run Returns ------- out : N-dimensional xarray data array (int) Total number of true values part of a consecutive runs of at least `window` long.
Return the index of the first item of a run of at least a given length. Parameters ---------- ---------- arr : N-dimensional Xarray data array (boolean) Input array window : int Minimum duration of consecutive run to accumulate values. dim : Xarray dimension (default = 'time') Dimension along which to calculate consecutive run Returns ------- out : N-dimensional xarray data array (int) Index of first item in first valid run. Returns np.nan if there are no valid run.
Return the length, starting position and value of consecutive identical values. Parameters ---------- arr : sequence Array of values to be parsed. Returns ------- (values, run lengths, start positions) values : np.array The values taken by arr over each run run lengths : np.array The length of each run start position : np.array The starting index of each run Examples -------- >>> a = [1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3] >>> rle_1d(a) (array([1, 2, 3]), array([2, 4, 6]), array([0, 2, 6]))
Return the number of consecutive true values in array for runs at least as long as given duration. Parameters ---------- arr : bool array Input array window : int Minimum duration of consecutive run to accumulate values. Returns ------- int Total number of true values part of a consecutive run at least `window` long.
Return the index of the first item of a run of at least a given length. Parameters ---------- ---------- arr : bool array Input array window : int Minimum duration of consecutive run to accumulate values. Returns ------- int Index of first item in first valid run. Returns np.nan if there are no valid run.
Return the length of the longest consecutive run of identical values. Parameters ---------- arr : bool array Input array Returns ------- int Length of longest run.
Return the number of runs of a minimum length. Parameters ---------- arr : bool array Input array window : int Minimum run length. Returns ------- out : func Number of distinct runs of a minimum length.